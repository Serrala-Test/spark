07:05:14.478 pool-1-thread-1 INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT
07:05:14.528 pool-1-thread-1 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
07:05:14.536 pool-1-thread-1 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
07:05:14.537 pool-1-thread-1 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
07:05:14.693 pool-1-thread-1 DEBUG KerberosName: Kerberos krb5 configuration not found, setting default realm to empty
07:05:14.695 pool-1-thread-1 DEBUG Groups:  Creating new Groups object
07:05:14.696 pool-1-thread-1 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
07:05:14.697 pool-1-thread-1 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
07:05:14.697 pool-1-thread-1 DEBUG NativeCodeLoader: java.library.path=/Users/lian/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
07:05:14.697 pool-1-thread-1 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
07:05:14.697 pool-1-thread-1 DEBUG JniBasedUnixGroupsMappingWithFallback: Falling back to shell based
07:05:14.698 pool-1-thread-1 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
07:05:14.727 pool-1-thread-1 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000
07:05:14.731 pool-1-thread-1 DEBUG UserGroupInformation: hadoop login
07:05:14.731 pool-1-thread-1 DEBUG UserGroupInformation: hadoop login commit
07:05:14.734 pool-1-thread-1 DEBUG UserGroupInformation: using local user:UnixPrincipal: lian
07:05:14.736 pool-1-thread-1 DEBUG UserGroupInformation: UGI loginUser:lian (auth:SIMPLE)
07:05:14.757 pool-1-thread-1 INFO SecurityManager: Changing view acls to: lian
07:05:14.757 pool-1-thread-1 INFO SecurityManager: Changing modify acls to: lian
07:05:14.757 pool-1-thread-1 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(lian); users with modify permissions: Set(lian)
07:05:14.763 pool-1-thread-1 DEBUG SecurityManager: Created SSL options for fs: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
07:05:14.803 pool-1-thread-1 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
07:05:14.806 pool-1-thread-1 DEBUG PlatformDependent0: java.nio.Buffer.address: available
07:05:14.806 pool-1-thread-1 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
07:05:14.806 pool-1-thread-1 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
07:05:14.807 pool-1-thread-1 DEBUG PlatformDependent0: java.nio.Bits.unaligned: true
07:05:14.808 pool-1-thread-1 DEBUG PlatformDependent: Java version: 8
07:05:14.808 pool-1-thread-1 DEBUG PlatformDependent: -Dio.netty.noUnsafe: false
07:05:14.808 pool-1-thread-1 DEBUG PlatformDependent: sun.misc.Unsafe: available
07:05:14.808 pool-1-thread-1 DEBUG PlatformDependent: -Dio.netty.noJavassist: false
07:05:14.883 pool-1-thread-1 DEBUG PlatformDependent: Javassist: available
07:05:14.883 pool-1-thread-1 DEBUG PlatformDependent: -Dio.netty.tmpdir: /Users/lian/local/src/spark/workspace-d/target/tmp (java.io.tmpdir)
07:05:14.884 pool-1-thread-1 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
07:05:14.884 pool-1-thread-1 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
07:05:14.885 pool-1-thread-1 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.org.apache.spark.network.protocol.MessageMatcher
07:05:14.887 pool-1-thread-1 DEBUG JavassistTypeParameterMatcherGenerator: Generated: io.netty.util.internal.__matchers__.io.netty.buffer.ByteBufMatcher
07:05:14.894 pool-1-thread-1 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 16
07:05:14.906 pool-1-thread-1 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
07:05:14.907 pool-1-thread-1 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
07:05:14.920 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 16
07:05:14.920 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 16
07:05:14.920 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
07:05:14.920 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
07:05:14.921 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
07:05:14.921 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
07:05:14.921 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
07:05:14.921 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
07:05:14.921 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
07:05:14.921 pool-1-thread-1 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
07:05:14.946 pool-1-thread-1 DEBUG ThreadLocalRandom: -Dio.netty.initialSeedUniquifier: 0xbfb65525fdd0042b (took 0 ms)
07:05:14.969 pool-1-thread-1 DEBUG ByteBufUtil: -Dio.netty.allocator.type: unpooled
07:05:14.969 pool-1-thread-1 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
07:05:14.970 pool-1-thread-1 DEBUG NetUtil: Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1)
07:05:14.971 pool-1-thread-1 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 128 (non-existent)
07:05:14.979 pool-1-thread-1 DEBUG TransportServer: Shuffle server started on port :52175
07:05:14.982 pool-1-thread-1 INFO Utils: Successfully started service 'sparkDriver' on port 52175.
07:05:14.982 pool-1-thread-1 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
07:05:14.990 pool-1-thread-1 INFO SparkEnv: Registering MapOutputTracker
07:05:15.001 pool-1-thread-1 INFO SparkEnv: Registering BlockManagerMaster
07:05:15.013 pool-1-thread-1 INFO DiskBlockManager: Created local directory at /Users/lian/local/src/spark/workspace-d/target/tmp/blockmgr-480d5544-174e-4939-be85-7e951312eba5
07:05:15.030 pool-1-thread-1 INFO MemoryStore: MemoryStore started with capacity 2.2 GB
07:05:15.118 pool-1-thread-1 INFO SparkEnv: Registering OutputCommitCoordinator
07:05:15.125 pool-1-thread-1 DEBUG SecurityManager: Created SSL options for ui: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
07:05:15.212 pool-1-thread-1 DEBUG log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.eclipse.jetty.util.log) via org.eclipse.jetty.util.log.Slf4jLog
07:05:15.226 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.226 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.226 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.226 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee}
07:05:15.227 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee=org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee}
07:05:15.227 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.227 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.227 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.227 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-ac83420}
07:05:15.227 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-ac83420=org.apache.spark.ui.JettyUtils$$anon$2-ac83420}
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-749369c2}
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-749369c2=org.apache.spark.ui.JettyUtils$$anon$2-749369c2}
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.228 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c=org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-31d55d07}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-31d55d07=org.apache.spark.ui.JettyUtils$$anon$2-31d55d07}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86=org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8}
07:05:15.229 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8=org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-4479d261}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-4479d261=org.apache.spark.ui.JettyUtils$$anon$2-4479d261}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef=org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9}
07:05:15.230 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9=org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9}
07:05:15.232 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.232 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.232 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.232 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-77c76850}
07:05:15.232 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-77c76850=org.apache.spark.ui.JettyUtils$$anon$2-77c76850}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1cc90593}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1cc90593=org.apache.spark.ui.JettyUtils$$anon$2-1cc90593}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-104bf26c}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-104bf26c=org.apache.spark.ui.JettyUtils$$anon$2-104bf26c}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd}
07:05:15.233 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd=org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd}
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-71f96237}
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-71f96237=org.apache.spark.ui.JettyUtils$$anon$2-71f96237}
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d}
07:05:15.234 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d=org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d}
07:05:15.236 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.236 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.236 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.236 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-30594f19}
07:05:15.236 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-30594f19=org.apache.spark.ui.JettyUtils$$anon$2-30594f19}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac=org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-4d765fca}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-4d765fca=org.apache.spark.ui.JettyUtils$$anon$2-4d765fca}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa}
07:05:15.237 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa=org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa}
07:05:15.241 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.241 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.241 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.241 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.eclipse.jetty.servlet.DefaultServlet-41922a8c}
07:05:15.241 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.eclipse.jetty.servlet.DefaultServlet-41922a8c=org.eclipse.jetty.servlet.DefaultServlet-41922a8c}
07:05:15.242 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.242 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.242 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.242 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df}
07:05:15.242 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df=org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df}
07:05:15.244 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.244 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.245 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.245 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/*=com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf}
07:05:15.245 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf=com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf}
07:05:15.245 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.245 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.245 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.245 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-3350a470}
07:05:15.245 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-3350a470=org.apache.spark.ui.JettyUtils$$anon$3-3350a470}
07:05:15.259 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.Server@7ec8f6ee + SelectChannelConnector@0.0.0.0:4040 as connector
07:05:15.262 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.Server@7ec8f6ee + SparkUI{8<=0<=0/254,-1} as threadpool
07:05:15.263 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@79ce262c + o.e.j.s.ServletContextHandler{/jobs,null} as handler
07:05:15.263 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@53367e85 + o.e.j.s.ServletContextHandler{/jobs/json,null} as handler
07:05:15.263 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@32638189 + o.e.j.s.ServletContextHandler{/jobs/job,null} as handler
07:05:15.263 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@5d3d10fa + o.e.j.s.ServletContextHandler{/jobs/job/json,null} as handler
07:05:15.263 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@554ab237 + o.e.j.s.ServletContextHandler{/stages,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@35e82c4d + o.e.j.s.ServletContextHandler{/stages/json,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@1ba98cb0 + o.e.j.s.ServletContextHandler{/stages/stage,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@2e26f5ff + o.e.j.s.ServletContextHandler{/stages/stage/json,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@2efd2106 + o.e.j.s.ServletContextHandler{/stages/pool,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@5a03ead0 + o.e.j.s.ServletContextHandler{/stages/pool/json,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@d98212c + o.e.j.s.ServletContextHandler{/storage,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@1408a46d + o.e.j.s.ServletContextHandler{/storage/json,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@533a42fb + o.e.j.s.ServletContextHandler{/storage/rdd,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@162a6726 + o.e.j.s.ServletContextHandler{/storage/rdd/json,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@5a77a58e + o.e.j.s.ServletContextHandler{/environment,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@b0d4961 + o.e.j.s.ServletContextHandler{/environment/json,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@a21ff61 + o.e.j.s.ServletContextHandler{/executors,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@4ff7ba54 + o.e.j.s.ServletContextHandler{/executors/json,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@284b2df1 + o.e.j.s.ServletContextHandler{/executors/threadDump,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@496c2fd8 + o.e.j.s.ServletContextHandler{/executors/threadDump/json,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@4cb30b63 + o.e.j.s.ServletContextHandler{/static,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@3ee43dbb + o.e.j.s.ServletContextHandler{/,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@4fb1bb33 + o.e.j.s.ServletContextHandler{/api,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.GzipHandler@678b4ff2 + o.e.j.s.ServletContextHandler{/stages/stage/kill,null} as handler
07:05:15.264 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@79ce262c as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@53367e85 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@32638189 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@5d3d10fa as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@554ab237 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@35e82c4d as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@1ba98cb0 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@2e26f5ff as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@2efd2106 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@5a03ead0 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@d98212c as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@1408a46d as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@533a42fb as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@162a6726 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@5a77a58e as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@b0d4961 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@a21ff61 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@4ff7ba54 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@284b2df1 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@496c2fd8 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@4cb30b63 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@3ee43dbb as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@4fb1bb33 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + org.eclipse.jetty.server.handler.GzipHandler@678b4ff2 as handler
07:05:15.265 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.Server@7ec8f6ee + org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 as handler
07:05:15.265 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.Server@7ec8f6ee
07:05:15.267 pool-1-thread-1 INFO Server: jetty-8.1.14.v20131031
07:05:15.276 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873
07:05:15.276 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@79ce262c
07:05:15.276 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/jobs,null}
07:05:15.283 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@4a2a22e9 + org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee as servlet
07:05:15.284 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@4a2a22e9 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee as servletMapping
07:05:15.284 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/jobs,null} + org.eclipse.jetty.servlet.ServletHandler@4a2a22e9 as handler
07:05:15.284 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@4a2a22e9
07:05:15.284 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.284 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.284 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.284 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee}
07:05:15.284 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee=org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee}
07:05:15.284 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@4a2a22e9
07:05:15.284 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@4a2a22e9
07:05:15.284 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/jobs,null}
07:05:15.284 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee
07:05:15.285 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee
07:05:15.285 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/jobs,null}
07:05:15.286 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@79ce262c
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@79ce262c
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@53367e85
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/jobs/json,null}
07:05:15.286 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@1b0107e3 + org.apache.spark.ui.JettyUtils$$anon$2-ac83420 as servlet
07:05:15.286 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@1b0107e3 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-ac83420 as servletMapping
07:05:15.286 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/jobs/json,null} + org.eclipse.jetty.servlet.ServletHandler@1b0107e3 as handler
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@1b0107e3
07:05:15.286 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.286 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.286 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.286 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-ac83420}
07:05:15.286 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-ac83420=org.apache.spark.ui.JettyUtils$$anon$2-ac83420}
07:05:15.286 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@1b0107e3
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@1b0107e3
07:05:15.286 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/jobs/json,null}
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-ac83420
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-ac83420
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/jobs/json,null}
07:05:15.286 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@53367e85
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@53367e85
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@32638189
07:05:15.286 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/jobs/job,null}
07:05:15.287 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@b920f35 + org.apache.spark.ui.JettyUtils$$anon$2-749369c2 as servlet
07:05:15.287 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@b920f35 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-749369c2 as servletMapping
07:05:15.287 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/jobs/job,null} + org.eclipse.jetty.servlet.ServletHandler@b920f35 as handler
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@b920f35
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-749369c2}
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-749369c2=org.apache.spark.ui.JettyUtils$$anon$2-749369c2}
07:05:15.287 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@b920f35
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@b920f35
07:05:15.287 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/jobs/job,null}
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-749369c2
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-749369c2
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/jobs/job,null}
07:05:15.287 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@32638189
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@32638189
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@5d3d10fa
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/jobs/job/json,null}
07:05:15.287 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@154c14d6 + org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c as servlet
07:05:15.287 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@154c14d6 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c as servletMapping
07:05:15.287 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/jobs/job/json,null} + org.eclipse.jetty.servlet.ServletHandler@154c14d6 as handler
07:05:15.287 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@154c14d6
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.287 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c}
07:05:15.288 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c=org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c}
07:05:15.288 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@154c14d6
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@154c14d6
07:05:15.288 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/jobs/job/json,null}
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/jobs/job/json,null}
07:05:15.288 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@5d3d10fa
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@5d3d10fa
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@554ab237
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/stages,null}
07:05:15.288 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@37581ab3 + org.apache.spark.ui.JettyUtils$$anon$2-31d55d07 as servlet
07:05:15.288 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@37581ab3 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-31d55d07 as servletMapping
07:05:15.288 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages,null} + org.eclipse.jetty.servlet.ServletHandler@37581ab3 as handler
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@37581ab3
07:05:15.288 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.288 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.288 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.288 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-31d55d07}
07:05:15.288 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-31d55d07=org.apache.spark.ui.JettyUtils$$anon$2-31d55d07}
07:05:15.288 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@37581ab3
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@37581ab3
07:05:15.288 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/stages,null}
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-31d55d07
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-31d55d07
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/stages,null}
07:05:15.288 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@554ab237
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@554ab237
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@35e82c4d
07:05:15.288 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/stages/json,null}
07:05:15.288 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@24752da4 + org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86 as servlet
07:05:15.288 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@24752da4 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86 as servletMapping
07:05:15.289 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/json,null} + org.eclipse.jetty.servlet.ServletHandler@24752da4 as handler
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@24752da4
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86}
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86=org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86}
07:05:15.289 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@24752da4
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@24752da4
07:05:15.289 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/stages/json,null}
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/stages/json,null}
07:05:15.289 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@35e82c4d
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@35e82c4d
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@1ba98cb0
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/stages/stage,null}
07:05:15.289 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3f0575b8 + org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8 as servlet
07:05:15.289 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3f0575b8 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8 as servletMapping
07:05:15.289 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/stage,null} + org.eclipse.jetty.servlet.ServletHandler@3f0575b8 as handler
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@3f0575b8
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8}
07:05:15.289 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8=org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8}
07:05:15.289 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@3f0575b8
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@3f0575b8
07:05:15.289 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/stages/stage,null}
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8
07:05:15.289 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/stages/stage,null}
07:05:15.290 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@1ba98cb0
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@1ba98cb0
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@2e26f5ff
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/stages/stage/json,null}
07:05:15.290 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a + org.apache.spark.ui.JettyUtils$$anon$2-4479d261 as servlet
07:05:15.290 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-4479d261 as servletMapping
07:05:15.290 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/stage/json,null} + org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a as handler
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a
07:05:15.290 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.290 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.290 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.290 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-4479d261}
07:05:15.290 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-4479d261=org.apache.spark.ui.JettyUtils$$anon$2-4479d261}
07:05:15.290 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a
07:05:15.290 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/stages/stage/json,null}
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-4479d261
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-4479d261
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/stages/stage/json,null}
07:05:15.290 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@2e26f5ff
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@2e26f5ff
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@2efd2106
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/stages/pool,null}
07:05:15.290 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3de02a58 + org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef as servlet
07:05:15.290 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3de02a58 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef as servletMapping
07:05:15.290 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/pool,null} + org.eclipse.jetty.servlet.ServletHandler@3de02a58 as handler
07:05:15.290 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@3de02a58
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef}
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef=org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef}
07:05:15.291 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@3de02a58
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@3de02a58
07:05:15.291 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/stages/pool,null}
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/stages/pool,null}
07:05:15.291 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@2efd2106
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@2efd2106
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@5a03ead0
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/stages/pool/json,null}
07:05:15.291 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@30f5284d + org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9 as servlet
07:05:15.291 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@30f5284d + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9 as servletMapping
07:05:15.291 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/pool/json,null} + org.eclipse.jetty.servlet.ServletHandler@30f5284d as handler
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@30f5284d
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9}
07:05:15.291 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9=org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9}
07:05:15.291 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@30f5284d
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@30f5284d
07:05:15.291 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/stages/pool/json,null}
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9
07:05:15.291 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/stages/pool/json,null}
07:05:15.292 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@5a03ead0
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@5a03ead0
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@d98212c
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/storage,null}
07:05:15.292 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@2449235e + org.apache.spark.ui.JettyUtils$$anon$2-77c76850 as servlet
07:05:15.292 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@2449235e + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-77c76850 as servletMapping
07:05:15.292 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/storage,null} + org.eclipse.jetty.servlet.ServletHandler@2449235e as handler
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@2449235e
07:05:15.292 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.292 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.292 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.292 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-77c76850}
07:05:15.292 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-77c76850=org.apache.spark.ui.JettyUtils$$anon$2-77c76850}
07:05:15.292 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@2449235e
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@2449235e
07:05:15.292 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/storage,null}
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-77c76850
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-77c76850
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/storage,null}
07:05:15.292 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@d98212c
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@d98212c
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@1408a46d
07:05:15.292 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/storage/json,null}
07:05:15.292 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3fa9a091 + org.apache.spark.ui.JettyUtils$$anon$2-1cc90593 as servlet
07:05:15.292 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3fa9a091 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-1cc90593 as servletMapping
07:05:15.292 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/storage/json,null} + org.eclipse.jetty.servlet.ServletHandler@3fa9a091 as handler
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@3fa9a091
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1cc90593}
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1cc90593=org.apache.spark.ui.JettyUtils$$anon$2-1cc90593}
07:05:15.293 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@3fa9a091
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@3fa9a091
07:05:15.293 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/storage/json,null}
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-1cc90593
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-1cc90593
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/storage/json,null}
07:05:15.293 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@1408a46d
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@1408a46d
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@533a42fb
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/storage/rdd,null}
07:05:15.293 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3a7efd6 + org.apache.spark.ui.JettyUtils$$anon$2-104bf26c as servlet
07:05:15.293 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3a7efd6 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-104bf26c as servletMapping
07:05:15.293 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/storage/rdd,null} + org.eclipse.jetty.servlet.ServletHandler@3a7efd6 as handler
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@3a7efd6
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-104bf26c}
07:05:15.293 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-104bf26c=org.apache.spark.ui.JettyUtils$$anon$2-104bf26c}
07:05:15.293 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@3a7efd6
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@3a7efd6
07:05:15.293 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/storage/rdd,null}
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-104bf26c
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-104bf26c
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/storage/rdd,null}
07:05:15.293 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@533a42fb
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@533a42fb
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@162a6726
07:05:15.293 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
07:05:15.293 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@1a0918af + org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd as servlet
07:05:15.294 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@1a0918af + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd as servletMapping
07:05:15.294 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/storage/rdd/json,null} + org.eclipse.jetty.servlet.ServletHandler@1a0918af as handler
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@1a0918af
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd}
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd=org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd}
07:05:15.294 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@1a0918af
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@1a0918af
07:05:15.294 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
07:05:15.294 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@162a6726
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@162a6726
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@5a77a58e
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/environment,null}
07:05:15.294 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@77b79b63 + org.apache.spark.ui.JettyUtils$$anon$2-71f96237 as servlet
07:05:15.294 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@77b79b63 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-71f96237 as servletMapping
07:05:15.294 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/environment,null} + org.eclipse.jetty.servlet.ServletHandler@77b79b63 as handler
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@77b79b63
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-71f96237}
07:05:15.294 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-71f96237=org.apache.spark.ui.JettyUtils$$anon$2-71f96237}
07:05:15.294 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@77b79b63
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@77b79b63
07:05:15.294 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/environment,null}
07:05:15.294 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-71f96237
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-71f96237
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/environment,null}
07:05:15.295 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@5a77a58e
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@5a77a58e
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@b0d4961
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/environment/json,null}
07:05:15.295 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@5e8002c + org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d as servlet
07:05:15.295 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@5e8002c + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d as servletMapping
07:05:15.295 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/environment/json,null} + org.eclipse.jetty.servlet.ServletHandler@5e8002c as handler
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@5e8002c
07:05:15.295 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.295 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.295 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.295 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d}
07:05:15.295 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d=org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d}
07:05:15.295 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@5e8002c
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@5e8002c
07:05:15.295 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/environment/json,null}
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/environment/json,null}
07:05:15.295 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@b0d4961
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@b0d4961
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@a21ff61
07:05:15.295 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/executors,null}
07:05:15.295 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@390d8618 + org.apache.spark.ui.JettyUtils$$anon$2-30594f19 as servlet
07:05:15.295 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@390d8618 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-30594f19 as servletMapping
07:05:15.296 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/executors,null} + org.eclipse.jetty.servlet.ServletHandler@390d8618 as handler
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@390d8618
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-30594f19}
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-30594f19=org.apache.spark.ui.JettyUtils$$anon$2-30594f19}
07:05:15.296 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@390d8618
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@390d8618
07:05:15.296 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/executors,null}
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-30594f19
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-30594f19
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/executors,null}
07:05:15.296 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@a21ff61
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@a21ff61
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@4ff7ba54
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/executors/json,null}
07:05:15.296 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@6b932df6 + org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac as servlet
07:05:15.296 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@6b932df6 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac as servletMapping
07:05:15.296 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/executors/json,null} + org.eclipse.jetty.servlet.ServletHandler@6b932df6 as handler
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@6b932df6
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac}
07:05:15.296 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac=org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac}
07:05:15.296 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@6b932df6
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@6b932df6
07:05:15.296 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/executors/json,null}
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/executors/json,null}
07:05:15.296 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@4ff7ba54
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@4ff7ba54
07:05:15.296 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@284b2df1
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/executors/threadDump,null}
07:05:15.297 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@20b4cf64 + org.apache.spark.ui.JettyUtils$$anon$2-4d765fca as servlet
07:05:15.297 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@20b4cf64 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-4d765fca as servletMapping
07:05:15.297 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/executors/threadDump,null} + org.eclipse.jetty.servlet.ServletHandler@20b4cf64 as handler
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@20b4cf64
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-4d765fca}
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-4d765fca=org.apache.spark.ui.JettyUtils$$anon$2-4d765fca}
07:05:15.297 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@20b4cf64
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@20b4cf64
07:05:15.297 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/executors/threadDump,null}
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-4d765fca
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-4d765fca
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/executors/threadDump,null}
07:05:15.297 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@284b2df1
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@284b2df1
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@496c2fd8
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
07:05:15.297 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@6518423e + org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa as servlet
07:05:15.297 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@6518423e + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa as servletMapping
07:05:15.297 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/executors/threadDump/json,null} + org.eclipse.jetty.servlet.ServletHandler@6518423e as handler
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@6518423e
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa}
07:05:15.297 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa=org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa}
07:05:15.297 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@6518423e
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@6518423e
07:05:15.297 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
07:05:15.297 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@496c2fd8
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@496c2fd8
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@4cb30b63
07:05:15.297 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/static,null}
07:05:15.297 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@307035c3 + org.eclipse.jetty.servlet.DefaultServlet-41922a8c as servlet
07:05:15.298 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@307035c3 + [/]=>org.eclipse.jetty.servlet.DefaultServlet-41922a8c as servletMapping
07:05:15.298 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/static,null} + org.eclipse.jetty.servlet.ServletHandler@307035c3 as handler
07:05:15.298 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@307035c3
07:05:15.298 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.298 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.298 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.298 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.eclipse.jetty.servlet.DefaultServlet-41922a8c}
07:05:15.298 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.eclipse.jetty.servlet.DefaultServlet-41922a8c=org.eclipse.jetty.servlet.DefaultServlet-41922a8c}
07:05:15.298 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@307035c3
07:05:15.298 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@307035c3
07:05:15.298 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/static,null}
07:05:15.298 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.DefaultServlet-41922a8c
07:05:15.302 pool-1-thread-1 DEBUG DefaultServlet: resource base = jar:file:/Users/lian/local/src/spark/workspace-d/core/target/scala-2.11/spark-core_2.11-2.0.0-SNAPSHOT.jar!/org/apache/spark/ui/static
07:05:15.302 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.DefaultServlet-41922a8c
07:05:15.302 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/static,null}
07:05:15.302 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@4cb30b63
07:05:15.302 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@4cb30b63
07:05:15.302 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@3ee43dbb
07:05:15.302 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/,null}
07:05:15.302 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@7e6a3001 + org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df as servlet
07:05:15.302 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@7e6a3001 + [/]=>org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df as servletMapping
07:05:15.302 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/,null} + org.eclipse.jetty.servlet.ServletHandler@7e6a3001 as handler
07:05:15.302 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@7e6a3001
07:05:15.302 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.302 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.302 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.302 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df}
07:05:15.302 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df=org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df}
07:05:15.302 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@7e6a3001
07:05:15.302 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@7e6a3001
07:05:15.302 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/,null}
07:05:15.302 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/,null}
07:05:15.303 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@3ee43dbb
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@3ee43dbb
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@4fb1bb33
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/api,null}
07:05:15.303 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3025cd3c + com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf as servlet
07:05:15.303 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@3025cd3c + [/*]=>com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf as servletMapping
07:05:15.303 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/api,null} + org.eclipse.jetty.servlet.ServletHandler@3025cd3c as handler
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@3025cd3c
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/*=com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf}
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf=com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf}
07:05:15.303 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@3025cd3c
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@3025cd3c
07:05:15.303 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/api,null}
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: starting com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/api,null}
07:05:15.303 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@4fb1bb33
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@4fb1bb33
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.GzipHandler@678b4ff2
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
07:05:15.303 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@a53afc7 + org.apache.spark.ui.JettyUtils$$anon$3-3350a470 as servlet
07:05:15.303 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@a53afc7 + [/]=>org.apache.spark.ui.JettyUtils$$anon$3-3350a470 as servletMapping
07:05:15.303 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/stage/kill,null} + org.eclipse.jetty.servlet.ServletHandler@a53afc7 as handler
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@a53afc7
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-3350a470}
07:05:15.303 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-3350a470=org.apache.spark.ui.JettyUtils$$anon$3-3350a470}
07:05:15.303 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@a53afc7
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@a53afc7
07:05:15.303 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$3-3350a470
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$3-3350a470
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
07:05:15.303 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.GzipHandler@678b4ff2
07:05:15.303 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.GzipHandler@678b4ff2
07:05:15.304 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873
07:05:15.304 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873
07:05:15.304 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.Server@7ec8f6ee
07:05:15.304 pool-1-thread-1 DEBUG AbstractLifeCycle: starting SparkUI{8<=0<=0/254,-1}
07:05:15.305 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED SparkUI{8<=7<=8/254,0}
07:05:15.305 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.handler.ErrorHandler@231967b4
07:05:15.305 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.server.handler.ErrorHandler@231967b4
07:05:15.305 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.handler.ErrorHandler@231967b4
07:05:15.305 pool-1-thread-1 DEBUG AbstractLifeCycle: starting SelectChannelConnector@0.0.0.0:4040
07:05:15.307 pool-1-thread-1 DEBUG AbstractLifeCycle: starting null/null
07:05:15.308 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED PooledBuffers [0/1024@6144,0/1024@16384,0/1024@-]/PooledBuffers [0/1024@6144,0/1024@32768,0/1024@-]
07:05:15.308 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.server.nio.SelectChannelConnector$ConnectorSelectorManager@78fd5c5f
07:05:15.311 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.nio.SelectChannelConnector$ConnectorSelectorManager@78fd5c5f
07:05:15.311 SparkUI-42 Selector1 DEBUG nio: Starting Thread[SparkUI-42 Selector1,5,main] on org.eclipse.jetty.io.nio.SelectorManager$1@4a3d1e79
07:05:15.311 SparkUI-41 Selector0 DEBUG nio: Starting Thread[SparkUI-41 Selector0,5,main] on org.eclipse.jetty.io.nio.SelectorManager$1@7fd0ddb4
07:05:15.311 pool-1-thread-1 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
07:05:15.311 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED SelectChannelConnector@0.0.0.0:4040
07:05:15.312 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.server.Server@7ec8f6ee
07:05:15.312 pool-1-thread-1 INFO Utils: Successfully started service 'SparkUI' on port 4040.
07:05:15.313 pool-1-thread-1 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.102:4040
07:05:15.374 pool-1-thread-1 INFO Executor: Starting executor ID driver on host localhost
07:05:15.388 pool-1-thread-1 DEBUG TransportServer: Shuffle server started on port :52176
07:05:15.388 pool-1-thread-1 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52176.
07:05:15.388 pool-1-thread-1 INFO NettyBlockTransferService: Server created on 192.168.1.102:52176
07:05:15.389 pool-1-thread-1 INFO BlockManagerMaster: Trying to register BlockManager
07:05:15.391 dispatcher-event-loop-2 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.102:52176 with 2.2 GB RAM, BlockManagerId(driver, 192.168.1.102, 52176)
07:05:15.393 pool-1-thread-1 INFO BlockManagerMaster: Registered BlockManager
07:05:15.488 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.488 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.488 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.488 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631}
07:05:15.488 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631=org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631}
07:05:15.489 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + o.e.j.s.ServletContextHandler{/metrics/json,null} as handler
07:05:15.489 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/metrics/json,null}
07:05:15.489 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@1ac7e89c + org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631 as servlet
07:05:15.489 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@1ac7e89c + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631 as servletMapping
07:05:15.489 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/metrics/json,null} + org.eclipse.jetty.servlet.ServletHandler@1ac7e89c as handler
07:05:15.489 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@1ac7e89c
07:05:15.489 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:15.489 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:15.490 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:15.490 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631}
07:05:15.490 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631=org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631}
07:05:15.490 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@1ac7e89c
07:05:15.490 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@1ac7e89c
07:05:15.490 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/metrics/json,null}
07:05:15.490 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631
07:05:15.490 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631
07:05:15.490 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/metrics/json,null}
07:05:16.411 pool-1-thread-1 DEBUG Shell: Failed to detect a valid hadoop home directory
java.io.IOException: HADOOP_HOME or hadoop.home.dir are not set.
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:225)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:250)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.findHadoopBinary(HiveConf.java:2327)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:365)
	at org.apache.spark.sql.hive.HiveContext$.newTemporaryConfiguration(HiveContext.scala:612)
	at org.apache.spark.sql.hive.test.TestHiveContext.<init>(TestHive.scala:100)
	at org.apache.spark.sql.hive.test.TestHive$.<init>(TestHive.scala:51)
	at org.apache.spark.sql.hive.test.TestHive$.<clinit>(TestHive.scala)
	at org.apache.spark.sql.hive.test.TestHiveSingleton$class.$init$(TestHiveSingleton.scala:27)
	at org.apache.spark.sql.sources.BucketedReadSuite.<init>(BucketedReadSuite.scala:36)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at java.lang.Class.newInstance(Class.java:438)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:468)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:16.416 pool-1-thread-1 DEBUG Shell: setsid is not available on this machine. So not using it.
07:05:16.416 pool-1-thread-1 DEBUG Shell: setsid exited with exit code 0
07:05:16.450 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-8dc8643}
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-8dc8643=org.apache.spark.ui.JettyUtils$$anon$2-8dc8643}
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66}
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66=org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66}
07:05:16.451 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + o.e.j.s.ServletContextHandler{/SQL,null} as handler
07:05:16.451 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/SQL,null}
07:05:16.451 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@591a2d21 + org.apache.spark.ui.JettyUtils$$anon$2-8dc8643 as servlet
07:05:16.451 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@591a2d21 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-8dc8643 as servletMapping
07:05:16.451 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/SQL,null} + org.eclipse.jetty.servlet.ServletHandler@591a2d21 as handler
07:05:16.451 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@591a2d21
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-8dc8643}
07:05:16.451 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-8dc8643=org.apache.spark.ui.JettyUtils$$anon$2-8dc8643}
07:05:16.451 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@591a2d21
07:05:16.451 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@591a2d21
07:05:16.452 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/SQL,null}
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-8dc8643
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-8dc8643
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/SQL,null}
07:05:16.452 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + o.e.j.s.ServletContextHandler{/SQL/json,null} as handler
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/SQL/json,null}
07:05:16.452 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@2af04b26 + org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66 as servlet
07:05:16.452 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@2af04b26 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66 as servletMapping
07:05:16.452 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/SQL/json,null} + org.eclipse.jetty.servlet.ServletHandler@2af04b26 as handler
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@2af04b26
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66}
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66=org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66}
07:05:16.452 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@2af04b26
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@2af04b26
07:05:16.452 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/SQL/json,null}
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66
07:05:16.452 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/SQL/json,null}
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-70d04fff}
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-70d04fff=org.apache.spark.ui.JettyUtils$$anon$2-70d04fff}
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-42241ea3}
07:05:16.452 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-42241ea3=org.apache.spark.ui.JettyUtils$$anon$2-42241ea3}
07:05:16.453 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + o.e.j.s.ServletContextHandler{/SQL/execution,null} as handler
07:05:16.453 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/SQL/execution,null}
07:05:16.453 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@28ba0350 + org.apache.spark.ui.JettyUtils$$anon$2-70d04fff as servlet
07:05:16.453 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@28ba0350 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-70d04fff as servletMapping
07:05:16.453 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/SQL/execution,null} + org.eclipse.jetty.servlet.ServletHandler@28ba0350 as handler
07:05:16.453 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@28ba0350
07:05:16.453 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.453 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.453 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.453 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-70d04fff}
07:05:16.453 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-70d04fff=org.apache.spark.ui.JettyUtils$$anon$2-70d04fff}
07:05:16.453 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@28ba0350
07:05:16.453 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@28ba0350
07:05:16.453 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/SQL/execution,null}
07:05:16.453 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-70d04fff
07:05:16.453 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-70d04fff
07:05:16.453 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/SQL/execution,null}
07:05:16.454 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + o.e.j.s.ServletContextHandler{/SQL/execution/json,null} as handler
07:05:16.454 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/SQL/execution/json,null}
07:05:16.454 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@7d05cc82 + org.apache.spark.ui.JettyUtils$$anon$2-42241ea3 as servlet
07:05:16.454 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@7d05cc82 + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-42241ea3 as servletMapping
07:05:16.454 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/SQL/execution/json,null} + org.eclipse.jetty.servlet.ServletHandler@7d05cc82 as handler
07:05:16.454 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@7d05cc82
07:05:16.454 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.454 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.454 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.454 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-42241ea3}
07:05:16.454 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-42241ea3=org.apache.spark.ui.JettyUtils$$anon$2-42241ea3}
07:05:16.454 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@7d05cc82
07:05:16.454 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@7d05cc82
07:05:16.454 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/SQL/execution/json,null}
07:05:16.454 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.apache.spark.ui.JettyUtils$$anon$2-42241ea3
07:05:16.454 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.apache.spark.ui.JettyUtils$$anon$2-42241ea3
07:05:16.454 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/SQL/execution/json,null}
07:05:16.455 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.455 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.455 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.455 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.eclipse.jetty.servlet.DefaultServlet-be2e3d}
07:05:16.455 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.eclipse.jetty.servlet.DefaultServlet-be2e3d=org.eclipse.jetty.servlet.DefaultServlet-be2e3d}
07:05:16.455 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873 + o.e.j.s.ServletContextHandler{/static/sql,null} as handler
07:05:16.455 pool-1-thread-1 DEBUG AbstractLifeCycle: starting o.e.j.s.ServletContextHandler{/static/sql,null}
07:05:16.455 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@28eb3784 + org.eclipse.jetty.servlet.DefaultServlet-be2e3d as servlet
07:05:16.455 pool-1-thread-1 DEBUG Container: Container org.eclipse.jetty.servlet.ServletHandler@28eb3784 + [/]=>org.eclipse.jetty.servlet.DefaultServlet-be2e3d as servletMapping
07:05:16.456 pool-1-thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/static/sql,null} + org.eclipse.jetty.servlet.ServletHandler@28eb3784 as handler
07:05:16.456 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.ServletHandler@28eb3784
07:05:16.456 pool-1-thread-1 DEBUG ServletHandler: filterNameMap={}
07:05:16.456 pool-1-thread-1 DEBUG ServletHandler: pathFilters=null
07:05:16.456 pool-1-thread-1 DEBUG ServletHandler: servletFilterMap=null
07:05:16.456 pool-1-thread-1 DEBUG ServletHandler: servletPathMap={/=org.eclipse.jetty.servlet.DefaultServlet-be2e3d}
07:05:16.456 pool-1-thread-1 DEBUG ServletHandler: servletNameMap={org.eclipse.jetty.servlet.DefaultServlet-be2e3d=org.eclipse.jetty.servlet.DefaultServlet-be2e3d}
07:05:16.456 pool-1-thread-1 DEBUG AbstractHandler: starting org.eclipse.jetty.servlet.ServletHandler@28eb3784
07:05:16.456 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.ServletHandler@28eb3784
07:05:16.456 pool-1-thread-1 DEBUG AbstractHandler: starting o.e.j.s.ServletContextHandler{/static/sql,null}
07:05:16.456 pool-1-thread-1 DEBUG AbstractLifeCycle: starting org.eclipse.jetty.servlet.DefaultServlet-be2e3d
07:05:16.456 pool-1-thread-1 DEBUG DefaultServlet: resource base = jar:file:/Users/lian/local/src/spark/workspace-d/sql/core/target/scala-2.11/spark-sql_2.11-2.0.0-SNAPSHOT.jar!/org/apache/spark/sql/execution/ui/static
07:05:16.456 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED org.eclipse.jetty.servlet.DefaultServlet-be2e3d
07:05:16.456 pool-1-thread-1 DEBUG AbstractLifeCycle: STARTED o.e.j.s.ServletContextHandler{/static/sql,null}
07:05:16.457 pool-1-thread-1 INFO HiveContext: Initializing execution hive, version 1.2.1
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for all properties in config...
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.reducers.bytes.per.reducer
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.createtable.group.grants
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.storeManagerType
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.aux.jars.path
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.stagingdir
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.rcfile.block.level
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.rack
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.default.partition.name
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.event.expiry.duration
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.compress
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.mode.local.auto.input.files.max
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.key.prefix
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hadoop.supports.splittable.combineinputformat
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.skewjoin.compiletime
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.smbjoin.cache.rows
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.overflow.repeated.threshold
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.log.level
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.mapfiles
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.post.hooks
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.client.socket.lifetime
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for fs.har.impl
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.variance
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.split.minsize
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.quorum
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for stream.stderr.reporter.prefix
07:05:16.490 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.reduce.speculative
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.hybridgrace.memcheckfrequency
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.filter.compact.maxsize
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.counters.pull.interval
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.command.whitelist
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.end.function.listeners
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.downloaded.resources.dir
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.join.emit.interval
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.zerocopy
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compute.query.using.stats
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.block.padding.tolerance
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lazysimple.extended_boolean_literal
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.error.on.empty.partition
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.orc.splits.include.file.footer
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.prewarm.enabled
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hadoop.bin.path
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.rcfile.record.buffer.size
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.default.rcfile.serde
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.multi.insert.move.tasks.share.dependencies
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.createtable.owner.grants
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.users.in.admin.role
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.autogen.columnalias.prefix.includefuncname
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.max.partition.factor
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.port
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.orc.cache.stripe.details.size
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.max.created.files
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.job.committer.task.cleanup.needed
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.prompt
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.input.dir.recursive
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.deserialization.factor
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metadata.export.location
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.log.explain.output
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.skewjoin
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.default.fileformat
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.optimized.hashtable.wbsize
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.metastore.authorization.auth.reads
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.NonTransactionalRead
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.remove.identity.project
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.infer.bucket.sort.num.buckets.power.two
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.worker.threads
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exim.strict.repl.tables
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.collect.tablekeys
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.future.timeout
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.display.partition.cols.separately
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.async.exec.shutdown.timeout
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.disallow.incompatible.col.type.changes
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.max.idle.time
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.dummystats.aggregator
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.querylog.enable.plan.progress
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.mode
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.cookie.auth.enabled
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.worker.keepalive.time
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.archive.intermediate.archived
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.warehouse.dir
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hwi.listen.host
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.collect.scancols
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.max.split.size
07:05:16.491 pool-1-thread-1 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hwi.war.file
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.input.format
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.dummystats.publisher
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.cookie.is.httponly
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.uris
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.querylog.location
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.localize.resource.num.wait.attempts
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.stripe.size
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.querylog.plan.progress.interval
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.optimize.enable
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.job.debug.timeout
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.createtable.role.grants
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.decode.partition.name
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.partition.inherit.table.properties
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cluster.delegation.token.store.class
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.autoStartMechanismMode
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.client.port
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.cookie.max.age
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.row.index.stride
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.alias
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.exponential.backoff.slot.length
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.native.enabled
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.tez.default.queues
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compat
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.partitioner
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.smallfiles.avgsize
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hbase.wal.enabled
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.entity.capture.transform
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.blockfilter.file
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.logging.operation.enabled
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lockmgr.zookeeper.default.partition.name
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.support.concurrency
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.file.max.footer
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.mode.prefix
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.print.header
07:05:16.491 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.table.type.mapping
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.event.db.listener.timetolive
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.tasklog.debug.timeout
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hashtable.loadfactor
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.filter.hook
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.local.mem
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.union.remove
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.global.init.file.location
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.client.drop.partitions.using.expressions
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.outerjoin.supports.filters
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.auto.progress
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.dynamic.partition
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.intermediate.compression.type
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.try.direct.sql
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.failure.retries
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.analyze.stmt.collect.partlevel.stats
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hbase.generatehfiles
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.join.factor
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.pre.event.listeners
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.map.fair.scheduler.queue
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.reducededuplication
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.localize.resource.wait.interval
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.filter.compact.minsize
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.copyfile.maxsize
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.enabled
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.sasl.enabled
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.manager
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.compression.strategy
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.rpc.query.plan
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.mapredfiles
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cache.expr.evaluation
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.counters.group.name
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.transactionIsolation
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.in.test
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.groupby.skewindata
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.hybridgrace.hashtable
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.clean.until
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.reliable
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.batch.retrieve.max
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.entity.separator
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.binary.record.max.length
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.max.dynamic.partitions
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.groupby.sorted
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hashtable.initialCapacity
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.check.memory.rows
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.idle.operation.timeout
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.block.size
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.hdfs.read
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.server.connect.timeout
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.transport.mode
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.path
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.query.max.entries
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.execute.setugi
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.split.maxsize
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.bucket.cache.size
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.drop.ignorenonexistent
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.serdes.using.metastore.for.schema
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.mode.nosamplelist
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.sparkfiles
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exim.uri.scheme.whitelist
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.query.redactor.hooks
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.log4j.file
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.fixedDatastore
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.sasl.qop
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.job.committer.setup.cleanup.needed
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.delta.num.threshold
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.plan
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.serde
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.log4j.file
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ddl.createtablelike.properties.whitelist
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.node
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.bucketmapjoin
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.aggr.hash.percentmemory
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.server.max.message.size
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.job.debug.capture.stacktraces
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cluster.delegation.token.store.zookeeper.acl
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.groupby.sorted.testmode
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.minmax.enabled
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.sample.seednumber
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.event.clean.freq
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.session.hook
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.reduce.tasks.speculative.execution
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.sortmerge.join.bigtable.selection.policy
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stageid.rearrange
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.groupby.flush.percent
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.temporary.table.storage
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.groupby.maxentries
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.optimized.hashtable
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.optimize.fetch.max
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authenticator.manager
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.client.stats.publishers
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.validateColumns
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.parallel
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.rcfile.record.interval
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.submitviachild
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fetch.task.conversion
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.udtf.auto.progress
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.archive.enabled
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.builtin.udf.whitelist
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.rpc.max.size
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication.spnego.principal
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.authz.sstd.hs2.mode
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.async.exec.threads
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.convert.join.bucket.mapjoin.tez
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.execution.engine
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.container.size
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.ConnectionPassword
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.use.SSL
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.null.scan
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.size
07:05:16.492 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.smalltable.filesize
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.session.silent
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.query.string
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.min.worker.threads
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.enforce.sorting
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.join.use.nonstaged
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.tez.sessions.per.default.queue
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.idle.session.check.operation
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.port
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.key.prefix.max.length
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.logging.operation.log.location
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.ConnectionURL
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.hybridgrace.minnumpartitions
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hmshandler.force.reload.conf
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.server.tcp.keepalive
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.semantic.analyzer.hook
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.rpc.threads
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.aggr.hash.min.reduction
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.rcfile.column.number.conf
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.cpu
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.clean.extra.nodes
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.metadataonly
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.insert.into.multilevel.dirs
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.added.archives.path
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hmshandler.retry.attempts
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.validateConstraints
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.retries.max
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.memory.pool
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.prewarm.numcontainers
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.identifierFactory
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.errors.ignore
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.multigroupby.singlereducer
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.reduce.tasks.speculative.execution
07:05:16.493 pool-1-thread-1 INFO deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.conf.restricted.list
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.sampling.orderby.number
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fetch.task.aggr
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.sortmerge.join.to.mapjoin
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.warehouse.subdir.inherit.perms
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.committer.job.setup.cleanup.needed
07:05:16.493 pool-1-thread-1 INFO deprecation: mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.txn.timeout
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.fetch.partition.stats
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.progress.timeout
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.returnpath.hiveop
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.dictionary.key.size.threshold
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.scratchdir
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.optimize.limit.file
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.server.max.threads
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.try.direct.sql.ddl
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.script.allow.partial.consumption
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.hybridgrace.minwbsize
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.namespace
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.long.polling.timeout
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.debug.localtask
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.createtable.user.grants
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server.tcp.keepalive
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.ppd
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.script.maxerrsize
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.worker.keepalive.time
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.enforce.bucketmapjoin
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.connect.timeout
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.session.id
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.allow.user.substitution
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.join.noconditionaltask
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.input.format
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.autoupdate
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ssl.protocol.blacklist
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.dynamic.partition.pruning
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.fetch.column.stats
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.max.dynamic.partitions.pernode
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.cleaner.run.interval
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.min.split.size.per.rack
07:05:16.493 pool-1-thread-1 INFO deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.skewjoin.mapjoin.map.tasks
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.schema.verification.record.version
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.job.reduces
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.abortedtxn.threshold
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.aggr
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.support.quoted.identifiers
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.PersistenceManagerFactoryClass
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.initiator.on
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.orc.row.index.stride.dictionary.check
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.fs.handler.class
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.task.factory
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lock.numretries
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.typecheck.on.insert
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.join
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.support.dynamic.service.discovery
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.distinct.rewrite
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.authorization.storage.checks
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.skip.corrupt.data
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.cache.level2
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.builtin.udf.blacklist
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.kerberos.principal
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.rdbms.useLegacyNativeValueStrategy
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.collect.rawdatasize
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.ppd.storage
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.binary.search
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.local.fs.read
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.full
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.enabled
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.correlation
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.cookie.is.secure
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.orcfile.stripe.level
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.reorder.nway.joins
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.compress.output
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.user.install.directory
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.list.num.entries
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.sqlstd.confwhitelist.append
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.insert.into.external.tables
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.groupby.checkinterval
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.explain.dependency.append.tasktype
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.bucketingsorting
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.login.timeout
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.print.current.db
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.scratch.dir.permission
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hashtable.key.count.adjustment
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.failure.hooks
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.integral.jdo.pushdown
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.exception.handlers
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.jobname.length
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.bind.host
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.added.jars.path
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.tez.initialize.default.sessions
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.client.socket.timeout
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.DetachAllOnCommit
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.txn.max.open.batch
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.check.interval
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.query.id
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.key.prefix.reserve.length
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.ConnectionDriverName
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.reduce.enabled
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.delta.pct.threshold
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.current.database
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.orm.retrieveMapNullsAsEmptyStrings
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.max.variable.length
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.start.cleanup.scratchdir
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.rcfile.use.explicit.header
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.tezfiles
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.split.strategy
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.async.exec.keepalive.time
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.filter
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.sqlstd.confwhitelist
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.default.serde
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.listbucketing
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.connection.basesleeptime
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.ds.connection.url.hook
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.query.result.fileformat
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.partition.name.whitelist.pattern
07:05:16.493 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.map.num.entries
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.dynamic.partition.pruning.max.event.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.enable
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.constant.propagation
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.mode.local.auto
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.reducededuplication.min.reducer
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.transform.escape.input
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.max.start.attempts
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.max.worker.threads
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.dynamic.partition.mode
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.network
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.fpp
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.driver.run.hooks
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.pre.hooks
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.conf.validation
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.added.files.path
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.session.history.enabled
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.operator.id.env.var
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.unlock.numretries
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.Multithreaded
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.rework.mapredwork
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.groupby
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.client.connect.retry.delay
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.min.split.size
07:05:16.494 pool-1-thread-1 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.check.crossproducts
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server.read.socket.timeout
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.retries.wait
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.reducers.max
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fetch.task.conversion.threshold
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.row.max.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.perf.logger
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.thrift.compact.protocol.enabled
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.plugin.pluginRegistryBundleCheck
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.join.noconditionaltask.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.operator.truncate.env
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.join.cache.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.parallel.thread.number
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.validateTables
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.skewjoin.key
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.reader.wait
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.metastore.authenticator.manager
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.default.fileformat.managed
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.enforce.bucketing
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.reloadable.aux.jars.path
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.file.ignore.hdfs
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hmshandler.retry.interval
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.local.scratchdir
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.max.message.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.mode
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.buffer.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.gather.num.threads
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.pushdown.memory.usage
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.archive.intermediate.original
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.mode.local.auto.inputbytes.max
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.localtask.max.memory.usage
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.support.sql11.reserved.keywords
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.batch.retrieve.table.partition.max
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.dynamic.partition.pruning.max.data.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metadata.move.exported.metadata.to.trash
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.pretty.output.num.cols
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.session.timeout
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fetch.output.serde
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hbase.snapshot.restoredir
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.skewjoin.mapjoin.min.split
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.resultset.use.unique.column.names
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.connection.max.retries
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.partitions
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.worker.timeout
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.session.check.interval
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compute.splits.in.am
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for parquet.memory.pool.ratio
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.supports.subdirectories
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication.kerberos.principal
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.new.job.grouping.set.cardinality
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.client.stats.counters
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.enforce.sortmergebucketmapjoin
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.smb.number.waves
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.writer.wait
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ppd.recognizetransivity
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication.spnego.keytab
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.rcfile.tolerate.corruptions
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.secret.bits
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for stream.stderr.reporter.enabled
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.autogen.columnalias.prefix.label
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.event.listeners
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.repl.task.factory
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.int.timestamp.conversion.in.seconds
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.auto.reducer.parallelism
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.rawstore.impl
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.metastore.authorization.manager
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.autoCreateSchema
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.jar.path
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.native.multikey.only.enabled
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.orc.compute.splits.num.threads
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.query.max.table.partition
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.rowoffset
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.default.publisher
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.recordwriter
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ppd.remove.duplicatefilters
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.keystore.password
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.logging.operation.level
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.variable.substitute
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.txn.manager
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.cache.level2.type
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.stats.ndv.densityfunction
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.direct.sql.batch.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.min.split.size.per.node
07:05:16.494 pool-1-thread-1 INFO deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.jdbc.timeout
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.intermediate.compression.codec
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.server.min.threads
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.exec.print.summary
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.compress.intermediate
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.enabled
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.expression.proxy
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.recordreader
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.autogather
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.sort.dynamic.partition
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.init.hooks
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.dml.events
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.plan.serialization.format
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.thrift.framed.transport.enabled
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.reduce.tasks
07:05:16.494 pool-1-thread-1 INFO deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.log.every.n.records
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.heartbeat.interval
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.query.max.size
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.reduce.groupby.enabled
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lock.sleep.between.retries
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.mode.samplefreq
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.dbclass
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.jdbcdriver
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.concatenate.check.index
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.connectionPoolingType
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.rcfile.use.sync.cache
07:05:16.494 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.aggr.hash.force.flush.memory.threshold
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.cache.pinobjtypes
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fileformat.check
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.async.exec.wait.queue.size
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.default.aggregator
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.input.dir.recursive
07:05:16.495 pool-1-thread-1 INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.explain.user
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.encoding.strategy
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.keystore.path
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.schema.verification
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.connect.retries
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cluster.delegation.token.store.zookeeper.connectString
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.infer.bucket.sort
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.ttl
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.submit.local.task.via.child
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.file
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hwi.listen.port
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.archive.intermediate.extracted
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cluster.delegation.token.store.zookeeper.znode
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.dbconnectionstring
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.hdfs.write
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication.kerberos.keytab
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.cpu.vcores
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.followby.map.aggr.hash.percentmemory
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.parquet.timestamp.skip.conversion
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.exec.inplace.progress
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lock.manager
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapper.cannot.span.multiple.partitions
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.variable.substitute.depth
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.size.per.task
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.table.parameters.default
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ignore.mapjoin.hint
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.sampling.orderby.percent
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lock.mapred.only.operation
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.min.partition.factor
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.rpc.sasl.mechanisms
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.block.padding
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.sampling.orderby
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.kerberos.keytab.file
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.groupby.mapaggr.checkinterval
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.script.trust
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.followby.gby.localtask.max.memory.usage
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.ConnectionUserName
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.job.monitor.timeout
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.show.job.failure.debug.info
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.groupby.orderby.position.alias
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.extended
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.operator.env.blacklist
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.local.fs.write
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.in.tez.test
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.ndv.error
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.bucketmapjoin.sortedmerge
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.enable.doAs
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.zookeeper.namespace
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.atomic
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.groupby
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.sortmerge.join
07:05:16.495 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.idle.session.timeout
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.rdbms.datastoreAdapterClassName=org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.identifierFactory=datanucleus1
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.DetachAllOnCommit=true
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.NonTransactionalRead=true
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.transactionIsolation=read-committed
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.cache.level2=false
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateColumns=false
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.integral.jdo.pushdown=false
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.warehouse.dir=file:/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.uris=
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateTables=false
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.rdbms.useLegacyNativeValueStrategy=true
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.connectionPoolingType=BONECP
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionUserName=APP
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.storeManagerType=rdbms
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.autoStartMechanismMode=checked
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.cache.level2.type=none
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.fixedDatastore=false
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionURL=jdbc:derby:memory:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore;create=true
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver
07:05:16.583 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.plugin.pluginRegistryBundleCheck=LOG
07:05:16.584 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionPassword=xxx
07:05:16.584 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateConstraints=false
07:05:16.584 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.autoCreateSchema=true
07:05:16.584 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.Multithreaded=true
07:05:16.589 pool-1-thread-1 DEBUG SessionState: SessionState user: null
07:05:16.687 pool-1-thread-1 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.storeManagerType value null from  jpox.properties with rdbms
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.autoStartMechanismMode value null from  jpox.properties with checked
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.fixedDatastore value null from  jpox.properties with false
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.validateColumns value null from  jpox.properties with false
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.validateConstraints value null from  jpox.properties with false
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.rdbms.datastoreAdapterClassName value null from  jpox.properties with org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.Multithreaded value null from  jpox.properties with true
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.validateTables value null from  jpox.properties with false
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.autoCreateSchema value null from  jpox.properties with true
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.cache.level2.type value null from  jpox.properties with none
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.connectionPoolingType value null from  jpox.properties with BONECP
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.ConnectionUserName value null from  jpox.properties with APP
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.NonTransactionalRead value null from  jpox.properties with true
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.transactionIsolation value null from  jpox.properties with read-committed
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.ConnectionURL value null from  jpox.properties with jdbc:derby:memory:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore;create=true
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.identifierFactory value null from  jpox.properties with datanucleus1
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.PersistenceManagerFactoryClass value null from  jpox.properties with org.datanucleus.api.jdo.JDOPersistenceManagerFactory
07:05:16.713 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.cache.level2 value null from  jpox.properties with false
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.rdbms.useLegacyNativeValueStrategy value null from  jpox.properties with true
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: Overriding hive.metastore.integral.jdo.pushdown value null from  jpox.properties with false
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.DetachAllOnCommit value null from  jpox.properties with true
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.ConnectionDriverName value null from  jpox.properties with org.apache.derby.jdbc.EmbeddedDriver
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.plugin.pluginRegistryBundleCheck value null from  jpox.properties with LOG
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.rdbms.useLegacyNativeValueStrategy = true
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: hive.metastore.integral.jdo.pushdown = false
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.autoStartMechanismMode = checked
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.Multithreaded = true
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.identifierFactory = datanucleus1
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.transactionIsolation = read-committed
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.validateTables = false
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.ConnectionURL = jdbc:derby:memory:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore;create=true
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.DetachAllOnCommit = true
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.NonTransactionalRead = true
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.fixedDatastore = false
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.validateConstraints = false
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.ConnectionDriverName = org.apache.derby.jdbc.EmbeddedDriver
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.ConnectionUserName = APP
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.validateColumns = false
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.rdbms.datastoreAdapterClassName = org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.cache.level2 = false
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.plugin.pluginRegistryBundleCheck = LOG
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.cache.level2.type = none
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.PersistenceManagerFactoryClass = org.datanucleus.api.jdo.JDOPersistenceManagerFactory
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.autoCreateSchema = true
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.storeManagerType = rdbms
07:05:16.714 pool-1-thread-1 DEBUG ObjectStore: datanucleus.connectionPoolingType = BONECP
07:05:16.714 pool-1-thread-1 INFO ObjectStore: ObjectStore, initialize called
07:05:16.766 pool-1-thread-1 DEBUG General: Using PluginRegistry org.datanucleus.plugin.NonManagedPluginRegistry
07:05:16.775 pool-1-thread-1 DEBUG General: Registering bundle org.datanucleus.store.rdbms version 3.2.9 at URL file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar.
07:05:16.785 pool-1-thread-1 DEBUG General: Loading extension points from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/plugin.xml.
07:05:16.797 pool-1-thread-1 DEBUG General: Loading extensions from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/plugin.xml.
07:05:16.805 pool-1-thread-1 DEBUG General: Registering bundle org.datanucleus.api.jdo version 3.2.6 at URL file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar.
07:05:16.806 pool-1-thread-1 DEBUG General: Loading extension points from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/plugin.xml.
07:05:16.806 pool-1-thread-1 DEBUG General: Loading extensions from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/plugin.xml.
07:05:16.807 pool-1-thread-1 DEBUG General: Registering bundle org.datanucleus version 3.2.10 at URL file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar.
07:05:16.809 pool-1-thread-1 DEBUG General: Loading extension points from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/plugin.xml.
07:05:16.817 pool-1-thread-1 DEBUG General: Loading extensions from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/plugin.xml.
07:05:16.829 pool-1-thread-1 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
07:05:16.829 pool-1-thread-1 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
07:05:16.837 pool-1-thread-1 DEBUG Persistence: Java types support initialising ...
07:05:16.840 pool-1-thread-1 DEBUG Persistence: Error in loading java type support for javax.time.calendar.LocalDate : Class "javax.time.calendar.LocalDate" was not found in the CLASSPATH. Please check your specification and your CLASSPATH.
07:05:16.840 pool-1-thread-1 DEBUG Persistence: Error in loading java type support for javax.time.calendar.LocalDateTime : Class "javax.time.calendar.LocalDateTime" was not found in the CLASSPATH. Please check your specification and your CLASSPATH.
07:05:16.840 pool-1-thread-1 DEBUG Persistence: Error in loading java type support for javax.time.calendar.LocalTime : Class "javax.time.calendar.LocalTime" was not found in the CLASSPATH. Please check your specification and your CLASSPATH.
07:05:16.874 pool-1-thread-1 DEBUG Persistence: Java types support loaded : supported java types=java.awt.Color, java.lang.Float, [Ljava.lang.Integer;, java.sql.Time, java.util.SortedSet, java.net.URL, java.util.Date, java.util.PriorityQueue, float, [Ljava.lang.Character;, [Ljava.lang.Double;, java.lang.Integer, java.math.BigDecimal, java.util.Vector, java.lang.Character, java.lang.Enum, java.lang.Long, java.lang.Short, java.util.Locale, java.util.Map, java.math.BigInteger, java.net.URI, java.lang.Byte, java.util.GregorianCalendar, java.awt.image.BufferedImage, byte, double, java.util.TimeZone, java.sql.Timestamp, java.util.Collection, java.util.Set, java.util.UUID, [Ljava.lang.String;, java.util.List, [Ljava.lang.Short;, java.util.Queue, java.util.SortedMap, [Ljava.lang.Enum;, [Ljava.lang.Boolean;, java.lang.Double, [Ljava.util.Date;, [B, [C, [D, java.util.HashMap, java.util.Currency, [F, long, java.util.Stack, java.util.TreeSet, [I, java.util.ArrayList, [J, java.util.HashSet, java.util.LinkedHashMap, java.util.Calendar, java.lang.StringBuffer, [S, [Ljava.math.BigInteger;, java.lang.Boolean, [Ljava.lang.Number;, java.lang.String, [Ljava.math.BigDecimal;, java.lang.Number, java.util.LinkedList, java.util.Hashtable, java.util.LinkedHashSet, [Z, [Ljava.lang.Float;, java.util.Properties, [Ljava.lang.Byte;, [Ljava.util.Locale;, int, java.sql.Date, boolean, java.util.TreeMap, [Ljava.lang.Long;, char, short, java.lang.Class, java.util.BitSet, java.util.Arrays$ArrayList
07:05:16.874 pool-1-thread-1 DEBUG Persistence: Type converter support initialising ...
07:05:16.875 pool-1-thread-1 DEBUG Persistence: Added converter for java.math.BigDecimal<->java.lang.String using org.datanucleus.store.types.converters.BigDecimalStringConverter
07:05:16.876 pool-1-thread-1 DEBUG Persistence: Added converter for java.math.BigInteger<->java.lang.String using org.datanucleus.store.types.converters.BigIntegerStringConverter
07:05:16.876 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.BitSet<->java.lang.String using org.datanucleus.store.types.converters.BitSetStringConverter
07:05:16.877 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Calendar<->java.lang.String using org.datanucleus.store.types.converters.CalendarStringConverter
07:05:16.878 pool-1-thread-1 DEBUG Persistence: Added converter for java.lang.Class<->java.lang.String using org.datanucleus.store.types.converters.ClassStringConverter
07:05:16.879 pool-1-thread-1 DEBUG Persistence: Added converter for java.awt.Color<->java.lang.String using org.datanucleus.store.types.converters.ColorStringConverter
07:05:16.879 pool-1-thread-1 DEBUG Persistence: Added converter for java.lang.Integer<->java.lang.String using org.datanucleus.store.types.converters.IntegerStringConverter
07:05:16.880 pool-1-thread-1 DEBUG Persistence: Added converter for java.lang.Long<->java.lang.String using org.datanucleus.store.types.converters.LongStringConverter
07:05:16.881 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Currency<->java.lang.String using org.datanucleus.store.types.converters.CurrencyStringConverter
07:05:16.881 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Date<->java.lang.Long using org.datanucleus.store.types.converters.DateLongConverter
07:05:16.882 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Date<->java.lang.String using org.datanucleus.store.types.converters.DateStringConverter
07:05:16.883 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Locale<->java.lang.String using org.datanucleus.store.types.converters.LocaleStringConverter
07:05:16.883 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Date<->java.lang.Long using org.datanucleus.store.types.converters.SqlDateLongConverter
07:05:16.884 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Date<->java.lang.String using org.datanucleus.store.types.converters.SqlDateStringConverter
07:05:16.884 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Time<->java.lang.Long using org.datanucleus.store.types.converters.SqlTimeLongConverter
07:05:16.884 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Time<->java.lang.String using org.datanucleus.store.types.converters.SqlTimeStringConverter
07:05:16.885 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Timestamp<->java.lang.Long using org.datanucleus.store.types.converters.SqlTimestampLongConverter
07:05:16.885 pool-1-thread-1 DEBUG Persistence: Added converter for java.lang.StringBuffer<->java.lang.String using org.datanucleus.store.types.converters.StringBufferStringConverter
07:05:16.886 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.TimeZone<->java.lang.String using org.datanucleus.store.types.converters.TimeZoneStringConverter
07:05:16.886 pool-1-thread-1 DEBUG Persistence: Added converter for java.net.URI<->java.lang.String using org.datanucleus.store.types.converters.URIStringConverter
07:05:16.886 pool-1-thread-1 DEBUG Persistence: Added converter for java.net.URL<->java.lang.String using org.datanucleus.store.types.converters.URLStringConverter
07:05:16.887 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.UUID<->java.lang.String using org.datanucleus.store.types.converters.UUIDStringConverter
07:05:16.887 pool-1-thread-1 DEBUG Persistence: TypeConverter for javax.time.calendar.LocalDate<->java.lang.String ignored since java type not present in CLASSPATH
07:05:16.888 pool-1-thread-1 DEBUG Persistence: TypeConverter for javax.time.calendar.LocalTime<->java.lang.String ignored since java type not present in CLASSPATH
07:05:16.889 pool-1-thread-1 DEBUG Persistence: TypeConverter for javax.time.calendar.LocalTime<->java.lang.Long ignored since java type not present in CLASSPATH
07:05:16.889 pool-1-thread-1 DEBUG Persistence: TypeConverter for javax.time.calendar.LocalDateTime<->java.lang.String ignored since java type not present in CLASSPATH
07:05:16.890 pool-1-thread-1 DEBUG Persistence: Added converter for java.io.Serializable<->java.lang.String using org.datanucleus.store.types.converters.SerializableStringConverter
07:05:16.890 pool-1-thread-1 DEBUG Persistence: Added converter for java.io.Serializable<->[B using org.datanucleus.store.types.converters.SerializableByteArrayConverter
07:05:16.890 pool-1-thread-1 DEBUG Persistence: Type converter support loaded
07:05:16.890 pool-1-thread-1 DEBUG MetaData: MetaDataManager : Input=(XML,Annotations), XML-Validation=false, XML-Suffices=(persistence=*.jdo, orm=orm, query=*.jdoquery), JDO-listener=true
07:05:16.890 pool-1-thread-1 DEBUG MetaData: Registering listener for metadata initialisation
07:05:16.896 pool-1-thread-1 DEBUG Datastore: Creating StoreManager for datastore
07:05:17.192 pool-1-thread-1 DEBUG Connection: Created tx data source using pooling type of BONECP
07:05:17.192 pool-1-thread-1 DEBUG Connection: Registered transactional connection factory under name "rdbms/tx"
07:05:17.192 pool-1-thread-1 DEBUG Connection: Registered nontransactional connection factory under name "rdbms/nontx"
07:05:17.224 pool-1-thread-1 DEBUG Connection: Created nontx data source using pooling type of BONECP
07:05:17.227 pool-1-thread-1 DEBUG BoneCPDataSource: JDBC URL = jdbc:derby:memory:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
07:05:17.563 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@9ed687" opened
07:05:17.575 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Boolean (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:17.575 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Boolean (jdbc-type=BOOLEAN, sql-type=BOOLEAN, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BooleanRDBMSMapping, default=false)
07:05:17.576 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Boolean (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=false)
07:05:17.576 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Byte (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=false)
07:05:17.576 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Character (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=true)
07:05:17.577 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Character (jdbc-type=INTEGER, sql-type=INTEGER, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:17.577 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Character (jdbc-type=INTEGER, sql-type=INT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:17.577 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Double (jdbc-type=DOUBLE, sql-type=DOUBLE, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping, default=true)
07:05:17.578 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Double (jdbc-type=DECIMAL, sql-type=DECIMAL, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DecimalRDBMSMapping, default=false)
07:05:17.578 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Float (jdbc-type=FLOAT, sql-type=FLOAT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.FloatRDBMSMapping, default=true)
07:05:17.578 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Float (jdbc-type=DOUBLE, sql-type=DOUBLE, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping, default=false)
07:05:17.578 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Float (jdbc-type=REAL, sql-type=REAL, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.RealRDBMSMapping, default=false)
07:05:17.578 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Float (jdbc-type=DECIMAL, sql-type=DECIMAL, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DecimalRDBMSMapping, default=false)
07:05:17.578 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=INTEGER, sql-type=INTEGER, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=true)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=INTEGER, sql-type=INT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=true)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Long (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=true)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Long (jdbc-type=INTEGER, sql-type=INT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Long (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Long (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Short (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=true)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Short (jdbc-type=INTEGER, sql-type=INTEGER, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Short (jdbc-type=INTEGER, sql-type=INT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=true)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:17.579 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:17.580 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=LONGVARCHAR, sql-type=LONGVARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.LongVarcharRDBMSMapping, default=false)
07:05:17.580 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=CLOB, sql-type=CLOB, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.ClobRDBMSMapping, default=false)
07:05:17.581 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=BLOB, sql-type=BLOB, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BlobRDBMSMapping, default=false)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=NVARCHAR, sql-type=NVARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NVarcharRDBMSMapping, default=false)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=NCHAR, sql-type=NCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NCharRDBMSMapping, default=false)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.math.BigDecimal (jdbc-type=DECIMAL, sql-type=DECIMAL, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DecimalRDBMSMapping, default=true)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.math.BigDecimal (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=false)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.math.BigInteger (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=true)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=DATE, sql-type=DATE, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DateRDBMSMapping, default=true)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=TIMESTAMP, sql-type=TIMESTAMP, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping, default=false)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:17.584 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=TIME, sql-type=TIME, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimeRDBMSMapping, default=true)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=TIMESTAMP, sql-type=TIMESTAMP, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Timestamp (jdbc-type=TIMESTAMP, sql-type=TIMESTAMP, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping, default=true)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Timestamp (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Timestamp (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=TIMESTAMP, sql-type=TIMESTAMP, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping, default=true)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=DATE, sql-type=DATE, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DateRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:17.585 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=TIME, sql-type=TIME, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimeRDBMSMapping, default=false)
07:05:17.586 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.io.Serializable (jdbc-type=LONGVARBINARY, sql-type=LONGVARBINARY, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.LongVarBinaryRDBMSMapping, default=true)
07:05:17.586 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.io.Serializable (jdbc-type=BLOB, sql-type=BLOB, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BlobRDBMSMapping, default=false)
07:05:17.586 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.io.Serializable (jdbc-type=VARBINARY, sql-type=VARBINARY, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarBinaryRDBMSMapping, default=false)
07:05:17.586 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.io.File (jdbc-type=LONGVARBINARY, sql-type=LONGVARBINARY, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BinaryStreamRDBMSMapping, default=true)
07:05:17.586 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=true)
07:05:17.586 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=INTEGER, sql-type=INTEGER, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:17.586 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=false)
07:05:17.587 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:17.587 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:17.854 pool-1-thread-1 DEBUG Datastore: Removing RDBMS support for Java type java.lang.String (jdbc-type=NVARCHAR, sql-type=NVARCHAR)
07:05:17.854 pool-1-thread-1 DEBUG Datastore: Removing RDBMS support for Java type java.lang.String (jdbc-type=NCHAR, sql-type=NCHAR)
07:05:17.873 pool-1-thread-1 DEBUG Datastore: ======================= Datastore =========================
07:05:17.873 pool-1-thread-1 DEBUG Datastore: StoreManager : "rdbms" (org.datanucleus.store.rdbms.RDBMSStoreManager)
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Datastore : read-write
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Schema Control : AutoCreate(Tables,Columns,Constraints), Validate(None)
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Query Languages : [JDOQL, JPQL, SQL, STOREDPROC]
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Queries : Timeout=0
07:05:17.873 pool-1-thread-1 DEBUG Datastore: ===========================================================
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Datastore Adapter : org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Datastore : name="Apache Derby" version="10.10.2.0 - (1582446)"
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Datastore Driver : name="Apache Derby Embedded JDBC Driver" version="10.10.2.0 - (1582446)"
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Primary Connection Factory : URL[jdbc:derby:memory:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore;create=true]
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Secondary Connection Factory : URL[jdbc:derby:memory:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore;create=true]
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Datastore Identifiers : factory="datanucleus1" case=UPPERCASE catalog= schema=APP
07:05:17.873 pool-1-thread-1 DEBUG Datastore: Supported Identifier Cases : "MixedCase" UPPERCASE "MixedCase-Sensitive"
07:05:17.874 pool-1-thread-1 DEBUG Datastore: Supported Identifier Lengths (max) : Table=128 Column=128 Constraint=128 Index=128 Delimiter="
07:05:17.874 pool-1-thread-1 DEBUG Datastore: Support for Identifiers in DDL : catalog=false schema=true
07:05:17.874 pool-1-thread-1 DEBUG Datastore: Datastore : checkTableViewExistence, rdbmsConstraintCreateMode=DataNucleus, initialiseColumnInfo=ALL
07:05:17.874 pool-1-thread-1 DEBUG Datastore: Support Statement Batching : yes (max-batch-size=50)
07:05:17.874 pool-1-thread-1 DEBUG Datastore: Queries : Results direction=forward, type=forward-only, concurrency=read-only
07:05:17.874 pool-1-thread-1 DEBUG Datastore: Java-Types : string-default-length=255
07:05:17.874 pool-1-thread-1 DEBUG Datastore: JDBC-Types : VARCHAR, LONGVARCHAR, BINARY, BOOLEAN, VARBINARY, LONGVARBINARY, BIGINT, JAVA_OBJECT, [id=2009], CHAR, NUMERIC, DECIMAL, INTEGER, CLOB, SMALLINT, BLOB, FLOAT, REAL, DOUBLE, DATE, TIME, TIMESTAMP
07:05:17.874 pool-1-thread-1 DEBUG Datastore: ===========================================================
07:05:17.874 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@9ed687" closed
07:05:17.874 pool-1-thread-1 DEBUG Datastore: StoreManager now created
07:05:17.875 pool-1-thread-1 DEBUG Persistence: Started pool of ExecutionContext (maxPool=20, reaperThread=false)
07:05:17.879 pool-1-thread-1 DEBUG Persistence: ================= NucleusContext ===============
07:05:17.879 pool-1-thread-1 DEBUG Persistence: DataNucleus Context : Version "3.2.10" with JRE "1.8.0_31" on "Mac OS X"
07:05:17.879 pool-1-thread-1 DEBUG Persistence: Persistence API : JDO
07:05:17.879 pool-1-thread-1 DEBUG Persistence: Plugin Registry : org.datanucleus.plugin.NonManagedPluginRegistry
07:05:17.879 pool-1-thread-1 DEBUG Persistence: ClassLoading : datanucleus
07:05:17.879 pool-1-thread-1 DEBUG Persistence: Persistence : pm-multithreaded, nontransactional-read, nontransactional-write, reachability-at-commit, detach-all-on-commit, copy-on-attach, managed-relations(checked), deletion-policy=JDO2, serverTimeZone=America/Los_Angeles
07:05:17.879 pool-1-thread-1 DEBUG Persistence: AutoStart : mechanism=None, mode=checked
07:05:17.879 pool-1-thread-1 DEBUG Persistence: Transactions : type=RESOURCE_LOCAL, mode=datastore, isolation=read-committed
07:05:17.879 pool-1-thread-1 DEBUG Persistence: ValueGeneration : txn-isolation=read-committed connection=New
07:05:17.879 pool-1-thread-1 DEBUG Persistence: Cache : Level1 (soft), Level2 (none, mode=UNSPECIFIED), QueryResults (soft), Collections/Maps
07:05:17.879 pool-1-thread-1 DEBUG Persistence: ================================================
07:05:17.880 pool-1-thread-1 DEBUG Cache: Level 2 Cache of type "none" initialised
07:05:17.924 pool-1-thread-1 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
07:05:17.941 pool-1-thread-1 DEBUG Persistence: ExecutionContext "org.datanucleus.ExecutionContextThreadedImpl@718838d0" opened for datastore "org.datanucleus.store.rdbms.RDBMSStoreManager@7be1af70" with txn="org.datanucleus.TransactionImpl@3ba12862"
07:05:17.942 pool-1-thread-1 DEBUG Cache: Level 1 Cache of type "soft" initialised
07:05:17.950 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:17.951 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:17.952 pool-1-thread-1 DEBUG BoneCPDataSource: JDBC URL = jdbc:derby:memory:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
07:05:17.955 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@40814444" opened with isolation level "read-committed" and auto-commit=false
07:05:17.955 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@39250ff6, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:17.955 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@39250ff6 is starting for transaction Xid=    with flags 0
07:05:17.956 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@4eda9f1f [conn=com.jolbox.bonecp.ConnectionHandle@40814444, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:17.959 pool-1-thread-1 DEBUG MetaStoreDirectSql: MySql check failed, assuming we are not on MySql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
07:05:17.959 pool-1-thread-1 DEBUG Transaction: Transaction rolling back for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:17.960 pool-1-thread-1 DEBUG Transaction: Rolling back [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@39250ff6]]
07:05:17.960 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@39250ff6 is rolling back for transaction Xid=   
07:05:17.960 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@39250ff6 rolled back connection for transaction Xid=   
07:05:17.960 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@40814444" closed
07:05:17.960 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@4eda9f1f [conn=com.jolbox.bonecp.ConnectionHandle@40814444, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:17.960 pool-1-thread-1 DEBUG Transaction: Transaction rolled back in 1 ms
07:05:17.960 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:17.960 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:17.960 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@16d9e8f8" opened with isolation level "read-committed" and auto-commit=false
07:05:17.960 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1746610f, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:17.960 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1746610f is starting for transaction Xid=    with flags 0
07:05:17.960 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2f1652e6 [conn=com.jolbox.bonecp.ConnectionHandle@16d9e8f8, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:17.961 pool-1-thread-1 DEBUG MetaStoreDirectSql: Oracle check failed, assuming we are not on Oracle: Lexical error at line 1, column 22.  Encountered: "$" (36), after : "".
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Transaction rolling back for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Rolling back [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1746610f]]
07:05:17.961 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1746610f is rolling back for transaction Xid=   
07:05:17.961 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1746610f rolled back connection for transaction Xid=   
07:05:17.961 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@16d9e8f8" closed
07:05:17.961 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2f1652e6 [conn=com.jolbox.bonecp.ConnectionHandle@16d9e8f8, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Transaction rolled back in 0 ms
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:17.961 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@52bbb818" opened with isolation level "read-committed" and auto-commit=false
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@47ed24c3, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:17.961 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@47ed24c3 is starting for transaction Xid=    with flags 0
07:05:17.961 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@23d74a71 [conn=com.jolbox.bonecp.ConnectionHandle@52bbb818, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:17.961 pool-1-thread-1 DEBUG MetaStoreDirectSql: MSSQL check failed, assuming we are not on MSSQL: Lexical error at line 1, column 8.  Encountered: "@" (64), after : "".
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Transaction rolling back for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Rolling back [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@47ed24c3]]
07:05:17.961 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@47ed24c3 is rolling back for transaction Xid=   
07:05:17.961 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@47ed24c3 rolled back connection for transaction Xid=   
07:05:17.961 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@52bbb818" closed
07:05:17.961 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@23d74a71 [conn=com.jolbox.bonecp.ConnectionHandle@52bbb818, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Transaction rolled back in 0 ms
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:17.961 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:17.961 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@246af10" opened with isolation level "read-committed" and auto-commit=false
07:05:17.962 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@569ebced, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:17.962 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@569ebced is starting for transaction Xid=    with flags 0
07:05:17.962 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@79fd3403 [conn=com.jolbox.bonecp.ConnectionHandle@246af10, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:17.963 pool-1-thread-1 DEBUG Cache: Query Cache of type "org.datanucleus.query.cache.SoftQueryCompilationCache" initialised
07:05:17.964 pool-1-thread-1 DEBUG Cache: Query Cache of type "org.datanucleus.store.query.cache.SoftQueryDatastoreCompilationCache" initialised
07:05:17.965 pool-1-thread-1 DEBUG Cache: Query Cache of type "org.datanucleus.store.query.cache.SoftQueryResultsCache" initialised
07:05:17.978 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == ''"
07:05:17.992 pool-1-thread-1 DEBUG MetaData: MetaData of type "jdo" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /META-INF/package.jdo
07:05:17.993 pool-1-thread-1 DEBUG MetaData: MetaData of type "jdo" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /WEB-INF/package.jdo
07:05:17.994 pool-1-thread-1 DEBUG MetaData: Parsing MetaData file "jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/package.jdo" using handler "jdo" (validation="false")
07:05:18.020 pool-1-thread-1 DEBUG MetaData: XML Entity Public="-//Sun Microsystems, Inc.//DTD Java Data Objects Metadata 2.0//EN" System="http://java.sun.com/dtd/jdo_2_0.dtd" : using local source "/org/datanucleus/api/jdo/jdo_2_0.dtd"
07:05:18.053 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDatabase" : Populating Meta-Data
07:05:18.054 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /META-INF/package.orm
07:05:18.054 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /WEB-INF/package.orm
07:05:18.055 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /package.orm
07:05:18.061 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/package.orm
07:05:18.061 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org.orm
07:05:18.062 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/package.orm
07:05:18.069 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache.orm
07:05:18.070 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/package.orm
07:05:18.070 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop.orm
07:05:18.071 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.072 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive.orm
07:05:18.073 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.073 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.074 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.075 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.076 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore/model/MDatabase.orm
07:05:18.076 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" not found
07:05:18.079 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MFieldSchema" : Populating Meta-Data
07:05:18.079 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /META-INF/package.orm
07:05:18.080 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /WEB-INF/package.orm
07:05:18.080 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /package.orm
07:05:18.085 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/package.orm
07:05:18.086 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org.orm
07:05:18.086 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/package.orm
07:05:18.090 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache.orm
07:05:18.091 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/package.orm
07:05:18.091 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop.orm
07:05:18.092 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.092 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive.orm
07:05:18.093 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.093 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.094 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.094 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.096 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore/model/MFieldSchema.orm
07:05:18.096 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" not found
07:05:18.097 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MType" : Populating Meta-Data
07:05:18.097 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /META-INF/package.orm
07:05:18.098 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /WEB-INF/package.orm
07:05:18.099 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /package.orm
07:05:18.103 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/package.orm
07:05:18.103 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org.orm
07:05:18.104 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/package.orm
07:05:18.108 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache.orm
07:05:18.109 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/package.orm
07:05:18.110 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop.orm
07:05:18.110 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.111 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive.orm
07:05:18.111 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.112 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.112 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.113 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.113 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore/model/MType.orm
07:05:18.113 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" not found
07:05:18.115 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTable" : Populating Meta-Data
07:05:18.116 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /META-INF/package.orm
07:05:18.116 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /WEB-INF/package.orm
07:05:18.117 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /package.orm
07:05:18.120 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/package.orm
07:05:18.121 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org.orm
07:05:18.121 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/package.orm
07:05:18.125 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache.orm
07:05:18.125 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/package.orm
07:05:18.126 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop.orm
07:05:18.126 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.127 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive.orm
07:05:18.127 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.128 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.128 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.129 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.129 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore/model/MTable.orm
07:05:18.129 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" not found
07:05:18.131 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" : Populating Meta-Data
07:05:18.132 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /META-INF/package.orm
07:05:18.132 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /WEB-INF/package.orm
07:05:18.133 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /package.orm
07:05:18.136 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/package.orm
07:05:18.137 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org.orm
07:05:18.137 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/package.orm
07:05:18.141 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache.orm
07:05:18.142 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/package.orm
07:05:18.142 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop.orm
07:05:18.143 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.143 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive.orm
07:05:18.144 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.144 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.145 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.145 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.146 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore/model/MSerDeInfo.orm
07:05:18.146 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" not found
07:05:18.146 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MOrder" : Populating Meta-Data
07:05:18.147 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /META-INF/package.orm
07:05:18.147 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /WEB-INF/package.orm
07:05:18.148 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /package.orm
07:05:18.151 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/package.orm
07:05:18.152 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org.orm
07:05:18.152 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/package.orm
07:05:18.156 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache.orm
07:05:18.156 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/package.orm
07:05:18.157 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop.orm
07:05:18.157 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.158 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive.orm
07:05:18.158 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.159 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.159 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.160 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.161 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore/model/MOrder.orm
07:05:18.161 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" not found
07:05:18.161 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" : Populating Meta-Data
07:05:18.162 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /META-INF/package.orm
07:05:18.163 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /WEB-INF/package.orm
07:05:18.163 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /package.orm
07:05:18.167 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/package.orm
07:05:18.168 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org.orm
07:05:18.168 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/package.orm
07:05:18.171 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache.orm
07:05:18.172 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/package.orm
07:05:18.172 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop.orm
07:05:18.173 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.173 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive.orm
07:05:18.174 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.174 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.175 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.175 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.176 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model/MColumnDescriptor.orm
07:05:18.176 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" not found
07:05:18.177 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MStringList" : Populating Meta-Data
07:05:18.178 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /META-INF/package.orm
07:05:18.179 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /WEB-INF/package.orm
07:05:18.179 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /package.orm
07:05:18.182 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/package.orm
07:05:18.183 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org.orm
07:05:18.184 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/package.orm
07:05:18.189 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache.orm
07:05:18.190 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/package.orm
07:05:18.190 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop.orm
07:05:18.191 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.191 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive.orm
07:05:18.192 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.192 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.193 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.193 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.194 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore/model/MStringList.orm
07:05:18.194 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" not found
07:05:18.194 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" : Populating Meta-Data
07:05:18.195 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /META-INF/package.orm
07:05:18.195 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /WEB-INF/package.orm
07:05:18.196 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /package.orm
07:05:18.199 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/package.orm
07:05:18.200 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org.orm
07:05:18.200 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/package.orm
07:05:18.204 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache.orm
07:05:18.204 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/package.orm
07:05:18.205 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop.orm
07:05:18.205 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.206 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive.orm
07:05:18.206 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.207 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.207 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.208 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.208 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model/MStorageDescriptor.orm
07:05:18.208 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" not found
07:05:18.212 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartition" : Populating Meta-Data
07:05:18.213 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /META-INF/package.orm
07:05:18.213 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /WEB-INF/package.orm
07:05:18.214 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /package.orm
07:05:18.217 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/package.orm
07:05:18.217 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org.orm
07:05:18.218 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/package.orm
07:05:18.221 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache.orm
07:05:18.221 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/package.orm
07:05:18.222 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop.orm
07:05:18.222 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.223 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive.orm
07:05:18.223 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.224 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.224 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.225 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.225 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore/model/MPartition.orm
07:05:18.225 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" not found
07:05:18.226 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MIndex" : Populating Meta-Data
07:05:18.227 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /META-INF/package.orm
07:05:18.228 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /WEB-INF/package.orm
07:05:18.228 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /package.orm
07:05:18.231 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/package.orm
07:05:18.232 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org.orm
07:05:18.232 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/package.orm
07:05:18.235 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache.orm
07:05:18.236 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/package.orm
07:05:18.236 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop.orm
07:05:18.237 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.237 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive.orm
07:05:18.238 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.238 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.239 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.239 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.240 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore/model/MIndex.orm
07:05:18.240 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" not found
07:05:18.240 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MRole" : Populating Meta-Data
07:05:18.241 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /META-INF/package.orm
07:05:18.242 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /WEB-INF/package.orm
07:05:18.242 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /package.orm
07:05:18.246 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/package.orm
07:05:18.246 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org.orm
07:05:18.246 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/package.orm
07:05:18.250 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache.orm
07:05:18.250 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/package.orm
07:05:18.251 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop.orm
07:05:18.251 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.252 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive.orm
07:05:18.252 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.253 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.254 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.255 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.255 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore/model/MRole.orm
07:05:18.255 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" not found
07:05:18.256 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MRoleMap" : Populating Meta-Data
07:05:18.257 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /META-INF/package.orm
07:05:18.258 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /WEB-INF/package.orm
07:05:18.259 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /package.orm
07:05:18.263 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/package.orm
07:05:18.264 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org.orm
07:05:18.264 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/package.orm
07:05:18.269 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache.orm
07:05:18.269 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/package.orm
07:05:18.270 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop.orm
07:05:18.270 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.271 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive.orm
07:05:18.272 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.272 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.273 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.274 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.275 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore/model/MRoleMap.orm
07:05:18.275 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" not found
07:05:18.275 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" : Populating Meta-Data
07:05:18.277 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /META-INF/package.orm
07:05:18.278 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /WEB-INF/package.orm
07:05:18.278 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /package.orm
07:05:18.283 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/package.orm
07:05:18.284 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org.orm
07:05:18.285 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/package.orm
07:05:18.290 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache.orm
07:05:18.290 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:18.291 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop.orm
07:05:18.292 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.292 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:18.293 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.294 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.294 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.295 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.296 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MGlobalPrivilege.orm
07:05:18.296 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" not found
07:05:18.296 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" : Populating Meta-Data
07:05:18.298 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /META-INF/package.orm
07:05:18.298 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /WEB-INF/package.orm
07:05:18.299 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /package.orm
07:05:18.304 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/package.orm
07:05:18.305 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org.orm
07:05:18.306 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/package.orm
07:05:18.310 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache.orm
07:05:18.311 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:18.311 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop.orm
07:05:18.312 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.312 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:18.313 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.314 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.314 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.315 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.316 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MDBPrivilege.orm
07:05:18.316 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" not found
07:05:18.317 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" : Populating Meta-Data
07:05:18.319 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /META-INF/package.orm
07:05:18.319 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /WEB-INF/package.orm
07:05:18.320 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /package.orm
07:05:18.324 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/package.orm
07:05:18.324 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org.orm
07:05:18.325 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/package.orm
07:05:18.330 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache.orm
07:05:18.330 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:18.331 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop.orm
07:05:18.331 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.332 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:18.333 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.334 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.335 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.336 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.336 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MTablePrivilege.orm
07:05:18.336 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" not found
07:05:18.337 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" : Populating Meta-Data
07:05:18.339 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /META-INF/package.orm
07:05:18.339 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /WEB-INF/package.orm
07:05:18.340 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /package.orm
07:05:18.344 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/package.orm
07:05:18.344 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org.orm
07:05:18.345 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/package.orm
07:05:18.349 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache.orm
07:05:18.350 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:18.351 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop.orm
07:05:18.351 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.352 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:18.352 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.353 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.354 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.354 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.355 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MPartitionPrivilege.orm
07:05:18.355 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" not found
07:05:18.356 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" : Populating Meta-Data
07:05:18.357 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /META-INF/package.orm
07:05:18.358 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /WEB-INF/package.orm
07:05:18.358 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /package.orm
07:05:18.362 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/package.orm
07:05:18.363 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org.orm
07:05:18.363 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/package.orm
07:05:18.368 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache.orm
07:05:18.368 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:18.369 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop.orm
07:05:18.369 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.370 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:18.371 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.371 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.372 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.372 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.373 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MTableColumnPrivilege.orm
07:05:18.373 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" not found
07:05:18.374 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" : Populating Meta-Data
07:05:18.375 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /META-INF/package.orm
07:05:18.376 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /WEB-INF/package.orm
07:05:18.376 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /package.orm
07:05:18.380 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/package.orm
07:05:18.381 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org.orm
07:05:18.381 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/package.orm
07:05:18.386 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache.orm
07:05:18.386 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:18.387 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop.orm
07:05:18.388 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.388 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:18.389 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.390 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.390 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.391 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.392 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MPartitionColumnPrivilege.orm
07:05:18.392 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" not found
07:05:18.392 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" : Populating Meta-Data
07:05:18.394 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /META-INF/package.orm
07:05:18.394 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /WEB-INF/package.orm
07:05:18.395 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /package.orm
07:05:18.399 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/package.orm
07:05:18.400 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org.orm
07:05:18.400 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/package.orm
07:05:18.412 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache.orm
07:05:18.412 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/package.orm
07:05:18.413 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop.orm
07:05:18.413 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.414 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive.orm
07:05:18.414 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.415 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.415 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.416 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.416 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore/model/MPartitionEvent.orm
07:05:18.416 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" not found
07:05:18.417 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MMasterKey" : Populating Meta-Data
07:05:18.418 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /META-INF/package.orm
07:05:18.418 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /WEB-INF/package.orm
07:05:18.419 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /package.orm
07:05:18.422 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/package.orm
07:05:18.422 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org.orm
07:05:18.423 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/package.orm
07:05:18.426 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache.orm
07:05:18.426 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/package.orm
07:05:18.427 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop.orm
07:05:18.427 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.428 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive.orm
07:05:18.428 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.428 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.429 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.429 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.430 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore/model/MMasterKey.orm
07:05:18.430 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" not found
07:05:18.433 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDelegationToken" : Populating Meta-Data
07:05:18.434 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /META-INF/package.orm
07:05:18.434 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /WEB-INF/package.orm
07:05:18.435 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /package.orm
07:05:18.438 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/package.orm
07:05:18.438 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org.orm
07:05:18.439 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/package.orm
07:05:18.445 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache.orm
07:05:18.445 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/package.orm
07:05:18.446 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop.orm
07:05:18.446 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.447 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive.orm
07:05:18.447 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.448 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.449 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.449 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.450 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore/model/MDelegationToken.orm
07:05:18.450 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" not found
07:05:18.450 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" : Populating Meta-Data
07:05:18.452 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /META-INF/package.orm
07:05:18.452 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /WEB-INF/package.orm
07:05:18.453 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /package.orm
07:05:18.455 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/package.orm
07:05:18.456 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org.orm
07:05:18.456 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/package.orm
07:05:18.459 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache.orm
07:05:18.460 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/package.orm
07:05:18.460 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop.orm
07:05:18.461 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.461 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive.orm
07:05:18.461 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.462 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.462 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.463 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.463 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.orm
07:05:18.463 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" not found
07:05:18.464 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" : Populating Meta-Data
07:05:18.466 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /META-INF/package.orm
07:05:18.466 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /WEB-INF/package.orm
07:05:18.466 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /package.orm
07:05:18.470 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/package.orm
07:05:18.470 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org.orm
07:05:18.470 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/package.orm
07:05:18.473 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache.orm
07:05:18.474 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/package.orm
07:05:18.474 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop.orm
07:05:18.475 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.475 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive.orm
07:05:18.476 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.476 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.476 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.477 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.478 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.orm
07:05:18.478 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" not found
07:05:18.479 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MVersionTable" : Populating Meta-Data
07:05:18.480 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /META-INF/package.orm
07:05:18.480 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /WEB-INF/package.orm
07:05:18.481 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /package.orm
07:05:18.484 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/package.orm
07:05:18.485 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org.orm
07:05:18.486 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/package.orm
07:05:18.491 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache.orm
07:05:18.491 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/package.orm
07:05:18.492 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop.orm
07:05:18.492 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.493 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive.orm
07:05:18.493 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.494 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.495 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.495 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.496 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore/model/MVersionTable.orm
07:05:18.496 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" not found
07:05:18.496 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MResourceUri" : Populating Meta-Data
07:05:18.497 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /META-INF/package.orm
07:05:18.498 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /WEB-INF/package.orm
07:05:18.498 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /package.orm
07:05:18.501 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/package.orm
07:05:18.502 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org.orm
07:05:18.502 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/package.orm
07:05:18.505 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache.orm
07:05:18.505 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/package.orm
07:05:18.506 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop.orm
07:05:18.506 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.507 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive.orm
07:05:18.507 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.508 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.508 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.508 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.509 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore/model/MResourceUri.orm
07:05:18.509 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" not found
07:05:18.509 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MFunction" : Populating Meta-Data
07:05:18.510 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /META-INF/package.orm
07:05:18.511 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /WEB-INF/package.orm
07:05:18.511 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /package.orm
07:05:18.514 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/package.orm
07:05:18.515 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org.orm
07:05:18.515 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/package.orm
07:05:18.518 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache.orm
07:05:18.519 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/package.orm
07:05:18.519 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop.orm
07:05:18.519 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.520 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive.orm
07:05:18.520 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.521 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.521 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.522 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.522 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore/model/MFunction.orm
07:05:18.522 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" not found
07:05:18.524 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MNotificationLog" : Populating Meta-Data
07:05:18.525 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /META-INF/package.orm
07:05:18.525 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /WEB-INF/package.orm
07:05:18.526 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /package.orm
07:05:18.529 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/package.orm
07:05:18.529 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org.orm
07:05:18.530 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/package.orm
07:05:18.534 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache.orm
07:05:18.534 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/package.orm
07:05:18.535 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop.orm
07:05:18.536 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.536 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive.orm
07:05:18.537 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.537 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.538 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.538 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.539 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore/model/MNotificationLog.orm
07:05:18.539 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" not found
07:05:18.540 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" : Populating Meta-Data
07:05:18.541 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /META-INF/package.orm
07:05:18.541 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /WEB-INF/package.orm
07:05:18.541 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /package.orm
07:05:18.544 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/package.orm
07:05:18.545 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org.orm
07:05:18.545 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/package.orm
07:05:18.548 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache.orm
07:05:18.549 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/package.orm
07:05:18.549 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop.orm
07:05:18.550 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/package.orm
07:05:18.550 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive.orm
07:05:18.551 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:18.551 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:18.552 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:18.552 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:18.553 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore/model/MNotificationNextId.orm
07:05:18.553 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" not found
07:05:18.553 pool-1-thread-1 DEBUG MetaData: MetaData of type "jdo" for class "org.apache.hadoop.hive.metastore.model.MDatabase" will use jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/package.jdo
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDatabase" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MFieldSchema" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MType" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTable" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MOrder" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MStringList" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartition" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MIndex" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MRole" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MRoleMap" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" : Initialising Meta-Data
07:05:18.554 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MMasterKey" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDelegationToken" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MVersionTable" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MResourceUri" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MFunction" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MNotificationLog" : Initialising Meta-Data
07:05:18.555 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" : Initialising Meta-Data
07:05:18.558 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 580 ms
07:05:18.558 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{name}  =  Literal{}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MDatabase]
07:05:18.559 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == ''" for datastore
07:05:18.576 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MDatabase [Table : DBS, InheritanceStrategy : new-table]
07:05:18.579 pool-1-thread-1 DEBUG Schema: Column "DBS.DB_ID" added to internal representation of table.
07:05:18.581 pool-1-thread-1 DEBUG Schema: Table DBS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MDatabase (inheritance strategy="new-table")
07:05:18.581 pool-1-thread-1 DEBUG Schema: Column "DBS."DESC"" added to internal representation of table.
07:05:18.582 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.description] -> Column(s) [DBS."DESC"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.582 pool-1-thread-1 DEBUG Schema: Column "DBS.DB_LOCATION_URI" added to internal representation of table.
07:05:18.582 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.locationUri] -> Column(s) [DBS.DB_LOCATION_URI] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.582 pool-1-thread-1 DEBUG Schema: Column "DBS."NAME"" added to internal representation of table.
07:05:18.582 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.name] -> Column(s) [DBS."NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.582 pool-1-thread-1 DEBUG Schema: Column "DBS.OWNER_NAME" added to internal representation of table.
07:05:18.582 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.ownerName] -> Column(s) [DBS.OWNER_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.582 pool-1-thread-1 DEBUG Schema: Column "DBS.OWNER_TYPE" added to internal representation of table.
07:05:18.583 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.ownerType] -> Column(s) [DBS.OWNER_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.586 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MDatabase.parameters [Table : DATABASE_PARAMS]
07:05:18.586 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:18.586 pool-1-thread-1 DEBUG Schema: Table/View DBS has been initialised
07:05:18.590 pool-1-thread-1 DEBUG Schema: Column "DATABASE_PARAMS.DB_ID" added to internal representation of table.
07:05:18.590 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MDatabase.parameters] -> Column(s) [DATABASE_PARAMS.DB_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.590 pool-1-thread-1 DEBUG Schema: Column "DATABASE_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:18.590 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MDatabase.parameters] -> Column(s) [DATABASE_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.590 pool-1-thread-1 DEBUG Schema: Column "DATABASE_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:18.590 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MDatabase.parameters] -> Column(s) [DATABASE_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.590 pool-1-thread-1 DEBUG Schema: Table/View DATABASE_PARAMS has been initialised
07:05:18.591 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@34a6cf62" opened with isolation level "serializable" and auto-commit=false
07:05:18.591 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@34a6cf62" with isolation "serializable"
07:05:18.654 pool-1-thread-1 DEBUG Schema: Check of existence of DBS returned no table
07:05:18.654 pool-1-thread-1 DEBUG Schema: Creating table DBS
07:05:18.657 pool-1-thread-1 DEBUG Schema: CREATE TABLE DBS
(
    DB_ID BIGINT NOT NULL,
    "DESC" VARCHAR(4000),
    DB_LOCATION_URI VARCHAR(4000) NOT NULL,
    "NAME" VARCHAR(128),
    OWNER_NAME VARCHAR(128),
    OWNER_TYPE VARCHAR(10)
)
07:05:18.667 pool-1-thread-1 DEBUG Schema: Execution Time = 10 ms
07:05:18.667 pool-1-thread-1 DEBUG Schema: ALTER TABLE DBS ADD CONSTRAINT DBS_PK PRIMARY KEY (DB_ID)
07:05:18.689 pool-1-thread-1 DEBUG Schema: Execution Time = 22 ms
07:05:18.722 pool-1-thread-1 DEBUG Schema: Check of existence of DATABASE_PARAMS returned no table
07:05:18.722 pool-1-thread-1 DEBUG Schema: Creating table DATABASE_PARAMS
07:05:18.722 pool-1-thread-1 DEBUG Schema: CREATE TABLE DATABASE_PARAMS
(
    DB_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(180) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:18.724 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:18.724 pool-1-thread-1 DEBUG Schema: ALTER TABLE DATABASE_PARAMS ADD CONSTRAINT DATABASE_PARAMS_PK PRIMARY KEY (DB_ID,PARAM_KEY)
07:05:18.727 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:18.742 pool-1-thread-1 DEBUG Schema: Creating index "UNIQUE_DATABASE" in catalog "" schema ""
07:05:18.742 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX UNIQUE_DATABASE ON DBS ("NAME")
07:05:18.746 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:18.748 pool-1-thread-1 DEBUG Schema: Creating index "DATABASE_PARAMS_N49" in catalog "" schema ""
07:05:18.748 pool-1-thread-1 DEBUG Schema: CREATE INDEX DATABASE_PARAMS_N49 ON DATABASE_PARAMS (DB_ID)
07:05:18.750 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:18.751 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "DATABASE_PARAMS_FK1" in catalog "" schema ""
07:05:18.751 pool-1-thread-1 DEBUG Schema: ALTER TABLE DATABASE_PARAMS ADD CONSTRAINT DATABASE_PARAMS_FK1 FOREIGN KEY (DB_ID) REFERENCES DBS (DB_ID)
07:05:18.758 pool-1-thread-1 DEBUG Schema: Execution Time = 7 ms
07:05:18.758 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@34a6cf62"
07:05:18.758 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@34a6cf62"
07:05:18.759 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@34a6cf62" non enlisted to a transaction is being committed.
07:05:18.759 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@34a6cf62" closed
07:05:18.785 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 226 ms
07:05:18.785 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == '' Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MDatabase' AS NUCLEUS_TYPE,A0."DESC",A0.DB_LOCATION_URI,A0."NAME",A0.OWNER_NAME,A0.OWNER_TYPE,A0.DB_ID FROM DBS A0 WHERE A0."NAME" = ''"
07:05:18.786 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@79fd3403 [conn=com.jolbox.bonecp.ConnectionHandle@246af10, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:18.786 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == ''" ...
07:05:18.790 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5490422c"
07:05:18.793 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MDatabase' AS NUCLEUS_TYPE,A0."DESC",A0.DB_LOCATION_URI,A0."NAME",A0.OWNER_NAME,A0.OWNER_TYPE,A0.DB_ID FROM DBS A0 WHERE A0."NAME" = ''
07:05:18.793 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:18.796 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 10 ms
07:05:18.796 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MTableColumnStatistics WHERE dbName == ''"
07:05:18.796 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:18.796 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{dbName}  =  Literal{}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MTableColumnStatistics]
07:05:18.796 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MTableColumnStatistics WHERE dbName == ''" for datastore
07:05:18.797 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
07:05:18.797 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MColumnDescriptor [Table : CDS, InheritanceStrategy : new-table]
07:05:18.797 pool-1-thread-1 DEBUG Schema: Column "CDS.CD_ID" added to internal representation of table.
07:05:18.797 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MSerDeInfo [Table : SERDES, InheritanceStrategy : new-table]
07:05:18.797 pool-1-thread-1 DEBUG Schema: Column "SERDES.SERDE_ID" added to internal representation of table.
07:05:18.797 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MStringList [Table : SKEWED_STRING_LIST, InheritanceStrategy : new-table]
07:05:18.797 pool-1-thread-1 DEBUG Schema: Column "SKEWED_STRING_LIST.STRING_LIST_ID" added to internal representation of table.
07:05:18.797 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
07:05:18.797 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MStorageDescriptor [Table : SDS, InheritanceStrategy : new-table]
07:05:18.797 pool-1-thread-1 DEBUG Schema: Column "SDS.SD_ID" added to internal representation of table.
07:05:18.797 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MTable [Table : TBLS, InheritanceStrategy : new-table]
07:05:18.797 pool-1-thread-1 DEBUG Schema: Column "TBLS.TBL_ID" added to internal representation of table.
07:05:18.797 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics [Table : TAB_COL_STATS, InheritanceStrategy : new-table]
07:05:18.797 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.CS_ID" added to internal representation of table.
07:05:18.797 pool-1-thread-1 DEBUG Schema: Table CDS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MColumnDescriptor (inheritance strategy="new-table")
07:05:18.799 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols [Table : COLUMNS_V2]
07:05:18.799 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:18.799 pool-1-thread-1 DEBUG Schema: Table/View CDS has been initialised
07:05:18.799 pool-1-thread-1 DEBUG Schema: Table TBLS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MTable (inheritance strategy="new-table")
07:05:18.800 pool-1-thread-1 DEBUG Schema: Column "TBLS.CREATE_TIME" added to internal representation of table.
07:05:18.800 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.createTime] -> Column(s) [TBLS.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.800 pool-1-thread-1 DEBUG Schema: Column "TBLS.DB_ID" added to internal representation of table.
07:05:18.800 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.database] -> Column(s) [TBLS.DB_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.800 pool-1-thread-1 DEBUG Schema: Column "TBLS.LAST_ACCESS_TIME" added to internal representation of table.
07:05:18.800 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.lastAccessTime] -> Column(s) [TBLS.LAST_ACCESS_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.800 pool-1-thread-1 DEBUG Schema: Column "TBLS.OWNER" added to internal representation of table.
07:05:18.800 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.owner] -> Column(s) [TBLS.OWNER] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.800 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.parameters [Table : TABLE_PARAMS]
07:05:18.800 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:18.800 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.partitionKeys [Table : PARTITION_KEYS]
07:05:18.801 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.partitionKeys] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:18.801 pool-1-thread-1 DEBUG Schema: Column "TBLS.RETENTION" added to internal representation of table.
07:05:18.801 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.retention] -> Column(s) [TBLS.RETENTION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.801 pool-1-thread-1 DEBUG Schema: Column "TBLS.SD_ID" added to internal representation of table.
07:05:18.801 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.sd] -> Column(s) [TBLS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.801 pool-1-thread-1 DEBUG Schema: Column "TBLS.TBL_NAME" added to internal representation of table.
07:05:18.801 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.tableName] -> Column(s) [TBLS.TBL_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.801 pool-1-thread-1 DEBUG Schema: Column "TBLS.TBL_TYPE" added to internal representation of table.
07:05:18.801 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.tableType] -> Column(s) [TBLS.TBL_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.801 pool-1-thread-1 DEBUG Schema: Column "TBLS.VIEW_EXPANDED_TEXT" added to internal representation of table.
07:05:18.801 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.viewExpandedText] -> Column(s) [TBLS.VIEW_EXPANDED_TEXT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.LongVarcharRDBMSMapping)
07:05:18.801 pool-1-thread-1 DEBUG Schema: Column "TBLS.VIEW_ORIGINAL_TEXT" added to internal representation of table.
07:05:18.801 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.viewOriginalText] -> Column(s) [TBLS.VIEW_ORIGINAL_TEXT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.LongVarcharRDBMSMapping)
07:05:18.801 pool-1-thread-1 DEBUG Schema: Table/View TBLS has been initialised
07:05:18.801 pool-1-thread-1 DEBUG Schema: Table SERDES will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MSerDeInfo (inheritance strategy="new-table")
07:05:18.801 pool-1-thread-1 DEBUG Schema: Column "SERDES."NAME"" added to internal representation of table.
07:05:18.801 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.name] -> Column(s) [SERDES."NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.802 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters [Table : SERDE_PARAMS]
07:05:18.802 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:18.802 pool-1-thread-1 DEBUG Schema: Column "SERDES.SLIB" added to internal representation of table.
07:05:18.802 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.serializationLib] -> Column(s) [SERDES.SLIB] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.802 pool-1-thread-1 DEBUG Schema: Table/View SERDES has been initialised
07:05:18.802 pool-1-thread-1 DEBUG Schema: Table SKEWED_STRING_LIST will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MStringList (inheritance strategy="new-table")
07:05:18.802 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStringList.internalList [Table : SKEWED_STRING_LIST_VALUES]
07:05:18.802 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStringList.internalList] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:18.802 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_STRING_LIST has been initialised
07:05:18.802 pool-1-thread-1 DEBUG Schema: Table SDS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MStorageDescriptor (inheritance strategy="new-table")
07:05:18.802 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols [Table : BUCKETING_COLS]
07:05:18.802 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:18.802 pool-1-thread-1 DEBUG Schema: Column "SDS.CD_ID" added to internal representation of table.
07:05:18.802 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd] -> Column(s) [SDS.CD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.802 pool-1-thread-1 DEBUG Schema: Column "SDS.INPUT_FORMAT" added to internal representation of table.
07:05:18.802 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.inputFormat] -> Column(s) [SDS.INPUT_FORMAT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.802 pool-1-thread-1 DEBUG Schema: Column "SDS.IS_COMPRESSED" added to internal representation of table.
07:05:18.802 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.isCompressed] -> Column(s) [SDS.IS_COMPRESSED] using mapping of type "org.datanucleus.store.rdbms.mapping.java.BooleanMapping" (org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping)
07:05:18.802 pool-1-thread-1 DEBUG Schema: Column "SDS.IS_STOREDASSUBDIRECTORIES" added to internal representation of table.
07:05:18.802 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.isStoredAsSubDirectories] -> Column(s) [SDS.IS_STOREDASSUBDIRECTORIES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.BooleanMapping" (org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping)
07:05:18.803 pool-1-thread-1 DEBUG Schema: Column "SDS.LOCATION" added to internal representation of table.
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.location] -> Column(s) [SDS.LOCATION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.803 pool-1-thread-1 DEBUG Schema: Column "SDS.NUM_BUCKETS" added to internal representation of table.
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.numBuckets] -> Column(s) [SDS.NUM_BUCKETS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.803 pool-1-thread-1 DEBUG Schema: Column "SDS.OUTPUT_FORMAT" added to internal representation of table.
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.outputFormat] -> Column(s) [SDS.OUTPUT_FORMAT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.803 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters [Table : SD_PARAMS]
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:18.803 pool-1-thread-1 DEBUG Schema: Column "SDS.SERDE_ID" added to internal representation of table.
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo] -> Column(s) [SDS.SERDE_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.803 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames [Table : SKEWED_COL_NAMES]
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:18.803 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps [Table : SKEWED_COL_VALUE_LOC_MAP]
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:18.803 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues [Table : SKEWED_VALUES]
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:18.803 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols [Table : SORT_COLS]
07:05:18.803 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:18.803 pool-1-thread-1 DEBUG Schema: Table/View SDS has been initialised
07:05:18.803 pool-1-thread-1 DEBUG Schema: Table TAB_COL_STATS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MTableColumnStatistics (inheritance strategy="new-table")
07:05:18.803 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.AVG_COL_LEN" added to internal representation of table.
07:05:18.804 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.avgColLen] -> Column(s) [TAB_COL_STATS.AVG_COL_LEN] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:18.804 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS."COLUMN_NAME"" added to internal representation of table.
07:05:18.804 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.colName] -> Column(s) [TAB_COL_STATS."COLUMN_NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.804 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.COLUMN_TYPE" added to internal representation of table.
07:05:18.804 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.colType] -> Column(s) [TAB_COL_STATS.COLUMN_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.804 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.DB_NAME" added to internal representation of table.
07:05:18.804 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.dbName] -> Column(s) [TAB_COL_STATS.DB_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.804 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.BIG_DECIMAL_HIGH_VALUE" added to internal representation of table.
07:05:18.804 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.decimalHighValue] -> Column(s) [TAB_COL_STATS.BIG_DECIMAL_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.804 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.BIG_DECIMAL_LOW_VALUE" added to internal representation of table.
07:05:18.804 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.decimalLowValue] -> Column(s) [TAB_COL_STATS.BIG_DECIMAL_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.804 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.DOUBLE_HIGH_VALUE" added to internal representation of table.
07:05:18.804 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.doubleHighValue] -> Column(s) [TAB_COL_STATS.DOUBLE_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:18.804 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.DOUBLE_LOW_VALUE" added to internal representation of table.
07:05:18.804 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.doubleLowValue] -> Column(s) [TAB_COL_STATS.DOUBLE_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:18.804 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.LAST_ANALYZED" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.lastAnalyzed] -> Column(s) [TAB_COL_STATS.LAST_ANALYZED] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.LONG_HIGH_VALUE" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.longHighValue] -> Column(s) [TAB_COL_STATS.LONG_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.LONG_LOW_VALUE" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.longLowValue] -> Column(s) [TAB_COL_STATS.LONG_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.MAX_COL_LEN" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.maxColLen] -> Column(s) [TAB_COL_STATS.MAX_COL_LEN] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.NUM_DISTINCTS" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.numDVs] -> Column(s) [TAB_COL_STATS.NUM_DISTINCTS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.NUM_FALSES" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.numFalses] -> Column(s) [TAB_COL_STATS.NUM_FALSES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.NUM_NULLS" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.numNulls] -> Column(s) [TAB_COL_STATS.NUM_NULLS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.NUM_TRUES" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.numTrues] -> Column(s) [TAB_COL_STATS.NUM_TRUES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.TBL_ID" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.table] -> Column(s) [TAB_COL_STATS.TBL_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS."TABLE_NAME"" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.tableName] -> Column(s) [TAB_COL_STATS."TABLE_NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.805 pool-1-thread-1 DEBUG Schema: Table/View TAB_COL_STATS has been initialised
07:05:18.805 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_VALUE_LOC_MAP.SD_ID" added to internal representation of table.
07:05:18.805 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps] -> Column(s) [SKEWED_COL_VALUE_LOC_MAP.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.806 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_VALUE_LOC_MAP.STRING_LIST_ID_KID" added to internal representation of table.
07:05:18.806 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps] -> Column(s) [SKEWED_COL_VALUE_LOC_MAP.STRING_LIST_ID_KID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.806 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_VALUE_LOC_MAP.LOCATION" added to internal representation of table.
07:05:18.806 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps] -> Column(s) [SKEWED_COL_VALUE_LOC_MAP.LOCATION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.806 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_COL_VALUE_LOC_MAP has been initialised
07:05:18.806 pool-1-thread-1 DEBUG Schema: Column "SKEWED_VALUES.SD_ID_OID" added to internal representation of table.
07:05:18.806 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues] -> Column(s) [SKEWED_VALUES.SD_ID_OID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.806 pool-1-thread-1 DEBUG Schema: Column "SKEWED_VALUES.STRING_LIST_ID_EID" added to internal representation of table.
07:05:18.806 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues] -> Column(s) [SKEWED_VALUES.STRING_LIST_ID_EID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.806 pool-1-thread-1 DEBUG Schema: Column "SKEWED_VALUES.INTEGER_IDX" added to internal representation of table.
07:05:18.806 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues] -> Column(s) [SKEWED_VALUES.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.806 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_VALUES has been initialised
07:05:18.806 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2.CD_ID" added to internal representation of table.
07:05:18.806 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols] -> Column(s) [COLUMNS_V2.CD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.807 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2.COMMENT" added to internal representation of table.
07:05:18.807 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2."COLUMN_NAME"" added to internal representation of table.
07:05:18.807 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2.TYPE_NAME" added to internal representation of table.
07:05:18.807 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols] -> Column(s) [COLUMNS_V2.COMMENT,COLUMNS_V2."COLUMN_NAME",COLUMNS_V2.TYPE_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.807 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2.INTEGER_IDX" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols] -> Column(s) [COLUMNS_V2.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Table/View COLUMNS_V2 has been initialised
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SKEWED_STRING_LIST_VALUES.STRING_LIST_ID" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStringList.internalList] -> Column(s) [SKEWED_STRING_LIST_VALUES.STRING_LIST_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SKEWED_STRING_LIST_VALUES.STRING_LIST_VALUE" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStringList.internalList] -> Column(s) [SKEWED_STRING_LIST_VALUES.STRING_LIST_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SKEWED_STRING_LIST_VALUES.INTEGER_IDX" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStringList.internalList] -> Column(s) [SKEWED_STRING_LIST_VALUES.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_STRING_LIST_VALUES has been initialised
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SORT_COLS.SD_ID" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols] -> Column(s) [SORT_COLS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SORT_COLS."COLUMN_NAME"" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SORT_COLS."ORDER"" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols] -> Column(s) [SORT_COLS."COLUMN_NAME",SORT_COLS."ORDER"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SORT_COLS.INTEGER_IDX" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols] -> Column(s) [SORT_COLS.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Table/View SORT_COLS has been initialised
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SD_PARAMS.SD_ID" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters] -> Column(s) [SD_PARAMS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SD_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters] -> Column(s) [SD_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "SD_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters] -> Column(s) [SD_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.808 pool-1-thread-1 DEBUG Schema: Table/View SD_PARAMS has been initialised
07:05:18.808 pool-1-thread-1 DEBUG Schema: Column "BUCKETING_COLS.SD_ID" added to internal representation of table.
07:05:18.808 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols] -> Column(s) [BUCKETING_COLS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.809 pool-1-thread-1 DEBUG Schema: Column "BUCKETING_COLS.BUCKET_COL_NAME" added to internal representation of table.
07:05:18.809 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols] -> Column(s) [BUCKETING_COLS.BUCKET_COL_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.809 pool-1-thread-1 DEBUG Schema: Column "BUCKETING_COLS.INTEGER_IDX" added to internal representation of table.
07:05:18.809 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols] -> Column(s) [BUCKETING_COLS.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.809 pool-1-thread-1 DEBUG Schema: Table/View BUCKETING_COLS has been initialised
07:05:18.810 pool-1-thread-1 DEBUG Schema: Column "TABLE_PARAMS.TBL_ID" added to internal representation of table.
07:05:18.810 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.parameters] -> Column(s) [TABLE_PARAMS.TBL_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.810 pool-1-thread-1 DEBUG Schema: Column "TABLE_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:18.810 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.parameters] -> Column(s) [TABLE_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.810 pool-1-thread-1 DEBUG Schema: Column "TABLE_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:18.810 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.parameters] -> Column(s) [TABLE_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.810 pool-1-thread-1 DEBUG Schema: Table/View TABLE_PARAMS has been initialised
07:05:18.810 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_NAMES.SD_ID" added to internal representation of table.
07:05:18.810 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames] -> Column(s) [SKEWED_COL_NAMES.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_NAMES.SKEWED_COL_NAME" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames] -> Column(s) [SKEWED_COL_NAMES.SKEWED_COL_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_NAMES.INTEGER_IDX" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames] -> Column(s) [SKEWED_COL_NAMES.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_COL_NAMES has been initialised
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.TBL_ID" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.partitionKeys] -> Column(s) [PARTITION_KEYS.TBL_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.PKEY_COMMENT" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.PKEY_NAME" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.PKEY_TYPE" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.partitionKeys] -> Column(s) [PARTITION_KEYS.PKEY_COMMENT,PARTITION_KEYS.PKEY_NAME,PARTITION_KEYS.PKEY_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.INTEGER_IDX" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.partitionKeys] -> Column(s) [PARTITION_KEYS.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Table/View PARTITION_KEYS has been initialised
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "SERDE_PARAMS.SERDE_ID" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters] -> Column(s) [SERDE_PARAMS.SERDE_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "SERDE_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters] -> Column(s) [SERDE_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Column "SERDE_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:18.811 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters] -> Column(s) [SERDE_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:18.811 pool-1-thread-1 DEBUG Schema: Table/View SERDE_PARAMS has been initialised
07:05:18.811 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@681d831d" opened with isolation level "serializable" and auto-commit=false
07:05:18.811 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@681d831d" with isolation "serializable"
07:05:18.816 pool-1-thread-1 DEBUG Schema: Check of existence of CDS returned no table
07:05:18.816 pool-1-thread-1 DEBUG Schema: Creating table CDS
07:05:18.816 pool-1-thread-1 DEBUG Schema: CREATE TABLE CDS
(
    CD_ID BIGINT NOT NULL
)
07:05:18.818 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:18.818 pool-1-thread-1 DEBUG Schema: ALTER TABLE CDS ADD CONSTRAINT CDS_PK PRIMARY KEY (CD_ID)
07:05:18.823 pool-1-thread-1 DEBUG Schema: Execution Time = 5 ms
07:05:18.845 pool-1-thread-1 DEBUG Schema: Check of existence of TBLS returned no table
07:05:18.845 pool-1-thread-1 DEBUG Schema: Creating table TBLS
07:05:18.845 pool-1-thread-1 DEBUG Schema: CREATE TABLE TBLS
(
    TBL_ID BIGINT NOT NULL,
    CREATE_TIME INTEGER NOT NULL,
    DB_ID BIGINT,
    LAST_ACCESS_TIME INTEGER NOT NULL,
    OWNER VARCHAR(767),
    RETENTION INTEGER NOT NULL,
    SD_ID BIGINT,
    TBL_NAME VARCHAR(128),
    TBL_TYPE VARCHAR(128),
    VIEW_EXPANDED_TEXT LONG VARCHAR,
    VIEW_ORIGINAL_TEXT LONG VARCHAR
)
07:05:18.848 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:18.848 pool-1-thread-1 DEBUG Schema: ALTER TABLE TBLS ADD CONSTRAINT TBLS_PK PRIMARY KEY (TBL_ID)
07:05:18.852 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:18.873 pool-1-thread-1 DEBUG Schema: Check of existence of SERDES returned no table
07:05:18.873 pool-1-thread-1 DEBUG Schema: Creating table SERDES
07:05:18.873 pool-1-thread-1 DEBUG Schema: CREATE TABLE SERDES
(
    SERDE_ID BIGINT NOT NULL,
    "NAME" VARCHAR(128),
    SLIB VARCHAR(4000)
)
07:05:18.874 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:18.874 pool-1-thread-1 DEBUG Schema: ALTER TABLE SERDES ADD CONSTRAINT SERDES_PK PRIMARY KEY (SERDE_ID)
07:05:18.877 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:18.999 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_STRING_LIST returned no table
07:05:18.999 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_STRING_LIST
07:05:18.999 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_STRING_LIST
(
    STRING_LIST_ID BIGINT NOT NULL
)
07:05:19.002 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.002 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_STRING_LIST ADD CONSTRAINT SKEWED_STRING_LIST_PK PRIMARY KEY (STRING_LIST_ID)
07:05:19.005 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.027 pool-1-thread-1 DEBUG Schema: Check of existence of SDS returned no table
07:05:19.027 pool-1-thread-1 DEBUG Schema: Creating table SDS
07:05:19.027 pool-1-thread-1 DEBUG Schema: CREATE TABLE SDS
(
    SD_ID BIGINT NOT NULL,
    CD_ID BIGINT,
    INPUT_FORMAT VARCHAR(4000),
    IS_COMPRESSED CHAR(1) NOT NULL CHECK (IS_COMPRESSED IN ('Y','N')),
    IS_STOREDASSUBDIRECTORIES CHAR(1) NOT NULL CHECK (IS_STOREDASSUBDIRECTORIES IN ('Y','N')),
    LOCATION VARCHAR(4000),
    NUM_BUCKETS INTEGER NOT NULL,
    OUTPUT_FORMAT VARCHAR(4000),
    SERDE_ID BIGINT
)
07:05:19.031 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.031 pool-1-thread-1 DEBUG Schema: ALTER TABLE SDS ADD CONSTRAINT SDS_PK PRIMARY KEY (SD_ID)
07:05:19.036 pool-1-thread-1 DEBUG Schema: Execution Time = 5 ms
07:05:19.063 pool-1-thread-1 DEBUG Schema: Check of existence of TAB_COL_STATS returned no table
07:05:19.063 pool-1-thread-1 DEBUG Schema: Creating table TAB_COL_STATS
07:05:19.063 pool-1-thread-1 DEBUG Schema: CREATE TABLE TAB_COL_STATS
(
    CS_ID BIGINT NOT NULL,
    AVG_COL_LEN DOUBLE,
    "COLUMN_NAME" VARCHAR(128) NOT NULL,
    COLUMN_TYPE VARCHAR(128) NOT NULL,
    DB_NAME VARCHAR(128) NOT NULL,
    BIG_DECIMAL_HIGH_VALUE VARCHAR(255),
    BIG_DECIMAL_LOW_VALUE VARCHAR(255),
    DOUBLE_HIGH_VALUE DOUBLE,
    DOUBLE_LOW_VALUE DOUBLE,
    LAST_ANALYZED BIGINT NOT NULL,
    LONG_HIGH_VALUE BIGINT,
    LONG_LOW_VALUE BIGINT,
    MAX_COL_LEN BIGINT,
    NUM_DISTINCTS BIGINT,
    NUM_FALSES BIGINT,
    NUM_NULLS BIGINT NOT NULL,
    NUM_TRUES BIGINT,
    TBL_ID BIGINT,
    "TABLE_NAME" VARCHAR(128) NOT NULL
)
07:05:19.072 pool-1-thread-1 DEBUG Schema: Execution Time = 9 ms
07:05:19.072 pool-1-thread-1 DEBUG Schema: ALTER TABLE TAB_COL_STATS ADD CONSTRAINT TAB_COL_STATS_PK PRIMARY KEY (CS_ID)
07:05:19.077 pool-1-thread-1 DEBUG Schema: Execution Time = 5 ms
07:05:19.105 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_COL_VALUE_LOC_MAP returned no table
07:05:19.105 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_COL_VALUE_LOC_MAP
07:05:19.105 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_COL_VALUE_LOC_MAP
(
    SD_ID BIGINT NOT NULL,
    STRING_LIST_ID_KID BIGINT NOT NULL,
    LOCATION VARCHAR(4000)
)
07:05:19.108 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.108 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_VALUE_LOC_MAP ADD CONSTRAINT SKEWED_COL_VALUE_LOC_MAP_PK PRIMARY KEY (SD_ID,STRING_LIST_ID_KID)
07:05:19.112 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.141 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_VALUES returned no table
07:05:19.141 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_VALUES
07:05:19.141 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_VALUES
(
    SD_ID_OID BIGINT NOT NULL,
    STRING_LIST_ID_EID BIGINT,
    INTEGER_IDX INTEGER NOT NULL
)
07:05:19.145 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.145 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_VALUES ADD CONSTRAINT SKEWED_VALUES_PK PRIMARY KEY (SD_ID_OID,INTEGER_IDX)
07:05:19.148 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.171 pool-1-thread-1 DEBUG Schema: Check of existence of COLUMNS_V2 returned no table
07:05:19.171 pool-1-thread-1 DEBUG Schema: Creating table COLUMNS_V2
07:05:19.171 pool-1-thread-1 DEBUG Schema: CREATE TABLE COLUMNS_V2
(
    CD_ID BIGINT NOT NULL,
    COMMENT VARCHAR(256),
    "COLUMN_NAME" VARCHAR(128) NOT NULL,
    TYPE_NAME VARCHAR(4000) NOT NULL,
    INTEGER_IDX INTEGER NOT NULL
)
07:05:19.173 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.173 pool-1-thread-1 DEBUG Schema: ALTER TABLE COLUMNS_V2 ADD CONSTRAINT COLUMNS_PK PRIMARY KEY (CD_ID,"COLUMN_NAME")
07:05:19.176 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.201 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_STRING_LIST_VALUES returned no table
07:05:19.201 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_STRING_LIST_VALUES
07:05:19.201 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_STRING_LIST_VALUES
(
    STRING_LIST_ID BIGINT NOT NULL,
    STRING_LIST_VALUE VARCHAR(255),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:19.203 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.203 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_STRING_LIST_VALUES ADD CONSTRAINT SKEWED_STRING_LIST_VALUES_PK PRIMARY KEY (STRING_LIST_ID,INTEGER_IDX)
07:05:19.204 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.222 pool-1-thread-1 DEBUG Schema: Check of existence of SORT_COLS returned no table
07:05:19.222 pool-1-thread-1 DEBUG Schema: Creating table SORT_COLS
07:05:19.223 pool-1-thread-1 DEBUG Schema: CREATE TABLE SORT_COLS
(
    SD_ID BIGINT NOT NULL,
    "COLUMN_NAME" VARCHAR(128),
    "ORDER" INTEGER NOT NULL,
    INTEGER_IDX INTEGER NOT NULL
)
07:05:19.224 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.224 pool-1-thread-1 DEBUG Schema: ALTER TABLE SORT_COLS ADD CONSTRAINT SORT_COLS_PK PRIMARY KEY (SD_ID,INTEGER_IDX)
07:05:19.228 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.247 pool-1-thread-1 DEBUG Schema: Check of existence of SD_PARAMS returned no table
07:05:19.247 pool-1-thread-1 DEBUG Schema: Creating table SD_PARAMS
07:05:19.247 pool-1-thread-1 DEBUG Schema: CREATE TABLE SD_PARAMS
(
    SD_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(256) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:19.248 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.248 pool-1-thread-1 DEBUG Schema: ALTER TABLE SD_PARAMS ADD CONSTRAINT SD_PARAMS_PK PRIMARY KEY (SD_ID,PARAM_KEY)
07:05:19.252 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.273 pool-1-thread-1 DEBUG Schema: Check of existence of BUCKETING_COLS returned no table
07:05:19.273 pool-1-thread-1 DEBUG Schema: Creating table BUCKETING_COLS
07:05:19.273 pool-1-thread-1 DEBUG Schema: CREATE TABLE BUCKETING_COLS
(
    SD_ID BIGINT NOT NULL,
    BUCKET_COL_NAME VARCHAR(255),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:19.275 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.275 pool-1-thread-1 DEBUG Schema: ALTER TABLE BUCKETING_COLS ADD CONSTRAINT BUCKETING_COLS_PK PRIMARY KEY (SD_ID,INTEGER_IDX)
07:05:19.278 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.297 pool-1-thread-1 DEBUG Schema: Check of existence of TABLE_PARAMS returned no table
07:05:19.297 pool-1-thread-1 DEBUG Schema: Creating table TABLE_PARAMS
07:05:19.297 pool-1-thread-1 DEBUG Schema: CREATE TABLE TABLE_PARAMS
(
    TBL_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(256) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:19.299 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.299 pool-1-thread-1 DEBUG Schema: ALTER TABLE TABLE_PARAMS ADD CONSTRAINT TABLE_PARAMS_PK PRIMARY KEY (TBL_ID,PARAM_KEY)
07:05:19.301 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.319 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_COL_NAMES returned no table
07:05:19.319 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_COL_NAMES
07:05:19.319 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_COL_NAMES
(
    SD_ID BIGINT NOT NULL,
    SKEWED_COL_NAME VARCHAR(255),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:19.321 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.321 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_NAMES ADD CONSTRAINT SKEWED_COL_NAMES_PK PRIMARY KEY (SD_ID,INTEGER_IDX)
07:05:19.323 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.342 pool-1-thread-1 DEBUG Schema: Check of existence of PARTITION_KEYS returned no table
07:05:19.342 pool-1-thread-1 DEBUG Schema: Creating table PARTITION_KEYS
07:05:19.342 pool-1-thread-1 DEBUG Schema: CREATE TABLE PARTITION_KEYS
(
    TBL_ID BIGINT NOT NULL,
    PKEY_COMMENT VARCHAR(4000),
    PKEY_NAME VARCHAR(128) NOT NULL,
    PKEY_TYPE VARCHAR(767) NOT NULL,
    INTEGER_IDX INTEGER NOT NULL
)
07:05:19.343 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.343 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_KEYS ADD CONSTRAINT PARTITION_KEY_PK PRIMARY KEY (TBL_ID,PKEY_NAME)
07:05:19.345 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.361 pool-1-thread-1 DEBUG Schema: Check of existence of SERDE_PARAMS returned no table
07:05:19.362 pool-1-thread-1 DEBUG Schema: Creating table SERDE_PARAMS
07:05:19.362 pool-1-thread-1 DEBUG Schema: CREATE TABLE SERDE_PARAMS
(
    SERDE_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(256) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:19.363 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.363 pool-1-thread-1 DEBUG Schema: ALTER TABLE SERDE_PARAMS ADD CONSTRAINT SERDE_PARAMS_PK PRIMARY KEY (SERDE_ID,PARAM_KEY)
07:05:19.366 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.377 pool-1-thread-1 DEBUG Schema: Creating index "TBLS_N50" in catalog "" schema ""
07:05:19.377 pool-1-thread-1 DEBUG Schema: CREATE INDEX TBLS_N50 ON TBLS (SD_ID)
07:05:19.379 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.379 pool-1-thread-1 DEBUG Schema: Creating index "TBLS_N49" in catalog "" schema ""
07:05:19.379 pool-1-thread-1 DEBUG Schema: CREATE INDEX TBLS_N49 ON TBLS (DB_ID)
07:05:19.380 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.380 pool-1-thread-1 DEBUG Schema: Creating index "UniqueTable" in catalog "" schema ""
07:05:19.380 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX UNIQUETABLE ON TBLS (TBL_NAME,DB_ID)
07:05:19.382 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.382 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "TBLS_FK1" in catalog "" schema ""
07:05:19.382 pool-1-thread-1 DEBUG Schema: ALTER TABLE TBLS ADD CONSTRAINT TBLS_FK1 FOREIGN KEY (DB_ID) REFERENCES DBS (DB_ID)
07:05:19.387 pool-1-thread-1 DEBUG Schema: Execution Time = 5 ms
07:05:19.387 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "TBLS_FK2" in catalog "" schema ""
07:05:19.387 pool-1-thread-1 DEBUG Schema: ALTER TABLE TBLS ADD CONSTRAINT TBLS_FK2 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:19.391 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.391 pool-1-thread-1 DEBUG Schema: Creating index "SDS_N50" in catalog "" schema ""
07:05:19.391 pool-1-thread-1 DEBUG Schema: CREATE INDEX SDS_N50 ON SDS (CD_ID)
07:05:19.393 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.393 pool-1-thread-1 DEBUG Schema: Creating index "SDS_N49" in catalog "" schema ""
07:05:19.393 pool-1-thread-1 DEBUG Schema: CREATE INDEX SDS_N49 ON SDS (SERDE_ID)
07:05:19.395 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.395 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SDS_FK1" in catalog "" schema ""
07:05:19.395 pool-1-thread-1 DEBUG Schema: ALTER TABLE SDS ADD CONSTRAINT SDS_FK1 FOREIGN KEY (SERDE_ID) REFERENCES SERDES (SERDE_ID)
07:05:19.399 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.399 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SDS_FK2" in catalog "" schema ""
07:05:19.399 pool-1-thread-1 DEBUG Schema: ALTER TABLE SDS ADD CONSTRAINT SDS_FK2 FOREIGN KEY (CD_ID) REFERENCES CDS (CD_ID)
07:05:19.402 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.402 pool-1-thread-1 DEBUG Schema: Creating index "TAB_COL_STATS_N49" in catalog "" schema ""
07:05:19.402 pool-1-thread-1 DEBUG Schema: CREATE INDEX TAB_COL_STATS_N49 ON TAB_COL_STATS (TBL_ID)
07:05:19.404 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.404 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "TAB_COL_STATS_FK1" in catalog "" schema ""
07:05:19.404 pool-1-thread-1 DEBUG Schema: ALTER TABLE TAB_COL_STATS ADD CONSTRAINT TAB_COL_STATS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID)
07:05:19.408 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.408 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_COL_VALUE_LOC_MAP_N50" in catalog "" schema ""
07:05:19.408 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_COL_VALUE_LOC_MAP_N50 ON SKEWED_COL_VALUE_LOC_MAP (STRING_LIST_ID_KID)
07:05:19.410 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.410 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_COL_VALUE_LOC_MAP_N49" in catalog "" schema ""
07:05:19.410 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_COL_VALUE_LOC_MAP_N49 ON SKEWED_COL_VALUE_LOC_MAP (SD_ID)
07:05:19.412 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.412 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_COL_VALUE_LOC_MAP_FK1" in catalog "" schema ""
07:05:19.412 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_VALUE_LOC_MAP ADD CONSTRAINT SKEWED_COL_VALUE_LOC_MAP_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:19.415 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.415 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_COL_VALUE_LOC_MAP_FK2" in catalog "" schema ""
07:05:19.415 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_VALUE_LOC_MAP ADD CONSTRAINT SKEWED_COL_VALUE_LOC_MAP_FK2 FOREIGN KEY (STRING_LIST_ID_KID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID)
07:05:19.419 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.420 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_VALUES_N50" in catalog "" schema ""
07:05:19.420 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_VALUES_N50 ON SKEWED_VALUES (SD_ID_OID)
07:05:19.422 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.422 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_VALUES_N49" in catalog "" schema ""
07:05:19.422 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_VALUES_N49 ON SKEWED_VALUES (STRING_LIST_ID_EID)
07:05:19.423 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.424 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_VALUES_FK1" in catalog "" schema ""
07:05:19.424 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_VALUES ADD CONSTRAINT SKEWED_VALUES_FK1 FOREIGN KEY (SD_ID_OID) REFERENCES SDS (SD_ID)
07:05:19.426 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.426 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_VALUES_FK2" in catalog "" schema ""
07:05:19.426 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_VALUES ADD CONSTRAINT SKEWED_VALUES_FK2 FOREIGN KEY (STRING_LIST_ID_EID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID)
07:05:19.429 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.429 pool-1-thread-1 DEBUG Schema: Creating index "COLUMNS_V2_N49" in catalog "" schema ""
07:05:19.429 pool-1-thread-1 DEBUG Schema: CREATE INDEX COLUMNS_V2_N49 ON COLUMNS_V2 (CD_ID)
07:05:19.431 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.431 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "COLUMNS_V2_FK1" in catalog "" schema ""
07:05:19.431 pool-1-thread-1 DEBUG Schema: ALTER TABLE COLUMNS_V2 ADD CONSTRAINT COLUMNS_V2_FK1 FOREIGN KEY (CD_ID) REFERENCES CDS (CD_ID)
07:05:19.434 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.434 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_STRING_LIST_VALUES_N49" in catalog "" schema ""
07:05:19.434 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_STRING_LIST_VALUES_N49 ON SKEWED_STRING_LIST_VALUES (STRING_LIST_ID)
07:05:19.435 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.436 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_STRING_LIST_VALUES_FK1" in catalog "" schema ""
07:05:19.436 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_STRING_LIST_VALUES ADD CONSTRAINT SKEWED_STRING_LIST_VALUES_FK1 FOREIGN KEY (STRING_LIST_ID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID)
07:05:19.438 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.438 pool-1-thread-1 DEBUG Schema: Creating index "SORT_COLS_N49" in catalog "" schema ""
07:05:19.438 pool-1-thread-1 DEBUG Schema: CREATE INDEX SORT_COLS_N49 ON SORT_COLS (SD_ID)
07:05:19.440 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.440 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SORT_COLS_FK1" in catalog "" schema ""
07:05:19.440 pool-1-thread-1 DEBUG Schema: ALTER TABLE SORT_COLS ADD CONSTRAINT SORT_COLS_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:19.443 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.443 pool-1-thread-1 DEBUG Schema: Creating index "SD_PARAMS_N49" in catalog "" schema ""
07:05:19.443 pool-1-thread-1 DEBUG Schema: CREATE INDEX SD_PARAMS_N49 ON SD_PARAMS (SD_ID)
07:05:19.445 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.445 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SD_PARAMS_FK1" in catalog "" schema ""
07:05:19.445 pool-1-thread-1 DEBUG Schema: ALTER TABLE SD_PARAMS ADD CONSTRAINT SD_PARAMS_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:19.448 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.448 pool-1-thread-1 DEBUG Schema: Creating index "BUCKETING_COLS_N49" in catalog "" schema ""
07:05:19.448 pool-1-thread-1 DEBUG Schema: CREATE INDEX BUCKETING_COLS_N49 ON BUCKETING_COLS (SD_ID)
07:05:19.450 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.450 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "BUCKETING_COLS_FK1" in catalog "" schema ""
07:05:19.450 pool-1-thread-1 DEBUG Schema: ALTER TABLE BUCKETING_COLS ADD CONSTRAINT BUCKETING_COLS_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:19.453 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.454 pool-1-thread-1 DEBUG Schema: Creating index "TABLE_PARAMS_N49" in catalog "" schema ""
07:05:19.454 pool-1-thread-1 DEBUG Schema: CREATE INDEX TABLE_PARAMS_N49 ON TABLE_PARAMS (TBL_ID)
07:05:19.455 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.456 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "TABLE_PARAMS_FK1" in catalog "" schema ""
07:05:19.456 pool-1-thread-1 DEBUG Schema: ALTER TABLE TABLE_PARAMS ADD CONSTRAINT TABLE_PARAMS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID)
07:05:19.459 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.459 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_COL_NAMES_N49" in catalog "" schema ""
07:05:19.459 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_COL_NAMES_N49 ON SKEWED_COL_NAMES (SD_ID)
07:05:19.461 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.461 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_COL_NAMES_FK1" in catalog "" schema ""
07:05:19.461 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_NAMES ADD CONSTRAINT SKEWED_COL_NAMES_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:19.464 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.464 pool-1-thread-1 DEBUG Schema: Creating index "PARTITION_KEYS_N49" in catalog "" schema ""
07:05:19.464 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITION_KEYS_N49 ON PARTITION_KEYS (TBL_ID)
07:05:19.467 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.467 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITION_KEYS_FK1" in catalog "" schema ""
07:05:19.467 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_KEYS ADD CONSTRAINT PARTITION_KEYS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID)
07:05:19.471 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.472 pool-1-thread-1 DEBUG Schema: Creating index "SERDE_PARAMS_N49" in catalog "" schema ""
07:05:19.472 pool-1-thread-1 DEBUG Schema: CREATE INDEX SERDE_PARAMS_N49 ON SERDE_PARAMS (SERDE_ID)
07:05:19.474 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.474 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SERDE_PARAMS_FK1" in catalog "" schema ""
07:05:19.474 pool-1-thread-1 DEBUG Schema: ALTER TABLE SERDE_PARAMS ADD CONSTRAINT SERDE_PARAMS_FK1 FOREIGN KEY (SERDE_ID) REFERENCES SERDES (SERDE_ID)
07:05:19.476 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.476 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@681d831d"
07:05:19.477 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@681d831d"
07:05:19.477 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@681d831d" non enlisted to a transaction is being committed.
07:05:19.477 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@681d831d" closed
07:05:19.478 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 682 ms
07:05:19.478 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MTableColumnStatistics WHERE dbName == '' Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MTableColumnStatistics' AS NUCLEUS_TYPE,A0.AVG_COL_LEN,A0."COLUMN_NAME",A0.COLUMN_TYPE,A0.DB_NAME,A0.BIG_DECIMAL_HIGH_VALUE,A0.BIG_DECIMAL_LOW_VALUE,A0.DOUBLE_HIGH_VALUE,A0.DOUBLE_LOW_VALUE,A0.LAST_ANALYZED,A0.LONG_HIGH_VALUE,A0.LONG_LOW_VALUE,A0.MAX_COL_LEN,A0.NUM_DISTINCTS,A0.NUM_FALSES,A0.NUM_NULLS,A0.NUM_TRUES,A0."TABLE_NAME",A0.CS_ID FROM TAB_COL_STATS A0 WHERE A0.DB_NAME = ''"
07:05:19.478 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@79fd3403 [conn=com.jolbox.bonecp.ConnectionHandle@246af10, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.478 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MTableColumnStatistics WHERE dbName == ''" ...
07:05:19.485 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@722b91c1"
07:05:19.485 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MTableColumnStatistics' AS NUCLEUS_TYPE,A0.AVG_COL_LEN,A0."COLUMN_NAME",A0.COLUMN_TYPE,A0.DB_NAME,A0.BIG_DECIMAL_HIGH_VALUE,A0.BIG_DECIMAL_LOW_VALUE,A0.DOUBLE_HIGH_VALUE,A0.DOUBLE_LOW_VALUE,A0.LAST_ANALYZED,A0.LONG_HIGH_VALUE,A0.LONG_LOW_VALUE,A0.MAX_COL_LEN,A0.NUM_DISTINCTS,A0.NUM_FALSES,A0.NUM_NULLS,A0.NUM_TRUES,A0."TABLE_NAME",A0.CS_ID FROM TAB_COL_STATS A0 WHERE A0.DB_NAME = ''
07:05:19.485 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.485 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 7 ms
07:05:19.485 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics WHERE dbName == ''"
07:05:19.485 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:19.485 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{dbName}  =  Literal{}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics]
07:05:19.486 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics WHERE dbName == ''" for datastore
07:05:19.486 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
07:05:19.486 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
07:05:19.486 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MPartition [Table : PARTITIONS, InheritanceStrategy : new-table]
07:05:19.486 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.PART_ID" added to internal representation of table.
07:05:19.486 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics [Table : PART_COL_STATS, InheritanceStrategy : new-table]
07:05:19.486 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.CS_ID" added to internal representation of table.
07:05:19.486 pool-1-thread-1 DEBUG Schema: Table PARTITIONS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MPartition (inheritance strategy="new-table")
07:05:19.487 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.CREATE_TIME" added to internal representation of table.
07:05:19.487 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.createTime] -> Column(s) [PARTITIONS.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:19.487 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.LAST_ACCESS_TIME" added to internal representation of table.
07:05:19.487 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.lastAccessTime] -> Column(s) [PARTITIONS.LAST_ACCESS_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:19.487 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.parameters [Table : PARTITION_PARAMS]
07:05:19.487 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:19.487 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.PART_NAME" added to internal representation of table.
07:05:19.487 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.partitionName] -> Column(s) [PARTITIONS.PART_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.487 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.SD_ID" added to internal representation of table.
07:05:19.487 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.sd] -> Column(s) [PARTITIONS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.487 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.TBL_ID" added to internal representation of table.
07:05:19.487 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.table] -> Column(s) [PARTITIONS.TBL_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.487 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.values [Table : PARTITION_KEY_VALS]
07:05:19.487 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.values] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:19.487 pool-1-thread-1 DEBUG Schema: Table/View PARTITIONS has been initialised
07:05:19.487 pool-1-thread-1 DEBUG Schema: Table PART_COL_STATS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics (inheritance strategy="new-table")
07:05:19.487 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.AVG_COL_LEN" added to internal representation of table.
07:05:19.487 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.avgColLen] -> Column(s) [PART_COL_STATS.AVG_COL_LEN] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:19.487 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS."COLUMN_NAME"" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.colName] -> Column(s) [PART_COL_STATS."COLUMN_NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.COLUMN_TYPE" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.colType] -> Column(s) [PART_COL_STATS.COLUMN_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.DB_NAME" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.dbName] -> Column(s) [PART_COL_STATS.DB_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.BIG_DECIMAL_HIGH_VALUE" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.decimalHighValue] -> Column(s) [PART_COL_STATS.BIG_DECIMAL_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.BIG_DECIMAL_LOW_VALUE" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.decimalLowValue] -> Column(s) [PART_COL_STATS.BIG_DECIMAL_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.DOUBLE_HIGH_VALUE" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.doubleHighValue] -> Column(s) [PART_COL_STATS.DOUBLE_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.DOUBLE_LOW_VALUE" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.doubleLowValue] -> Column(s) [PART_COL_STATS.DOUBLE_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.LAST_ANALYZED" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.lastAnalyzed] -> Column(s) [PART_COL_STATS.LAST_ANALYZED] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.LONG_HIGH_VALUE" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.longHighValue] -> Column(s) [PART_COL_STATS.LONG_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.LONG_LOW_VALUE" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.longLowValue] -> Column(s) [PART_COL_STATS.LONG_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.MAX_COL_LEN" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.maxColLen] -> Column(s) [PART_COL_STATS.MAX_COL_LEN] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.NUM_DISTINCTS" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.numDVs] -> Column(s) [PART_COL_STATS.NUM_DISTINCTS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.NUM_FALSES" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.numFalses] -> Column(s) [PART_COL_STATS.NUM_FALSES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.488 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.NUM_NULLS" added to internal representation of table.
07:05:19.488 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.numNulls] -> Column(s) [PART_COL_STATS.NUM_NULLS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.489 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.NUM_TRUES" added to internal representation of table.
07:05:19.489 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.numTrues] -> Column(s) [PART_COL_STATS.NUM_TRUES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.489 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.PART_ID" added to internal representation of table.
07:05:19.489 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.partition] -> Column(s) [PART_COL_STATS.PART_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.489 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.PARTITION_NAME" added to internal representation of table.
07:05:19.489 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.partitionName] -> Column(s) [PART_COL_STATS.PARTITION_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.489 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS."TABLE_NAME"" added to internal representation of table.
07:05:19.489 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.tableName] -> Column(s) [PART_COL_STATS."TABLE_NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.489 pool-1-thread-1 DEBUG Schema: Table/View PART_COL_STATS has been initialised
07:05:19.489 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEY_VALS.PART_ID" added to internal representation of table.
07:05:19.489 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.values] -> Column(s) [PARTITION_KEY_VALS.PART_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.489 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEY_VALS.PART_KEY_VAL" added to internal representation of table.
07:05:19.489 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.values] -> Column(s) [PARTITION_KEY_VALS.PART_KEY_VAL] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.489 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEY_VALS.INTEGER_IDX" added to internal representation of table.
07:05:19.489 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.values] -> Column(s) [PARTITION_KEY_VALS.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:19.489 pool-1-thread-1 DEBUG Schema: Table/View PARTITION_KEY_VALS has been initialised
07:05:19.490 pool-1-thread-1 DEBUG Schema: Column "PARTITION_PARAMS.PART_ID" added to internal representation of table.
07:05:19.490 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.parameters] -> Column(s) [PARTITION_PARAMS.PART_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:19.490 pool-1-thread-1 DEBUG Schema: Column "PARTITION_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:19.490 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.parameters] -> Column(s) [PARTITION_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.490 pool-1-thread-1 DEBUG Schema: Column "PARTITION_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:19.490 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.parameters] -> Column(s) [PARTITION_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.490 pool-1-thread-1 DEBUG Schema: Table/View PARTITION_PARAMS has been initialised
07:05:19.490 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@46fb28c9" opened with isolation level "serializable" and auto-commit=false
07:05:19.490 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@46fb28c9" with isolation "serializable"
07:05:19.494 pool-1-thread-1 DEBUG Schema: Check of existence of PARTITIONS returned no table
07:05:19.494 pool-1-thread-1 DEBUG Schema: Creating table PARTITIONS
07:05:19.494 pool-1-thread-1 DEBUG Schema: CREATE TABLE PARTITIONS
(
    PART_ID BIGINT NOT NULL,
    CREATE_TIME INTEGER NOT NULL,
    LAST_ACCESS_TIME INTEGER NOT NULL,
    PART_NAME VARCHAR(767),
    SD_ID BIGINT,
    TBL_ID BIGINT
)
07:05:19.496 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.496 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITIONS ADD CONSTRAINT PARTITIONS_PK PRIMARY KEY (PART_ID)
07:05:19.497 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.518 pool-1-thread-1 DEBUG Schema: Check of existence of PART_COL_STATS returned no table
07:05:19.518 pool-1-thread-1 DEBUG Schema: Creating table PART_COL_STATS
07:05:19.518 pool-1-thread-1 DEBUG Schema: CREATE TABLE PART_COL_STATS
(
    CS_ID BIGINT NOT NULL,
    AVG_COL_LEN DOUBLE,
    "COLUMN_NAME" VARCHAR(128) NOT NULL,
    COLUMN_TYPE VARCHAR(128) NOT NULL,
    DB_NAME VARCHAR(128) NOT NULL,
    BIG_DECIMAL_HIGH_VALUE VARCHAR(255),
    BIG_DECIMAL_LOW_VALUE VARCHAR(255),
    DOUBLE_HIGH_VALUE DOUBLE,
    DOUBLE_LOW_VALUE DOUBLE,
    LAST_ANALYZED BIGINT NOT NULL,
    LONG_HIGH_VALUE BIGINT,
    LONG_LOW_VALUE BIGINT,
    MAX_COL_LEN BIGINT,
    NUM_DISTINCTS BIGINT,
    NUM_FALSES BIGINT,
    NUM_NULLS BIGINT NOT NULL,
    NUM_TRUES BIGINT,
    PART_ID BIGINT,
    PARTITION_NAME VARCHAR(767) NOT NULL,
    "TABLE_NAME" VARCHAR(128) NOT NULL
)
07:05:19.520 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.520 pool-1-thread-1 DEBUG Schema: ALTER TABLE PART_COL_STATS ADD CONSTRAINT PART_COL_STATS_PK PRIMARY KEY (CS_ID)
07:05:19.522 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.542 pool-1-thread-1 DEBUG Schema: Check of existence of PARTITION_KEY_VALS returned no table
07:05:19.542 pool-1-thread-1 DEBUG Schema: Creating table PARTITION_KEY_VALS
07:05:19.543 pool-1-thread-1 DEBUG Schema: CREATE TABLE PARTITION_KEY_VALS
(
    PART_ID BIGINT NOT NULL,
    PART_KEY_VAL VARCHAR(255),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:19.543 pool-1-thread-1 DEBUG Schema: Execution Time = 0 ms
07:05:19.543 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_KEY_VALS ADD CONSTRAINT PARTITION_KEY_VALS_PK PRIMARY KEY (PART_ID,INTEGER_IDX)
07:05:19.545 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.563 pool-1-thread-1 DEBUG Schema: Check of existence of PARTITION_PARAMS returned no table
07:05:19.563 pool-1-thread-1 DEBUG Schema: Creating table PARTITION_PARAMS
07:05:19.563 pool-1-thread-1 DEBUG Schema: CREATE TABLE PARTITION_PARAMS
(
    PART_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(256) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:19.564 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.564 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_PARAMS ADD CONSTRAINT PARTITION_PARAMS_PK PRIMARY KEY (PART_ID,PARAM_KEY)
07:05:19.566 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.575 pool-1-thread-1 DEBUG Schema: Creating index "UniquePartition" in catalog "" schema ""
07:05:19.575 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX UNIQUEPARTITION ON PARTITIONS (PART_NAME,TBL_ID)
07:05:19.577 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.577 pool-1-thread-1 DEBUG Schema: Creating index "PARTITIONS_N50" in catalog "" schema ""
07:05:19.577 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITIONS_N50 ON PARTITIONS (SD_ID)
07:05:19.578 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.578 pool-1-thread-1 DEBUG Schema: Creating index "PARTITIONS_N49" in catalog "" schema ""
07:05:19.578 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITIONS_N49 ON PARTITIONS (TBL_ID)
07:05:19.580 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.580 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITIONS_FK1" in catalog "" schema ""
07:05:19.580 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITIONS ADD CONSTRAINT PARTITIONS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID)
07:05:19.584 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.584 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITIONS_FK2" in catalog "" schema ""
07:05:19.584 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITIONS ADD CONSTRAINT PARTITIONS_FK2 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:19.588 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.588 pool-1-thread-1 DEBUG Schema: Creating index "PART_COL_STATS_N49" in catalog "" schema ""
07:05:19.588 pool-1-thread-1 DEBUG Schema: CREATE INDEX PART_COL_STATS_N49 ON PART_COL_STATS (PART_ID)
07:05:19.590 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.590 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PART_COL_STATS_FK1" in catalog "" schema ""
07:05:19.590 pool-1-thread-1 DEBUG Schema: ALTER TABLE PART_COL_STATS ADD CONSTRAINT PART_COL_STATS_FK1 FOREIGN KEY (PART_ID) REFERENCES PARTITIONS (PART_ID)
07:05:19.594 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.594 pool-1-thread-1 DEBUG Schema: Creating index "PARTITION_KEY_VALS_N49" in catalog "" schema ""
07:05:19.594 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITION_KEY_VALS_N49 ON PARTITION_KEY_VALS (PART_ID)
07:05:19.595 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.595 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITION_KEY_VALS_FK1" in catalog "" schema ""
07:05:19.595 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_KEY_VALS ADD CONSTRAINT PARTITION_KEY_VALS_FK1 FOREIGN KEY (PART_ID) REFERENCES PARTITIONS (PART_ID)
07:05:19.599 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.599 pool-1-thread-1 DEBUG Schema: Creating index "PARTITION_PARAMS_N49" in catalog "" schema ""
07:05:19.599 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITION_PARAMS_N49 ON PARTITION_PARAMS (PART_ID)
07:05:19.601 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.601 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITION_PARAMS_FK1" in catalog "" schema ""
07:05:19.601 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_PARAMS ADD CONSTRAINT PARTITION_PARAMS_FK1 FOREIGN KEY (PART_ID) REFERENCES PARTITIONS (PART_ID)
07:05:19.604 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.604 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@46fb28c9"
07:05:19.604 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@46fb28c9"
07:05:19.604 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@46fb28c9" non enlisted to a transaction is being committed.
07:05:19.604 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@46fb28c9" closed
07:05:19.605 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 119 ms
07:05:19.605 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics WHERE dbName == '' Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics' AS NUCLEUS_TYPE,A0.AVG_COL_LEN,A0."COLUMN_NAME",A0.COLUMN_TYPE,A0.DB_NAME,A0.BIG_DECIMAL_HIGH_VALUE,A0.BIG_DECIMAL_LOW_VALUE,A0.DOUBLE_HIGH_VALUE,A0.DOUBLE_LOW_VALUE,A0.LAST_ANALYZED,A0.LONG_HIGH_VALUE,A0.LONG_LOW_VALUE,A0.MAX_COL_LEN,A0.NUM_DISTINCTS,A0.NUM_FALSES,A0.NUM_NULLS,A0.NUM_TRUES,A0.PARTITION_NAME,A0."TABLE_NAME",A0.CS_ID FROM PART_COL_STATS A0 WHERE A0.DB_NAME = ''"
07:05:19.605 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@79fd3403 [conn=com.jolbox.bonecp.ConnectionHandle@246af10, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.605 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics WHERE dbName == ''" ...
07:05:19.610 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@b145e1f"
07:05:19.610 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics' AS NUCLEUS_TYPE,A0.AVG_COL_LEN,A0."COLUMN_NAME",A0.COLUMN_TYPE,A0.DB_NAME,A0.BIG_DECIMAL_HIGH_VALUE,A0.BIG_DECIMAL_LOW_VALUE,A0.DOUBLE_HIGH_VALUE,A0.DOUBLE_LOW_VALUE,A0.LAST_ANALYZED,A0.LONG_HIGH_VALUE,A0.LONG_LOW_VALUE,A0.MAX_COL_LEN,A0.NUM_DISTINCTS,A0.NUM_FALSES,A0.NUM_NULLS,A0.NUM_TRUES,A0.PARTITION_NAME,A0."TABLE_NAME",A0.CS_ID FROM PART_COL_STATS A0 WHERE A0.DB_NAME = ''
07:05:19.610 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.610 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 5 ms
07:05:19.613 pool-1-thread-1 DEBUG Query: SQL Query : "select "DB_ID" from "DBS""
07:05:19.613 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@79fd3403 [conn=com.jolbox.bonecp.ConnectionHandle@246af10, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.616 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1512ba5b"
07:05:19.616 pool-1-thread-1 DEBUG Native: select "DB_ID" from "DBS"
07:05:19.616 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.623 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.623 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.623 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.623 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.623 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.623 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@569ebced]]
07:05:19.623 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@569ebced is committing for transaction Xid=    with onePhase=true
07:05:19.623 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@569ebced committed connection for transaction Xid=    with onePhase=true
07:05:19.624 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@246af10" closed
07:05:19.624 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@79fd3403 [conn=com.jolbox.bonecp.ConnectionHandle@246af10, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.624 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:19.624 pool-1-thread-1 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
07:05:19.626 pool-1-thread-1 DEBUG ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@7a78e105, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@6f477150 created in the thread with id: 11
07:05:19.626 pool-1-thread-1 INFO ObjectStore: Initialized ObjectStore
07:05:19.680 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.681 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:19.681 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6717)
07:05:19.681 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable"
07:05:19.681 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:19.681 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MVersionTable]
07:05:19.681 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable" for datastore
07:05:19.681 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MVersionTable [Table : VERSION, InheritanceStrategy : new-table]
07:05:19.682 pool-1-thread-1 DEBUG Schema: Column "VERSION.VER_ID" added to internal representation of table.
07:05:19.682 pool-1-thread-1 DEBUG Schema: Table VERSION will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MVersionTable (inheritance strategy="new-table")
07:05:19.682 pool-1-thread-1 DEBUG Schema: Column "VERSION.SCHEMA_VERSION" added to internal representation of table.
07:05:19.682 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MVersionTable.schemaVersion] -> Column(s) [VERSION.SCHEMA_VERSION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.682 pool-1-thread-1 DEBUG Schema: Column "VERSION.VERSION_COMMENT" added to internal representation of table.
07:05:19.682 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MVersionTable.versionComment] -> Column(s) [VERSION.VERSION_COMMENT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.682 pool-1-thread-1 DEBUG Schema: Table/View VERSION has been initialised
07:05:19.682 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@73f9bdee" opened with isolation level "serializable" and auto-commit=false
07:05:19.682 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@73f9bdee" with isolation "serializable"
07:05:19.686 pool-1-thread-1 DEBUG Schema: Check of existence of VERSION returned no table
07:05:19.686 pool-1-thread-1 DEBUG Schema: Creating table VERSION
07:05:19.686 pool-1-thread-1 DEBUG Schema: CREATE TABLE VERSION
(
    VER_ID BIGINT NOT NULL,
    SCHEMA_VERSION VARCHAR(127) NOT NULL,
    VERSION_COMMENT VARCHAR(255) NOT NULL
)
07:05:19.687 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.687 pool-1-thread-1 DEBUG Schema: ALTER TABLE VERSION ADD CONSTRAINT VERSION_PK PRIMARY KEY (VER_ID)
07:05:19.689 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.696 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@73f9bdee"
07:05:19.697 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@73f9bdee"
07:05:19.697 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@73f9bdee" non enlisted to a transaction is being committed.
07:05:19.697 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@73f9bdee" closed
07:05:19.697 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 16 ms
07:05:19.697 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MVersionTable' AS NUCLEUS_TYPE,A0.SCHEMA_VERSION,A0.VERSION_COMMENT,A0.VER_ID FROM VERSION A0"
07:05:19.697 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1b104b84" opened with isolation level "read-committed" and auto-commit=false
07:05:19.697 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@37a0b739, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.697 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@37a0b739 is starting for transaction Xid=    with flags 0
07:05:19.697 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@189aee65 [conn=com.jolbox.bonecp.ConnectionHandle@1b104b84, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.697 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable" ...
07:05:19.699 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@89dd0c3"
07:05:19.699 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MVersionTable' AS NUCLEUS_TYPE,A0.SCHEMA_VERSION,A0.VERSION_COMMENT,A0.VER_ID FROM VERSION A0
07:05:19.699 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.699 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 2 ms
07:05:19.701 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6731)
07:05:19.701 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.701 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.701 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.701 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.701 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.701 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@37a0b739]]
07:05:19.701 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@37a0b739 is committing for transaction Xid=    with onePhase=true
07:05:19.701 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@37a0b739 committed connection for transaction Xid=    with onePhase=true
07:05:19.701 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1b104b84" closed
07:05:19.701 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@189aee65 [conn=com.jolbox.bonecp.ConnectionHandle@1b104b84, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.701 pool-1-thread-1 DEBUG Transaction: Transaction committed in 0 ms
07:05:19.705 pool-1-thread-1 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
07:05:19.705 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.705 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:19.705 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6717)
07:05:19.705 pool-1-thread-1 DEBUG Query: Query "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTableFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:19.706 pool-1-thread-1 DEBUG Query: Query "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTableFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:19.706 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@561c85d9" opened with isolation level "read-committed" and auto-commit=false
07:05:19.706 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1979696a, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.706 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1979696a is starting for transaction Xid=    with flags 0
07:05:19.706 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@46e9d11 [conn=com.jolbox.bonecp.ConnectionHandle@561c85d9, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.706 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable" ...
07:05:19.706 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@366ef208"
07:05:19.706 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MVersionTable' AS NUCLEUS_TYPE,A0.SCHEMA_VERSION,A0.VERSION_COMMENT,A0.VER_ID FROM VERSION A0
07:05:19.706 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.706 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:19.706 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6731)
07:05:19.706 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.706 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.706 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.706 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.706 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.706 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1979696a]]
07:05:19.706 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1979696a is committing for transaction Xid=    with onePhase=true
07:05:19.706 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1979696a committed connection for transaction Xid=    with onePhase=true
07:05:19.706 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@561c85d9" closed
07:05:19.706 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@46e9d11 [conn=com.jolbox.bonecp.ConnectionHandle@561c85d9, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.706 pool-1-thread-1 DEBUG Transaction: Transaction committed in 0 ms
07:05:19.707 pool-1-thread-1 DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MVersionTable
07:05:19.708 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.708 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:19.708 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6772)
07:05:19.708 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903"
07:05:19.721 pool-1-thread-1 DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MVersionTable"
07:05:19.723 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:19.723 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:19.723 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4bd17f53" opened with isolation level "read-committed" and auto-commit=false
07:05:19.726 pool-1-thread-1 DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned no table
07:05:19.726 pool-1-thread-1 DEBUG Schema: Creating table APP.SEQUENCE_TABLE
07:05:19.727 pool-1-thread-1 DEBUG Schema: CREATE TABLE APP.SEQUENCE_TABLE
(
    SEQUENCE_NAME VARCHAR(255) NOT NULL,
    NEXT_VAL BIGINT NOT NULL
)
07:05:19.728 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.728 pool-1-thread-1 DEBUG Schema: ALTER TABLE APP.SEQUENCE_TABLE ADD CONSTRAINT SEQUENCE_TABLE_PK PRIMARY KEY (SEQUENCE_NAME)
07:05:19.729 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.742 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@242d526e"
07:05:19.742 pool-1-thread-1 DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MVersionTable'> FOR UPDATE
07:05:19.742 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.751 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5403a1ee"
07:05:19.751 pool-1-thread-1 DEBUG Native: SELECT MAX(VER_ID) FROM VERSION
07:05:19.762 pool-1-thread-1 DEBUG Retrieve: Execution Time = 11 ms
07:05:19.773 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@27fbe16a" for connection "com.jolbox.bonecp.ConnectionHandle@4bd17f53"
07:05:19.774 pool-1-thread-1 DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MVersionTable'>,<6>)
07:05:19.778 pool-1-thread-1 DEBUG Persist: Execution Time = 4 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@27fbe16a"
07:05:19.778 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@27fbe16a"
07:05:19.779 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@61ec0eb"
07:05:19.779 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@488016eb"
07:05:19.779 pool-1-thread-1 DEBUG ValueGeneration: Reserved a block of 5 values
07:05:19.779 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4bd17f53" non enlisted to a transaction is being committed.
07:05:19.780 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4bd17f53" closed
07:05:19.780 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MVersionTable (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:19.782 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") added to Level 1 cache (loadedFlags="[YY]")
07:05:19.782 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") enlisted in transactional cache
07:05:19.791 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903" being inserted into table "VERSION"
07:05:19.792 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@18ec93b7" opened with isolation level "read-committed" and auto-commit=false
07:05:19.792 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a6acb48, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.792 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a6acb48 is starting for transaction Xid=    with flags 0
07:05:19.792 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@45efb919 [conn=com.jolbox.bonecp.ConnectionHandle@18ec93b7, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.794 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@edd4208" for connection "com.jolbox.bonecp.ConnectionHandle@18ec93b7"
07:05:19.794 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO VERSION (VER_ID,VERSION_COMMENT,SCHEMA_VERSION) VALUES (?,?,?)" has been made batchable
07:05:19.796 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO VERSION (VER_ID,VERSION_COMMENT,SCHEMA_VERSION) VALUES (?,?,?)" for processing (batch size = 1)
07:05:19.796 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6774)
07:05:19.797 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO VERSION (VER_ID,VERSION_COMMENT,SCHEMA_VERSION) VALUES (<1>,<'Set by MetaStore lian@192.168.1.102'>,<'1.2.0'>)]
07:05:19.797 pool-1-thread-1 DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@edd4208"
07:05:19.797 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.797 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:19.797 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.797 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.797 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable"
07:05:19.797 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") taken from Level 1 cache (loadedFlags="[YY]") [cache size = 1]
07:05:19.798 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:19.798 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.798 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.798 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.799 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a6acb48]]
07:05:19.800 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a6acb48 is committing for transaction Xid=    with onePhase=true
07:05:19.800 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a6acb48 committed connection for transaction Xid=    with onePhase=true
07:05:19.800 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@18ec93b7" closed
07:05:19.800 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@45efb919 [conn=com.jolbox.bonecp.ConnectionHandle@18ec93b7, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.800 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903" (depth=0)
07:05:19.801 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:19.801 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") being evicted from transactional cache
07:05:19.801 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MVersionTable@62d0c903, lifecycle=DETACHED_CLEAN]
07:05:19.801 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable" being removed from Level 1 cache [current cache size = 1]
07:05:19.801 pool-1-thread-1 DEBUG Transaction: Transaction committed in 4 ms
07:05:19.804 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.804 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:19.805 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
07:05:19.805 pool-1-thread-1 DEBUG Query: SQL Query : "select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = ?"
07:05:19.805 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5ac53c2b" opened with isolation level "read-committed" and auto-commit=false
07:05:19.805 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6ed63cbe, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.805 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6ed63cbe is starting for transaction Xid=    with flags 0
07:05:19.805 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5cc3d10 [conn=com.jolbox.bonecp.ConnectionHandle@5ac53c2b, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.809 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@8dacc55"
07:05:19.809 pool-1-thread-1 DEBUG Native: select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = <'default'>
07:05:19.809 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.810 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
07:05:19.810 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.810 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.810 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.810 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.810 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.810 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6ed63cbe]]
07:05:19.810 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6ed63cbe is committing for transaction Xid=    with onePhase=true
07:05:19.811 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6ed63cbe committed connection for transaction Xid=    with onePhase=true
07:05:19.811 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5ac53c2b" closed
07:05:19.811 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5cc3d10 [conn=com.jolbox.bonecp.ConnectionHandle@5ac53c2b, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.811 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:19.811 pool-1-thread-1 DEBUG ObjectStore: db details for db default retrieved using SQL in 6.40588ms
07:05:19.811 pool-1-thread-1 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
07:05:19.855 pool-1-thread-1 DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MDatabase
07:05:19.855 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   	, enlisted resources=[]]
07:05:19.855 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:19.856 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:520)
07:05:19.856 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1"
07:05:19.856 pool-1-thread-1 DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:19.856 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:19.856 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:19.856 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1303d64a" opened with isolation level "read-committed" and auto-commit=false
07:05:19.861 pool-1-thread-1 DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:19.864 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6705c2"
07:05:19.864 pool-1-thread-1 DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MDatabase'> FOR UPDATE
07:05:19.864 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.866 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1d259228"
07:05:19.866 pool-1-thread-1 DEBUG Native: SELECT MAX(DB_ID) FROM DBS
07:05:19.867 pool-1-thread-1 DEBUG Retrieve: Execution Time = 1 ms
07:05:19.870 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@54613d28" for connection "com.jolbox.bonecp.ConnectionHandle@1303d64a"
07:05:19.870 pool-1-thread-1 DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MDatabase'>,<6>)
07:05:19.871 pool-1-thread-1 DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@54613d28"
07:05:19.871 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@54613d28"
07:05:19.871 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7df26faf"
07:05:19.871 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4fb49726"
07:05:19.871 pool-1-thread-1 DEBUG ValueGeneration: Reserved a block of 5 values
07:05:19.872 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1303d64a" non enlisted to a transaction is being committed.
07:05:19.872 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1303d64a" closed
07:05:19.872 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MDatabase (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:19.872 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") added to Level 1 cache (loadedFlags="[YYYYYY]")
07:05:19.872 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") enlisted in transactional cache
07:05:19.872 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" being inserted into table "DBS"
07:05:19.873 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@133042ea" opened with isolation level "read-committed" and auto-commit=false
07:05:19.873 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@78d6ea0d, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   	, enlisted resources=[]]
07:05:19.873 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@78d6ea0d is starting for transaction Xid=   	 with flags 0
07:05:19.873 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@1af4d2e2 [conn=com.jolbox.bonecp.ConnectionHandle@133042ea, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.875 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@33835a3f" for connection "com.jolbox.bonecp.ConnectionHandle@133042ea"
07:05:19.875 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO DBS (DB_ID,DB_LOCATION_URI,"NAME","DESC",OWNER_NAME,OWNER_TYPE) VALUES (?,?,?,?,?,?)" has been made batchable
07:05:19.875 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO DBS (DB_ID,DB_LOCATION_URI,"NAME","DESC",OWNER_NAME,OWNER_TYPE) VALUES (?,?,?,?,?,?)" for processing (batch size = 1)
07:05:19.875 pool-1-thread-1 DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MDatabase.parameters"
07:05:19.876 pool-1-thread-1 DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MDatabase.parameters"
07:05:19.885 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:19.885 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:522)
07:05:19.885 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO DBS (DB_ID,DB_LOCATION_URI,"NAME","DESC",OWNER_NAME,OWNER_TYPE) VALUES (<1>,<'file:/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore'>,<'default'>,<'Default Hive database'>,<'public'>,<'ROLE'>)]
07:05:19.886 pool-1-thread-1 DEBUG Datastore: Execution Time = 1 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@33835a3f"
07:05:19.886 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.886 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:19.886 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.886 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.886 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:19.886 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") taken from Level 1 cache (loadedFlags="[YYYYYY]") [cache size = 1]
07:05:19.886 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:19.886 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.886 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.886 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.886 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   	, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@78d6ea0d]]
07:05:19.886 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@78d6ea0d is committing for transaction Xid=   	 with onePhase=true
07:05:19.886 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@78d6ea0d committed connection for transaction Xid=   	 with onePhase=true
07:05:19.886 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@133042ea" closed
07:05:19.886 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@1af4d2e2 [conn=com.jolbox.bonecp.ConnectionHandle@133042ea, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.886 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" (depth=0)
07:05:19.886 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" field "parameters" loading contents to SCO wrapper from the datastore
07:05:19.891 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5db03910 [conn=null, commitOnRelease=true, closeOnRelease=true, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:nontx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@7a240b0c]
07:05:19.891 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@279362fc" opened with isolation level "read-committed" and auto-commit=false
07:05:19.896 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@65c2e56a"
07:05:19.896 pool-1-thread-1 DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM DATABASE_PARAMS A0 WHERE A0.DB_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:19.896 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.897 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1556713b"
07:05:19.897 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@279362fc" non enlisted to a transaction is being committed.
07:05:19.897 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@279362fc" closed
07:05:19.897 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5db03910 [conn=com.jolbox.bonecp.ConnectionHandle@279362fc, commitOnRelease=true, closeOnRelease=true, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:nontx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@7a240b0c]
07:05:19.897 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:19.898 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:19.898 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") being evicted from transactional cache
07:05:19.898 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MDatabase@136f15d1, lifecycle=DETACHED_CLEAN]
07:05:19.898 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" being removed from Level 1 cache [current cache size = 1]
07:05:19.898 pool-1-thread-1 DEBUG Transaction: Transaction committed in 13 ms
07:05:19.898 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   
, enlisted resources=[]]
07:05:19.898 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:19.898 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3220)
07:05:19.898 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
07:05:19.899 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1"
07:05:19.900 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 1 ms
07:05:19.900 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{roleName}  =  ParameterExpression{t1}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MRole, t1 type=java.lang.String]
07:05:19.900 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1" for datastore
07:05:19.900 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MRole [Table : ROLES, InheritanceStrategy : new-table]
07:05:19.900 pool-1-thread-1 DEBUG Schema: Column "ROLES.ROLE_ID" added to internal representation of table.
07:05:19.901 pool-1-thread-1 DEBUG Schema: Table ROLES will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MRole (inheritance strategy="new-table")
07:05:19.901 pool-1-thread-1 DEBUG Schema: Column "ROLES.CREATE_TIME" added to internal representation of table.
07:05:19.901 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MRole.createTime] -> Column(s) [ROLES.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:19.901 pool-1-thread-1 DEBUG Schema: Column "ROLES.OWNER_NAME" added to internal representation of table.
07:05:19.901 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MRole.ownerName] -> Column(s) [ROLES.OWNER_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.901 pool-1-thread-1 DEBUG Schema: Column "ROLES.ROLE_NAME" added to internal representation of table.
07:05:19.901 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MRole.roleName] -> Column(s) [ROLES.ROLE_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.901 pool-1-thread-1 DEBUG Schema: Table/View ROLES has been initialised
07:05:19.901 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4c9fc0d3" opened with isolation level "serializable" and auto-commit=false
07:05:19.901 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@4c9fc0d3" with isolation "serializable"
07:05:19.902 pool-1-thread-1 DEBUG Schema: Check of existence of ROLES returned no table
07:05:19.902 pool-1-thread-1 DEBUG Schema: Creating table ROLES
07:05:19.902 pool-1-thread-1 DEBUG Schema: CREATE TABLE ROLES
(
    ROLE_ID BIGINT NOT NULL,
    CREATE_TIME INTEGER NOT NULL,
    OWNER_NAME VARCHAR(128),
    ROLE_NAME VARCHAR(128)
)
07:05:19.904 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.904 pool-1-thread-1 DEBUG Schema: ALTER TABLE ROLES ADD CONSTRAINT ROLES_PK PRIMARY KEY (ROLE_ID)
07:05:19.906 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.916 pool-1-thread-1 DEBUG Schema: Creating index "RoleEntityINDEX" in catalog "" schema ""
07:05:19.916 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX ROLEENTITYINDEX ON ROLES (ROLE_NAME)
07:05:19.918 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:19.918 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@4c9fc0d3"
07:05:19.918 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@4c9fc0d3"
07:05:19.918 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4c9fc0d3" non enlisted to a transaction is being committed.
07:05:19.918 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4c9fc0d3" closed
07:05:19.919 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 19 ms
07:05:19.919 pool-1-thread-1 DEBUG Query: SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1 Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MRole' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.OWNER_NAME,A0.ROLE_NAME,A0.ROLE_ID FROM ROLES A0 WHERE A0.ROLE_NAME = ?"
07:05:19.919 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4e0403e1" opened with isolation level "read-committed" and auto-commit=false
07:05:19.919 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@387c47e2, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   
, enlisted resources=[]]
07:05:19.919 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@387c47e2 is starting for transaction Xid=   
 with flags 0
07:05:19.919 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@17c33db7 [conn=com.jolbox.bonecp.ConnectionHandle@4e0403e1, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.919 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1" ...
07:05:19.922 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@7ab0a26f"
07:05:19.923 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MRole' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.OWNER_NAME,A0.ROLE_NAME,A0.ROLE_ID FROM ROLES A0 WHERE A0.ROLE_NAME = <'admin'>
07:05:19.923 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.923 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 4 ms
07:05:19.923 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
07:05:19.923 pool-1-thread-1 DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MRole
07:05:19.923 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MRole@45743c7c"
07:05:19.923 pool-1-thread-1 DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MRole"
07:05:19.924 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:19.924 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:19.924 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@3a9e60e2" opened with isolation level "read-committed" and auto-commit=false
07:05:19.928 pool-1-thread-1 DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:19.928 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@57f5d0ad"
07:05:19.928 pool-1-thread-1 DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MRole'> FOR UPDATE
07:05:19.928 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.930 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@33516cf9"
07:05:19.930 pool-1-thread-1 DEBUG Native: SELECT MAX(ROLE_ID) FROM ROLES
07:05:19.930 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.930 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4854c51" for connection "com.jolbox.bonecp.ConnectionHandle@3a9e60e2"
07:05:19.930 pool-1-thread-1 DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MRole'>,<6>)
07:05:19.931 pool-1-thread-1 DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4854c51"
07:05:19.931 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4854c51"
07:05:19.931 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@76accd5f"
07:05:19.931 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@72717c56"
07:05:19.931 pool-1-thread-1 DEBUG ValueGeneration: Reserved a block of 5 values
07:05:19.931 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@3a9e60e2" non enlisted to a transaction is being committed.
07:05:19.931 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@3a9e60e2" closed
07:05:19.931 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MRole (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:19.931 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@45743c7c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") added to Level 1 cache (loadedFlags="[YYY]")
07:05:19.931 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@45743c7c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") enlisted in transactional cache
07:05:19.931 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@45743c7c" being inserted into table "ROLES"
07:05:19.931 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@17c33db7 [conn=com.jolbox.bonecp.ConnectionHandle@4e0403e1, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.933 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@598154ff" for connection "com.jolbox.bonecp.ConnectionHandle@4e0403e1"
07:05:19.933 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO ROLES (ROLE_ID,ROLE_NAME,CREATE_TIME,OWNER_NAME) VALUES (?,?,?,?)" has been made batchable
07:05:19.933 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO ROLES (ROLE_ID,ROLE_NAME,CREATE_TIME,OWNER_NAME) VALUES (?,?,?,?)" for processing (batch size = 1)
07:05:19.933 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3229)
07:05:19.933 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO ROLES (ROLE_ID,ROLE_NAME,CREATE_TIME,OWNER_NAME) VALUES (<1>,<'admin'>,<1461247519>,<'admin'>)]
07:05:19.933 pool-1-thread-1 DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@598154ff"
07:05:19.933 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.933 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:19.933 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.933 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.933 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MRole"
07:05:19.934 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@45743c7c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") taken from Level 1 cache (loadedFlags="[YYY]") [cache size = 1]
07:05:19.934 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@45743c7c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:19.934 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.934 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.934 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.934 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   
, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@387c47e2]]
07:05:19.934 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@387c47e2 is committing for transaction Xid=   
 with onePhase=true
07:05:19.934 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@387c47e2 committed connection for transaction Xid=   
 with onePhase=true
07:05:19.934 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4e0403e1" closed
07:05:19.934 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@17c33db7 [conn=com.jolbox.bonecp.ConnectionHandle@4e0403e1, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.934 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MRole@45743c7c" (depth=0)
07:05:19.934 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MRole@45743c7c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:19.934 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@45743c7c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") being evicted from transactional cache
07:05:19.934 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MRole@45743c7c from StateManager[pc=org.apache.hadoop.hive.metastore.model.MRole@45743c7c, lifecycle=DETACHED_CLEAN]
07:05:19.934 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MRole" being removed from Level 1 cache [current cache size = 1]
07:05:19.934 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:19.934 pool-1-thread-1 INFO HiveMetaStore: Added admin role in metastore
07:05:19.934 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.934 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:19.934 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3220)
07:05:19.934 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
07:05:19.934 pool-1-thread-1 DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1FetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:19.935 pool-1-thread-1 DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1FetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:19.935 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@55d8ef7e" opened with isolation level "read-committed" and auto-commit=false
07:05:19.935 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@175ca8e9, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.935 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@175ca8e9 is starting for transaction Xid=    with flags 0
07:05:19.935 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2d89e559 [conn=com.jolbox.bonecp.ConnectionHandle@55d8ef7e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.935 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1" ...
07:05:19.935 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@3fb84c89"
07:05:19.935 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MRole' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.OWNER_NAME,A0.ROLE_NAME,A0.ROLE_ID FROM ROLES A0 WHERE A0.ROLE_NAME = <'public'>
07:05:19.935 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.935 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:19.935 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
07:05:19.935 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MRole@3b91955f"
07:05:19.935 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MRole (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=2
07:05:19.935 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@3b91955f" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") added to Level 1 cache (loadedFlags="[YYY]")
07:05:19.935 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@3b91955f" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") enlisted in transactional cache
07:05:19.936 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@3b91955f" being inserted into table "ROLES"
07:05:19.936 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2d89e559 [conn=com.jolbox.bonecp.ConnectionHandle@55d8ef7e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.936 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@59d00c59" for connection "com.jolbox.bonecp.ConnectionHandle@55d8ef7e"
07:05:19.936 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO ROLES (ROLE_ID,ROLE_NAME,CREATE_TIME,OWNER_NAME) VALUES (?,?,?,?)" has been made batchable
07:05:19.936 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO ROLES (ROLE_ID,ROLE_NAME,CREATE_TIME,OWNER_NAME) VALUES (?,?,?,?)" for processing (batch size = 1)
07:05:19.936 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3229)
07:05:19.936 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO ROLES (ROLE_ID,ROLE_NAME,CREATE_TIME,OWNER_NAME) VALUES (<2>,<'public'>,<1461247519>,<'public'>)]
07:05:19.936 pool-1-thread-1 DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@59d00c59"
07:05:19.936 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.936 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:19.936 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.936 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.936 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "2[OID]org.apache.hadoop.hive.metastore.model.MRole"
07:05:19.936 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@3b91955f" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") taken from Level 1 cache (loadedFlags="[YYY]") [cache size = 1]
07:05:19.936 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@3b91955f" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:19.936 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.936 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.936 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.936 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@175ca8e9]]
07:05:19.936 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@175ca8e9 is committing for transaction Xid=    with onePhase=true
07:05:19.936 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@175ca8e9 committed connection for transaction Xid=    with onePhase=true
07:05:19.937 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@55d8ef7e" closed
07:05:19.937 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2d89e559 [conn=com.jolbox.bonecp.ConnectionHandle@55d8ef7e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.937 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MRole@3b91955f" (depth=0)
07:05:19.937 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MRole@3b91955f" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:19.937 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@3b91955f" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") being evicted from transactional cache
07:05:19.937 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MRole@3b91955f from StateManager[pc=org.apache.hadoop.hive.metastore.model.MRole@3b91955f, lifecycle=DETACHED_CLEAN]
07:05:19.937 pool-1-thread-1 DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MRole" being removed from Level 1 cache [current cache size = 1]
07:05:19.937 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:19.937 pool-1-thread-1 INFO HiveMetaStore: Added public role in metastore
07:05:19.943 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.943 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:19.944 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:3912)
07:05:19.944 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
07:05:19.944 pool-1-thread-1 DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1FetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:19.944 pool-1-thread-1 DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1FetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:19.944 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@3f769d56" opened with isolation level "read-committed" and auto-commit=false
07:05:19.944 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@5ffd1190, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:19.944 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@5ffd1190 is starting for transaction Xid=    with flags 0
07:05:19.944 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7767c29b [conn=com.jolbox.bonecp.ConnectionHandle@3f769d56, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.944 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1" ...
07:05:19.944 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@7c707d0c"
07:05:19.944 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MRole' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.OWNER_NAME,A0.ROLE_NAME,A0.ROLE_ID FROM ROLES A0 WHERE A0.ROLE_NAME = <'admin'>
07:05:19.944 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.944 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:19.945 pool-1-thread-1 DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MRole"
07:05:19.945 pool-1-thread-1 DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MRole" not found in Level 1 cache [cache size = 0]
07:05:19.946 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@2b0855d4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") added to Level 1 cache (loadedFlags="[NNN]")
07:05:19.948 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MRole@2b0855d4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:19.948 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@2b0855d4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") enlisted in transactional cache
07:05:19.948 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
07:05:19.948 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalGlobalGrants(ObjectStore.java:4397)
07:05:19.948 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MGlobalPrivilege WHERE principalName == t1 && principalType == t2 PARAMETERS java.lang.String t1, java.lang.String t2"
07:05:19.949 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 1 ms
07:05:19.949 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{DyadicExpression{PrimaryExpression{principalName}  =  ParameterExpression{t1}}  AND  DyadicExpression{PrimaryExpression{principalType}  =  ParameterExpression{t2}}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MGlobalPrivilege, t1 type=java.lang.String, t2 type=java.lang.String]
07:05:19.949 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MGlobalPrivilege WHERE principalName == t1 && principalType == t2 PARAMETERS java.lang.String t1, java.lang.String t2" for datastore
07:05:19.949 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege [Table : GLOBAL_PRIVS, InheritanceStrategy : new-table]
07:05:19.949 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.USER_GRANT_ID" added to internal representation of table.
07:05:19.949 pool-1-thread-1 DEBUG Schema: Table GLOBAL_PRIVS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MGlobalPrivilege (inheritance strategy="new-table")
07:05:19.949 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.CREATE_TIME" added to internal representation of table.
07:05:19.949 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.createTime] -> Column(s) [GLOBAL_PRIVS.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:19.949 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.GRANT_OPTION" added to internal representation of table.
07:05:19.950 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.grantOption] -> Column(s) [GLOBAL_PRIVS.GRANT_OPTION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.BooleanMapping" (org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping)
07:05:19.950 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.GRANTOR" added to internal representation of table.
07:05:19.950 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.grantor] -> Column(s) [GLOBAL_PRIVS.GRANTOR] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.950 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.GRANTOR_TYPE" added to internal representation of table.
07:05:19.950 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.grantorType] -> Column(s) [GLOBAL_PRIVS.GRANTOR_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.950 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.PRINCIPAL_NAME" added to internal representation of table.
07:05:19.950 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.principalName] -> Column(s) [GLOBAL_PRIVS.PRINCIPAL_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.950 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.PRINCIPAL_TYPE" added to internal representation of table.
07:05:19.950 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.principalType] -> Column(s) [GLOBAL_PRIVS.PRINCIPAL_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.950 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.USER_PRIV" added to internal representation of table.
07:05:19.950 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.privilege] -> Column(s) [GLOBAL_PRIVS.USER_PRIV] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:19.950 pool-1-thread-1 DEBUG Schema: Table/View GLOBAL_PRIVS has been initialised
07:05:19.950 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@18693647" opened with isolation level "serializable" and auto-commit=false
07:05:19.950 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@18693647" with isolation "serializable"
07:05:19.951 pool-1-thread-1 DEBUG Schema: Check of existence of GLOBAL_PRIVS returned no table
07:05:19.951 pool-1-thread-1 DEBUG Schema: Creating table GLOBAL_PRIVS
07:05:19.951 pool-1-thread-1 DEBUG Schema: CREATE TABLE GLOBAL_PRIVS
(
    USER_GRANT_ID BIGINT NOT NULL,
    CREATE_TIME INTEGER NOT NULL,
    GRANT_OPTION SMALLINT NOT NULL CHECK (GRANT_OPTION IN (0,1)),
    GRANTOR VARCHAR(128),
    GRANTOR_TYPE VARCHAR(128),
    PRINCIPAL_NAME VARCHAR(128),
    PRINCIPAL_TYPE VARCHAR(128),
    USER_PRIV VARCHAR(128)
)
07:05:19.952 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:19.953 pool-1-thread-1 DEBUG Schema: ALTER TABLE GLOBAL_PRIVS ADD CONSTRAINT GLOBAL_PRIVS_PK PRIMARY KEY (USER_GRANT_ID)
07:05:19.956 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:19.966 pool-1-thread-1 DEBUG Schema: Creating index "GlobalPrivilegeIndex" in catalog "" schema ""
07:05:19.966 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX GLOBALPRIVILEGEINDEX ON GLOBAL_PRIVS (PRINCIPAL_NAME,PRINCIPAL_TYPE,USER_PRIV,GRANTOR,GRANTOR_TYPE)
07:05:19.970 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:19.970 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@18693647"
07:05:19.970 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@18693647"
07:05:19.970 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@18693647" non enlisted to a transaction is being committed.
07:05:19.970 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@18693647" closed
07:05:19.971 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 22 ms
07:05:19.971 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MGlobalPrivilege WHERE principalName == t1 && principalType == t2 PARAMETERS java.lang.String t1, java.lang.String t2 Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.GRANT_OPTION,A0.GRANTOR,A0.GRANTOR_TYPE,A0.PRINCIPAL_NAME,A0.PRINCIPAL_TYPE,A0.USER_PRIV,A0.USER_GRANT_ID FROM GLOBAL_PRIVS A0 WHERE A0.PRINCIPAL_NAME = ? AND A0.PRINCIPAL_TYPE = ?"
07:05:19.971 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7767c29b [conn=com.jolbox.bonecp.ConnectionHandle@3f769d56, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.971 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MGlobalPrivilege WHERE principalName == t1 && principalType == t2 PARAMETERS java.lang.String t1, java.lang.String t2" ...
07:05:19.975 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@3f9fc1c1"
07:05:19.975 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.GRANT_OPTION,A0.GRANTOR,A0.GRANTOR_TYPE,A0.PRINCIPAL_NAME,A0.PRINCIPAL_TYPE,A0.USER_PRIV,A0.USER_GRANT_ID FROM GLOBAL_PRIVS A0 WHERE A0.PRINCIPAL_NAME = <'admin'> AND A0.PRINCIPAL_TYPE = <'ROLE'>
07:05:19.976 pool-1-thread-1 DEBUG Retrieve: Execution Time = 1 ms
07:05:19.976 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 5 ms
07:05:19.976 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalGlobalGrants(ObjectStore.java:4407)
07:05:19.976 pool-1-thread-1 DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MGlobalPrivilege
07:05:19.977 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4"
07:05:19.977 pool-1-thread-1 DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege"
07:05:19.977 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:19.977 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:19.977 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@7a1311c0" opened with isolation level "read-committed" and auto-commit=false
07:05:19.982 pool-1-thread-1 DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:19.982 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2afe342c"
07:05:19.982 pool-1-thread-1 DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege'> FOR UPDATE
07:05:19.982 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.985 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@48700cb"
07:05:19.985 pool-1-thread-1 DEBUG Native: SELECT MAX(USER_GRANT_ID) FROM GLOBAL_PRIVS
07:05:19.985 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:19.985 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7f88bf3e" for connection "com.jolbox.bonecp.ConnectionHandle@7a1311c0"
07:05:19.985 pool-1-thread-1 DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege'>,<6>)
07:05:19.986 pool-1-thread-1 DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7f88bf3e"
07:05:19.986 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7f88bf3e"
07:05:19.986 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@970168c"
07:05:19.986 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@482786e8"
07:05:19.986 pool-1-thread-1 DEBUG ValueGeneration: Reserved a block of 5 values
07:05:19.986 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@7a1311c0" non enlisted to a transaction is being committed.
07:05:19.986 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@7a1311c0" closed
07:05:19.986 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:19.986 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") added to Level 1 cache (loadedFlags="[YYYYYYY]")
07:05:19.986 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") enlisted in transactional cache
07:05:19.986 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4" being inserted into table "GLOBAL_PRIVS"
07:05:19.986 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7767c29b [conn=com.jolbox.bonecp.ConnectionHandle@3f769d56, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.989 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7998deb3" for connection "com.jolbox.bonecp.ConnectionHandle@3f769d56"
07:05:19.989 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO GLOBAL_PRIVS (USER_GRANT_ID,CREATE_TIME,GRANTOR_TYPE,GRANTOR,GRANT_OPTION,USER_PRIV,PRINCIPAL_NAME,PRINCIPAL_TYPE) VALUES (?,?,?,?,?,?,?,?)" has been made batchable
07:05:19.989 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO GLOBAL_PRIVS (USER_GRANT_ID,CREATE_TIME,GRANTOR_TYPE,GRANTOR,GRANT_OPTION,USER_PRIV,PRINCIPAL_NAME,PRINCIPAL_TYPE) VALUES (?,?,?,?,?,?,?,?)" for processing (batch size = 1)
07:05:19.989 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:4112)
07:05:19.989 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO GLOBAL_PRIVS (USER_GRANT_ID,CREATE_TIME,GRANTOR_TYPE,GRANTOR,GRANT_OPTION,USER_PRIV,PRINCIPAL_NAME,PRINCIPAL_TYPE) VALUES (<1>,<1461247519>,<'ROLE'>,<'admin'>,<1>,<'All'>,<'admin'>,<'ROLE'>)]
07:05:19.990 pool-1-thread-1 DEBUG Datastore: Execution Time = 1 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7998deb3"
07:05:19.990 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:19.990 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:19.990 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege"
07:05:19.990 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") taken from Level 1 cache (loadedFlags="[YYYYYYY]") [cache size = 2]
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MRole"
07:05:19.990 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@2b0855d4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") taken from Level 1 cache (loadedFlags="[YYY]") [cache size = 2]
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@2b0855d4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:19.990 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:19.990 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:19.990 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@5ffd1190]]
07:05:19.990 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@5ffd1190 is committing for transaction Xid=    with onePhase=true
07:05:19.990 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@5ffd1190 committed connection for transaction Xid=    with onePhase=true
07:05:19.990 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@3f769d56" closed
07:05:19.990 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7767c29b [conn=com.jolbox.bonecp.ConnectionHandle@3f769d56, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4" (depth=0)
07:05:19.990 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:19.990 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") being evicted from transactional cache
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5161e0e4, lifecycle=DETACHED_CLEAN]
07:05:19.990 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" being removed from Level 1 cache [current cache size = 2]
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MRole@2b0855d4" (depth=0)
07:05:19.990 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MRole@2b0855d4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:19.990 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@2b0855d4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") being evicted from transactional cache
07:05:19.990 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MRole@2b0855d4 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MRole@2b0855d4, lifecycle=DETACHED_CLEAN]
07:05:19.990 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MRole" being removed from Level 1 cache [current cache size = 1]
07:05:19.990 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:19.990 pool-1-thread-1 INFO HiveMetaStore: No user is added in admin role, since config is empty
07:05:20.077 pool-1-thread-1 INFO HiveMetaStore: 0: get_all_databases
07:05:20.078 pool-1-thread-1 INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_all_databases
07:05:20.080 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:20.080 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:20.080 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getDatabases(ObjectStore.java:677)
07:05:20.081 pool-1-thread-1 DEBUG Query: JDOQL Single-String with "select name from org.apache.hadoop.hive.metastore.model.MDatabase where ( name.matches("(?i)..*"))"
07:05:20.082 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT name FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE ( name.matches("(?i)..*")) ORDER BY name ascending"
07:05:20.083 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 1 ms
07:05:20.083 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [result:PrimaryExpression{name}]
  [filter:InvokeExpression{[PrimaryExpression{name}].matches(Literal{(?i)..*})}]
  [ordering:OrderExpression{PrimaryExpression{name} ascendingnull}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MDatabase]
07:05:20.083 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT name FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE ( name.matches("(?i)..*")) ORDER BY name ascending" for datastore
07:05:20.087 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 4 ms
07:05:20.087 pool-1-thread-1 DEBUG Query: SELECT name FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE ( name.matches("(?i)..*")) ORDER BY name ascending Query compiled to datastore query "SELECT A0."NAME" AS NUCORDER0 FROM DBS A0 WHERE LOWER(A0."NAME") LIKE '_%' ESCAPE '\' ORDER BY NUCORDER0"
07:05:20.087 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@2c2514e0" opened with isolation level "read-committed" and auto-commit=false
07:05:20.087 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4de72bb8, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:20.087 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4de72bb8 is starting for transaction Xid=    with flags 0
07:05:20.087 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@74667cb3 [conn=com.jolbox.bonecp.ConnectionHandle@2c2514e0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:20.087 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT name FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE ( name.matches("(?i)..*")) ORDER BY name ascending" ...
07:05:20.091 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@29cf678a"
07:05:20.091 pool-1-thread-1 DEBUG Native: SELECT A0."NAME" AS NUCORDER0 FROM DBS A0 WHERE LOWER(A0."NAME") LIKE '_%' ESCAPE '\' ORDER BY NUCORDER0
07:05:20.091 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:20.091 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 4 ms
07:05:20.091 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getDatabases(ObjectStore.java:701)
07:05:20.092 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:20.092 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:20.092 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:20.092 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:20.092 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:20.092 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4de72bb8]]
07:05:20.092 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4de72bb8 is committing for transaction Xid=    with onePhase=true
07:05:20.092 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4de72bb8 committed connection for transaction Xid=    with onePhase=true
07:05:20.092 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@2c2514e0" closed
07:05:20.092 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@74667cb3 [conn=com.jolbox.bonecp.ConnectionHandle@2c2514e0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:20.092 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:20.092 pool-1-thread-1 INFO HiveMetaStore: 0: get_functions: db=default pat=*
07:05:20.092 pool-1-thread-1 INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*
07:05:20.092 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:20.093 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0 (optimistic=false)
07:05:20.093 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getFunctions(ObjectStore.java:7013)
07:05:20.093 pool-1-thread-1 DEBUG Query: JDOQL Single-String with "select functionName from org.apache.hadoop.hive.metastore.model.MFunction where database.name == dbName && ( functionName.matches("(?i).*"))"
07:05:20.093 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT functionName FROM org.apache.hadoop.hive.metastore.model.MFunction WHERE database.name == dbName && ( functionName.matches("(?i).*")) PARAMETERS java.lang.String dbName ORDER BY functionName ascending"
07:05:20.093 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:20.093 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [result:PrimaryExpression{functionName}]
  [filter:DyadicExpression{DyadicExpression{PrimaryExpression{database.name}  =  ParameterExpression{dbName}}  AND  InvokeExpression{[PrimaryExpression{functionName}].matches(Literal{(?i).*})}}]
  [ordering:OrderExpression{PrimaryExpression{functionName} ascendingnull}]
  [symbols: dbName type=java.lang.String, this type=org.apache.hadoop.hive.metastore.model.MFunction]
07:05:20.093 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT functionName FROM org.apache.hadoop.hive.metastore.model.MFunction WHERE database.name == dbName && ( functionName.matches("(?i).*")) PARAMETERS java.lang.String dbName ORDER BY functionName ascending" for datastore
07:05:20.094 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
07:05:20.094 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MFunction [Table : FUNCS, InheritanceStrategy : new-table]
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNCS.FUNC_ID" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Schema: Table FUNCS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MFunction (inheritance strategy="new-table")
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNCS.CLASS_NAME" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.className] -> Column(s) [FUNCS.CLASS_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNCS.CREATE_TIME" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.createTime] -> Column(s) [FUNCS.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNCS.DB_ID" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.database] -> Column(s) [FUNCS.DB_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNCS.FUNC_NAME" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.functionName] -> Column(s) [FUNCS.FUNC_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNCS.FUNC_TYPE" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.functionType] -> Column(s) [FUNCS.FUNC_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNCS.OWNER_NAME" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.ownerName] -> Column(s) [FUNCS.OWNER_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNCS.OWNER_TYPE" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.ownerType] -> Column(s) [FUNCS.OWNER_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:20.094 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MFunction.resourceUris [Table : FUNC_RU]
07:05:20.094 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.resourceUris] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:20.094 pool-1-thread-1 DEBUG Schema: Table/View FUNCS has been initialised
07:05:20.094 pool-1-thread-1 DEBUG Schema: Column "FUNC_RU.FUNC_ID" added to internal representation of table.
07:05:20.094 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MFunction.resourceUris] -> Column(s) [FUNC_RU.FUNC_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:20.095 pool-1-thread-1 DEBUG Schema: Column "FUNC_RU.RESOURCE_TYPE" added to internal representation of table.
07:05:20.095 pool-1-thread-1 DEBUG Schema: Column "FUNC_RU.RESOURCE_URI" added to internal representation of table.
07:05:20.095 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MFunction.resourceUris] -> Column(s) [FUNC_RU.RESOURCE_TYPE,FUNC_RU.RESOURCE_URI] using mapping of type "org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:20.095 pool-1-thread-1 DEBUG Schema: Column "FUNC_RU.INTEGER_IDX" added to internal representation of table.
07:05:20.095 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MFunction.resourceUris] -> Column(s) [FUNC_RU.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:20.095 pool-1-thread-1 DEBUG Schema: Table/View FUNC_RU has been initialised
07:05:20.095 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@c4cda89" opened with isolation level "serializable" and auto-commit=false
07:05:20.095 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@c4cda89" with isolation "serializable"
07:05:20.095 pool-1-thread-1 DEBUG Schema: Check of existence of FUNCS returned no table
07:05:20.095 pool-1-thread-1 DEBUG Schema: Creating table FUNCS
07:05:20.095 pool-1-thread-1 DEBUG Schema: CREATE TABLE FUNCS
(
    FUNC_ID BIGINT NOT NULL,
    CLASS_NAME VARCHAR(4000),
    CREATE_TIME INTEGER NOT NULL,
    DB_ID BIGINT,
    FUNC_NAME VARCHAR(128),
    FUNC_TYPE INTEGER NOT NULL,
    OWNER_NAME VARCHAR(128),
    OWNER_TYPE VARCHAR(10)
)
07:05:20.097 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:20.097 pool-1-thread-1 DEBUG Schema: ALTER TABLE FUNCS ADD CONSTRAINT FUNCS_PK PRIMARY KEY (FUNC_ID)
07:05:20.098 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:20.114 pool-1-thread-1 DEBUG Schema: Check of existence of FUNC_RU returned no table
07:05:20.114 pool-1-thread-1 DEBUG Schema: Creating table FUNC_RU
07:05:20.114 pool-1-thread-1 DEBUG Schema: CREATE TABLE FUNC_RU
(
    FUNC_ID BIGINT NOT NULL,
    RESOURCE_TYPE INTEGER NOT NULL,
    RESOURCE_URI VARCHAR(4000),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:20.116 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:20.116 pool-1-thread-1 DEBUG Schema: ALTER TABLE FUNC_RU ADD CONSTRAINT FUNC_RU_PK PRIMARY KEY (FUNC_ID,INTEGER_IDX)
07:05:20.118 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:20.127 pool-1-thread-1 DEBUG Schema: Creating index "FUNCS_N49" in catalog "" schema ""
07:05:20.127 pool-1-thread-1 DEBUG Schema: CREATE INDEX FUNCS_N49 ON FUNCS (DB_ID)
07:05:20.129 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:20.129 pool-1-thread-1 DEBUG Schema: Creating index "UniqueFunction" in catalog "" schema ""
07:05:20.129 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX UNIQUEFUNCTION ON FUNCS (FUNC_NAME,DB_ID)
07:05:20.130 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:20.130 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "FUNCS_FK1" in catalog "" schema ""
07:05:20.130 pool-1-thread-1 DEBUG Schema: ALTER TABLE FUNCS ADD CONSTRAINT FUNCS_FK1 FOREIGN KEY (DB_ID) REFERENCES DBS (DB_ID)
07:05:20.132 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:20.132 pool-1-thread-1 DEBUG Schema: Creating index "FUNC_RU_N49" in catalog "" schema ""
07:05:20.132 pool-1-thread-1 DEBUG Schema: CREATE INDEX FUNC_RU_N49 ON FUNC_RU (FUNC_ID)
07:05:20.134 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:20.134 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "FUNC_RU_FK1" in catalog "" schema ""
07:05:20.134 pool-1-thread-1 DEBUG Schema: ALTER TABLE FUNC_RU ADD CONSTRAINT FUNC_RU_FK1 FOREIGN KEY (FUNC_ID) REFERENCES FUNCS (FUNC_ID)
07:05:20.137 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:20.137 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@c4cda89"
07:05:20.138 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@c4cda89"
07:05:20.138 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@c4cda89" non enlisted to a transaction is being committed.
07:05:20.138 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@c4cda89" closed
07:05:20.141 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 48 ms
07:05:20.141 pool-1-thread-1 DEBUG Query: SELECT functionName FROM org.apache.hadoop.hive.metastore.model.MFunction WHERE database.name == dbName && ( functionName.matches("(?i).*")) PARAMETERS java.lang.String dbName ORDER BY functionName ascending Query compiled to datastore query "SELECT A0.FUNC_NAME AS NUCORDER0 FROM FUNCS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE B0."NAME" = ? AND LOWER(A0.FUNC_NAME) LIKE '%' ESCAPE '\' ORDER BY NUCORDER0"
07:05:20.141 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5be534f0" opened with isolation level "read-committed" and auto-commit=false
07:05:20.141 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@dea3eaf, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:20.141 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@dea3eaf is starting for transaction Xid=    with flags 0
07:05:20.141 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5c3af89f [conn=com.jolbox.bonecp.ConnectionHandle@5be534f0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:20.141 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT functionName FROM org.apache.hadoop.hive.metastore.model.MFunction WHERE database.name == dbName && ( functionName.matches("(?i).*")) PARAMETERS java.lang.String dbName ORDER BY functionName ascending" ...
07:05:20.151 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@41671a6a"
07:05:20.151 pool-1-thread-1 DEBUG Native: SELECT A0.FUNC_NAME AS NUCORDER0 FROM FUNCS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE B0."NAME" = <'default'> AND LOWER(A0.FUNC_NAME) LIKE '%' ESCAPE '\' ORDER BY NUCORDER0
07:05:20.151 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:20.151 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 10 ms
07:05:20.151 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getFunctions(ObjectStore.java:7041)
07:05:20.151 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@718838d0
07:05:20.151 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:20.152 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:20.152 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:20.152 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:20.152 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@dea3eaf]]
07:05:20.152 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@dea3eaf is committing for transaction Xid=    with onePhase=true
07:05:20.152 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@dea3eaf committed connection for transaction Xid=    with onePhase=true
07:05:20.152 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5be534f0" closed
07:05:20.152 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5c3af89f [conn=com.jolbox.bonecp.ConnectionHandle@5be534f0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@718838d0 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@5b7cfa69]
07:05:20.152 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:20.178 pool-1-thread-1 DEBUG SessionState: HDFS root scratch dir: /tmp/hive with schema null, permission: rwx-wx-wx
07:05:20.189 pool-1-thread-1 INFO SessionState: Created local directory: /Users/lian/local/src/spark/workspace-d/target/tmp/70f69927-5d75-43fe-897a-75e2e6388ce1_resources
07:05:20.196 pool-1-thread-1 INFO SessionState: Created HDFS directory: /tmp/hive/lian/70f69927-5d75-43fe-897a-75e2e6388ce1
07:05:20.200 pool-1-thread-1 INFO SessionState: Created local directory: /Users/lian/local/src/spark/workspace-d/target/tmp/lian/70f69927-5d75-43fe-897a-75e2e6388ce1
07:05:20.205 pool-1-thread-1 INFO SessionState: Created HDFS directory: /tmp/hive/lian/70f69927-5d75-43fe-897a-75e2e6388ce1/_tmp_space.db
07:05:20.248 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.rdbms.datastoreAdapterClassName=org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:20.248 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.identifierFactory=datanucleus1
07:05:20.248 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.DetachAllOnCommit=true
07:05:20.248 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.NonTransactionalRead=true
07:05:20.248 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.transactionIsolation=read-committed
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.cache.level2=false
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateColumns=false
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.integral.jdo.pushdown=false
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.warehouse.dir=file:/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.uris=
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateTables=false
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.rdbms.useLegacyNativeValueStrategy=true
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.connectionPoolingType=BONECP
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionUserName=APP
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.storeManagerType=rdbms
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.autoStartMechanismMode=checked
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.cache.level2.type=none
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.fixedDatastore=false
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionURL=jdbc:derby:memory:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10/metastore;create=true
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.plugin.pluginRegistryBundleCheck=LOG
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionPassword=xxx
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateConstraints=false
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.autoCreateSchema=true
07:05:20.249 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.Multithreaded=true
07:05:20.249 pool-1-thread-1 DEBUG SessionState: SessionState user: null
07:05:20.257 pool-1-thread-1 DEBUG SessionState: HDFS root scratch dir: /tmp/hive with schema null, permission: rwx-wx-wx
07:05:20.262 pool-1-thread-1 INFO SessionState: Created local directory: /Users/lian/local/src/spark/workspace-d/target/tmp/f1faa641-476a-45ef-a018-3f1271358b53_resources
07:05:20.266 pool-1-thread-1 INFO SessionState: Created HDFS directory: /tmp/hive/lian/f1faa641-476a-45ef-a018-3f1271358b53
07:05:20.270 pool-1-thread-1 INFO SessionState: Created local directory: /Users/lian/local/src/spark/workspace-d/target/tmp/lian/f1faa641-476a-45ef-a018-3f1271358b53
07:05:20.275 pool-1-thread-1 INFO SessionState: Created HDFS directory: /tmp/hive/lian/f1faa641-476a-45ef-a018-3f1271358b53/_tmp_space.db
07:05:20.312 pool-1-thread-1 INFO HiveContext: default warehouse location is /user/hive/warehouse
07:05:20.316 pool-1-thread-1 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
07:05:20.316 pool-1-thread-1 DEBUG IsolatedClientLoader: Initializing the logger to avoid disaster...
07:05:20.321 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl - 1022460032
07:05:20.322 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.HiveClient
07:05:20.322 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.internal.Logging
07:05:20.323 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Object
07:05:20.323 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim_v0_12 - 1419598417
07:05:20.324 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim - 1028742206
07:05:20.324 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim_v0_13 - 292532315
07:05:20.324 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim_v0_14 - -530748652
07:05:20.325 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim_v1_0 - 519688247
07:05:20.325 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim_v1_1 - 2131206696
07:05:20.325 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim_v1_2 - 1390747764
07:05:20.325 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.Function0
07:05:20.325 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.collection.Seq
07:05:20.325 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Throwable
07:05:20.325 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.SparkException
07:05:20.325 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.Function1
07:05:20.326 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.OutputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/OutputStream.class
07:05:20.326 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.util.CircularBuffer
07:05:20.326 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.MatchError
07:05:20.326 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.NonLocalReturnControl
07:05:20.326 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Exception
07:05:20.326 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.Nothing$
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.collection.Map
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.package$HiveVersion
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.SparkConf
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.conf.Configuration
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.collection.immutable.Map
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ClassLoader
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.IsolatedClientLoader
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.HiveClient$class
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.internal.Logging$class
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.util.CircularBuffer$
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.package$hive$v12$
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.package$hive$v13$
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.package$hive$v14$
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.package$hive$v1_0$
07:05:20.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.package$hive$v1_1$
07:05:20.328 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.hive.client.package$hive$v1_2$
07:05:20.328 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.List - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/List.class
07:05:20.328 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Collection - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Collection.class
07:05:20.328 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.PartialFunction
07:05:20.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConf - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf.class
07:05:20.339 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Thread
07:05:20.339 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.SessionState - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/SessionState.class
07:05:20.340 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Map - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Map.class
07:05:20.340 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Set - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Set.class
07:05:20.340 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ThreadLocal
07:05:20.340 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.SessionState$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/SessionState$1.class
07:05:20.341 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.IOException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/IOException.class
07:05:20.341 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.history.HiveHistory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/history/HiveHistory.class
07:05:20.341 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.RuntimeException
07:05:20.341 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.FileSystem
07:05:20.341 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.LocalFileSystem
07:05:20.342 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.HiveException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/HiveException.class
07:05:20.342 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.authorization.plugin.HiveMetastoreClientFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveMetastoreClientFactory.class
07:05:20.342 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.AssertionError
07:05:20.342 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.IllegalArgumentException
07:05:20.342 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.URLClassLoader
07:05:20.343 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.URISyntaxException
07:05:20.343 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ClassNotFoundException
07:05:20.344 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.LogFactory - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/LogFactory.class
07:05:20.345 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.PrivilegedAction - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/PrivilegedAction.class
07:05:20.346 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.NoClassDefFoundError
07:05:20.346 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.SecurityException
07:05:20.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.FileOutputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/FileOutputStream.class
07:05:20.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Enumeration - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Enumeration.class
07:05:20.346 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.LinkageError
07:05:20.346 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ClassCastException
07:05:20.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.LogConfigurationException - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/LogConfigurationException.class
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.NoSuchMethodException
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.InvocationTargetException
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.IllegalAccessException
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.Method
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ThreadDeath
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.VirtualMachineError
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Class
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.UnsupportedEncodingException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/UnsupportedEncodingException.class
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.String
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.Reader - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/Reader.class
07:05:20.347 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.InputStreamReader - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/InputStreamReader.class
07:05:20.348 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.StringBuffer
07:05:20.348 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.System
07:05:20.348 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.LogFactory$6 - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/LogFactory$6.class
07:05:20.348 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.AccessController - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/AccessController.class
07:05:20.348 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.impl.WeakHashtable - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/impl/WeakHashtable.class
07:05:20.349 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Hashtable - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Hashtable.class
07:05:20.349 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.NullPointerException
07:05:20.349 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ref.ReferenceQueue
07:05:20.350 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.LogFactory$1 - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/LogFactory$1.class
07:05:20.350 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.impl.WeakHashtable$Referenced - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/impl/WeakHashtable$Referenced.class
07:05:20.351 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ref.Reference
07:05:20.351 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ref.WeakReference
07:05:20.351 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.impl.WeakHashtable$WeakKey - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/impl/WeakHashtable$WeakKey.class
07:05:20.351 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.LogFactory$4 - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/LogFactory$4.class
07:05:20.352 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.NoSuchMethodError
07:05:20.356 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.LogFactory$3 - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/LogFactory$3.class
07:05:20.357 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.LogFactory$2 - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/LogFactory$2.class
07:05:20.357 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.impl.LogFactoryImpl - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/impl/LogFactoryImpl.class
07:05:20.358 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.Writer - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/Writer.class
07:05:20.358 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.StringWriter - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/StringWriter.class
07:05:20.358 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ExceptionInInitializerError
07:05:20.360 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.impl.LogFactoryImpl$2 - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/impl/LogFactoryImpl$2.class
07:05:20.360 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.impl.LogFactoryImpl$1 - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/impl/LogFactoryImpl$1.class
07:05:20.361 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.impl.LogFactoryImpl$3 - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/impl/LogFactoryImpl$3.class
07:05:20.361 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.impl.Log4JLogger - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/impl/Log4JLogger.class
07:05:20.362 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.logging.Log - jar:file:/Users/lian/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar!/org/apache/commons/logging/Log.class
07:05:20.362 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.Serializable - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/Serializable.class
07:05:20.362 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.log4j.Category
07:05:20.362 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.log4j.Logger
07:05:20.362 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.log4j.Priority
07:05:20.362 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.log4j.Level
07:05:20.362 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.InstantiationError
07:05:20.362 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.Field
07:05:20.363 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.Constructor
07:05:20.363 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.SessionState$SessionStates - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/SessionState$SessionStates.class
07:05:20.364 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.security.auth.login.LoginException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/security/auth/login/LoginException.class
07:05:20.364 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.ByteArrayOutputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/ByteArrayOutputStream.class
07:05:20.364 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.InputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/InputStream.class
07:05:20.364 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.LoopingByteArrayInputStream - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/LoopingByteArrayInputStream.class
07:05:20.364 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.CharSequence
07:05:20.365 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.HashMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/HashMap.class
07:05:20.365 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConf$ConfVars - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf$ConfVars.class
07:05:20.367 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Enum
07:05:20.368 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator.class
07:05:20.370 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConf$ConfVars$VarType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf$ConfVars$VarType.class
07:05:20.371 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConf$ConfVars$VarType$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf$ConfVars$VarType$1.class
07:05:20.371 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConf$ConfVars$VarType$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf$ConfVars$VarType$2.class
07:05:20.371 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConf$ConfVars$VarType$3 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf$ConfVars$VarType$3.class
07:05:20.371 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConf$ConfVars$VarType$4 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf$ConfVars$VarType$4.class
07:05:20.372 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConf$ConfVars$VarType$5 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConf$ConfVars$VarType$5.class
07:05:20.372 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.SystemVariables - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/SystemVariables.class
07:05:20.372 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.IllegalStateException
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.regex.Pattern - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/regex/Pattern.class
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.StringBuilder
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.regex.Matcher - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/regex/Matcher.class
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.File - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/File.class
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Boolean
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Integer
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Long
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Float
07:05:20.373 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.util.Shell
07:05:20.374 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.ShimLoader - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/ShimLoader.class
07:05:20.374 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims.class
07:05:20.374 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.util.VersionInfo
07:05:20.375 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.Hadoop23Shims - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/Hadoop23Shims.class
07:05:20.376 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShimsSecure - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShimsSecure.class
07:05:20.376 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$MiniMrShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$MiniMrShim.class
07:05:20.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.Hadoop23Shims$MiniMrShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/Hadoop23Shims$MiniMrShim.class
07:05:20.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.Hadoop23Shims$MiniTezShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/Hadoop23Shims$MiniTezShim.class
07:05:20.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.Hadoop23Shims$MiniSparkShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/Hadoop23Shims$MiniSparkShim.class
07:05:20.378 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$MiniDFSShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$MiniDFSShim.class
07:05:20.378 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$CombineFileInputFormatShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$CombineFileInputFormatShim.class
07:05:20.378 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapreduce.TaskAttemptContext
07:05:20.378 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapreduce.JobContext
07:05:20.378 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Comparator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Comparator.class
07:05:20.379 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$HdfsFileStatus - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$HdfsFileStatus.class
07:05:20.379 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Iterable
07:05:20.379 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$HCatHadoopShims - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$HCatHadoopShims.class
07:05:20.379 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$WebHCatJTShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$WebHCatJTShim.class
07:05:20.380 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.Hadoop23Shims$ProxyFileSystem23 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/Hadoop23Shims$ProxyFileSystem23.class
07:05:20.380 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.ProxyFileSystem
07:05:20.380 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$StoragePolicyShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$StoragePolicyShim.class
07:05:20.380 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$KerberosNameShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$KerberosNameShim.class
07:05:20.381 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.HadoopShims$HdfsEncryptionShim - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/HadoopShims$HdfsEncryptionShim.class
07:05:20.381 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.base.Predicate
07:05:20.381 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.AccessControlException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/AccessControlException.class
07:05:20.381 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.Path
07:05:20.381 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.permission.FsAction
07:05:20.381 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.CacheFlag
07:05:20.382 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$TimeValidator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$TimeValidator.class
07:05:20.382 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.TimeUnit - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/TimeUnit.class
07:05:20.384 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$StringSet - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$StringSet.class
07:05:20.384 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.LinkedHashSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/LinkedHashSet.class
07:05:20.386 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$RatioValidator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$RatioValidator.class
07:05:20.386 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.NumberFormatException
07:05:20.386 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$PatternSet - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$PatternSet.class
07:05:20.386 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.ArrayList - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/ArrayList.class
07:05:20.388 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$RangeValidator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$RangeValidator.class
07:05:20.389 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$TYPE - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$TYPE.class
07:05:20.389 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$TYPE$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$TYPE$1.class
07:05:20.389 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$TYPE$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$TYPE$2.class
07:05:20.389 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.Validator$TYPE$3 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/Validator$TYPE$3.class
07:05:20.391 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hive.common.HiveCompat - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/common/HiveCompat.class
07:05:20.391 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hive.common.HiveCompat$CompatLevel - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/common/HiveCompat$CompatLevel.class
07:05:20.397 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.JobConf
07:05:20.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.jaxp.DocumentBuilderFactoryImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/jaxp/DocumentBuilderFactoryImpl.class
07:05:20.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.parsers.DocumentBuilderFactory - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/parsers/DocumentBuilderFactory.class
07:05:20.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.SAXException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/SAXException.class
07:05:20.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.SAXNotRecognizedException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/SAXNotRecognizedException.class
07:05:20.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.SAXNotSupportedException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/SAXNotSupportedException.class
07:05:20.405 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.parsers.ParserConfigurationException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/parsers/ParserConfigurationException.class
07:05:20.405 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.parsers.DocumentBuilder - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/parsers/DocumentBuilder.class
07:05:20.406 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.jaxp.DocumentBuilderImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/jaxp/DocumentBuilderImpl.class
07:05:20.407 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.jaxp.JAXPConstants - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/jaxp/JAXPConstants.class
07:05:20.408 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.Document - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/Document.class
07:05:20.408 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLConfigurationException - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLConfigurationException.class
07:05:20.409 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.XNIException - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/XNIException.class
07:05:20.409 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.ErrorHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/ErrorHandler.class
07:05:20.409 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.XMLDTDHandler - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/XMLDTDHandler.class
07:05:20.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLDTDSource - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLDTDSource.class
07:05:20.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLComponentManager - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLComponentManager.class
07:05:20.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.xs.XMLSchemaValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/xs/XMLSchemaValidator.class
07:05:20.411 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLComponent - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLComponent.class
07:05:20.412 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLDocumentFilter - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLDocumentFilter.class
07:05:20.412 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.XMLDocumentHandler - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/XMLDocumentHandler.class
07:05:20.412 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLDocumentSource - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLDocumentSource.class
07:05:20.413 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.xs.identity.FieldActivator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/xs/identity/FieldActivator.class
07:05:20.413 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.RevalidationHandler - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/RevalidationHandler.class
07:05:20.413 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.xs.XSElementDeclHelper - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/xs/XSElementDeclHelper.class
07:05:20.413 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.jaxp.JAXPValidatorComponent - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/jaxp/JAXPValidatorComponent.class
07:05:20.414 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.jaxp.TeeXMLDocumentFilterImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/jaxp/TeeXMLDocumentFilterImpl.class
07:05:20.414 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.DOMParser - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/DOMParser.class
07:05:20.415 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.AbstractDOMParser - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/AbstractDOMParser.class
07:05:20.416 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.AbstractXMLDocumentParser - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/AbstractXMLDocumentParser.class
07:05:20.416 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.XMLDTDContentModelHandler - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/XMLDTDContentModelHandler.class
07:05:20.416 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.XMLParser - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/XMLParser.class
07:05:20.417 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.AbstractDOMParser$Abort - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/AbstractDOMParser$Abort.class
07:05:20.417 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.Node - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/Node.class
07:05:20.418 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xs.XSSimpleTypeDefinition - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xs/XSSimpleTypeDefinition.class
07:05:20.418 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xs.XSTypeDefinition - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xs/XSTypeDefinition.class
07:05:20.418 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xs.XSObject - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xs/XSObject.class
07:05:20.420 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.DOMError - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/DOMError.class
07:05:20.420 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLEntityResolver - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLEntityResolver.class
07:05:20.420 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLErrorHandler - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLErrorHandler.class
07:05:20.421 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.EntityResolver - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/EntityResolver.class
07:05:20.421 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLParseException - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLParseException.class
07:05:20.421 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.Locator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/Locator.class
07:05:20.421 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.SAXParseException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/SAXParseException.class
07:05:20.421 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.ObjectFactory - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/ObjectFactory.class
07:05:20.422 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.FileInputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/FileInputStream.class
07:05:20.422 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Properties - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Properties.class
07:05:20.422 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.ObjectFactory$ConfigurationError - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/ObjectFactory$ConfigurationError.class
07:05:20.422 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Error
07:05:20.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.SecuritySupport - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/SecuritySupport.class
07:05:20.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.PrivilegedActionException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/PrivilegedActionException.class
07:05:20.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.PrivilegedExceptionAction - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/PrivilegedExceptionAction.class
07:05:20.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.FileNotFoundException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/FileNotFoundException.class
07:05:20.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.SecuritySupport$4 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/SecuritySupport$4.class
07:05:20.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.SecuritySupport$1 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/SecuritySupport$1.class
07:05:20.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.SecuritySupport$2 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/SecuritySupport$2.class
07:05:20.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.SecuritySupport$3 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/SecuritySupport$3.class
07:05:20.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.SecuritySupport$7 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/SecuritySupport$7.class
07:05:20.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.SecuritySupport$6 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/SecuritySupport$6.class
07:05:20.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.XIncludeAwareParserConfiguration - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/XIncludeAwareParserConfiguration.class
07:05:20.426 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.XML11Configuration - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/XML11Configuration.class
07:05:20.426 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLPullParserConfiguration - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLPullParserConfiguration.class
07:05:20.427 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLParserConfiguration - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLParserConfiguration.class
07:05:20.427 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.XML11Configurable - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/XML11Configurable.class
07:05:20.427 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.ParserConfigurationSettings - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/ParserConfigurationSettings.class
07:05:20.428 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLDTDScanner - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLDTDScanner.class
07:05:20.428 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLDTDContentModelSource - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLDTDContentModelSource.class
07:05:20.429 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLDocumentScanner - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLDocumentScanner.class
07:05:20.429 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLDTDValidatorFilter - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLDTDValidatorFilter.class
07:05:20.429 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.MessageFormatter - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/MessageFormatter.class
07:05:20.430 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.XMLLocator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/XMLLocator.class
07:05:20.431 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLDTDValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLDTDValidator.class
07:05:20.432 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLNSDTDValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLNSDTDValidator.class
07:05:20.432 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.NamespaceContext - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/NamespaceContext.class
07:05:20.433 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.SymbolTable - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/SymbolTable.class
07:05:20.433 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.grammars.XMLGrammarPool - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/grammars/XMLGrammarPool.class
07:05:20.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.SymbolTable$Entry - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/SymbolTable$Entry.class
07:05:20.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager.class
07:05:20.435 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.io.Latin1Reader - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/io/Latin1Reader.class
07:05:20.435 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.io.ASCIIReader - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/io/ASCIIReader.class
07:05:20.436 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.io.UTF16Reader - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/io/UTF16Reader.class
07:05:20.436 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.io.UTF8Reader - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/io/UTF8Reader.class
07:05:20.437 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.io.UCSReader - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/io/UCSReader.class
07:05:20.437 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.URLConnection
07:05:20.437 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.URI$MalformedURIException - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/URI$MalformedURIException.class
07:05:20.437 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityScanner - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityScanner.class
07:05:20.438 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XML11EntityScanner - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XML11EntityScanner.class
07:05:20.438 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$RewindableInputStream - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$RewindableInputStream.class
07:05:20.439 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.XMLResourceIdentifier - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/XMLResourceIdentifier.class
07:05:20.439 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.StringReader - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/StringReader.class
07:05:20.439 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$ExternalEntity - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$ExternalEntity.class
07:05:20.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$Entity - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$Entity.class
07:05:20.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$InternalEntity - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$InternalEntity.class
07:05:20.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$ScannedEntity - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$ScannedEntity.class
07:05:20.441 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.Augmentations - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/Augmentations.class
07:05:20.442 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$1 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$1.class
07:05:20.442 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Stack - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Stack.class
07:05:20.442 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.XMLResourceIdentifierImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/XMLResourceIdentifierImpl.class
07:05:20.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.AugmentationsImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/AugmentationsImpl.class
07:05:20.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.AugmentationsImpl$AugmentationsItemsContainer - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/AugmentationsImpl$AugmentationsItemsContainer.class
07:05:20.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.AugmentationsImpl$SmallContainer - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/AugmentationsImpl$SmallContainer.class
07:05:20.444 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.AugmentationsImpl$LargeContainer - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/AugmentationsImpl$LargeContainer.class
07:05:20.444 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$ByteBufferPool - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$ByteBufferPool.class
07:05:20.445 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$CharacterBufferPool - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$CharacterBufferPool.class
07:05:20.445 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$CharacterBuffer - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$CharacterBuffer.class
07:05:20.446 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.EOFException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/EOFException.class
07:05:20.446 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityScanner$1 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityScanner$1.class
07:05:20.447 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLErrorReporter - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLErrorReporter.class
07:05:20.447 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLNSDocumentScannerImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLNSDocumentScannerImpl.class
07:05:20.448 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentScannerImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentScannerImpl.class
07:05:20.448 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentFragmentScannerImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentFragmentScannerImpl.class
07:05:20.449 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityHandler - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityHandler.class
07:05:20.449 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLScanner - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLScanner.class
07:05:20.450 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.XMLString - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/XMLString.class
07:05:20.450 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.XMLStringBuffer - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/XMLStringBuffer.class
07:05:20.451 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$Dispatcher - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentFragmentScannerImpl$Dispatcher.class
07:05:20.452 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.XMLAttributes - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/XMLAttributes.class
07:05:20.453 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.grammars.XMLDTDDescription - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/grammars/XMLDTDDescription.class
07:05:20.453 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.grammars.XMLGrammarDescription - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/grammars/XMLGrammarDescription.class
07:05:20.454 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$ElementStack - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentFragmentScannerImpl$ElementStack.class
07:05:20.455 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.QName - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/QName.class
07:05:20.455 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Cloneable
07:05:20.455 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLNSDocumentScannerImpl$NSContentDispatcher - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLNSDocumentScannerImpl$NSContentDispatcher.class
07:05:20.455 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentScannerImpl$ContentDispatcher - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentScannerImpl$ContentDispatcher.class
07:05:20.456 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.class
07:05:20.456 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.CharConversionException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/CharConversionException.class
07:05:20.456 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.io.MalformedByteSequenceException - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/io/MalformedByteSequenceException.class
07:05:20.457 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.XMLAttributesImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/XMLAttributesImpl.class
07:05:20.458 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.XMLAttributesImpl$Attribute - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/XMLAttributesImpl$Attribute.class
07:05:20.458 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.NamespaceSupport - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/NamespaceSupport.class
07:05:20.459 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentScannerImpl$XMLDeclDispatcher - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentScannerImpl$XMLDeclDispatcher.class
07:05:20.459 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentScannerImpl$PrologDispatcher - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentScannerImpl$PrologDispatcher.class
07:05:20.460 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentScannerImpl$DTDDispatcher - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentScannerImpl$DTDDispatcher.class
07:05:20.461 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDocumentScannerImpl$TrailingMiscDispatcher - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDocumentScannerImpl$TrailingMiscDispatcher.class
07:05:20.461 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLDTDDescription - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLDTDDescription.class
07:05:20.462 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLDTDScannerImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLDTDScannerImpl.class
07:05:20.464 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLDTDProcessor - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLDTDProcessor.class
07:05:20.465 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLDTDFilter - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLDTDFilter.class
07:05:20.465 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLDTDContentModelFilter - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLDTDContentModelFilter.class
07:05:20.466 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLEntityDecl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLEntityDecl.class
07:05:20.467 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.InvalidDatatypeValueException - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/InvalidDatatypeValueException.class
07:05:20.467 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.DatatypeException - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/DatatypeException.class
07:05:20.467 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.ValidationContext - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/ValidationContext.class
07:05:20.468 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.validation.EntityState - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/validation/EntityState.class
07:05:20.469 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.DTDGrammar - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/DTDGrammar.class
07:05:20.469 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.grammars.Grammar - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/grammars/Grammar.class
07:05:20.470 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.BalancedDTDGrammar - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/BalancedDTDGrammar.class
07:05:20.471 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.validation.ValidationState - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/validation/ValidationState.class
07:05:20.471 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLElementDecl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLElementDecl.class
07:05:20.472 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLSimpleType - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLSimpleType.class
07:05:20.472 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLAttributeDecl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLAttributeDecl.class
07:05:20.473 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.DTDGrammarBucket - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/DTDGrammarBucket.class
07:05:20.473 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.DTDDVFactory - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/DTDDVFactory.class
07:05:20.474 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.DVFactoryException - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/DVFactoryException.class
07:05:20.474 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.ObjectFactory - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/ObjectFactory.class
07:05:20.475 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.ObjectFactory$ConfigurationError - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/ObjectFactory$ConfigurationError.class
07:05:20.475 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.SecuritySupport - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/SecuritySupport.class
07:05:20.476 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.SecuritySupport$4 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/SecuritySupport$4.class
07:05:20.476 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.SecuritySupport$1 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/SecuritySupport$1.class
07:05:20.476 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.SecuritySupport$2 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/SecuritySupport$2.class
07:05:20.477 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.SecuritySupport$3 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/SecuritySupport$3.class
07:05:20.477 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.dtd.DTDDVFactoryImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/dtd/DTDDVFactoryImpl.class
07:05:20.477 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.DatatypeValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/DatatypeValidator.class
07:05:20.478 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.dtd.StringDatatypeValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/dtd/StringDatatypeValidator.class
07:05:20.478 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.dtd.IDDatatypeValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/dtd/IDDatatypeValidator.class
07:05:20.478 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.dtd.IDREFDatatypeValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/dtd/IDREFDatatypeValidator.class
07:05:20.479 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.dtd.ListDatatypeValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/dtd/ListDatatypeValidator.class
07:05:20.479 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.dtd.ENTITYDatatypeValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/dtd/ENTITYDatatypeValidator.class
07:05:20.479 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.dtd.NOTATIONDatatypeValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/dtd/NOTATIONDatatypeValidator.class
07:05:20.480 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dv.dtd.NMTOKENDatatypeValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dv/dtd/NMTOKENDatatypeValidator.class
07:05:20.480 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.validation.ValidationManager - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/validation/ValidationManager.class
07:05:20.480 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLVersionDetector - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLVersionDetector.class
07:05:20.481 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.msg.XMLMessageFormatter - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/msg/XMLMessageFormatter.class
07:05:20.481 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.MissingResourceException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/MissingResourceException.class
07:05:20.482 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Locale - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Locale.class
07:05:20.482 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xni.parser.XMLInputSource - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xni/parser/XMLInputSource.class
07:05:20.482 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.InputSource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/InputSource.class
07:05:20.483 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.URI - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/URI.class
07:05:20.485 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.XMLEntityManager$EncodingInfo - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/XMLEntityManager$EncodingInfo.class
07:05:20.486 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.XMLChar - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/XMLChar.class
07:05:20.487 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Arrays - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Arrays.class
07:05:20.488 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xinclude.XIncludeHandler - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xinclude/XIncludeHandler.class
07:05:20.489 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.HTTPInputSource - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/HTTPInputSource.class
07:05:20.490 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xinclude.XIncludeHandler$Notation - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xinclude/XIncludeHandler$Notation.class
07:05:20.490 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xinclude.XIncludeHandler$UnparsedEntity - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xinclude/XIncludeHandler$UnparsedEntity.class
07:05:20.491 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xpointer.XPointerProcessor - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xpointer/XPointerProcessor.class
07:05:20.491 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xinclude.XIncludeTextReader - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xinclude/XIncludeTextReader.class
07:05:20.492 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xinclude.XInclude11TextReader - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xinclude/XInclude11TextReader.class
07:05:20.493 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.SecurityManager - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/SecurityManager.class
07:05:20.494 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.XMLSymbols - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/XMLSymbols.class
07:05:20.495 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.XMLLocatorWrapper - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/XMLLocatorWrapper.class
07:05:20.495 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xinclude.XIncludeMessageFormatter - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xinclude/XIncludeMessageFormatter.class
07:05:20.495 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.IntStack - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/IntStack.class
07:05:20.496 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.Constants - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/Constants.class
07:05:20.496 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.Constants$ArrayEnumeration - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/Constants$ArrayEnumeration.class
07:05:20.497 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.NoSuchElementException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/NoSuchElementException.class
07:05:20.497 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xinclude.XIncludeNamespaceSupport - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xinclude/XIncludeNamespaceSupport.class
07:05:20.497 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xinclude.MultipleScopeNamespaceSupport - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xinclude/MultipleScopeNamespaceSupport.class
07:05:20.498 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredDocumentImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredDocumentImpl.class
07:05:20.499 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredNode - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredNode.class
07:05:20.499 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DocumentImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DocumentImpl.class
07:05:20.500 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.traversal.DocumentTraversal - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/traversal/DocumentTraversal.class
07:05:20.500 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.events.DocumentEvent - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/events/DocumentEvent.class
07:05:20.500 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.ranges.DocumentRange - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/ranges/DocumentRange.class
07:05:20.500 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.CoreDocumentImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/CoreDocumentImpl.class
07:05:20.501 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.ParentNode - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/ParentNode.class
07:05:20.501 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.ChildNode - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/ChildNode.class
07:05:20.502 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.NodeImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/NodeImpl.class
07:05:20.502 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.NodeList - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/NodeList.class
07:05:20.502 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.events.EventTarget - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/events/EventTarget.class
07:05:20.502 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.DOMException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/DOMException.class
07:05:20.502 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.Element - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/Element.class
07:05:20.503 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.CloneNotSupportedException
07:05:20.504 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.Attr - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/Attr.class
07:05:20.504 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.AttrImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/AttrImpl.class
07:05:20.504 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.TypeInfo - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/TypeInfo.class
07:05:20.505 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.AttrNSImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/AttrNSImpl.class
07:05:20.505 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.DOMConfiguration - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/DOMConfiguration.class
07:05:20.505 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.EntityReference - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/EntityReference.class
07:05:20.505 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.ProcessingInstruction - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/ProcessingInstruction.class
07:05:20.505 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.CDATASection - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/CDATASection.class
07:05:20.505 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.Comment - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/Comment.class
07:05:20.505 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.DocumentFragment - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/DocumentFragment.class
07:05:20.505 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.DocumentType - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/DocumentType.class
07:05:20.506 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.ElementImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/ElementImpl.class
07:05:20.507 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.ElementTraversal - jar:file:/Users/lian/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.4.01.jar!/org/w3c/dom/ElementTraversal.class
07:05:20.507 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.ElementNSImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/ElementNSImpl.class
07:05:20.508 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.Text - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/Text.class
07:05:20.508 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.Notation - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/Notation.class
07:05:20.508 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.Entity - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/Entity.class
07:05:20.508 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.events.Event - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/events/Event.class
07:05:20.508 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.events.MutationEvent - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/events/MutationEvent.class
07:05:20.509 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.events.EventException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/events/EventException.class
07:05:20.509 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.ranges.Range - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/ranges/Range.class
07:05:20.509 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.traversal.TreeWalker - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/traversal/TreeWalker.class
07:05:20.509 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.traversal.NodeIterator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/traversal/NodeIterator.class
07:05:20.510 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredAttrNSImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredAttrNSImpl.class
07:05:20.510 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredAttrImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredAttrImpl.class
07:05:20.511 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredCDATASectionImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredCDATASectionImpl.class
07:05:20.511 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.CDATASectionImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/CDATASectionImpl.class
07:05:20.511 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.TextImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/TextImpl.class
07:05:20.512 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.CharacterData - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/CharacterData.class
07:05:20.512 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.CharacterDataImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/CharacterDataImpl.class
07:05:20.512 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredCommentImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredCommentImpl.class
07:05:20.513 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.CommentImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/CommentImpl.class
07:05:20.513 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredDocumentTypeImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredDocumentTypeImpl.class
07:05:20.513 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DocumentTypeImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DocumentTypeImpl.class
07:05:20.514 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredElementNSImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredElementNSImpl.class
07:05:20.514 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredElementImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredElementImpl.class
07:05:20.514 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredEntityImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredEntityImpl.class
07:05:20.515 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.EntityImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/EntityImpl.class
07:05:20.515 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredEntityReferenceImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredEntityReferenceImpl.class
07:05:20.515 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.EntityReferenceImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/EntityReferenceImpl.class
07:05:20.516 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredNotationImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredNotationImpl.class
07:05:20.516 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.NotationImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/NotationImpl.class
07:05:20.516 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredProcessingInstructionImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredProcessingInstructionImpl.class
07:05:20.517 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.ProcessingInstructionImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/ProcessingInstructionImpl.class
07:05:20.517 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredTextImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredTextImpl.class
07:05:20.517 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredElementDefinitionImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredElementDefinitionImpl.class
07:05:20.518 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.ElementDefinitionImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/ElementDefinitionImpl.class
07:05:20.521 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeferredDocumentImpl$RefCount - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeferredDocumentImpl$RefCount.class
07:05:20.532 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.w3c.dom.NamedNodeMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/w3c/dom/NamedNodeMap.class
07:05:20.533 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.StringIndexOutOfBoundsException
07:05:20.534 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.CharacterDataImpl$1 - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/CharacterDataImpl$1.class
07:05:20.535 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.NodeListCache - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/NodeListCache.class
07:05:20.580 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: sun.reflect.ConstructorAccessorImpl - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/sun/reflect/ConstructorAccessorImpl.class
07:05:20.582 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Iterator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Iterator.class
07:05:20.582 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Map$Entry - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Map$Entry.class
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for all properties in config...
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.reducers.bytes.per.reducer
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.createtable.group.grants
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.storeManagerType
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.aux.jars.path
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.stagingdir
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.rcfile.block.level
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.rack
07:05:20.583 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.default.partition.name
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.event.expiry.duration
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.compress
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.mode.local.auto.input.files.max
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.key.prefix
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hadoop.supports.splittable.combineinputformat
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.skewjoin.compiletime
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.smbjoin.cache.rows
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.overflow.repeated.threshold
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.log.level
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.mapfiles
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.post.hooks
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.client.socket.lifetime
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for fs.har.impl
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.variance
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.split.minsize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.quorum
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for stream.stderr.reporter.prefix
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.reduce.speculative
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.hybridgrace.memcheckfrequency
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.filter.compact.maxsize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.counters.pull.interval
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.command.whitelist
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.end.function.listeners
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.downloaded.resources.dir
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.join.emit.interval
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.zerocopy
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compute.query.using.stats
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.block.padding.tolerance
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lazysimple.extended_boolean_literal
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.error.on.empty.partition
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.orc.splits.include.file.footer
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.prewarm.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hadoop.bin.path
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.rcfile.record.buffer.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.default.rcfile.serde
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.multi.insert.move.tasks.share.dependencies
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.createtable.owner.grants
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.users.in.admin.role
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.autogen.columnalias.prefix.includefuncname
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.max.partition.factor
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.port
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.orc.cache.stripe.details.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.max.created.files
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.job.committer.task.cleanup.needed
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.prompt
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.input.dir.recursive
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.deserialization.factor
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metadata.export.location
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.log.explain.output
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.skewjoin
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.default.fileformat
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.optimized.hashtable.wbsize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.metastore.authorization.auth.reads
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.NonTransactionalRead
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.remove.identity.project
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.infer.bucket.sort.num.buckets.power.two
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.worker.threads
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exim.strict.repl.tables
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.collect.tablekeys
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.future.timeout
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.display.partition.cols.separately
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.async.exec.shutdown.timeout
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.disallow.incompatible.col.type.changes
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.max.idle.time
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.dummystats.aggregator
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.querylog.enable.plan.progress
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.mode
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.cookie.auth.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.worker.keepalive.time
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.archive.intermediate.archived
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.warehouse.dir
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hwi.listen.host
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.collect.scancols
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.max.split.size
07:05:20.584 pool-1-thread-1 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hwi.war.file
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.input.format
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.dummystats.publisher
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.cookie.is.httponly
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.uris
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.querylog.location
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.localize.resource.num.wait.attempts
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.stripe.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.querylog.plan.progress.interval
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.optimize.enable
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.job.debug.timeout
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.createtable.role.grants
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.decode.partition.name
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.partition.inherit.table.properties
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cluster.delegation.token.store.class
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.autoStartMechanismMode
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.client.port
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.cookie.max.age
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.row.index.stride
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.alias
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.exponential.backoff.slot.length
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.native.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.tez.default.queues
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compat
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.partitioner
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.smallfiles.avgsize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hbase.wal.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.entity.capture.transform
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.blockfilter.file
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.logging.operation.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lockmgr.zookeeper.default.partition.name
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.support.concurrency
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.file.max.footer
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.mode.prefix
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.print.header
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.table.type.mapping
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.event.db.listener.timetolive
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.tasklog.debug.timeout
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hashtable.loadfactor
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.filter.hook
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.local.mem
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.union.remove
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.global.init.file.location
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.client.drop.partitions.using.expressions
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.outerjoin.supports.filters
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.auto.progress
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.dynamic.partition
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.intermediate.compression.type
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.try.direct.sql
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.failure.retries
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.analyze.stmt.collect.partlevel.stats
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hbase.generatehfiles
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.join.factor
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.pre.event.listeners
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.map.fair.scheduler.queue
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.reducededuplication
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.localize.resource.wait.interval
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.filter.compact.minsize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.copyfile.maxsize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.sasl.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.manager
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.compression.strategy
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.rpc.query.plan
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.mapredfiles
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cache.expr.evaluation
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.counters.group.name
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.transactionIsolation
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.in.test
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.groupby.skewindata
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.hybridgrace.hashtable
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.clean.until
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.reliable
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.batch.retrieve.max
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.entity.separator
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.binary.record.max.length
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.max.dynamic.partitions
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.groupby.sorted
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hashtable.initialCapacity
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.check.memory.rows
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.idle.operation.timeout
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.block.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.hdfs.read
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.server.connect.timeout
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.transport.mode
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.path
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.query.max.entries
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.execute.setugi
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.split.maxsize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.bucket.cache.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.drop.ignorenonexistent
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.serdes.using.metastore.for.schema
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.mode.nosamplelist
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.sparkfiles
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exim.uri.scheme.whitelist
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.query.redactor.hooks
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.log4j.file
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.fixedDatastore
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.sasl.qop
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.job.committer.setup.cleanup.needed
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.delta.num.threshold
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.plan
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.serde
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.log4j.file
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ddl.createtablelike.properties.whitelist
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.input.fileinputformat.split.minsize.per.node
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.bucketmapjoin
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.aggr.hash.percentmemory
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.server.max.message.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.job.debug.capture.stacktraces
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cluster.delegation.token.store.zookeeper.acl
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.groupby.sorted.testmode
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.minmax.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.sample.seednumber
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.event.clean.freq
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.session.hook
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.reduce.tasks.speculative.execution
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.sortmerge.join.bigtable.selection.policy
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stageid.rearrange
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.groupby.flush.percent
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.temporary.table.storage
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.groupby.maxentries
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.optimized.hashtable
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.optimize.fetch.max
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authenticator.manager
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.client.stats.publishers
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.validateColumns
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.parallel
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.rcfile.record.interval
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.submitviachild
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fetch.task.conversion
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.udtf.auto.progress
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.archive.enabled
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.builtin.udf.whitelist
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.rpc.max.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication.spnego.principal
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.authz.sstd.hs2.mode
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.async.exec.threads
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.convert.join.bucket.mapjoin.tez
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.execution.engine
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.container.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.ConnectionPassword
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.use.SSL
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.null.scan
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.size
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.smalltable.filesize
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.session.silent
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.query.string
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.min.worker.threads
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.enforce.sorting
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.join.use.nonstaged
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.tez.sessions.per.default.queue
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.idle.session.check.operation
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.port
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.key.prefix.max.length
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.logging.operation.log.location
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.ConnectionURL
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.hybridgrace.minnumpartitions
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hmshandler.force.reload.conf
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.server.tcp.keepalive
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.semantic.analyzer.hook
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.rpc.threads
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.aggr.hash.min.reduction
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.rcfile.column.number.conf
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.cpu
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.clean.extra.nodes
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.metadataonly
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.insert.into.multilevel.dirs
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.added.archives.path
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hmshandler.retry.attempts
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.validateConstraints
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.retries.max
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.memory.pool
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.prewarm.numcontainers
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.identifierFactory
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.errors.ignore
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.multigroupby.singlereducer
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.reduce.tasks.speculative.execution
07:05:20.584 pool-1-thread-1 INFO deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.conf.restricted.list
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.sampling.orderby.number
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fetch.task.aggr
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.sortmerge.join.to.mapjoin
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.warehouse.subdir.inherit.perms
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.committer.job.setup.cleanup.needed
07:05:20.584 pool-1-thread-1 INFO deprecation: mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.txn.timeout
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.fetch.partition.stats
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.progress.timeout
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.returnpath.hiveop
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.dictionary.key.size.threshold
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.scratchdir
07:05:20.584 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.optimize.limit.file
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.server.max.threads
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.try.direct.sql.ddl
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.script.allow.partial.consumption
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.hybridgrace.minwbsize
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.namespace
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.long.polling.timeout
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.debug.localtask
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.createtable.user.grants
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server.tcp.keepalive
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.ppd
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.script.maxerrsize
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.worker.keepalive.time
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.enforce.bucketmapjoin
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.connect.timeout
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.session.id
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.allow.user.substitution
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.join.noconditionaltask
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.input.format
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.autoupdate
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ssl.protocol.blacklist
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.dynamic.partition.pruning
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.fetch.column.stats
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.max.dynamic.partitions.pernode
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.cleaner.run.interval
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.min.split.size.per.rack
07:05:20.585 pool-1-thread-1 INFO deprecation: mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.skewjoin.mapjoin.map.tasks
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.schema.verification.record.version
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapreduce.job.reduces
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.abortedtxn.threshold
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.aggr
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.support.quoted.identifiers
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.PersistenceManagerFactoryClass
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.initiator.on
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.orc.row.index.stride.dictionary.check
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.fs.handler.class
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.task.factory
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lock.numretries
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.typecheck.on.insert
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.join
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.support.dynamic.service.discovery
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.distinct.rewrite
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.authorization.storage.checks
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.skip.corrupt.data
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.cache.level2
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.builtin.udf.blacklist
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.kerberos.principal
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.rdbms.useLegacyNativeValueStrategy
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.collect.rawdatasize
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.ppd.storage
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.binary.search
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.local.fs.read
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.full
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.correlation
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.http.cookie.is.secure
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.orcfile.stripe.level
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.reorder.nway.joins
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.compress.output
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.user.install.directory
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.list.num.entries
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.sqlstd.confwhitelist.append
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.insert.into.external.tables
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.groupby.checkinterval
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.explain.dependency.append.tasktype
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.bucketingsorting
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.login.timeout
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.print.current.db
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.scratch.dir.permission
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hashtable.key.count.adjustment
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.failure.hooks
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.integral.jdo.pushdown
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.exception.handlers
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.jobname.length
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.bind.host
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.added.jars.path
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.tez.initialize.default.sessions
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.client.socket.timeout
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.DetachAllOnCommit
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.txn.max.open.batch
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.check.interval
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.query.id
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.key.prefix.reserve.length
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.ConnectionDriverName
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.reduce.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.delta.pct.threshold
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.current.database
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.orm.retrieveMapNullsAsEmptyStrings
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.max.variable.length
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.start.cleanup.scratchdir
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.rcfile.use.explicit.header
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.tezfiles
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.split.strategy
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.async.exec.keepalive.time
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.filter
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.authorization.sqlstd.confwhitelist
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.default.serde
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.listbucketing
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.connection.basesleeptime
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.ds.connection.url.hook
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.query.result.fileformat
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.partition.name.whitelist.pattern
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.map.num.entries
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.dynamic.partition.pruning.max.event.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.enable
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.constant.propagation
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.mode.local.auto
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.reducededuplication.min.reducer
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.transform.escape.input
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.max.start.attempts
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.max.worker.threads
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.dynamic.partition.mode
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.network
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.fpp
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.driver.run.hooks
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.pre.hooks
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.conf.validation
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.added.files.path
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.session.history.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.operator.id.env.var
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.unlock.numretries
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.Multithreaded
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.rework.mapredwork
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.groupby
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.client.connect.retry.delay
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.min.split.size
07:05:20.585 pool-1-thread-1 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.check.crossproducts
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server.read.socket.timeout
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.retries.wait
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.reducers.max
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fetch.task.conversion.threshold
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.row.max.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.perf.logger
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.thrift.compact.protocol.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.plugin.pluginRegistryBundleCheck
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.join.noconditionaltask.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.operator.truncate.env
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.join.cache.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.parallel.thread.number
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.validateTables
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.skewjoin.key
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.reader.wait
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.metastore.authenticator.manager
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.default.fileformat.managed
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.enforce.bucketing
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.reloadable.aux.jars.path
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.file.ignore.hdfs
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hmshandler.retry.interval
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.local.scratchdir
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.thrift.max.message.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.mode
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.buffer.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.gather.num.threads
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.pushdown.memory.usage
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.archive.intermediate.original
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.mode.local.auto.inputbytes.max
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.localtask.max.memory.usage
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.support.sql11.reserved.keywords
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.batch.retrieve.table.partition.max
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.dynamic.partition.pruning.max.data.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metadata.move.exported.metadata.to.trash
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cli.pretty.output.num.cols
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.session.timeout
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fetch.output.serde
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hbase.snapshot.restoredir
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.skewjoin.mapjoin.min.split
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.resultset.use.unique.column.names
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.zookeeper.connection.max.retries
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.partitions
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compactor.worker.timeout
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.session.check.interval
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.compute.splits.in.am
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for parquet.memory.pool.ratio
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapred.supports.subdirectories
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication.kerberos.principal
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.new.job.grouping.set.cardinality
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.client.stats.counters
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.enforce.sortmergebucketmapjoin
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.smb.number.waves
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.max.writer.wait
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ppd.recognizetransivity
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication.spnego.keytab
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.io.rcfile.tolerate.corruptions
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.secret.bits
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for stream.stderr.reporter.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.autogen.columnalias.prefix.label
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.event.listeners
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.repl.task.factory
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.int.timestamp.conversion.in.seconds
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.auto.reducer.parallelism
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.rawstore.impl
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.security.metastore.authorization.manager
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.autoCreateSchema
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.jar.path
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.mapjoin.native.multikey.only.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.orc.compute.splits.num.threads
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.limit.query.max.table.partition
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.rowoffset
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.default.publisher
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.recordwriter
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ppd.remove.duplicatefilters
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.keystore.password
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.logging.operation.level
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.variable.substitute
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.txn.manager
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.cache.level2.type
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.stats.ndv.densityfunction
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.direct.sql.batch.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.min.split.size.per.node
07:05:20.585 pool-1-thread-1 INFO deprecation: mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.jdbc.timeout
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.intermediate.compression.codec
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.server.min.threads
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.exec.print.summary
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.compress.intermediate
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.expression.proxy
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.recordreader
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.autogather
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.sort.dynamic.partition
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.init.hooks
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.dml.events
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.plan.serialization.format
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.thrift.framed.transport.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.reduce.tasks
07:05:20.585 pool-1-thread-1 INFO deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.log.every.n.records
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.heartbeat.interval
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.query.max.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.vectorized.execution.reduce.groupby.enabled
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lock.sleep.between.retries
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.test.mode.samplefreq
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.dbclass
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.jdbcdriver
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.concatenate.check.index
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for datanucleus.connectionPoolingType
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.rcfile.use.sync.cache
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.map.aggr.hash.force.flush.memory.threshold
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.cache.pinobjtypes
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.fileformat.check
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.async.exec.wait.queue.size
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.default.aggregator
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for mapred.input.dir.recursive
07:05:20.585 pool-1-thread-1 INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.explain.user
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.encoding.strategy
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.keystore.path
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.schema.verification
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.connect.retries
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cluster.delegation.token.store.zookeeper.connectString
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.infer.bucket.sort
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.aggregate.stats.cache.ttl
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.submit.local.task.via.child
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.index.compact.file
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.hwi.listen.port
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.archive.intermediate.extracted
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cluster.delegation.token.store.zookeeper.znode
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.dbconnectionstring
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.hdfs.write
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.authentication.kerberos.keytab
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.cpu.vcores
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.followby.map.aggr.hash.percentmemory
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.parquet.timestamp.skip.conversion
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.exec.inplace.progress
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lock.manager
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapper.cannot.span.multiple.partitions
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.variable.substitute.depth
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.merge.size.per.task
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.table.parameters.default
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.ignore.mapjoin.hint
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.sampling.orderby.percent
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.lock.mapred.only.operation
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.tez.min.partition.factor
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.client.rpc.sasl.mechanisms
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.orc.default.block.padding
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.sampling.orderby
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.metastore.kerberos.keytab.file
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.groupby.mapaggr.checkinterval
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.script.trust
07:05:20.585 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.mapjoin.followby.gby.localtask.max.memory.usage
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for javax.jdo.option.ConnectionUserName
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.spark.job.monitor.timeout
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.exec.show.job.failure.debug.info
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.groupby.orderby.position.alias
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.extended
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.script.operator.env.blacklist
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.cbo.costmodel.local.fs.write
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.in.tez.test
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.ndv.error
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.bucketmapjoin.sortedmerge
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.enable.doAs
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.zookeeper.namespace
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.stats.atomic
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.optimize.index.groupby
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.auto.convert.sortmerge.join
07:05:20.586 pool-1-thread-1 DEBUG Configuration: Handling deprecation for hive.server2.idle.session.timeout
07:05:20.593 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.processor.TransformerFactoryImpl - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/processor/TransformerFactoryImpl.class
07:05:20.594 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.sax.SAXTransformerFactory - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/sax/SAXTransformerFactory.class
07:05:20.594 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.TransformerException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/TransformerException.class
07:05:20.594 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.TransformerConfigurationException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/TransformerConfigurationException.class
07:05:20.594 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.sax.TransformerHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/sax/TransformerHandler.class
07:05:20.594 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.XMLFilter - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/XMLFilter.class
07:05:20.595 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.sax.TemplatesHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/sax/TemplatesHandler.class
07:05:20.595 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.ContentHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/ContentHandler.class
07:05:20.595 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.utils.DOMHelper - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xml/utils/DOMHelper.class
07:05:20.596 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.utils.DOM2Helper - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xml/utils/DOM2Helper.class
07:05:20.596 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.utils.StopParseException - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xml/utils/StopParseException.class
07:05:20.596 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.AbstractMethodError
07:05:20.596 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.parsers.FactoryConfigurationError - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/parsers/FactoryConfigurationError.class
07:05:20.597 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.Transformer - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/Transformer.class
07:05:20.597 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.transformer.TransformerIdentityImpl - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/transformer/TransformerIdentityImpl.class
07:05:20.597 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.ext.DeclHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/ext/DeclHandler.class
07:05:20.598 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.ErrorListener - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/ErrorListener.class
07:05:20.598 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.utils.DefaultErrorHandler - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xml/utils/DefaultErrorHandler.class
07:05:20.598 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.utils.SAXSourceLocator - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xml/utils/SAXSourceLocator.class
07:05:20.598 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.SourceLocator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/SourceLocator.class
07:05:20.598 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.helpers.LocatorImpl - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/helpers/LocatorImpl.class
07:05:20.599 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.PrintStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/PrintStream.class
07:05:20.599 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.ext.LexicalHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/ext/LexicalHandler.class
07:05:20.599 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.sax.SAXResult - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/sax/SAXResult.class
07:05:20.599 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.dom.DOMResult - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/dom/DOMResult.class
07:05:20.599 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.utils.WrappedRuntimeException - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xml/utils/WrappedRuntimeException.class
07:05:20.600 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.Source - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/Source.class
07:05:20.600 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.TreeWalker - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/TreeWalker.class
07:05:20.601 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.templates.OutputProperties - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/templates/OutputProperties.class
07:05:20.601 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.templates.ElemTemplateElement - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/templates/ElemTemplateElement.class
07:05:20.602 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.utils.PrefixResolver - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xml/utils/PrefixResolver.class
07:05:20.602 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xpath.ExpressionNode - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xpath/ExpressionNode.class
07:05:20.603 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xpath.WhitespaceStrippingElementMatcher - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xpath/WhitespaceStrippingElementMatcher.class
07:05:20.603 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.templates.XSLTVisitable - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/templates/XSLTVisitable.class
07:05:20.604 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.utils.UnImplNode - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xml/utils/UnImplNode.class
07:05:20.606 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.templates.StylesheetRoot - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/templates/StylesheetRoot.class
07:05:20.607 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.Templates - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/Templates.class
07:05:20.607 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.templates.StylesheetComposed - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/templates/StylesheetComposed.class
07:05:20.607 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.templates.Stylesheet - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/templates/Stylesheet.class
07:05:20.608 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ExtendedContentHandler - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ExtendedContentHandler.class
07:05:20.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.OutputPropertiesFactory - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/OutputPropertiesFactory.class
07:05:20.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.utils.WrappedRuntimeException - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/utils/WrappedRuntimeException.class
07:05:20.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.BufferedInputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/BufferedInputStream.class
07:05:20.611 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SerializerBase - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SerializerBase.class
07:05:20.611 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SerializationHandler - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SerializationHandler.class
07:05:20.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ExtendedLexicalHandler - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ExtendedLexicalHandler.class
07:05:20.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.XSLOutputAttributes - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/XSLOutputAttributes.class
07:05:20.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.DTDHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/DTDHandler.class
07:05:20.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.DOMSerializer - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/DOMSerializer.class
07:05:20.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.Serializer - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/Serializer.class
07:05:20.613 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SerializerConstants - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SerializerConstants.class
07:05:20.614 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.Attributes - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/Attributes.class
07:05:20.614 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.helpers.AttributesImpl - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/helpers/AttributesImpl.class
07:05:20.614 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.AttributesImplSerializer - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/AttributesImplSerializer.class
07:05:20.615 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.OutputPropertiesFactory$1 - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/OutputPropertiesFactory$1.class
07:05:20.616 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.stream.StreamResult - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/stream/StreamResult.class
07:05:20.616 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SerializerFactory - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SerializerFactory.class
07:05:20.616 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ObjectFactory - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ObjectFactory.class
07:05:20.617 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ObjectFactory$ConfigurationError - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ObjectFactory$ConfigurationError.class
07:05:20.618 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SecuritySupport - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SecuritySupport.class
07:05:20.618 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SecuritySupport12 - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SecuritySupport12.class
07:05:20.619 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SecuritySupport12$1 - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SecuritySupport12$1.class
07:05:20.619 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SecuritySupport12$2 - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SecuritySupport12$2.class
07:05:20.620 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SecuritySupport12$3 - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SecuritySupport12$3.class
07:05:20.620 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ToXMLStream - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ToXMLStream.class
07:05:20.620 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ToStream - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ToStream.class
07:05:20.622 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ToStream$WritertoStringBuffer - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ToStream$WritertoStringBuffer.class
07:05:20.622 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SerializerTraceWriter - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SerializerTraceWriter.class
07:05:20.623 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.WriterChain - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/WriterChain.class
07:05:20.623 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.WriterToUTF8Buffered - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/WriterToUTF8Buffered.class
07:05:20.624 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.WriterToASCI - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/WriterToASCI.class
07:05:20.624 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.OutputStreamWriter - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/OutputStreamWriter.class
07:05:20.626 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SecuritySupport12$4 - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SecuritySupport12$4.class
07:05:20.626 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ElemContext - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ElemContext.class
07:05:20.627 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.ToStream$BoolStack - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/ToStream$BoolStack.class
07:05:20.627 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.EncodingInfo - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/EncodingInfo.class
07:05:20.628 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.EncodingInfo$InEncoding - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/EncodingInfo$InEncoding.class
07:05:20.628 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.CharInfo - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/CharInfo.class
07:05:20.629 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.CharInfo$1 - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/CharInfo$1.class
07:05:20.630 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.CharInfo$CharKey - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/CharInfo$CharKey.class
07:05:20.630 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.ResourceBundle - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/ResourceBundle.class
07:05:20.630 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.XMLEntities - null
07:05:20.631 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.XMLEntities_en - null
07:05:20.631 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.XMLEntities_en_US - null
07:05:20.632 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.NamespaceMappings - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/NamespaceMappings.class
07:05:20.632 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.NamespaceMappings$Stack - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/NamespaceMappings$Stack.class
07:05:20.633 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.NamespaceMappings$MappingRecord - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/NamespaceMappings$MappingRecord.class
07:05:20.633 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.Encodings - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/Encodings.class
07:05:20.634 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.MalformedURLException
07:05:20.634 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.SecuritySupport12$6 - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/SecuritySupport12$6.class
07:05:20.635 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.StringTokenizer - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/StringTokenizer.class
07:05:20.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.stream.StreamSource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/stream/StreamSource.class
07:05:20.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.sax.SAXSource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/sax/SAXSource.class
07:05:20.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.transform.dom.DOMSource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/transform/dom/DOMSource.class
07:05:20.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.utils.DOM2Helper - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/utils/DOM2Helper.class
07:05:20.637 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.AttributeMap - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/AttributeMap.class
07:05:20.637 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.NamedNodeMapImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/NamedNodeMapImpl.class
07:05:20.640 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xml.serializer.utils.AttList - jar:file:/Users/lian/.ivy2/cache/xalan/serializer/jars/serializer-2.7.1.jar!/org/apache/xml/serializer/utils/AttList.class
07:05:20.640 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xalan.transformer.SerializerSwitcher - jar:file:/Users/lian/.ivy2/cache/xalan/xalan/jars/xalan-2.7.1.jar!/org/apache/xalan/transformer/SerializerSwitcher.class
07:05:20.656 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.LoopingByteArrayInputStream$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/LoopingByteArrayInputStream$1.class
07:05:20.678 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.ByteArrayInputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/ByteArrayInputStream.class
07:05:20.684 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.conf.HiveConfUtil - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/conf/HiveConfUtil.class
07:05:20.685 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.base.Joiner
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$2 - 325769008
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.Serializable
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.AbstractFunction1
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.Tuple2
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$2$$anonfun$apply$2 - -549050405
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.AbstractFunction0
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.StringContext
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.Predef$
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.cookie.is.httponly=true
07:05:20.686 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.BoxedUnit
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: nfs3.mountd.port=4242
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.healthchecker.script.timeout=600000
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.input.dir.recursive=false
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.orc.compute.splits.num.threads=10
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.classloader.system.classes=java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3n.awsSecretAccessKey=lkJm1rO2asxYC6VQtuSma3SlElf3Ywiz9r7n8XgU
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.sortmerge.join.to.mapjoin=false
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.reduce.groupby.enabled=true
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.max.partition.factor=2.0
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.framework.name=local
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.script.maxerrsize=100000
07:05:20.686 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.txns=1000000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: tfile.fs.output.buffer.size=262144
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.job.task.listener.thread-count=30
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3a.secret.key=lkJm1rO2asxYC6VQtuSma3SlElf3Ywiz9r7n8XgU
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.local.dir.minspacekill=0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.support.concurrency=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.block.size=67108864
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.repl.task.factory=org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.retries.max=0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.hdfs.configuration.version=1
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.bytes-per-checksum=512
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.buffer.dir=${hadoop.tmp.dir}/s3
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.acl-view-job=
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.typecheck.on.insert=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.loadedjobs.cache.size=5
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.persist.jobstatus.hours=1
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.hybridgrace.memcheckfrequency=1024
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.unlock.numretries=10
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.handler.count=10
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.copyfile.maxsize=33554432
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.split.minsize=1
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.plan.serialization.format=kryo
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.failed.volumes.tolerated=0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.amliveliness-monitor.interval-ms=1000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.client.thread-count=50
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.seqfile.compress.blocksize=1000000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.http.threads=40
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.explain.dependency.append.tasktype=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.retrycache.expirytime.millis=600000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.backup.address=0.0.0.0:50100
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hwi.listen.host=0.0.0.0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.replication=3
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.jobhistory.block.size=3145728
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.profile.maps=0-2
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.reliable=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.global.init.file.location=${env:HIVE_CONF_DIR}
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.io.rcfile.record.buffer.size=4194304
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.retiredjobs.cache.size=1000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.ppd=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.am.max-attempts=2
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.print.current.db=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.trash.checkpoint.interval=0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.check.period=60
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.compress.intermediate=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.rdbms.datastoreAdapterClassName=org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.container-monitor.interval-ms=3000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hashtable.loadfactor=0.75
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.test.mode.samplefreq=32
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.jetty.logs.serve.aliases=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.groupby.checkinterval=100000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.max-completed-applications=10000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.skip.proc.count.autoincr=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.identifierFactory=datanucleus1
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.decode.partition.name=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hbase.generatehfiles=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.fallback-to-simple-auth-allowed=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.localize.resource.wait.interval=5000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.file.max.footer=100
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.fs-limits.max-component-length=0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.DetachAllOnCommit=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.constant.propagation=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.health-monitor.check-interval.ms=1000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.prompt=hive
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.check.crossproducts=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.connect.timeout=1000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.explain.user=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.tasktracker.maxblacklists=4
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapred.child.java.opts=-Xmx200m
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.common.configuration.version=0.23.0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.remote-app-log-dir-suffix=logs
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.concatenate.check.index=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.blocksize=67108864
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.convert.join.bucket.mapjoin.tez=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hmshandler.retry.interval=2000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.binary.record.max.length=1000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.profile.reduces=0-2
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.gather.num.threads=10
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.address=0.0.0.0:50010
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.https.server.keystore.resource=ssl-server.xml
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.job.debug.timeout=30000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.script.allow.partial.consumption=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.memory.pool=0.5
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.rpc.threads=8
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.row.index.stride=10000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.submit.local.task.via.child=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.NonTransactionalRead=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.aggr.hash.min.reduction=0.5
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.test.authz.sstd.hs2.mode=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.timeout=600000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.entity.separator=@
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.insert.into.external.tables=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.connect.max-wait.ms=900000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.defaultFS=file:///
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.transactionIsolation=read-committed
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.compression.codec.bzip2.library=system-native
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.audit.loggers=default
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.warehouse.subdir.inherit.perms=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.sample.seednumber=0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.split.strategy=HYBRID
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.default.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.block.access.key.update.interval=600
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.authentication=NONE
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.block.padding=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.full=0.9
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.returnpath.hiveop=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.max.objects=0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: stream.stderr.reporter.prefix=reporter:
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.address=0.0.0.0:10020
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.address=${yarn.nodemanager.hostname}:0
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.dbconnectionstring=jdbc:derby:;databaseName=TempStatsStore;create=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.smallfiles.avgsize=16000000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.am.max-attempts=2
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.rcfile.use.explicit.header=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.orm.retrieveMapNullsAsEmptyStrings=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.clean.extra.nodes=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hbase.snapshot.restoredir=/tmp
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authorization.sqlstd.confwhitelist=hive\.auto\..*|hive\.cbo\..*|hive\.convert\..*|hive\.exec\.dynamic\.partition.*|hive\.exec\..*\.dynamic\.partitions\..*|hive\.exec\.compress\..*|hive\.exec\.infer\..*|hive\.exec\.mode.local\..*|hive\.exec\.orc\..*|hive\.exec\.parallel.*|hive\.explain\..*|hive\.fetch.task\..*|hive\.groupby\..*|hive\.hbase\..*|hive\.index\..*|hive\.index\..*|hive\.intermediate\..*|hive\.join\..*|hive\.limit\..*|hive\.log\..*|hive\.mapjoin\..*|hive\.merge\..*|hive\.optimize\..*|hive\.orc\..*|hive\.outerjoin\..*|hive\.parquet\..*|hive\.ppd\..*|hive\.prewarm\..*|hive\.server2\.proxy\.user|hive\.skewjoin\..*|hive\.smbjoin\..*|hive\.stats\..*|hive\.tez\..*|hive\.vectorized\..*|mapred\.map\..*|mapred\.reduce\..*|mapred\.output\.compression\.codec|mapred\.job\.queuename|mapred\.output\.compression\.type|mapred\.min\.split\.size|mapreduce\.job\.reduce\.slowstart\.completedmaps|mapreduce\.job\.queuename|mapreduce\.job\.tags|mapreduce\.input\.fileinputformat\.split\.minsize|mapreduce\.map\..*|mapreduce\.reduce\..*|mapreduce\.output\.fileoutputformat\.compress\.codec|mapreduce\.output\.fileoutputformat\.compress\.type|tez\.am\..*|tez\.task\..*|tez\.runtime\..*|tez.queue.name|hive\.exec\.reducers\.bytes\.per\.reducer|hive\.client\.stats\.counters|hive\.exec\.default\.partition\.name|hive\.exec\.drop\.ignorenonexistent|hive\.counters\.group\.name|hive\.default\.fileformat\.managed|hive\.enforce\.bucketing|hive\.enforce\.bucketmapjoin|hive\.enforce\.sorting|hive\.enforce\.sortmergebucketmapjoin|hive\.cache\.expr\.evaluation|hive\.hashtable\.loadfactor|hive\.hashtable\.initialCapacity|hive\.ignore\.mapjoin\.hint|hive\.limit\.row\.max\.size|hive\.mapred\.mode|hive\.map\.aggr|hive\.compute\.query\.using\.stats|hive\.exec\.rowoffset|hive\.variable\.substitute|hive\.variable\.substitute\.depth|hive\.autogen\.columnalias\.prefix\.includefuncname|hive\.autogen\.columnalias\.prefix\.label|hive\.exec\.check\.crossproducts|hive\.compat|hive\.exec\.concatenate\.check\.index|hive\.display\.partition\.cols\.separately|hive\.error\.on\.empty\.partition|hive\.execution\.engine|hive\.exim\.uri\.scheme\.whitelist|hive\.file\.max\.footer|hive\.mapred\.supports\.subdirectories|hive\.insert\.into\.multilevel\.dirs|hive\.localize\.resource\.num\.wait\.attempts|hive\.multi\.insert\.move\.tasks\.share\.dependencies|hive\.support\.quoted\.identifiers|hive\.resultset\.use\.unique\.column\.names|hive\.analyze\.stmt\.collect\.partlevel\.stats|hive\.server2\.logging\.operation\.level|hive\.support\.sql11\.reserved\.keywords|hive\.exec\.job\.debug\.capture\.stacktraces|hive\.exec\.job\.debug\.timeout|hive\.exec\.max\.created\.files|hive\.exec\.reducers\.max|hive\.reorder\.nway\.joins|hive\.output\.file\.extension|hive\.exec\.show\.job\.failure\.debug\.info|hive\.exec\.tasklog\.debug\.timeout
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.reducers.bytes.per.reducer=256000000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.start.cleanup.scratchdir=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.drop.cache.behind.reads=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.auto.reducer.parallelism=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.size=10000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.cache.level2=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.try.direct.sql=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.bucket.cache.size=100
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateColumns=false
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.block.write.replace-datanode-on-failure.enable=true
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.bytes-per-checksum=512
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.resource.memory-mb=8192
07:05:20.687 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.local.fs.read=4.0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.sampling.orderby=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.heartbeat.interval=3
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.joblist.cache.size=20000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hmshandler.force.reload.conf=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.ftp.host=0.0.0.0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.ha.tail-edits.period=60
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.dynamic.partition=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hwi.war.file=${env:HWI_WAR_FILE}
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.resultset.use.unique.column.names=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.expire.trackers.interval=600000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.default.fileformat.managed=none
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.ha.fencing.ssh.connect-timeout=30000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.log-aggregation-enable=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.markreset.buffer.percent=0.0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapper.cannot.span.multiple.partitions=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.future.timeout=60
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.edits.noeditlogchannelflush=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.support.append=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.io.sort.factor=10
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.outofband.heartbeat=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.failover-controller.new-active.rpc-timeout.ms=60000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.datestring.cache.size=200000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.acl-modify-job=
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.https-address=0.0.0.0:50470
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.ppd.remove.duplicatefilters=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.job.committer.commit-window=10000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.container-manager.thread-count=20
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.integral.jdo.pushdown=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.zookeeper.session-timeout.ms=5000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: tfile.io.chunk.size=1048576
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.speculative.slowtaskthreshold=1.0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.initiator.on=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.directory.search.timeout=10000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.ha.automatic-failover.enabled=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.warehouse.dir=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.decommission.interval=30
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.local-cache.max-files-per-directory=8192
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.direct.sql.batch.size=0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.handler.count=10
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.client.drop.partitions.using.expressions=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.safemode.threshold-pct=0.999f
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.batch.retrieve.table.partition.max=1000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.sort.spill.percent=0.80
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.metadataonly=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.sync.behind.writes=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.ttl=600
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.stale.datanode.interval=30000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.ifile.readahead=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.hybridgrace.minwbsize=524288
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.max.dynamic.partitions.pernode=100
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.orc.splits.include.file.footer=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.smbjoin.cache.rows=10000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.display.partition.cols.separately=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.groupby.sorted=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.resourcemanager.connect.wait.secs=900
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.dynamic.partition.pruning=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.enabled=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.command-opts=-Xmx1024m
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.outerjoin.supports.filters=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.logging.operation.enabled=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.cluster.local.dir=${hadoop.tmp.dir}/mapred/local
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.mapfile.bloom.error.rate=0.005
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.bucketmapjoin=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.replication=3
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.uid.cache.secs=14400
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.maxtaskfailures.per.tracker=3
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.enforce.bucketmapjoin=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.skip.checksum.errors=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.server.max.message.size=104857600
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.directoryscan.interval=21600
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: nfs3.server.port=2049
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.dbclass=fs
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.write.stale.datanode.ratio=0.5f
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.hdfs.write=10.0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.user.install.directory=hdfs:///user/
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.connection.max.retries=3
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.downloaded.resources.dir=/Users/lian/local/src/spark/workspace-d/target/tmp/${hive.session.id}_resources
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.taskcache.levels=2
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.http.policy=HTTP_ONLY
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.cpu.vcores=-1
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.token.validity=36000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.writer.wait=5000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.shuffle.max.connections=0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.enforce.sortmergebucketmapjoin=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.thrift.compact.protocol.enabled=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.dictionary.key.size.threshold=0.8
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.mode.local.auto.inputbytes.max=134217728
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.move.thread-count=3
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.admin.client.thread-count=1
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.persist.jobstatus.active=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.sparkfiles=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.txn.max.open.batch=1000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.uris=
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.expression.proxy=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.io.rcfile.tolerate.corruptions=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.shuffle.port=13562
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.hybridgrace.hashtable=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.job.debug.capture.stacktraces=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.log.explain.output=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.health-checker.interval-ms=600000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.report.address=127.0.0.1:0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateTables=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.in.test=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.seqfile.lazydecompress=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.blocksize=67108864
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.smb.number.waves=0.5
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.backup.http-address=0.0.0.0:50105
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.groupby.sorted.testmode=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.distinct.rewrite=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.rpc.max.size=52428800
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.delete.debug-delay-sec=0
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.filter.hook=org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.http.address=0.0.0.0:50030
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.enable.doAs=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.pmem-check-enabled=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server.read.socket.timeout=10
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.tez.sessions.per.default.queue=1
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.groups.cache.secs=300
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.autoupdate=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.variance=0.01
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.server.tcpnodelay=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.authorization=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.complete.cancel.delegation.tokens=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.http.policy=HTTP_ONLY
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.dns.interface=default
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.replication=3
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.ssl=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.rdbms.useLegacyNativeValueStrategy=true
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.max.worker.threads=500
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.dynamic.partition.pruning.max.event.size=1048576
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.key.prefix.reserve.length=24
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.server.max.threads=1000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.secret.bits=256
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.insert.into.multilevel.dirs=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.reducers.max=1009
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.parallel.thread.number=8
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.query.max.table.partition=-1
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.bin.path=/Users/lian/local/opt/hadoop/bin/hadoop
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.zookeeper.parent-znode=/hadoop-ha
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.safemode.extension=30000
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.transport.mode=binary
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.merge.percent=0.66
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.blocksize=134217728
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.schema.verification=false
07:05:20.688 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.localize.resource.num.wait.attempts=5
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.idle.operation.timeout=432000000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.autogen.columnalias.prefix.label=_c
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.admin.acl=*
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compat=0.12
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.compression.strategy=SPEED
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.jdbc.timeout=30
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.skip.maxgroups=0
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.connect.timeout=180000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.shuffle.ssl.file.buffer.size=65536
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.tez.initialize.default.sessions=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.journalnode.http-address=0.0.0.0:8480
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.enabled=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.transform.escape.input=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.end-notification.max.attempts=5
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.jobname.length=50
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.cleaner.run.interval=5000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.edits.dir=${dfs.namenode.name.dir}
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.health-monitor.connect-retry-interval.ms=1000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.batch.retrieve.max=300
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.keytab=/etc/krb5.keytab
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.support.allow.format=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.connect.retries=3
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.event.db.listener.timetolive=86400
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.authorization.storage.checks=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.async.exec.keepalive.time=10
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.input.buffer.percent=0.70
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.replication=3
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.try.direct.sql.ddl=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.rowoffset=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.long.polling.timeout=5000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.connection.retries=0
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.allow.user.substitution=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.querylog.enable.plan.progress=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.ppd.recognizetransivity=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.tmp.dir=/tmp/hadoop-${user.name}
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.maps=2
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.index.compact.query.max.size=10737418240
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.secondary.http-address=0.0.0.0:50090
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.end-notification.max.retry.interval=5000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.log-aggregation.retain-check-interval-seconds=-1
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.resource-tracker.client.thread-count=50
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.collect.rawdatasize=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.ipc.serializer.type=protocolbuffers
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.hybridgrace.minnumpartitions=16
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.test.mode=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.seqfile.local.dir=${hadoop.tmp.dir}/io/local
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.list.num.entries=10
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.client.submit.file.replication=10
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.minicluster.fixed.ports=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.print.header=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.counters.group.name=HIVE
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.data.dir.perm=700
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.session.history.enabled=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/name
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.rpc.sasl.mechanisms=DIGEST-MD5
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.namespace=hive_zookeeper_namespace
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.idlethreshold=4000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lazysimple.extended_boolean_literal=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.input.buffer.percent=0.0
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cluster.delegation.token.store.zookeeper.znode=/hivedelegation
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.ftp.host.port=21
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.num.checkpoints.retained=2
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.dml.events=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.client-write-packet-size=65536
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.async.exec.wait.queue.size=100
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.localtask.max.memory.usage=0.9
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.file-block-storage-locations.timeout=60
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.kill.max=10
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.session.silent=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.speculative=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.temporary.table.storage=default
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compute.splits.in.am=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.disk-health-checker.interval-ms=120000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.archive.enabled=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.connection.maxidletime=10000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.io.sort.mb=100
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.client.thread-count=5
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hashtable.key.count.adjustment=1.0
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.max-retries=3
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.in.tez.test=false
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.connectionPoolingType=BONECP
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.delegation.token.max-lifetime=604800000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.process-kill-wait.ms=2000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.num.extra.edits.retained=1000000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.hdfs-servers=${fs.defaultFS}
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.ignore.mapjoin.hint=true
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.df.interval=60000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.groupby.mapaggr.checkinterval=100000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.sleepTimeSeconds=10
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.file.buffer.size=65536
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.tasklog.debug.timeout=20000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.check.memory.rows=100000
07:05:20.689 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.collect.scancols=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.groupby=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.work.around.non.threadsafe.getpwuid=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.permissions.superusergroup=supergroup
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.search.attr.member=member
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.dns.interface=default
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.groupby.maxentries=1000000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fetch.task.conversion.threshold=1073741824
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.permissions.enabled=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.connect.retry-interval.ms=30000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.ssl.protocol.blacklist=SSLv2,SSLv3
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.scheduler.minimum-allocation-mb=1024
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.read.timeout=180000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.https.address=0.0.0.0:50475
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.mode.local.auto.input.files.max=4
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.support.sql11.reserved.keywords=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.abortedtxn.threshold=1000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.execute.setugi=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.querylog.plan.progress.interval=60000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.io.rcfile.column.number.conf=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.require.client.cert=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.kerberos.kinit.command=kinit
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.fuse.connection.timeout=300
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.worker.keepalive.time=60
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.log.level=INFO
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.hdfs.read=1.5
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.ubertask.enable=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.skewjoin.mapjoin.min.split=33554432
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.rcfile.block.level=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.prewarm.enabled=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.vmem-pmem-ratio=2.1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.metastore.authorization.auth.reads=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.rpc.protection=authentication
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.health-monitor.rpc-timeout.ms=45000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.stream-buffer-size=4096
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.remote-app-log-dir=/tmp/logs
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.check.interval=300
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.enabled=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3a.access.key=AKIAJTMRDVCTJGAZZECQ
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.instrumentation.requires.admin=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.delete.thread-count=4
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.txn.timeout=300
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.balance.bandwidthPerSec=1048576
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.udtf.auto.progress=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.name.dir.restore=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.sleep.max.millis=15000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.blocksize=67108864
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.script.trust=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.sasl.enabled=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionUserName=APP
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.scratch.dir.permission=700
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.map.index.interval=128
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.login.timeout=20
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.counters.max=120
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.event.clean.freq=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.reducededuplication.min.reducer=4
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.move.interval-ms=180000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authorization.task.factory=org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.operator.truncate.env=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.orcfile.stripe.level=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.parquet.timestamp.skip.conversion=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server.tcp.keepalive=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.fetch.thread-count=4
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.client.thread-count=50
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.querylog.location=/Users/lian/local/src/spark/workspace-d/target/tmp/lian
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.mode.local.auto=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.hostname.verifier=DEFAULT
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.classloader=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.enforce.sorting=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.connect.timeout=20000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.stream-buffer-size=4096
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.nm.liveness-monitor.interval-ms=1000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nm.liveness-monitor.expiry-interval-ms=600000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.bytes-per-checksum=512
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.client.connect.retry.delay=1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.pushdown.memory.usage=-1.0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.path=cliservice
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.aggr.hash.percentmemory=0.5
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.fs-limits.max-directory-items=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.delta.pct.threshold=0.1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.smalltable.filesize=25000000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.delegation.token.renew-interval=86400000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.address=local
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.end-notification.retry.interval=1000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: parquet.memory.pool.ratio=0.5
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.delta.num.threshold=10
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.command.whitelist=set,reset,dfs,add,list,delete,reload,compile
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.bytes-per-checksum=512
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.period=3600
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hadoop.supports.splittable.combineinputformat=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.new.job.grouping.set.cardinality=30
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.block.padding.tolerance=0.05
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: net.topology.script.number.args=100
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.merge.progress.records=10000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.skewjoin.key=100000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.optimize.limit.file=10
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cache.expr.evaluation=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.deserialization.factor=1.0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.groupby=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.client.max-nodemanagers-proxies=500
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.session.check.interval=21600000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.index.compact.query.max.entries=10000000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.optimized.hashtable.wbsize=10485760
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.skip.maxrecords=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.decommission.nodes.per.interval=5
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.show.job.failure.debug.info=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.thrift.framed.transport.enabled=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.port=10001
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.handler.count=10
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.type=simple
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.taskscheduler=org.apache.hadoop.mapred.JobQueueTaskScheduler
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.jvm.numtasks=1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.dynamic.partition.pruning.max.data.size=104857600
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.mapredfiles=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.userlog.limit.kb=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.monitor.enable=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3n.block.size=67108864
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.connect.max.retries=10
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.sampling.orderby.percent=0.1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.orc.cache.stripe.details.size=10000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.split.minsize.per.rack=1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.http-address=0.0.0.0:50070
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.clean.until=0.8
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.aggr.hash.force.flush.memory.threshold=0.9
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hwi.listen.port=9999
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.logging.operation.level=EXECUTION
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.infer.bucket.sort.num.buckets.power.two=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.jobhistory.lru.cache.size=5
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.directoryscan.threads=1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.fs-limits.max-blocks-per-file=1048576
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.sampling.orderby.number=1000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lock.mapred.only.operation=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.local.dir.minspacestart=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.cpu.vcores=1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metadata.move.exported.metadata.to.trash=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.filter.compact.maxsize=-1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.exec.inplace.progress=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.client.port=2181
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.log.every.n.records=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.idle.session.check.operation=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.listbucketing=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.ppd.storage=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.parallel=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.native.enabled=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.ha.log-roll.period=120
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.sleep.base.millis=500
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hashtable.initialCapacity=100000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.groupby.skewindata=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.util.hash.type=murmur
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.native.multikey.only.enabled=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.storeManagerType=rdbms
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.accesstime.precision=3600000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.buffer.size=262144
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.retry-delay.max.ms=60000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.job.monitor.timeout=60
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.event.expiry.duration=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.port=10000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.worker.timeout=86400
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.ndv.error=20.0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.enforce.bucketing=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.map.fair.scheduler.queue=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.connection.basesleeptime=1000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.error.on.empty.partition=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.cleaner.interval-ms=86400000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.errors.ignore=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.output.fileoutputformat.compress=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.entity.capture.transform=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.server.conf=ssl-server.xml
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.skip.corrupt.data=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.https.keystore.resource=ssl-client.xml
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.client.completion.pollinterval=5000
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.min.worker.threads=5
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.resource.cpu-vcores=1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.acl.enable=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.speculative.speculativecap=0.1
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.drop.ignorenonexistent=true
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.map.tasks.maximum=2
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.profile=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.webhdfs.enabled=false
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.client.socket.lifetime=0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.local.fs.write=4.0
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.cpu=0.000001
07:05:20.690 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.local.mem=0
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.awsSecretAccessKey=lkJm1rO2asxYC6VQtuSma3SlElf3Ywiz9r7n8XgU
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.zookeeper.namespace=hiveserver2
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.client.thread-count=10
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: tfile.fs.input.buffer.size=262144
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hbase.wal.enabled=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.client.progressmonitor.pollinterval=1000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.seqfile.sorter.recordlimit=1000000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.blockreport.initialDelay=0
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.automatic.close=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.replication.min=1
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.sortmerge.join=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lock.numretries=100
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: stream.stderr.reporter.enabled=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.hostname=0.0.0.0
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.stream-buffer-size=4096
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.optimize.fetch.max=50000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.map.num.entries=10
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.join.noconditionaltask=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.replication.work.multiplier.per.iteration=2
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.ubertask.maxmaps=9
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.avoid.write.stale.datanode=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.files.preserve.failedtasks=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.async.exec.shutdown.timeout=10
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.reduce.enabled=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.failover-controller.graceful-fence.connection.retries=1
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.max.dynamic.partitions=1000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.multi.insert.move.tasks.share.dependencies=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.worker.keepalive.time=60
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.join.cache.size=25000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.cpu.vcores=1
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.worker.threads=0
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.client.resolve.remote.symlinks=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.index.compact.binary.search=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.restart.recover=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.server.tcp.keepalive=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.optimized.hashtable=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.fetch.partition.stats=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.reduce.tasks.maximum=2
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.split.maxsize=256000000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.filter=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.reduce.tasks.speculative.execution=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.join.factor=1.1
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.max.start.attempts=30
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: net.topology.impl=org.apache.hadoop.net.NetworkTopology
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.map.index.skip=0
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.remove.identity.project=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.safemode.min.datanodes=0
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.userlog.retain.hours=24
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.scheduler.maximum-allocation-vcores=32
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.log-aggregation.compression-type=none
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.enable.retrycache=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.maxattempts=4
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.committer.setup.cleanup.needed=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.readahead.bytes=4193404
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.heartbeats.in.second=100
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.conf.validation=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.submitviachild=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.extended=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.token.tracking.ids.enabled=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.analyze.stmt.collect.partlevel.stats=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.autoStartMechanismMode=checked
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.tmp.dir=./tmp
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.enable=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.delegation.key.update-interval=86400000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.server.min.threads=200
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.rcfile.use.sync.cache=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.file-block-storage-locations.num-threads=10
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.healthchecker.interval=60000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.block.size=268435456
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.simple.anonymous.allowed=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fetch.task.conversion=more
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.autogen.columnalias.prefix.includefuncname=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.rework.mapredwork=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.avoid.read.stale.datanode=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.health-checker.script.timeout-ms=1200000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.cookie.auth.enabled=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compute.query.using.stats=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.logging.operation.log.location=/Users/lian/local/src/spark/workspace-d/target/tmp/lian/operation_logs
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.log.level=INFO
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.cookie.max.age=86400
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.output.fileoutputformat.compress.type=RECORD
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.table.type.mapping=CLASSIC
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.log.level=INFO
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.ifile.readahead.bytes=4194304
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.blockreport.intervalMsec=21600000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.cluster.acls.enabled=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.test.mode.prefix=test_
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.zerocopy=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.stream-buffer-size=4096
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.speculative=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.jdbcdriver=org.apache.derby.jdbc.EmbeddedDriver
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.size.per.task=256000000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.use.datanode.hostname=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.stagingdir=.hive-staging
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.min.partition.factor=0.25
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.fs-limits.min-block-size=1048576
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.linux-container-executor.cgroups.mount=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.prewarm.numcontainers=10
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.reduce.slowstart.completedmaps=0.05
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.local.scratchdir=/Users/lian/local/src/spark/workspace-d/target/tmp/lian
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.search.attr.group.name=cn
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.skewjoin=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.invalidate.work.pct.per.iteration=0.32f
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.pretty.output.num.cols=-1
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.index.compact.file.ignore.hdfs=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.replication.max=512
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.hostname=0.0.0.0
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fileformat.check=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.skip.start.attempts=2
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.io.rcfile.record.interval=2147483647
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.heartbeat.interval=1000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.reader.wait=1000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.cache.level2.type=none
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.tcpnodelay=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.application-tokens.master-key-rolling-interval-secs=86400
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.optimize.enable=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.maxRetries=4
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exim.strict.repl.tables=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.fixedDatastore=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.default.chunk.view.size=32768
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8/metastore;create=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.connect.max.retries.on.timeouts=45
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.split.metainfo.maxsize=10000000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.am.liveness-monitor.expiry-interval-ms=600000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.progress.timeout=0
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.awsAccessKeyId=AKIAJTMRDVCTJGAZZECQ
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.image.compress=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.application.classpath=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,$HADOOP_COMMON_HOME/share/hadoop/common/lib/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,$HADOOP_YARN_HOME/share/hadoop/yarn/*,$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.http.address=0.0.0.0:50060
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.fetch.column.stats=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.fuse.timer.period=5
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.stats.ndv.densityfunction=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.max.idle.time=1800000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.hdfs-blocks-metadata.enabled=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lock.sleep.between.retries=60
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.max.transfer.threads=4096
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.bucketingsorting=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.client.output.filter=FAILED
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.support.dynamic.service.discovery=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stageid.rearrange=none
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.client-write-packet-size=65536
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.admin.user.env=LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.key.prefix.max.length=150
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.replication=1
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.max.variable.length=100
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.resourcemanager.connect.retry_interval.secs=30
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.supports.subdirectories=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.log.retain-seconds=10800
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.plugin.pluginRegistryBundleCheck=LOG
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.collect.tablekeys=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.max.created.files=100000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.keytab=/etc/krb5.keytab
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.merge.inmem.threshold=1000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.https.need-auth=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.minmax.enabled=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.shuffle.ssl.enabled=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.orc.row.index.stride.dictionary.check=true
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.join.emit.interval=1000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.block.access.token.enable=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.partitions=10000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.groupby.orderby.position.alias=false
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.skewjoin.mapjoin.map.tasks=10000
07:05:20.691 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.groupby.flush.percent=0.1
07:05:20.692 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$2$$anonfun$apply$1 - 1435826786
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionPassword=xxx
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.multigroupby.singlereducer=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateConstraints=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.https.enable=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.log-aggregation.retain-seconds=-1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.retries.wait=3000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.session.timeout=1200000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.replication.considerLoad=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.max-age-ms=604800000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.retrycache.heap.percent=0.03f
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.webapp.address=0.0.0.0:19888
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.join.noconditionaltask.size=10000000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.journalnode.rpc-address=0.0.0.0:8485
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.reorder.nway.joins=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3n.awsAccessKeyId=AKIAJTMRDVCTJGAZZECQ
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.parallelcopies=5
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.trash.interval=0
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.replication.interval=3
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.mode=nonstrict
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.client.max-retries=3
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.speculative.slownodethreshold=1.0
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.authentication=simple
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.reducededuplication=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.du.reserved=0
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.union.remove=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.stripe.size=67108864
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.resource.mb=1536
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hmshandler.retry.attempts=10
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.mapfile.bloom.size=1048576
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.resource.cpu-vcores=8
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.reduces=-1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.scheduler.minimum-allocation-vcores=1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.http.address=0.0.0.0:50075
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.client.conf=ssl-client.xml
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.int.timestamp.conversion.in.seconds=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.variable.substitute=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.queuename=default
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.health-monitor.sleep-after-disconnect.ms=1000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.bytes-per-checksum=512
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.server.connect.timeout=90000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fetch.task.aggr=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.skewjoin.compiletime=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.join.use.nonstaged=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.encrypt.data.transfer=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.compress=ZLIB
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.staticuser.user=dr.who
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.variable.substitute.depth=40
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.default.fileformat=TextFile
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.memory.limit.percent=0.25
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.client-write-packet-size=65536
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.container.size=-1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.output.compress=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.zookeeper.acl=world:anyone:rwcda
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.committer.task.cleanup.needed=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.use.SSL=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.schema.verification.record.version=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.network=150.0
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.skip.proc.count.autoincr=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.scheduler.maximum-allocation-mb=8192
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.autoCreateSchema=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.blocksize=67108864
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.max.message.size=104857600
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.query.result.fileformat=TextFile
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.failure.retries=1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.sort.dynamic.partition=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.cleaner.enable=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.idle.session.timeout=604800000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.counters.pull.interval=1000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.use.datanode.hostname=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.stream-buffer-size=4096
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.disallow.incompatible.col.type.changes=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.split.minsize.per.node=1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.null.scan=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.client.nodemanager-client-async.thread-pool-max-size=500
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.maxattempts=4
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.async.exec.threads=100
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.drop.cache.behind.writes=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.dns.nameserver=default
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.scratchdir=file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.support.quoted.identifiers=column
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.sleep-delay-before-sigkill.ms=250
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.end-notification.retry.attempts=0
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.dynamic.partition.mode=strict
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exim.uri.scheme.whitelist=hdfs,pfile
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.atomic=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.row.max.size=100000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.max.attempts=15
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.indexcache.mb=10
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.cache.target-size-mb=10240
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.Multithreaded=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.compress.output=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.bucketmapjoin.sortedmerge=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.filter.compact.minsize=5368709120
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.fpp=0.01
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.cookie.is.secure=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.client-write-packet-size=65536
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.join=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.maxtasks.perjob=-1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.block.access.token.lifetime=600
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.max.extra.edits.segments.retained=10000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.sasl.qop=auth
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.correlation=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.rpc.query.plan=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.image.transfer.bandwidthPerSec=0
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.native.lib.available=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.exec.print.summary=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.server.listen.queue.size=128
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.client.socket.timeout=600
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.client.app-submission.poll-interval=1000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.debug.localtask=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.tezfiles=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: map.sort.class=org.apache.hadoop.util.QuickSort
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.auto.progress=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.permissions.umask-mode=022
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.ipc.address=0.0.0.0:50020
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.vmem-check-enabled=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.exponential.backoff.slot.length=100
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.client-am.ipc.max-retries=3
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.mapfiles=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.block.write.retries=3
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.ubertask.maxreduces=1
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.encoding.strategy=SPEED
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.logging.level=info
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authorization.enabled=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.failover-controller.cli-check.rpc-timeout.ms=20000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client-write-packet-size=65536
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.dns.nameserver=default
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.autogather=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.aggr=true
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.image.transfer.timeout=600000
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.recovery.enabled=false
07:05:20.692 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.infer.bucket.sort=false
07:05:20.693 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.execution.engine=mr
07:05:20.693 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240
07:05:20.693 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.connection.retries.on.timeouts=0
07:05:20.693 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.HashSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/HashSet.class
07:05:20.693 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Registry - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Registry.class
07:05:20.693 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.regex.PatternSyntaxException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/regex/PatternSyntaxException.class
07:05:20.694 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.udf.generic.GenericUDF - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/udf/generic/GenericUDF.class
07:05:20.694 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.Closeable - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/Closeable.class
07:05:20.694 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.class
07:05:20.695 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFResolver.class
07:05:20.695 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.FunctionInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/FunctionInfo.class
07:05:20.695 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.WindowFunctionInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/WindowFunctionInfo.class
07:05:20.696 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMacro.class
07:05:20.696 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.parse.SemanticException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/parse/SemanticException.class
07:05:20.697 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.udf.generic.GenericUDAFParameterInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFParameterInfo.class
07:05:20.697 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.FunctionRegistry - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/FunctionRegistry.class
07:05:20.698 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.LinkedHashMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/LinkedHashMap.class
07:05:20.698 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Collections - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Collections.class
07:05:20.698 pool-1-thread-1 DEBUG SessionState: SessionState user: null
07:05:20.698 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.LineageState - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/LineageState.class
07:05:20.699 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Operator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Operator.class
07:05:20.700 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.lib.Node - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/lib/Node.class
07:05:20.700 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.FileSinkOperator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/FileSinkOperator.class
07:05:20.701 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.TerminalOperator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/TerminalOperator.class
07:05:20.701 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.hooks.LineageInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/hooks/LineageInfo.class
07:05:20.702 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.ResourceMaps - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/ResourceMaps.class
07:05:20.702 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.DependencyResolver - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/DependencyResolver.class
07:05:20.703 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.SessionState$LogHelper - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/SessionState$LogHelper.class
07:05:20.703 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.lang.StringUtils - jar:file:/Users/lian/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.6.jar!/org/apache/commons/lang/StringUtils.class
07:05:20.705 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.IndexOutOfBoundsException
07:05:20.707 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.UUID - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/UUID.class
07:05:20.708 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.JavaUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/JavaUtils.class
07:05:20.709 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.history.HiveHistoryProxyHandler - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/history/HiveHistoryProxyHandler.class
07:05:20.709 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.InvocationHandler
07:05:20.709 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.Proxy
07:05:20.709 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Task - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Task.class
07:05:20.710 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.history.HiveHistory$Keys - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/history/HiveHistory$Keys.class
07:05:20.712 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.Counters
07:05:20.712 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.QueryPlan - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/QueryPlan.class
07:05:20.713 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.UndeclaredThrowableException
07:05:20.714 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.PTFUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/PTFUtils.class
07:05:20.714 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.beans.ExceptionListener - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/beans/ExceptionListener.class
07:05:20.714 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.beans.IntrospectionException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/beans/IntrospectionException.class
07:05:20.714 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.beans.PersistenceDelegate - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/beans/PersistenceDelegate.class
07:05:20.715 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.PTFUtils$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/PTFUtils$1.class
07:05:20.715 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.PTFUtils$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/PTFUtils$2.class
07:05:20.715 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.PTFUtils$3 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/PTFUtils$3.class
07:05:20.716 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.PTFUtils$4 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/PTFUtils$4.class
07:05:20.716 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.PTFUtils$5 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/PTFUtils$5.class
07:05:20.716 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$EnumDelegate - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$EnumDelegate.class
07:05:20.717 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.beans.DefaultPersistenceDelegate - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/beans/DefaultPersistenceDelegate.class
07:05:20.717 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.beans.Introspector - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/beans/Introspector.class
07:05:20.717 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.TaskBeanInfo - null
07:05:20.717 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.TaskBeanInfo - null
07:05:20.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.TaskCustomizer - null
07:05:20.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.TaskCustomizer - null
07:05:20.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.api.StageType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/api/StageType.class
07:05:20.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.TEnum - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/TEnum.class
07:05:20.719 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.DriverContext - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/DriverContext.class
07:05:20.719 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.TaskHandle - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/TaskHandle.class
07:05:20.720 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.MapWork - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/MapWork.class
07:05:20.720 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.BaseWork - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/BaseWork.class
07:05:20.721 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.class
07:05:20.721 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.OperatorDesc - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/OperatorDesc.class
07:05:20.724 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.beans.BeanInfo - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/beans/BeanInfo.class
07:05:20.725 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.beans.PropertyDescriptor - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/beans/PropertyDescriptor.class
07:05:20.725 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.TException - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/TException.class
07:05:20.725 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TTransport - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TTransport.class
07:05:20.726 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TMemoryBuffer - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TMemoryBuffer.class
07:05:20.726 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.protocol.TProtocol - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/protocol/TProtocol.class
07:05:20.727 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.protocol.TBinaryProtocol - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/protocol/TBinaryProtocol.class
07:05:20.728 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Queue - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Queue.class
07:05:20.728 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.protocol.TJSONProtocol - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/protocol/TJSONProtocol.class
07:05:20.729 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.Hive - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/Hive.class
07:05:20.731 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.Hive$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/Hive$1.class
07:05:20.732 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NoSuchObjectException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NoSuchObjectException.class
07:05:20.733 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.TBase - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/TBase.class
07:05:20.733 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Comparable
07:05:20.733 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.InvalidTableException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/InvalidTableException.class
07:05:20.734 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.InvalidOperationException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/InvalidOperationException.class
07:05:20.735 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AlreadyExistsException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AlreadyExistsException.class
07:05:20.735 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.MetaException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/MetaException.class
07:05:20.737 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.SerDeException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/SerDeException.class
07:05:20.738 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaHookLoader - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaHookLoader.class
07:05:20.739 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.security.UserGroupInformation
07:05:20.739 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.Hive$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/Hive$2.class
07:05:20.740 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.class
07:05:20.740 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.IMetaStoreClient - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/IMetaStoreClient.class
07:05:20.741 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStoreClient - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.class
07:05:20.743 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.class
07:05:20.744 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreUtils.class
07:05:20.745 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.PathFilter
07:05:20.746 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.base.Function
07:05:20.746 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Runnable
07:05:20.746 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.SocketAddress
07:05:20.746 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.InetSocketAddress
07:05:20.747 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy$PartitionIterator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy$PartitionIterator.class
07:05:20.747 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde.serdeConstants - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde/serdeConstants.class
07:05:20.748 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreUtils$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreUtils$2.class
07:05:20.748 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreUtils$3 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreUtils$3.class
07:05:20.749 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TTransportException - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TTransportException.class
07:05:20.749 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.InterruptedException
07:05:20.749 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TSocket - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TSocket.class
07:05:20.750 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TIOStreamTransport - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TIOStreamTransport.class
07:05:20.750 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TFramedTransport - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TFramedTransport.class
07:05:20.751 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.protocol.TCompactProtocol - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/protocol/TCompactProtocol.class
07:05:20.752 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Iface - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore$Iface.class
07:05:20.752 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.facebook.fb303.FacebookService$Iface - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libfb303/jars/libfb303-0.9.2.jar!/com/facebook/fb303/FacebookService$Iface.class
07:05:20.753 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.UnsupportedOperationException
07:05:20.753 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.UnknownHostException
07:05:20.753 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.InstantiationException
07:05:20.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.TApplicationException - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/TApplicationException.class
07:05:20.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.IMetaStoreClient$IncompatibleMetastoreException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/IMetaStoreClient$IncompatibleMetastoreException.class
07:05:20.756 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Character
07:05:20.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/DefaultMetaStoreFilterHookImpl.class
07:05:20.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreFilterHook - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreFilterHook.class
07:05:20.757 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore.class
07:05:20.758 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.class
07:05:20.759 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$1.class
07:05:20.759 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.LogUtils$LogInitializationException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/LogUtils$LogInitializationException.class
07:05:20.760 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$2.class
07:05:20.760 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.locks.Lock - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/locks/Lock.class
07:05:20.760 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TServerSocket - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TServerSocket.class
07:05:20.761 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TServerTransport - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TServerTransport.class
07:05:20.761 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.TServerSocketKeepAlive - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/TServerSocketKeepAlive.class
07:05:20.762 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.protocol.TProtocolFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/protocol/TProtocolFactory.class
07:05:20.762 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.IHMSHandler - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/IHMSHandler.class
07:05:20.762 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.conf.Configurable
07:05:20.763 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaException.class
07:05:20.763 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.TProcessor - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/TProcessor.class
07:05:20.763 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TTransportFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TTransportFactory.class
07:05:20.763 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.thrift.TUGIContainingTransport$Factory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/thrift/TUGIContainingTransport$Factory.class
07:05:20.764 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.transport.TFramedTransport$Factory - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/transport/TFramedTransport$Factory.class
07:05:20.764 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$ChainedTTransportFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$ChainedTTransportFactory.class
07:05:20.764 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.server.TServer - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/server/TServer.class
07:05:20.765 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.server.TThreadPoolServer - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/server/TThreadPoolServer.class
07:05:20.765 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$3 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$3.class
07:05:20.765 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$4 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$4.class
07:05:20.766 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.DateFormat - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/DateFormat.class
07:05:20.766 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.SimpleDateFormat - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/SimpleDateFormat.class
07:05:20.766 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$HMSHandler.class
07:05:20.770 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.facebook.fb303.FacebookBase - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libfb303/jars/libfb303-0.9.2.jar!/com/facebook/fb303/FacebookBase.class
07:05:20.771 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.AbstractMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/AbstractMap.class
07:05:20.771 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.ConcurrentHashMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/ConcurrentHashMap.class
07:05:20.772 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$HMSHandler$1.class
07:05:20.773 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$HMSHandler$2.class
07:05:20.773 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$3 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$HMSHandler$3.class
07:05:20.774 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$4 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$HMSHandler$4.class
07:05:20.774 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$5 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$HMSHandler$5.class
07:05:20.775 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$6 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStore$HMSHandler$6.class
07:05:20.775 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.TimerTask - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/TimerTask.class
07:05:20.775 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.EventCleanerTask - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/EventCleanerTask.class
07:05:20.776 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.UnknownTableException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/UnknownTableException.class
07:05:20.777 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.UnknownDBException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/UnknownDBException.class
07:05:20.778 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.InvalidObjectException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/InvalidObjectException.class
07:05:20.778 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreEventContext - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreEventContext.class
07:05:20.779 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreLoadPartitionDoneEvent.class
07:05:20.779 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.UnknownPartitionException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/UnknownPartitionException.class
07:05:20.780 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.InvalidPartitionException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/InvalidPartitionException.class
07:05:20.780 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreAuthorizationCallEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreAuthorizationCallEvent.class
07:05:20.781 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreAlterPartitionEvent.class
07:05:20.782 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreDropPartitionEvent.class
07:05:20.783 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreReadDatabaseEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreReadDatabaseEvent.class
07:05:20.783 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreReadTableEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreReadTableEvent.class
07:05:20.784 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ConfigValSecurityException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ConfigValSecurityException.class
07:05:20.785 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreAlterIndexEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreAlterIndexEvent.class
07:05:20.786 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOException.class
07:05:20.787 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreCreateDatabaseEvent.class
07:05:20.787 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreDropDatabaseEvent.class
07:05:20.788 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreCreateTableEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreCreateTableEvent.class
07:05:20.788 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreDropTableEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreDropTableEvent.class
07:05:20.789 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreAddPartitionEvent.class
07:05:20.790 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreAlterTableEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreAlterTableEvent.class
07:05:20.790 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreAddIndexEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreAddIndexEvent.class
07:05:20.791 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreDropIndexEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreDropIndexEvent.class
07:05:20.792 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Appendable
07:05:20.792 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.RetryingHMSHandler - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/RetryingHMSHandler.class
07:05:20.793 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreInit$MetaStoreInitData - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreInit$MetaStoreInitData.class
07:05:20.793 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreInit - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreInit.class
07:05:20.794 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreInitListener - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreInitListener.class
07:05:20.794 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveAlterHandler - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveAlterHandler.class
07:05:20.795 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.AlterHandler - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/AlterHandler.class
07:05:20.795 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.InvalidInputException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/InvalidInputException.class
07:05:20.797 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.util.ReflectionUtils
07:05:20.797 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.JobConfigurable
07:05:20.797 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.Warehouse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/Warehouse.class
07:05:20.799 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/HiveMetaStoreFsImpl.class
07:05:20.799 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreFS - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreFS.class
07:05:20.799 pool-1-thread-1 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
07:05:20.799 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.RawStoreProxy - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/RawStoreProxy.class
07:05:20.800 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.ObjectStore - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/ObjectStore.class
07:05:20.802 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.RawStore - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/RawStore.class
07:05:20.805 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOObjectNotFoundException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOObjectNotFoundException.class
07:05:20.805 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDODataStoreException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDODataStoreException.class
07:05:20.805 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOCanRetryException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOCanRetryException.class
07:05:20.809 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.parser.ExpressionTree$TreeVisitor - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/parser/ExpressionTree$TreeVisitor.class
07:05:20.809 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.ObjectStore$LikeChecker - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/ObjectStore$LikeChecker.class
07:05:20.810 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.Query - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/Query.class
07:05:20.810 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.antlr.runtime.RecognitionException - jar:file:/Users/lian/.ivy2/cache/org.antlr/antlr-runtime/jars/antlr-runtime-3.4.jar!/org/antlr/runtime/RecognitionException.class
07:05:20.811 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.antlr.runtime.CharStream - jar:file:/Users/lian/.ivy2/cache/org.antlr/antlr-runtime/jars/antlr-runtime-3.4.jar!/org/antlr/runtime/CharStream.class
07:05:20.811 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.antlr.runtime.IntStream - jar:file:/Users/lian/.ivy2/cache/org.antlr/antlr-runtime/jars/antlr-runtime-3.4.jar!/org/antlr/runtime/IntStream.class
07:05:20.811 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.antlr.runtime.TokenSource - jar:file:/Users/lian/.ivy2/cache/org.antlr/antlr-runtime/jars/antlr-runtime-3.4.jar!/org/antlr/runtime/TokenSource.class
07:05:20.811 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.antlr.runtime.TokenStream - jar:file:/Users/lian/.ivy2/cache/org.antlr/antlr-runtime/jars/antlr-runtime-3.4.jar!/org/antlr/runtime/TokenStream.class
07:05:20.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.locks.ReentrantLock - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/locks/ReentrantLock.class
07:05:20.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.atomic.AtomicBoolean - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/atomic/AtomicBoolean.class
07:05:20.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MTable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MTable.class
07:05:20.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.Detachable - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/Detachable.class
07:05:20.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.PersistenceCapable - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/PersistenceCapable.class
07:05:20.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MStorageDescriptor - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MStorageDescriptor.class
07:05:20.816 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MSerDeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MSerDeInfo.class
07:05:20.816 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MPartition - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MPartition.class
07:05:20.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MDatabase - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MDatabase.class
07:05:20.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MType.class
07:05:20.818 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MFieldSchema - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MFieldSchema.class
07:05:20.818 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MOrder - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MOrder.class
07:05:20.819 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.InetAddress
07:05:20.819 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.ObjectStore$TXN_STATUS - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/ObjectStore$TXN_STATUS.class
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.storeManagerType value null from  jpox.properties with rdbms
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.autoStartMechanismMode value null from  jpox.properties with checked
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.fixedDatastore value null from  jpox.properties with false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.validateColumns value null from  jpox.properties with false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.validateConstraints value null from  jpox.properties with false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.rdbms.datastoreAdapterClassName value null from  jpox.properties with org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.Multithreaded value null from  jpox.properties with true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.validateTables value null from  jpox.properties with false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.autoCreateSchema value null from  jpox.properties with true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.cache.level2.type value null from  jpox.properties with none
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.connectionPoolingType value null from  jpox.properties with BONECP
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.ConnectionUserName value null from  jpox.properties with APP
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.NonTransactionalRead value null from  jpox.properties with true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.transactionIsolation value null from  jpox.properties with read-committed
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.ConnectionURL value null from  jpox.properties with jdbc:derby:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8/metastore;create=true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.identifierFactory value null from  jpox.properties with datanucleus1
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.PersistenceManagerFactoryClass value null from  jpox.properties with org.datanucleus.api.jdo.JDOPersistenceManagerFactory
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.cache.level2 value null from  jpox.properties with false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.rdbms.useLegacyNativeValueStrategy value null from  jpox.properties with true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding hive.metastore.integral.jdo.pushdown value null from  jpox.properties with true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.DetachAllOnCommit value null from  jpox.properties with true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding javax.jdo.option.ConnectionDriverName value null from  jpox.properties with org.apache.derby.jdbc.EmbeddedDriver
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: Overriding datanucleus.plugin.pluginRegistryBundleCheck value null from  jpox.properties with LOG
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.rdbms.useLegacyNativeValueStrategy = true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: hive.metastore.integral.jdo.pushdown = true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.autoStartMechanismMode = checked
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.Multithreaded = true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.identifierFactory = datanucleus1
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.transactionIsolation = read-committed
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.validateTables = false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.ConnectionURL = jdbc:derby:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8/metastore;create=true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.DetachAllOnCommit = true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.NonTransactionalRead = true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.fixedDatastore = false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.validateConstraints = false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.ConnectionDriverName = org.apache.derby.jdbc.EmbeddedDriver
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.option.ConnectionUserName = APP
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.validateColumns = false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.rdbms.datastoreAdapterClassName = org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.cache.level2 = false
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.plugin.pluginRegistryBundleCheck = LOG
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.cache.level2.type = none
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: javax.jdo.PersistenceManagerFactoryClass = org.datanucleus.api.jdo.JDOPersistenceManagerFactory
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.autoCreateSchema = true
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.storeManagerType = rdbms
07:05:20.820 pool-1-thread-1 DEBUG ObjectStore: datanucleus.connectionPoolingType = BONECP
07:05:20.820 pool-1-thread-1 INFO ObjectStore: ObjectStore, initialize called
07:05:20.820 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper.class
07:05:20.821 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.Constants - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/Constants.class
07:05:20.822 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOFatalUserException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOFatalUserException.class
07:05:20.822 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOFatalException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOFatalException.class
07:05:20.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOFatalInternalException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOFatalInternalException.class
07:05:20.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.NamingException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/NamingException.class
07:05:20.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.Context - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/Context.class
07:05:20.824 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper$StateInterrogationObjectReturn - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper$StateInterrogationObjectReturn.class
07:05:20.824 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper$StateInterrogationBooleanReturn - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper$StateInterrogationBooleanReturn.class
07:05:20.824 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.I18NHelper - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/I18NHelper.class
07:05:20.825 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.Bundle - null
07:05:20.826 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.Bundle_en - null
07:05:20.826 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.Bundle_en_US - null
07:05:20.826 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$1 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$1.class
07:05:20.827 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper.class
07:05:20.827 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.Permission - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/Permission.class
07:05:20.827 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOPermission - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOPermission.class
07:05:20.828 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.BasicPermission - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/BasicPermission.class
07:05:20.828 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOUserException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOUserException.class
07:05:20.828 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper$StringConstructor - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper$StringConstructor.class
07:05:20.828 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.WeakHashMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/WeakHashMap.class
07:05:20.829 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper$4 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper$4.class
07:05:20.829 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Currency - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Currency.class
07:05:20.829 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper$1 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper$1.class
07:05:20.829 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper$2 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper$2.class
07:05:20.830 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Date - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Date.class
07:05:20.830 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper$3 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper$3.class
07:05:20.830 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$2 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$2.class
07:05:20.830 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$3 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$3.class
07:05:20.831 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$4 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$4.class
07:05:20.831 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$5 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$5.class
07:05:20.831 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$6 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$6.class
07:05:20.831 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$7 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$7.class
07:05:20.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$8 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$8.class
07:05:20.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$9 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$9.class
07:05:20.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$10 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$10.class
07:05:20.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$11 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$11.class
07:05:20.833 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$13 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$13.class
07:05:20.833 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$18 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$18.class
07:05:20.833 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOPersistenceManagerFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOPersistenceManagerFactory.class
07:05:20.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.PersistenceManagerFactory - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/PersistenceManagerFactory.class
07:05:20.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.spi.ObjectFactory - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/spi/ObjectFactory.class
07:05:20.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.Referenceable - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/Referenceable.class
07:05:20.835 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.ClassNotResolvedException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/ClassNotResolvedException.class
07:05:20.835 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.NucleusException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/NucleusException.class
07:05:20.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.metadata.JDOMetadata - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/metadata/JDOMetadata.class
07:05:20.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.metadata.Metadata - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/metadata/Metadata.class
07:05:20.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.FetchGroup - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/FetchGroup.class
07:05:20.837 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOUnsupportedOptionException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOUnsupportedOptionException.class
07:05:20.837 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.PersistenceManager - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/PersistenceManager.class
07:05:20.838 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.TransactionIsolationNotSupportedException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/TransactionIsolationNotSupportedException.class
07:05:20.838 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.NucleusUserException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/NucleusUserException.class
07:05:20.838 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.datastore.DataStoreCache - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/datastore/DataStoreCache.class
07:05:20.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.metadata.TypeMetadata - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/metadata/TypeMetadata.class
07:05:20.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.RefAddr - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/RefAddr.class
07:05:20.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.StringRefAddr - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/StringRefAddr.class
07:05:20.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOPersistenceManagerFactory$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOPersistenceManagerFactory$1.class
07:05:20.840 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.InheritableThreadLocal
07:05:20.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.Localiser - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/Localiser.class
07:05:20.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLException.class
07:05:20.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.Localisation - null
07:05:20.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.Localisation_en - null
07:05:20.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.Localisation_en_US - null
07:05:20.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$15 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$15.class
07:05:20.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.Reference - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/Reference.class
07:05:20.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.NucleusContext - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/NucleusContext.class
07:05:20.843 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.Name - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/Name.class
07:05:20.844 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOPersistenceManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOPersistenceManager.class
07:05:20.845 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOQueryCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOQueryCache.class
07:05:20.845 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.cache.QueryCompilationCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/cache/QueryCompilationCache.class
07:05:20.845 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.cache.QueryDatastoreCompilationCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/cache/QueryDatastoreCompilationCache.class
07:05:20.846 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.listener.InstanceLifecycleListener - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/listener/InstanceLifecycleListener.class
07:05:20.846 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.datastore.Sequence - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/datastore/Sequence.class
07:05:20.846 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOHelper$16 - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOHelper$16.class
07:05:20.847 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.MetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/MetaData.class
07:05:20.847 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.AbstractClassMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/AbstractClassMetaData.class
07:05:20.849 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.exceptions.DatastoreInitialisationException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/exceptions/DatastoreInitialisationException.class
07:05:20.849 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ImplementationCreator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ImplementationCreator.class
07:05:20.850 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.NucleusTransactionException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/NucleusTransactionException.class
07:05:20.850 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.jta.JTASyncRegistryUnavailableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/jta/JTASyncRegistryUnavailableException.class
07:05:20.850 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.CallbackHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/CallbackHandler.class
07:05:20.851 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.StoreManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/StoreManager.class
07:05:20.851 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.ObjectProviderFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/ObjectProviderFactory.class
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ClassConstants - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ClassConstants.class
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Byte
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Double
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Short
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.math.BigDecimal - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/math/BigDecimal.class
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.math.BigInteger - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/math/BigInteger.class
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Date - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Date.class
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Time - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Time.class
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Timestamp - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Timestamp.class
07:05:20.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ClassLoaderResolver - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ClassLoaderResolver.class
07:05:20.853 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.identity.OIDImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/identity/OIDImpl.class
07:05:20.853 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.identity.OID - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/identity/OID.class
07:05:20.853 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.MetaDataManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/MetaDataManager.class
07:05:20.855 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContext - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContext.class
07:05:20.855 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.Localisation - null
07:05:20.857 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.Localisation_en - null
07:05:20.857 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.Localisation_en_US - null
07:05:20.858 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Random - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Random.class
07:05:20.858 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.NucleusContext$ContextType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/NucleusContext$ContextType.class
07:05:20.858 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.PluginManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/PluginManager.class
07:05:20.859 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ClassLoaderResolverImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ClassLoaderResolverImpl.class
07:05:20.860 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.WeakValueMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/WeakValueMap.class
07:05:20.860 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.ReferenceValueMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/ReferenceValueMap.class
07:05:20.861 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.ReferenceValueMap$ValueReference - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/ReferenceValueMap$ValueReference.class
07:05:20.861 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.PluginRegistryFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/PluginRegistryFactory.class
07:05:20.861 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.PluginRegistry - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/PluginRegistry.class
07:05:20.862 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ClassNameConstants - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ClassNameConstants.class
07:05:20.862 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.eclipse.core.runtime.RegistryFactory - null
07:05:20.862 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.eclipse.core.runtime.RegistryFactory - null
07:05:20.863 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.eclipse.core.runtime.RegistryFactory - null
07:05:20.863 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.MessageFormat - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/MessageFormat.class
07:05:20.863 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.NucleusLogger - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/NucleusLogger.class
07:05:20.863 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.Log4JLogger - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/Log4JLogger.class
07:05:20.864 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.NonManagedPluginRegistry - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/NonManagedPluginRegistry.class
07:05:20.864 pool-1-thread-1 DEBUG General: Using PluginRegistry org.datanucleus.plugin.NonManagedPluginRegistry
07:05:20.864 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.FilenameFilter - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/FilenameFilter.class
07:05:20.865 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.NonManagedPluginRegistry$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/NonManagedPluginRegistry$1.class
07:05:20.865 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.ExtensionPoint - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/ExtensionPoint.class
07:05:20.866 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.PluginParser - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/PluginParser.class
07:05:20.867 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ClassLoaderResolverImpl$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ClassLoaderResolverImpl$1.class
07:05:20.868 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.URL
07:05:20.868 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.StringUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/StringUtils.class
07:05:20.869 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.URLDecoder
07:05:20.869 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.jar.JarFile - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/jar/JarFile.class
07:05:20.869 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.URI
07:05:20.869 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.jar.Manifest - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/jar/Manifest.class
07:05:20.869 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.jar.Attributes - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/jar/Attributes.class
07:05:20.869 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.Bundle - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/Bundle.class
07:05:20.869 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.PluginParser$Parser - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/PluginParser$Parser.class
07:05:20.870 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.CharacterIterator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/CharacterIterator.class
07:05:20.870 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.StringCharacterIterator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/StringCharacterIterator.class
07:05:20.870 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.Bundle$BundleDescription - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/Bundle$BundleDescription.class
07:05:20.870 pool-1-thread-1 DEBUG General: Registering bundle org.datanucleus.store.rdbms version 3.2.9 at URL file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar.
07:05:20.879 pool-1-thread-1 DEBUG General: Loading extension points from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/plugin.xml.
07:05:20.879 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.dom.DeepNodeListImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/dom/DeepNodeListImpl.class
07:05:20.888 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ClassLoaderResolverImpl$2 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ClassLoaderResolverImpl$2.class
07:05:20.889 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.WeakValueMap$WeakValueReference - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/WeakValueMap$WeakValueReference.class
07:05:20.889 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.Extension - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/Extension.class
07:05:20.892 pool-1-thread-1 DEBUG General: Loading extensions from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/plugin.xml.
07:05:20.892 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.ConfigurationElement - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/ConfigurationElement.class
07:05:20.897 pool-1-thread-1 DEBUG General: Registering bundle org.datanucleus.api.jdo version 3.2.6 at URL file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar.
07:05:20.898 pool-1-thread-1 DEBUG General: Loading extension points from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/plugin.xml.
07:05:20.899 pool-1-thread-1 DEBUG General: Loading extensions from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/plugin.xml.
07:05:20.900 pool-1-thread-1 DEBUG General: Registering bundle org.datanucleus version 3.2.10 at URL file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar.
07:05:20.901 pool-1-thread-1 DEBUG General: Loading extension points from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/plugin.xml.
07:05:20.908 pool-1-thread-1 DEBUG General: Loading extensions from plug-in file jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/plugin.xml.
07:05:20.909 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.NonManagedPluginRegistry$ExtensionSorter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/NonManagedPluginRegistry$ExtensionSorter.class
07:05:20.909 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.PersistenceConfiguration - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/PersistenceConfiguration.class
07:05:20.910 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.properties.PropertyStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/properties/PropertyStore.class
07:05:20.911 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.properties.PropertyTypeInvalidException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/properties/PropertyTypeInvalidException.class
07:05:20.912 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.properties.BooleanPropertyValidator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/properties/BooleanPropertyValidator.class
07:05:20.912 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.properties.PersistencePropertyValidator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/properties/PersistencePropertyValidator.class
07:05:20.912 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.PersistenceConfiguration$PropertyMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/PersistenceConfiguration$PropertyMapping.class
07:05:20.913 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.properties.CorePropertyValidator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/properties/CorePropertyValidator.class
07:05:20.913 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.properties.IntegerPropertyValidator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/properties/IntegerPropertyValidator.class
07:05:20.914 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.properties.StringPropertyValidator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/properties/StringPropertyValidator.class
07:05:20.914 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.LinkedList - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/LinkedList.class
07:05:20.914 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.plugin.PluginManager$ConfigurationElementPriorityComparator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/plugin/PluginManager$ConfigurationElementPriorityComparator.class
07:05:20.916 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.ApiAdapterFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/api/ApiAdapterFactory.class
07:05:20.916 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOAdapter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOAdapter.class
07:05:20.917 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.ApiAdapter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/api/ApiAdapter.class
07:05:20.918 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.PersistenceCapable$ObjectIdFieldConsumer - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/PersistenceCapable$ObjectIdFieldConsumer.class
07:05:20.918 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDONullIdentityException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDONullIdentityException.class
07:05:20.919 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.InvalidPrimaryKeyException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/InvalidPrimaryKeyException.class
07:05:20.919 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.InvalidClassMetaDataException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/InvalidClassMetaDataException.class
07:05:20.919 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.InvalidMetaDataException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/InvalidMetaDataException.class
07:05:20.919 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.NucleusFatalUserException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/NucleusFatalUserException.class
07:05:20.920 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.TransactionType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/TransactionType.class
07:05:20.921 pool-1-thread-1 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
07:05:20.921 pool-1-thread-1 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
07:05:20.921 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.metadata.JDOMetaDataManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/metadata/JDOMetaDataManager.class
07:05:20.922 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.annotations.AnnotationManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/annotations/AnnotationManager.class
07:05:20.923 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ClassMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ClassMetaData.class
07:05:20.924 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.InterfaceMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/InterfaceMetaData.class
07:05:20.924 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.NoPersistenceInformationException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/NoPersistenceInformationException.class
07:05:20.926 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.RegisterClassListener - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/RegisterClassListener.class
07:05:20.926 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.EventListener - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/EventListener.class
07:05:20.927 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.MultiMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/MultiMap.class
07:05:20.927 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.annotations.AnnotationManagerImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/annotations/AnnotationManagerImpl.class
07:05:20.928 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.TypeManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/TypeManager.class
07:05:20.929 pool-1-thread-1 DEBUG Persistence: Java types support initialising ...
07:05:20.929 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.TypeManager$JavaType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/TypeManager$JavaType.class
07:05:20.930 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Number
07:05:20.930 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.awt.image.BufferedImage - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/awt/image/BufferedImage.class
07:05:20.930 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.awt.Color - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/awt/Color.class
07:05:20.930 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.SqlDate - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/SqlDate.class
07:05:20.930 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.SCO - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/SCO.class
07:05:20.930 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.SqlTime - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/SqlTime.class
07:05:20.931 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.SqlTimestamp - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/SqlTimestamp.class
07:05:20.931 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalDate - null
07:05:20.932 pool-1-thread-1 DEBUG Persistence: Error in loading java type support for javax.time.calendar.LocalDate : Class "javax.time.calendar.LocalDate" was not found in the CLASSPATH. Please check your specification and your CLASSPATH.
07:05:20.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalDateTime - null
07:05:20.932 pool-1-thread-1 DEBUG Persistence: Error in loading java type support for javax.time.calendar.LocalDateTime : Class "javax.time.calendar.LocalDateTime" was not found in the CLASSPATH. Please check your specification and your CLASSPATH.
07:05:20.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalTime - null
07:05:20.932 pool-1-thread-1 DEBUG Persistence: Error in loading java type support for javax.time.calendar.LocalTime : Class "javax.time.calendar.LocalTime" was not found in the CLASSPATH. Please check your specification and your CLASSPATH.
07:05:20.933 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Date - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Date.class
07:05:20.933 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Calendar - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Calendar.class
07:05:20.933 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.GregorianCalendar - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/GregorianCalendar.class
07:05:20.933 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.GregorianCalendar - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/GregorianCalendar.class
07:05:20.933 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.TimeZone - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/TimeZone.class
07:05:20.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.ArrayList - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/ArrayList.class
07:05:20.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.SCOList - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/SCOList.class
07:05:20.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.SCOCollection - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/SCOCollection.class
07:05:20.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.SCOContainer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/SCOContainer.class
07:05:20.935 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.ArrayList - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/ArrayList.class
07:05:20.935 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.BackedSCO - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/BackedSCO.class
07:05:20.936 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Arrays$ArrayList - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Arrays$ArrayList.class
07:05:20.936 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.List - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/List.class
07:05:20.936 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.AbstractList - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/AbstractList.class
07:05:20.936 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.List - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/List.class
07:05:20.937 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.BitSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/BitSet.class
07:05:20.937 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.BitSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/BitSet.class
07:05:20.937 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.BitSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/BitSet.class
07:05:20.937 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Collection - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Collection.class
07:05:20.938 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.SCOMtoN - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/SCOMtoN.class
07:05:20.938 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.AbstractCollection - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/AbstractCollection.class
07:05:20.938 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.Collection - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/Collection.class
07:05:20.939 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.HashMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/HashMap.class
07:05:20.939 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.SCOMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/SCOMap.class
07:05:20.939 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.HashMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/HashMap.class
07:05:20.940 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.HashSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/HashSet.class
07:05:20.940 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.HashSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/HashSet.class
07:05:20.941 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Hashtable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Hashtable.class
07:05:20.941 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.Hashtable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/Hashtable.class
07:05:20.942 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.LinkedHashMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/LinkedHashMap.class
07:05:20.942 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.LinkedHashMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/LinkedHashMap.class
07:05:20.943 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.LinkedHashSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/LinkedHashSet.class
07:05:20.943 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.LinkedHashSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/LinkedHashSet.class
07:05:20.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.LinkedList - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/LinkedList.class
07:05:20.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.LinkedList - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/LinkedList.class
07:05:20.945 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Map - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Map.class
07:05:20.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.Map - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/Map.class
07:05:20.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.PriorityQueue - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/PriorityQueue.class
07:05:20.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.PriorityQueue - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/PriorityQueue.class
07:05:20.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.PriorityQueue - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/PriorityQueue.class
07:05:20.947 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.PriorityQueue - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/PriorityQueue.class
07:05:20.947 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Properties - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Properties.class
07:05:20.948 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.Properties - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/Properties.class
07:05:20.948 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Queue - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Queue.class
07:05:20.949 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.AbstractQueue - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/AbstractQueue.class
07:05:20.949 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.Queue - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/Queue.class
07:05:20.949 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Set - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Set.class
07:05:20.950 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.AbstractSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/AbstractSet.class
07:05:20.950 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.Set - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/Set.class
07:05:20.950 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.SortedSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/SortedSet.class
07:05:20.950 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.SortedSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/SortedSet.class
07:05:20.951 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.SortedSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/SortedSet.class
07:05:20.951 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.SortedSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/SortedSet.class
07:05:20.951 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.SortedMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/SortedMap.class
07:05:20.951 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.SortedMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/SortedMap.class
07:05:20.952 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.SortedMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/SortedMap.class
07:05:20.952 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.SortedMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/SortedMap.class
07:05:20.952 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Stack - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Stack.class
07:05:20.953 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.Stack - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/Stack.class
07:05:20.954 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.TreeMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/TreeMap.class
07:05:20.954 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.TreeMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/TreeMap.class
07:05:20.954 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.TreeMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/TreeMap.class
07:05:20.954 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.TreeMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/TreeMap.class
07:05:20.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.TreeSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/TreeSet.class
07:05:20.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.TreeSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/TreeSet.class
07:05:20.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.TreeSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/TreeSet.class
07:05:20.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.TreeSet - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/TreeSet.class
07:05:20.956 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Vector - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Vector.class
07:05:20.956 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.simple.Vector - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/simple/Vector.class
07:05:20.956 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Vector - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Vector.class
07:05:20.957 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.backed.Vector - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/backed/Vector.class
07:05:20.957 pool-1-thread-1 DEBUG Persistence: Java types support loaded : supported java types=java.awt.Color, java.lang.Float, [Ljava.lang.Integer;, java.sql.Time, java.util.SortedSet, java.net.URL, java.util.Date, java.util.PriorityQueue, float, [Ljava.lang.Character;, [Ljava.lang.Double;, java.lang.Integer, java.math.BigDecimal, java.util.Vector, java.lang.Character, java.lang.Enum, java.lang.Long, java.lang.Short, java.util.Locale, java.util.Map, java.math.BigInteger, java.net.URI, java.lang.Byte, java.util.GregorianCalendar, java.awt.image.BufferedImage, byte, double, java.util.TimeZone, java.sql.Timestamp, java.util.Collection, java.util.Set, java.util.UUID, [Ljava.lang.String;, java.util.List, [Ljava.lang.Short;, java.util.Queue, java.util.SortedMap, [Ljava.lang.Enum;, [Ljava.lang.Boolean;, java.lang.Double, [Ljava.util.Date;, [B, [C, [D, java.util.HashMap, java.util.Currency, [F, long, java.util.Stack, java.util.TreeSet, [I, java.util.ArrayList, [J, java.util.HashSet, java.util.LinkedHashMap, java.util.Calendar, java.lang.StringBuffer, [S, [Ljava.math.BigInteger;, java.lang.Boolean, [Ljava.lang.Number;, java.lang.String, [Ljava.math.BigDecimal;, java.lang.Number, java.util.LinkedList, java.util.Hashtable, java.util.LinkedHashSet, [Z, [Ljava.lang.Float;, java.util.Properties, [Ljava.lang.Byte;, [Ljava.util.Locale;, int, java.sql.Date, boolean, java.util.TreeMap, [Ljava.lang.Long;, char, short, java.lang.Class, java.util.BitSet, java.util.Arrays$ArrayList
07:05:20.957 pool-1-thread-1 DEBUG Persistence: Type converter support initialising ...
07:05:20.958 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.BigDecimalStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/BigDecimalStringConverter.class
07:05:20.958 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.TypeConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/TypeConverter.class
07:05:20.958 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.NucleusDataStoreException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/NucleusDataStoreException.class
07:05:20.958 pool-1-thread-1 DEBUG Persistence: Added converter for java.math.BigDecimal<->java.lang.String using org.datanucleus.store.types.converters.BigDecimalStringConverter
07:05:20.959 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.BigIntegerStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/BigIntegerStringConverter.class
07:05:20.959 pool-1-thread-1 DEBUG Persistence: Added converter for java.math.BigInteger<->java.lang.String using org.datanucleus.store.types.converters.BigIntegerStringConverter
07:05:20.959 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.BitSetStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/BitSetStringConverter.class
07:05:20.959 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.BitSet<->java.lang.String using org.datanucleus.store.types.converters.BitSetStringConverter
07:05:20.959 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.CalendarStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/CalendarStringConverter.class
07:05:20.960 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.CalendarStringConverter$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/CalendarStringConverter$1.class
07:05:20.960 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.ParseException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/ParseException.class
07:05:20.960 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Calendar<->java.lang.String using org.datanucleus.store.types.converters.CalendarStringConverter
07:05:20.960 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.ClassStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/ClassStringConverter.class
07:05:20.960 pool-1-thread-1 DEBUG Persistence: Added converter for java.lang.Class<->java.lang.String using org.datanucleus.store.types.converters.ClassStringConverter
07:05:20.960 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.ColorStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/ColorStringConverter.class
07:05:20.961 pool-1-thread-1 DEBUG Persistence: Added converter for java.awt.Color<->java.lang.String using org.datanucleus.store.types.converters.ColorStringConverter
07:05:20.961 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.IntegerStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/IntegerStringConverter.class
07:05:20.961 pool-1-thread-1 DEBUG Persistence: Added converter for java.lang.Integer<->java.lang.String using org.datanucleus.store.types.converters.IntegerStringConverter
07:05:20.961 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.LongStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/LongStringConverter.class
07:05:20.961 pool-1-thread-1 DEBUG Persistence: Added converter for java.lang.Long<->java.lang.String using org.datanucleus.store.types.converters.LongStringConverter
07:05:20.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.CurrencyStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/CurrencyStringConverter.class
07:05:20.962 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Currency<->java.lang.String using org.datanucleus.store.types.converters.CurrencyStringConverter
07:05:20.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.DateLongConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/DateLongConverter.class
07:05:20.962 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Date<->java.lang.Long using org.datanucleus.store.types.converters.DateLongConverter
07:05:20.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.DateStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/DateStringConverter.class
07:05:20.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.DateStringConverter$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/DateStringConverter$1.class
07:05:20.963 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Date<->java.lang.String using org.datanucleus.store.types.converters.DateStringConverter
07:05:20.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.LocaleStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/LocaleStringConverter.class
07:05:20.963 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.Locale<->java.lang.String using org.datanucleus.store.types.converters.LocaleStringConverter
07:05:20.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.SqlDateLongConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/SqlDateLongConverter.class
07:05:20.964 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Date<->java.lang.Long using org.datanucleus.store.types.converters.SqlDateLongConverter
07:05:20.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.SqlDateStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/SqlDateStringConverter.class
07:05:20.964 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Date<->java.lang.String using org.datanucleus.store.types.converters.SqlDateStringConverter
07:05:20.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.SqlTimeLongConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/SqlTimeLongConverter.class
07:05:20.964 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Time<->java.lang.Long using org.datanucleus.store.types.converters.SqlTimeLongConverter
07:05:20.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.SqlTimeStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/SqlTimeStringConverter.class
07:05:20.965 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Time<->java.lang.String using org.datanucleus.store.types.converters.SqlTimeStringConverter
07:05:20.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.SqlTimestampLongConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/SqlTimestampLongConverter.class
07:05:20.965 pool-1-thread-1 DEBUG Persistence: Added converter for java.sql.Timestamp<->java.lang.Long using org.datanucleus.store.types.converters.SqlTimestampLongConverter
07:05:20.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.StringBufferStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/StringBufferStringConverter.class
07:05:20.965 pool-1-thread-1 DEBUG Persistence: Added converter for java.lang.StringBuffer<->java.lang.String using org.datanucleus.store.types.converters.StringBufferStringConverter
07:05:20.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.TimeZoneStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/TimeZoneStringConverter.class
07:05:20.966 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.TimeZone<->java.lang.String using org.datanucleus.store.types.converters.TimeZoneStringConverter
07:05:20.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.URIStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/URIStringConverter.class
07:05:20.966 pool-1-thread-1 DEBUG Persistence: Added converter for java.net.URI<->java.lang.String using org.datanucleus.store.types.converters.URIStringConverter
07:05:20.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.URLStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/URLStringConverter.class
07:05:20.967 pool-1-thread-1 DEBUG Persistence: Added converter for java.net.URL<->java.lang.String using org.datanucleus.store.types.converters.URLStringConverter
07:05:20.967 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.UUIDStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/UUIDStringConverter.class
07:05:20.967 pool-1-thread-1 DEBUG Persistence: Added converter for java.util.UUID<->java.lang.String using org.datanucleus.store.types.converters.UUIDStringConverter
07:05:20.967 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.LocalDateStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/LocalDateStringConverter.class
07:05:20.967 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalDate - null
07:05:20.968 pool-1-thread-1 DEBUG Persistence: TypeConverter for javax.time.calendar.LocalDate<->java.lang.String ignored since java type not present in CLASSPATH
07:05:20.968 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.LocalTimeStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/LocalTimeStringConverter.class
07:05:20.968 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalTime - null
07:05:20.968 pool-1-thread-1 DEBUG Persistence: TypeConverter for javax.time.calendar.LocalTime<->java.lang.String ignored since java type not present in CLASSPATH
07:05:20.968 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.LocalTimeLongConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/LocalTimeLongConverter.class
07:05:20.969 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalTime - null
07:05:20.969 pool-1-thread-1 DEBUG Persistence: TypeConverter for javax.time.calendar.LocalTime<->java.lang.Long ignored since java type not present in CLASSPATH
07:05:20.969 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.LocalDateTimeStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/LocalDateTimeStringConverter.class
07:05:20.969 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalDateTime - null
07:05:20.970 pool-1-thread-1 DEBUG Persistence: TypeConverter for javax.time.calendar.LocalDateTime<->java.lang.String ignored since java type not present in CLASSPATH
07:05:20.970 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.SerializableStringConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/SerializableStringConverter.class
07:05:20.970 pool-1-thread-1 DEBUG Persistence: Added converter for java.io.Serializable<->java.lang.String using org.datanucleus.store.types.converters.SerializableStringConverter
07:05:20.970 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.converters.SerializableByteArrayConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/converters/SerializableByteArrayConverter.class
07:05:20.970 pool-1-thread-1 DEBUG Persistence: Added converter for java.io.Serializable<->[B using org.datanucleus.store.types.converters.SerializableByteArrayConverter
07:05:20.970 pool-1-thread-1 DEBUG Persistence: Type converter support loaded
07:05:20.971 pool-1-thread-1 DEBUG MetaData: MetaDataManager : Input=(XML,Annotations), XML-Validation=false, XML-Suffices=(persistence=*.jdo, orm=orm, query=*.jdoquery), JDO-listener=true
07:05:20.971 pool-1-thread-1 DEBUG MetaData: Registering listener for metadata initialisation
07:05:20.971 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.metadata.JDOMetaDataManager$MetaDataRegisterClassListener - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/metadata/JDOMetaDataManager$MetaDataRegisterClassListener.class
07:05:20.971 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.NucleusJDOHelper - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/NucleusJDOHelper.class
07:05:20.972 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.exceptions.ClassNotPersistenceCapableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/exceptions/ClassNotPersistenceCapableException.class
07:05:20.972 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.exceptions.NoPersistenceInformationException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/exceptions/NoPersistenceInformationException.class
07:05:20.972 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.exceptions.TransactionNotReadableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/exceptions/TransactionNotReadableException.class
07:05:20.973 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.exceptions.TransactionNotActiveException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/exceptions/TransactionNotActiveException.class
07:05:20.973 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.exceptions.TransactionNotWritableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/exceptions/TransactionNotWritableException.class
07:05:20.973 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOQueryInterruptedException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOQueryInterruptedException.class
07:05:20.973 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOFatalDataStoreException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOFatalDataStoreException.class
07:05:20.974 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOOptimisticVerificationException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOOptimisticVerificationException.class
07:05:20.974 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.NucleusJDOHelper$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/NucleusJDOHelper$1.class
07:05:20.975 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.StateManager - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/StateManager.class
07:05:20.975 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.PersistenceCapable$ObjectIdFieldSupplier - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/PersistenceCapable$ObjectIdFieldSupplier.class
07:05:20.975 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.JDOImplHelper$Meta - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/JDOImplHelper$Meta.class
07:05:20.976 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.StateInterrogation - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/StateInterrogation.class
07:05:20.976 pool-1-thread-1 DEBUG Datastore: Creating StoreManager for datastore
07:05:20.976 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.RDBMSStoreManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/RDBMSStoreManager.class
07:05:20.978 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.BackedSCOStoreManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/BackedSCOStoreManager.class
07:05:20.978 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.SchemaAwareStoreManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/SchemaAwareStoreManager.class
07:05:20.978 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.SchemaScriptAwareStoreManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/SchemaScriptAwareStoreManager.class
07:05:20.979 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.AbstractStoreManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/AbstractStoreManager.class
07:05:20.980 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContext$LifecycleListener - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContext$LifecycleListener.class
07:05:20.981 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.flush.FlushProcess - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/flush/FlushProcess.class
07:05:20.981 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.naming.NamingFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/naming/NamingFactory.class
07:05:20.981 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.NucleusSequence - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/NucleusSequence.class
07:05:20.982 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.NucleusConnection - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/NucleusConnection.class
07:05:20.982 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.exceptions.NoExtentException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/exceptions/NoExtentException.class
07:05:20.982 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.Extent - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/Extent.class
07:05:20.982 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.ConnectionManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/ConnectionManager.class
07:05:20.983 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.valuegenerator.ValueGenerationConnectionProvider - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/valuegenerator/ValueGenerationConnectionProvider.class
07:05:20.983 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.locks.ReadWriteLock - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/locks/ReadWriteLock.class
07:05:20.983 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.StorePersistenceHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/StorePersistenceHandler.class
07:05:20.984 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.StoreSchemaHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/StoreSchemaHandler.class
07:05:20.985 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.FileWriter - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/FileWriter.class
07:05:20.985 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.NoTableManagedException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/NoTableManagedException.class
07:05:20.986 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.scostore.Store - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/scostore/Store.class
07:05:20.986 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.IncompatibleFieldTypeException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/IncompatibleFieldTypeException.class
07:05:20.986 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.scostore.CollectionStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/scostore/CollectionStore.class
07:05:20.987 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.FieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/FieldManager.class
07:05:20.987 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.FieldConsumer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/FieldConsumer.class
07:05:20.987 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.FieldSupplier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/FieldSupplier.class
07:05:20.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.UnsupportedDataTypeException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/UnsupportedDataTypeException.class
07:05:20.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.Table - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/Table.class
07:05:20.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultObjectFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultObjectFactory.class
07:05:20.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.scostore.ArrayStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/scostore/ArrayStore.class
07:05:20.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.scostore.ListStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/scostore/ListStore.class
07:05:20.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.scostore.SetStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/scostore/SetStore.class
07:05:20.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.scostore.MapStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/scostore/MapStore.class
07:05:20.990 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.scostore.PersistableRelationStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/scostore/PersistableRelationStore.class
07:05:20.990 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Calendar - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Calendar.class
07:05:20.990 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.IdentityMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/IdentityMetaData.class
07:05:20.990 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.AbstractMemberMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/AbstractMemberMetaData.class
07:05:20.991 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ColumnMetaDataContainer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ColumnMetaDataContainer.class
07:05:20.992 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.SequenceMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/SequenceMetaData.class
07:05:20.993 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.Localisation - null
07:05:20.994 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.Localisation_en - null
07:05:20.994 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.Localisation_en_US - null
07:05:20.995 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.StoreDataManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/StoreDataManager.class
07:05:20.995 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.ConnectionManagerImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/ConnectionManagerImpl.class
07:05:20.996 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.TransactionEventListener - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/TransactionEventListener.class
07:05:20.996 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.ManagedConnectionResourceListener - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/ManagedConnectionResourceListener.class
07:05:20.997 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.ConnectionManagerImpl$ManagedConnectionPool - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/ConnectionManagerImpl$ManagedConnectionPool.class
07:05:20.998 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.ConnectionFactoryImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/ConnectionFactoryImpl.class
07:05:20.998 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.AbstractConnectionFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/AbstractConnectionFactory.class
07:05:20.999 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.ConnectionFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/ConnectionFactory.class
07:05:21.000 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.ManagedConnection - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/ManagedConnection.class
07:05:21.000 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.UnsupportedConnectionFactoryException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/UnsupportedConnectionFactoryException.class
07:05:21.001 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.ConnectionFactoryNotFoundException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/ConnectionFactoryNotFoundException.class
07:05:21.001 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.sql.DataSource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/sql/DataSource.class
07:05:21.002 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.BoneCPDataSource - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/BoneCPDataSource.class
07:05:21.002 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.BoneCPConfig - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/BoneCPConfig.class
07:05:21.003 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.BoneCPConfigMBean - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/BoneCPConfigMBean.class
07:05:21.004 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.slf4j.Logger
07:05:21.004 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.collect.Multiset
07:05:21.004 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/connectionpool/BoneCPConnectionPoolFactory.class
07:05:21.004 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/connectionpool/AbstractConnectionPoolFactory.class
07:05:21.005 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.connectionpool.ConnectionPoolFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/connectionpool/ConnectionPoolFactory.class
07:05:21.005 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/connectionpool/DatastoreDriverNotFoundException.class
07:05:21.005 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.connectionpool.ConnectionPool - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/connectionpool/ConnectionPool.class
07:05:21.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.EmbeddedDriver - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/EmbeddedDriver.class
07:05:21.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Driver - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Driver.class
07:05:21.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.DriverManager - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/DriverManager.class
07:05:21.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.PrintWriter - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/PrintWriter.class
07:05:21.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.JDBCBoot - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/JDBCBoot.class
07:05:21.007 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.error.StandardException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/error/StandardException.class
07:05:21.007 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.InternalDriver - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/InternalDriver.class
07:05:21.008 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.monitor.ModuleControl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/monitor/ModuleControl.class
07:05:21.008 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.DatabaseMetaData - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/DatabaseMetaData.class
07:05:21.008 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Statement - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Statement.class
07:05:21.008 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.security.SystemPermission - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/security/SystemPermission.class
07:05:21.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatableProperties - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatableProperties.class
07:05:21.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.Formatable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/Formatable.class
07:05:21.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.Externalizable - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/Externalizable.class
07:05:21.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.TypedFormat - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/TypedFormat.class
07:05:21.009 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.OutOfMemoryError
07:05:21.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Connection - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Connection.class
07:05:21.010 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.monitor.Monitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/monitor/Monitor.class
07:05:21.010 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.monitor.ModuleFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/monitor/ModuleFactory.class
07:05:21.011 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.FileMonitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/FileMonitor.class
07:05:21.011 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.BaseMonitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/BaseMonitor.class
07:05:21.012 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.i18n.BundleFinder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/i18n/BundleFinder.class
07:05:21.012 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.AccessibleByteArrayOutputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/AccessibleByteArrayOutputStream.class
07:05:21.013 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.UpdateServiceProperties - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/UpdateServiceProperties.class
07:05:21.013 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.io.StorageFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/io/StorageFactory.class
07:05:21.013 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.context.Context - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/context/Context.class
07:05:21.014 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.monitor.PersistentService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/monitor/PersistentService.class
07:05:21.015 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ArrayIndexOutOfBoundsException
07:05:21.015 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.InstanceGetter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/InstanceGetter.class
07:05:21.015 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.error.ShutdownException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/error/ShutdownException.class
07:05:21.015 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.TopService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/TopService.class
07:05:21.016 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.info.JVMInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/info/JVMInfo.class
07:05:21.017 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.ModuleInstance - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/ModuleInstance.class
07:05:21.018 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.FileMonitor$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/FileMonitor$1.class
07:05:21.018 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ThreadGroup
07:05:21.018 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.info.ProductVersionHolder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/info/ProductVersionHolder.class
07:05:21.019 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.i18n.MessageService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/i18n/MessageService.class
07:05:21.020 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.FileMonitor$6 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/FileMonitor$6.class
07:05:21.020 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FileUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FileUtil.class
07:05:21.020 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.NoSuchFieldException
07:05:21.021 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.FileMonitor$2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/FileMonitor$2.class
07:05:21.022 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.DataDictionaryImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/DataDictionaryImpl.class
07:05:21.024 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.DataDictionary - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/DataDictionary.class
07:05:21.025 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.cache.CacheableFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/cache/CacheableFactory.class
07:05:21.025 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.monitor.ModuleSupportable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/monitor/ModuleSupportable.class
07:05:21.026 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.DataValueDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/DataValueDescriptor.class
07:05:21.026 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.Storable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/Storable.class
07:05:21.027 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.Orderable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/Orderable.class
07:05:21.027 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.DependableFinder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/DependableFinder.class
07:05:21.027 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.daemon.IndexStatisticsDaemon - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/daemon/IndexStatisticsDaemon.class
07:05:21.028 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.PermissionsDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/PermissionsDescriptor.class
07:05:21.028 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.depend.Provider - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/depend/Provider.class
07:05:21.028 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.Dependable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/Dependable.class
07:05:21.029 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.TupleDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/TupleDescriptor.class
07:05:21.029 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.PermDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/PermDescriptor.class
07:05:21.030 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.RoutinePermsDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/RoutinePermsDescriptor.class
07:05:21.031 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ColPermsDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ColPermsDescriptor.class
07:05:21.031 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.TablePermsDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/TablePermsDescriptor.class
07:05:21.032 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ExecRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ExecRow.class
07:05:21.032 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.Row - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/Row.class
07:05:21.033 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.AliasInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/AliasInfo.class
07:05:21.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.AliasDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/AliasDescriptor.class
07:05:21.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.PrivilegedSQLObject - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/PrivilegedSQLObject.class
07:05:21.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.UniqueSQLObjectDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/UniqueSQLObjectDescriptor.class
07:05:21.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.UniqueTupleDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/UniqueTupleDescriptor.class
07:05:21.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.depend.Dependent - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/depend/Dependent.class
07:05:21.036 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.TableKey - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/TableKey.class
07:05:21.036 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.UUID - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/UUID.class
07:05:21.037 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.cache.Cacheable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/cache/Cacheable.class
07:05:21.038 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.CatalogRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/CatalogRowFactory.class
07:05:21.039 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSCONSTRAINTSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSCONSTRAINTSRowFactory.class
07:05:21.040 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSKEYSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSKEYSRowFactory.class
07:05:21.041 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSDEPENDSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSDEPENDSRowFactory.class
07:05:21.041 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSVIEWSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSVIEWSRowFactory.class
07:05:21.042 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSCHECKSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSCHECKSRowFactory.class
07:05:21.042 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSFOREIGNKEYSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSFOREIGNKEYSRowFactory.class
07:05:21.043 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSSTATEMENTSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSSTATEMENTSRowFactory.class
07:05:21.043 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSFILESRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSFILESRowFactory.class
07:05:21.044 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSALIASESRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSALIASESRowFactory.class
07:05:21.045 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSTRIGGERSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSTRIGGERSRowFactory.class
07:05:21.046 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSSTATISTICSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSSTATISTICSRowFactory.class
07:05:21.046 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSDUMMY1RowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSDUMMY1RowFactory.class
07:05:21.047 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSTABLEPERMSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSTABLEPERMSRowFactory.class
07:05:21.047 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.PermissionsCatalogRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/PermissionsCatalogRowFactory.class
07:05:21.048 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSCOLPERMSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSCOLPERMSRowFactory.class
07:05:21.049 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSROUTINEPERMSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSROUTINEPERMSRowFactory.class
07:05:21.049 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSROLESRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSROLESRowFactory.class
07:05:21.050 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSSEQUENCESRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSSEQUENCESRowFactory.class
07:05:21.050 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSPERMSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSPERMSRowFactory.class
07:05:21.051 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSUSERSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSUSERSRowFactory.class
07:05:21.051 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.Qualifier - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/Qualifier.class
07:05:21.052 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSTABLESRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSTABLESRowFactory.class
07:05:21.053 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSCOLUMNSRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSCOLUMNSRowFactory.class
07:05:21.053 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSCONGLOMERATESRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSCONGLOMERATESRowFactory.class
07:05:21.054 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SYSSCHEMASRowFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SYSSCHEMASRowFactory.class
07:05:21.054 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.TableDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/TableDescriptor.class
07:05:21.055 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.SchemaDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/SchemaDescriptor.class
07:05:21.056 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ConglomerateDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ConglomerateDescriptor.class
07:05:21.056 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ColumnDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ColumnDescriptor.class
07:05:21.058 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.UserDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/UserDescriptor.class
07:05:21.059 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.TupleFilter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/TupleFilter.class
07:05:21.060 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ConstraintDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ConstraintDescriptor.class
07:05:21.061 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ReferencedKeyConstraintDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ReferencedKeyConstraintDescriptor.class
07:05:21.061 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.KeyConstraintDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/KeyConstraintDescriptor.class
07:05:21.062 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ForeignKeyConstraintDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ForeignKeyConstraintDescriptor.class
07:05:21.062 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.SubKeyConstraintDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/SubKeyConstraintDescriptor.class
07:05:21.063 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.SubConstraintDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/SubConstraintDescriptor.class
07:05:21.063 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.SubCheckConstraintDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/SubCheckConstraintDescriptor.class
07:05:21.064 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.property.PersistentSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/property/PersistentSet.class
07:05:21.065 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.RoleClosureIterator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/RoleClosureIterator.class
07:05:21.066 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.DefaultDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/DefaultDescriptor.class
07:05:21.067 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.locks.Lockable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/locks/Lockable.class
07:05:21.068 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.depend.DependencyManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/depend/DependencyManager.class
07:05:21.068 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.NoSuchAlgorithmException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/NoSuchAlgorithmException.class
07:05:21.069 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.SPSDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/SPSDescriptor.class
07:05:21.070 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.Visitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/Visitor.class
07:05:21.071 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.TriggerDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/TriggerDescriptor.class
07:05:21.072 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.DataDictionaryImpl$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/DataDictionaryImpl$1.class
07:05:21.073 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.timer.Java5SingletonTimerFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/timer/Java5SingletonTimerFactory.class
07:05:21.073 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.timer.SingletonTimerFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/timer/SingletonTimerFactory.class
07:05:21.074 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.timer.TimerFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/timer/TimerFactory.class
07:05:21.074 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.SinglePool - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/SinglePool.class
07:05:21.075 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.AbstractPool - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/AbstractPool.class
07:05:21.075 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.locks.LockFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/locks/LockFactory.class
07:05:21.075 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.property.PropertySetCallback - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/property/PropertySetCallback.class
07:05:21.076 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.locks.CompatibilitySpace - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/locks/CompatibilitySpace.class
07:05:21.077 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.LockTable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/LockTable.class
07:05:21.077 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.BCJava - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/BCJava.class
07:05:21.077 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.compiler.JavaFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/compiler/JavaFactory.class
07:05:21.078 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.compiler.ClassBuilder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/compiler/ClassBuilder.class
07:05:21.078 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.VMTypeIdCacheable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/VMTypeIdCacheable.class
07:05:21.079 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.replication.slave.SlaveController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/replication/slave/SlaveController.class
07:05:21.080 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.replication.slave.SlaveFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/replication/slave/SlaveFactory.class
07:05:21.080 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.SocketTimeoutException
07:05:21.081 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.GenericExecutionFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/GenericExecutionFactory.class
07:05:21.081 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ExecutionFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ExecutionFactory.class
07:05:21.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ExecIndexRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ExecIndexRow.class
07:05:21.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ExecutionContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ExecutionContext.class
07:05:21.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.RowChanger - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/RowChanger.class
07:05:21.083 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.ResultColumnDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/ResultColumnDescriptor.class
07:05:21.083 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.ResultDescription - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/ResultDescription.class
07:05:21.084 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ResultSetFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ResultSetFactory.class
07:05:21.084 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.db.BasicDatabase - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/db/BasicDatabase.class
07:05:21.085 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.db.Database - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/db/Database.class
07:05:21.085 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.database.Database - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/database/Database.class
07:05:21.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.i18n.LocaleFinder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/i18n/LocaleFinder.class
07:05:21.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.JarReader - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/JarReader.class
07:05:21.087 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.DoubleProperties - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/DoubleProperties.class
07:05:21.087 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.authentication.NoneAuthenticationServiceImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/authentication/NoneAuthenticationServiceImpl.class
07:05:21.088 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.authentication.UserAuthenticator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/authentication/UserAuthenticator.class
07:05:21.088 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.authentication.AuthenticationServiceBase - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/authentication/AuthenticationServiceBase.class
07:05:21.089 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.AuthenticationService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/AuthenticationService.class
07:05:21.090 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.DRDAServerStarter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/DRDAServerStarter.class
07:05:21.091 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.J2SEDataValueFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/J2SEDataValueFactory.class
07:05:21.091 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.DataValueFactoryImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/DataValueFactoryImpl.class
07:05:21.092 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.DataValueFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/DataValueFactory.class
07:05:21.093 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.XMLDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/XMLDataValue.class
07:05:21.093 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.DateTimeDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/DateTimeDataValue.class
07:05:21.094 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.RefDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/RefDataValue.class
07:05:21.094 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.UserDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/UserDataValue.class
07:05:21.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.StringDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/StringDataValue.class
07:05:21.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.ConcatableDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/ConcatableDataValue.class
07:05:21.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.VariableSizeDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/VariableSizeDataValue.class
07:05:21.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.BitDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/BitDataValue.class
07:05:21.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.StreamStorable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/StreamStorable.class
07:05:21.097 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.BooleanDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/BooleanDataValue.class
07:05:21.097 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.NumberDataValue - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/NumberDataValue.class
07:05:21.098 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Number
07:05:21.098 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.jmxnone.NoManagementService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/jmxnone/NoManagementService.class
07:05:21.099 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.jmx.ManagementService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/jmx/ManagementService.class
07:05:21.099 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.mbeans.ManagementMBean - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/mbeans/ManagementMBean.class
07:05:21.100 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.authentication.NativeAuthenticationServiceImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/authentication/NativeAuthenticationServiceImpl.class
07:05:21.100 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Dictionary - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Dictionary.class
07:05:21.100 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLWarning - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLWarning.class
07:05:21.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.replication.master.MasterController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/replication/master/MasterController.class
07:05:21.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.replication.master.MasterFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/replication/master/MasterFactory.class
07:05:21.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.replication.net.ReplicationMessage - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/replication/net/ReplicationMessage.class
07:05:21.102 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.replication.buffer.LogBufferFullException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/replication/buffer/LogBufferFullException.class
07:05:21.102 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.CDCDataValueFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/CDCDataValueFactory.class
07:05:21.103 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IFactory.class
07:05:21.103 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.ConglomerateFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/ConglomerateFactory.class
07:05:21.103 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.MethodFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/MethodFactory.class
07:05:21.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2I - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2I.class
07:05:21.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BTree - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BTree.class
07:05:21.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.GenericConglomerate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/GenericConglomerate.class
07:05:21.105 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.Conglomerate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/Conglomerate.class
07:05:21.105 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.DataType - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/DataType.class
07:05:21.106 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2I_10_3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2I_10_3.class
07:05:21.106 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2I_v10_2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2I_v10_2.class
07:05:21.106 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.uuid.BasicUUIDFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/uuid/BasicUUIDFactory.class
07:05:21.107 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.uuid.UUIDFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/uuid/UUIDFactory.class
07:05:21.107 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.crypto.SecretKey - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/jce.jar!/javax/crypto/SecretKey.class
07:05:21.107 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.jce.JCECipherFactoryBuilder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/jce/JCECipherFactoryBuilder.class
07:05:21.107 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.crypto.CipherFactoryBuilder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/crypto/CipherFactoryBuilder.class
07:05:21.108 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.crypto.CipherFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/crypto/CipherFactory.class
07:05:21.108 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.RllRAMAccessManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/RllRAMAccessManager.class
07:05:21.108 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.RAMAccessManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/RAMAccessManager.class
07:05:21.109 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.AccessFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/AccessFactory.class
07:05:21.109 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.TransactionController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/TransactionController.class
07:05:21.110 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.Driver169 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/Driver169.class
07:05:21.110 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.CallableStatement - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/CallableStatement.class
07:05:21.110 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.PreparedStatement - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/PreparedStatement.class
07:05:21.110 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.Level2OptimizerFactoryImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/Level2OptimizerFactoryImpl.class
07:05:21.111 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OptimizerFactoryImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OptimizerFactoryImpl.class
07:05:21.111 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.OptimizerFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/OptimizerFactory.class
07:05:21.111 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.Optimizer - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/Optimizer.class
07:05:21.112 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.CostEstimate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/CostEstimate.class
07:05:21.112 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.StoreCostResult - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/StoreCostResult.class
07:05:21.113 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.authentication.SpecificAuthenticationServiceImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/authentication/SpecificAuthenticationServiceImpl.class
07:05:21.113 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.naming.directory.InitialDirContext - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/naming/directory/InitialDirContext.class
07:05:21.114 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.authentication.JNDIAuthenticationService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/authentication/JNDIAuthenticationService.class
07:05:21.114 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.authentication.BasicAuthenticationServiceImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/authentication/BasicAuthenticationServiceImpl.class
07:05:21.115 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.nio.Buffer - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/nio/Buffer.class
07:05:21.115 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BaseDataFileFactoryJ4 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BaseDataFileFactoryJ4.class
07:05:21.115 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BaseDataFileFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BaseDataFileFactory.class
07:05:21.116 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.data.DataFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/data/DataFactory.class
07:05:21.117 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.Corruptable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/Corruptable.class
07:05:21.117 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.ContainerKey - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/ContainerKey.class
07:05:21.118 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.Matchable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/Matchable.class
07:05:21.118 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.ContainerHandle - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/ContainerHandle.class
07:05:21.119 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.DatabaseInstant - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/DatabaseInstant.class
07:05:21.120 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.io.StorageFile - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/io/StorageFile.class
07:05:21.120 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.AllocationActions - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/AllocationActions.class
07:05:21.121 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.PageActions - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/PageActions.class
07:05:21.121 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.StreamContainerHandle - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/StreamContainerHandle.class
07:05:21.123 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.Loggable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/Loggable.class
07:05:21.123 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.Transaction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/Transaction.class
07:05:21.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.daemon.Serviceable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/daemon/Serviceable.class
07:05:21.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.FileResource - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/FileResource.class
07:05:21.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.data.RawContainerHandle - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/data/RawContainerHandle.class
07:05:21.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BaseContainer - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BaseContainer.class
07:05:21.126 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.FileContainer - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/FileContainer.class
07:05:21.128 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.property.PropertyValidation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/property/PropertyValidation.class
07:05:21.128 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.property.PropertyFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/property/PropertyFactory.class
07:05:21.129 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.Java5ClassFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/Java5ClassFactory.class
07:05:21.129 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.ReflectClassesJava2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/ReflectClassesJava2.class
07:05:21.129 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.DatabaseClasses - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/DatabaseClasses.class
07:05:21.130 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.ClassFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/ClassFactory.class
07:05:21.130 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.GeneratedClass - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/GeneratedClass.class
07:05:21.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.LoadedGeneratedClass - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/LoadedGeneratedClass.class
07:05:21.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.ReflectGeneratedClass - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/ReflectGeneratedClass.class
07:05:21.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.ReflectLoaderJava2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/ReflectLoaderJava2.class
07:05:21.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.ClassInspector - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/ClassInspector.class
07:05:21.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.Java5ClassInspector - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/Java5ClassInspector.class
07:05:21.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.stream.SingleStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/stream/SingleStream.class
07:05:21.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.stream.InfoStreams - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/stream/InfoStreams.class
07:05:21.133 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.Member
07:05:21.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.stream.HeaderPrintWriter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/stream/HeaderPrintWriter.class
07:05:21.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.BufferedOutputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/BufferedOutputStream.class
07:05:21.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.stream.PrintWriterGetHeader - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/stream/PrintWriterGetHeader.class
07:05:21.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.Driver42 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/Driver42.class
07:05:21.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.Driver40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/Driver40.class
07:05:21.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.Driver30 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/Driver30.class
07:05:21.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.Driver20 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/Driver20.class
07:05:21.136 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedResultSet.class
07:05:21.137 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.EngineResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/EngineResultSet.class
07:05:21.137 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.ResultSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/ResultSet.class
07:05:21.138 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.ConnectionChild - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/ConnectionChild.class
07:05:21.138 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedResultSet20 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedResultSet20.class
07:05:21.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.TimeoutException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/TimeoutException.class
07:05:21.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.ExecutionException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/ExecutionException.class
07:05:21.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.Callable - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/Callable.class
07:05:21.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.BlockingQueue - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/BlockingQueue.class
07:05:21.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.ThreadFactory - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/ThreadFactory.class
07:05:21.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.sql.PooledConnection - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/sql/PooledConnection.class
07:05:21.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.sql.XAConnection - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/sql/XAConnection.class
07:05:21.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedConnection - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedConnection.class
07:05:21.140 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.EngineConnection - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/EngineConnection.class
07:05:21.140 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedConnection40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedConnection40.class
07:05:21.141 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.EngineConnection40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/EngineConnection40.class
07:05:21.141 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedResultSet40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedResultSet40.class
07:05:21.141 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedResultSetMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedResultSetMetaData.class
07:05:21.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.ResultSetMetaData - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/ResultSetMetaData.class
07:05:21.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedResultSetMetaData40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedResultSetMetaData40.class
07:05:21.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.SQLExceptionFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/SQLExceptionFactory.class
07:05:21.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.ExceptionFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/ExceptionFactory.class
07:05:21.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.SQLExceptionFactory40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/SQLExceptionFactory40.class
07:05:21.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLFeatureNotSupportedException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLFeatureNotSupportedException.class
07:05:21.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.BrokeredConnection - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/BrokeredConnection.class
07:05:21.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.BrokeredConnection40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/BrokeredConnection40.class
07:05:21.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedResultSet42 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedResultSet42.class
07:05:21.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.BrokeredConnection42 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/BrokeredConnection42.class
07:05:21.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.ThreadPoolExecutor - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/ThreadPoolExecutor.class
07:05:21.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.SynchronousQueue - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/SynchronousQueue.class
07:05:21.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.Driver20$DaemonThreadFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/Driver20$DaemonThreadFactory.class
07:05:21.145 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.log.LogToFile - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/log/LogToFile.class
07:05:21.145 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.log.LogFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/log/LogFactory.class
07:05:21.146 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.log.LogInstant - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/log/LogInstant.class
07:05:21.147 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.log.LogScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/log/LogScan.class
07:05:21.147 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.log.LogCounter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/log/LogCounter.class
07:05:21.148 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.ScanHandle - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/ScanHandle.class
07:05:21.148 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.io.StorageRandomAccessFile - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/io/StorageRandomAccessFile.class
07:05:21.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.DataInput - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/DataInput.class
07:05:21.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.DataOutput - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/DataOutput.class
07:05:21.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.log.Logger - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/log/Logger.class
07:05:21.150 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.SyncFailedException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/SyncFailedException.class
07:05:21.151 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.log.ReadOnly - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/log/ReadOnly.class
07:05:21.151 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.heap.HeapConglomerateFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/heap/HeapConglomerateFactory.class
07:05:21.151 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.heap.Heap - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/heap/Heap.class
07:05:21.152 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.StaticCompiledOpenConglomInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/StaticCompiledOpenConglomInfo.class
07:05:21.152 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.heap.Heap_v10_2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/heap/Heap_v10_2.class
07:05:21.153 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.daemon.SingleThreadDaemonFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/daemon/SingleThreadDaemonFactory.class
07:05:21.153 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.daemon.DaemonFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/daemon/DaemonFactory.class
07:05:21.153 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.daemon.DaemonService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/daemon/DaemonService.class
07:05:21.153 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TypeCompilerFactoryImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TypeCompilerFactoryImpl.class
07:05:21.154 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.TypeCompilerFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/TypeCompilerFactory.class
07:05:21.154 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BaseTypeCompiler - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BaseTypeCompiler.class
07:05:21.154 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.TypeCompiler - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/TypeCompiler.class
07:05:21.155 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UserDefinedTypeCompiler - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UserDefinedTypeCompiler.class
07:05:21.155 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.UniqueWithDuplicateNullsExternalSortFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/UniqueWithDuplicateNullsExternalSortFactory.class
07:05:21.155 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.ExternalSortFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/ExternalSortFactory.class
07:05:21.155 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.SortFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/SortFactory.class
07:05:21.156 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.SortCostController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/SortCostController.class
07:05:21.156 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.Sort - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/Sort.class
07:05:21.156 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.MergeSort - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/MergeSort.class
07:05:21.157 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.UniqueWithDuplicateNullsMergeSort - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/UniqueWithDuplicateNullsMergeSort.class
07:05:21.157 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.ConcurrentCacheFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/ConcurrentCacheFactory.class
07:05:21.157 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.cache.CacheFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/cache/CacheFactory.class
07:05:21.157 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.cache.CacheManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/cache/CacheManager.class
07:05:21.158 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.RealResultSetStatisticsFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/RealResultSetStatisticsFactory.class
07:05:21.158 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ResultSetStatisticsFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ResultSetStatisticsFactory.class
07:05:21.159 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.rts.RealInsertResultSetStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/rts/RealInsertResultSetStatistics.class
07:05:21.159 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.rts.RealNoRowsResultSetStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/rts/RealNoRowsResultSetStatistics.class
07:05:21.159 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ResultSetStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ResultSetStatistics.class
07:05:21.159 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.xplain.XPLAINable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/xplain/XPLAINable.class
07:05:21.160 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.rts.RealInsertVTIResultSetStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/rts/RealInsertVTIResultSetStatistics.class
07:05:21.160 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.InsertResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/InsertResultSet.class
07:05:21.161 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.TargetResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/TargetResultSet.class
07:05:21.161 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.ResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/ResultSet.class
07:05:21.161 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DMLWriteResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DMLWriteResultSet.class
07:05:21.162 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.NoRowsResultSetImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/NoRowsResultSetImpl.class
07:05:21.162 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.InsertVTIResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/InsertVTIResultSet.class
07:05:21.162 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DMLVTIResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DMLVTIResultSet.class
07:05:21.163 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.rts.RealUpdateResultSetStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/rts/RealUpdateResultSetStatistics.class
07:05:21.163 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.UpdateResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/UpdateResultSet.class
07:05:21.163 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.rts.RealDeleteCascadeResultSetStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/rts/RealDeleteCascadeResultSetStatistics.class
07:05:21.164 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.rts.RealDeleteResultSetStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/rts/RealDeleteResultSetStatistics.class
07:05:21.164 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DeleteCascadeResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DeleteCascadeResultSet.class
07:05:21.164 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DeleteResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DeleteResultSet.class
07:05:21.165 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.rts.RealDeleteVTIResultSetStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/rts/RealDeleteVTIResultSetStatistics.class
07:05:21.165 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.PreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/PreparedStatement.class
07:05:21.166 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.RunTimeStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/RunTimeStatistics.class
07:05:21.167 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.ClockFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/ClockFactory.class
07:05:21.167 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.db.SlaveDatabase - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/db/SlaveDatabase.class
07:05:21.167 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.xplain.XPLAINFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/xplain/XPLAINFactory.class
07:05:21.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.xplain.XPLAINFactoryIF - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/xplain/XPLAINFactoryIF.class
07:05:21.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.xplain.XPLAINVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/xplain/XPLAINVisitor.class
07:05:21.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.transaction.xa.Xid - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/transaction/xa/Xid.class
07:05:21.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.transaction.xa.XAResource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/transaction/xa/XAResource.class
07:05:21.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.transaction.xa.XAException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/transaction/xa/XAException.class
07:05:21.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.sql.ConnectionPoolDataSource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/sql/ConnectionPoolDataSource.class
07:05:21.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.sql.XADataSource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/sql/XADataSource.class
07:05:21.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.ResourceAdapterImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/ResourceAdapterImpl.class
07:05:21.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.ResourceAdapter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/ResourceAdapter.class
07:05:21.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.jmx.JMXManagementService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/jmx/JMXManagementService.class
07:05:21.170 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.JMException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/JMException.class
07:05:21.170 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.StandardMBean - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/StandardMBean.class
07:05:21.170 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.jmx.JMXManagementService$2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/jmx/JMXManagementService$2.class
07:05:21.171 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.ConcurrentXactFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/ConcurrentXactFactory.class
07:05:21.171 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.XactFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/XactFactory.class
07:05:21.171 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.xact.TransactionFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/xact/TransactionFactory.class
07:05:21.172 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.GlobalTransactionId - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/GlobalTransactionId.class
07:05:21.172 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.Xact - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/Xact.class
07:05:21.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.locks.Limit - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/locks/Limit.class
07:05:21.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.locks.LockOwner - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/locks/LockOwner.class
07:05:21.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.xact.RawTransaction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/xact/RawTransaction.class
07:05:21.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Observable - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Observable.class
07:05:21.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.InternalXact - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/InternalXact.class
07:05:21.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.xact.TransactionId - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/xact/TransactionId.class
07:05:21.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.TransactionMapFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/TransactionMapFactory.class
07:05:21.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.ConcurrentTransactionMapFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/ConcurrentTransactionMapFactory.class
07:05:21.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.conn.GenericLanguageConnectionFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/conn/GenericLanguageConnectionFactory.class
07:05:21.176 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.conn.LanguageConnectionFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/conn/LanguageConnectionFactory.class
07:05:21.176 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.conn.LanguageConnectionContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/conn/LanguageConnectionContext.class
07:05:21.176 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.Statement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/Statement.class
07:05:21.177 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.Parser - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/Parser.class
07:05:21.177 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.RawStore - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/RawStore.class
07:05:21.178 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.RawStoreFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/RawStoreFactory.class
07:05:21.180 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericLanguageFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericLanguageFactory.class
07:05:21.181 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.LanguageFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/LanguageFactory.class
07:05:21.181 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.ParameterValueSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/ParameterValueSet.class
07:05:21.181 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.NodeFactoryImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/NodeFactoryImpl.class
07:05:21.181 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.NodeFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/NodeFactory.class
07:05:21.182 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.Node - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/Node.class
07:05:21.182 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.ConcurrentPool - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/ConcurrentPool.class
07:05:21.183 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.ProtocolKey - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/ProtocolKey.class
07:05:21.183 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.stream.BasicGetLogHeader - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/stream/BasicGetLogHeader.class
07:05:21.184 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.property.PropertyUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/property/PropertyUtil.class
07:05:21.185 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.FileMonitor$3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/FileMonitor$3.class
07:05:21.185 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.stream.BasicHeaderPrintWriter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/stream/BasicHeaderPrintWriter.class
07:05:21.185 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.context.ContextService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/context/ContextService.class
07:05:21.186 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Runtime
07:05:21.186 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.timer.SingletonTimerFactory$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/timer/SingletonTimerFactory$1.class
07:05:21.186 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.timer.SingletonTimerFactory$2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/timer/SingletonTimerFactory$2.class
07:05:21.187 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Timer - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Timer.class
07:05:21.187 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.timer.SingletonTimerFactory$3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/timer/SingletonTimerFactory$3.class
07:05:21.187 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.atomic.AtomicInteger - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/atomic/AtomicInteger.class
07:05:21.187 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.uuid.BasicUUID - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/uuid/BasicUUID.class
07:05:21.188 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.jmx.JMXManagementService$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/jmx/JMXManagementService$1.class
07:05:21.188 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.management.ManagementFactory
07:05:21.188 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.MBeanServer - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/MBeanServer.class
07:05:21.188 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.ObjectName - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/ObjectName.class
07:05:21.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.jmx.JMXManagementService$3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/jmx/JMXManagementService$3.class
07:05:21.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.MBeanInfo - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/MBeanInfo.class
07:05:21.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.info.Version - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/info/Version.class
07:05:21.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.mbeans.VersionMBean - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/mbeans/VersionMBean.class
07:05:21.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.context.ContextManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/context/ContextManager.class
07:05:21.191 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.context.SystemContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/context/SystemContext.class
07:05:21.191 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.context.ContextImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/context/ContextImpl.class
07:05:21.198 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.context.ContextManager$CtxStack - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/context/ContextManager$CtxStack.class
07:05:21.199 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.ServiceBootContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/ServiceBootContext.class
07:05:21.199 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedSQLException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedSQLException.class
07:05:21.201 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.error.DerbySQLException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/error/DerbySQLException.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLNonTransientConnectionException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLNonTransientConnectionException.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLDataException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLDataException.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLIntegrityConstraintViolationException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLIntegrityConstraintViolationException.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLInvalidAuthorizationSpecException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLInvalidAuthorizationSpecException.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLTransactionRollbackException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLTransactionRollbackException.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLSyntaxErrorException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLSyntaxErrorException.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLTimeoutException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLTimeoutException.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.Util - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/Util.class
07:05:21.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.BatchUpdateException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/BatchUpdateException.class
07:05:21.204 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.JDBC - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/JDBC.class
07:05:21.204 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.mbeans.JDBCMBean - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/mbeans/JDBCMBean.class
07:05:21.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.AutoloadedDriver - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/AutoloadedDriver.class
07:05:21.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.jdbc.AutoloadedDriver40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/jdbc/AutoloadedDriver40.class
07:05:21.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.error.StandardException$BadMessageArgumentException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/error/StandardException$BadMessageArgumentException.class
07:05:21.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.error.ErrorStringBuilder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/error/ErrorStringBuilder.class
07:05:21.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.error.PassThroughException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/error/PassThroughException.class
07:05:21.207 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.ClassUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/ClassUtils.class
07:05:21.209 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.SoftValueMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/SoftValueMap.class
07:05:21.210 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.slf4j.LoggerFactory
07:05:21.213 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.hooks.ConnectionHook - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/hooks/ConnectionHook.class
07:05:21.213 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.base.Preconditions
07:05:21.214 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.cache.CacheLoader
07:05:21.214 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.BoneCPDataSource$1 - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/BoneCPDataSource$1.class
07:05:21.216 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.cache.CacheBuilder
07:05:21.216 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.slf4j.Logger
07:05:21.217 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory$BoneCPConnectionPool - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/connectionpool/BoneCPConnectionPoolFactory$BoneCPConnectionPool.class
07:05:21.217 pool-1-thread-1 DEBUG Connection: Created tx data source using pooling type of BONECP
07:05:21.218 pool-1-thread-1 DEBUG Connection: Registered transactional connection factory under name "rdbms/tx"
07:05:21.218 pool-1-thread-1 DEBUG Connection: Registered nontransactional connection factory under name "rdbms/nontx"
07:05:21.218 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.AbstractStoreManager$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/AbstractStoreManager$1.class
07:05:21.218 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.locks.ReentrantReadWriteLock - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/locks/ReentrantReadWriteLock.class
07:05:21.218 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.MappedTypeManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/MappedTypeManager.class
07:05:21.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.JavaUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/JavaUtils.class
07:05:21.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.BooleanMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/BooleanMapping.class
07:05:21.220 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.SingleFieldMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/SingleFieldMapping.class
07:05:21.220 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.JavaTypeMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/JavaTypeMapping.class
07:05:21.221 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.MappedTypeManager$MappedType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/MappedTypeManager$MappedType.class
07:05:21.221 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.ByteMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/ByteMapping.class
07:05:21.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.CharacterMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/CharacterMapping.class
07:05:21.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.DoubleMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/DoubleMapping.class
07:05:21.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.FloatMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/FloatMapping.class
07:05:21.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.IntegerMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/IntegerMapping.class
07:05:21.223 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.LongMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/LongMapping.class
07:05:21.223 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.ShortMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/ShortMapping.class
07:05:21.223 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.ClassMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/ClassMapping.class
07:05:21.223 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.ObjectAsStringMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/ObjectAsStringMapping.class
07:05:21.224 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.NumberMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/NumberMapping.class
07:05:21.224 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.SerialisedMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/SerialisedMapping.class
07:05:21.224 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.StringMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/StringMapping.class
07:05:21.225 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.StringBufferMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/StringBufferMapping.class
07:05:21.225 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.EnumMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/EnumMapping.class
07:05:21.226 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.ColorMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/ColorMapping.class
07:05:21.226 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.SingleFieldMultiMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/SingleFieldMultiMapping.class
07:05:21.227 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.BufferedImageMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/BufferedImageMapping.class
07:05:21.227 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.FileMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/FileMapping.class
07:05:21.228 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.BigDecimalMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/BigDecimalMapping.class
07:05:21.228 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.BigIntegerMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/BigIntegerMapping.class
07:05:21.228 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.URLMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/URLMapping.class
07:05:21.229 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.URIMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/URIMapping.class
07:05:21.229 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.GregorianCalendarMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/GregorianCalendarMapping.class
07:05:21.229 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.DateMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/DateMapping.class
07:05:21.230 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.TemporalMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/TemporalMapping.class
07:05:21.230 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.LocaleMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/LocaleMapping.class
07:05:21.230 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.TimeZoneMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/TimeZoneMapping.class
07:05:21.230 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.LocalDateMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/LocalDateMapping.class
07:05:21.231 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalDate - null
07:05:21.231 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.LocalDateTimeMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/LocalDateTimeMapping.class
07:05:21.232 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalDateTime - null
07:05:21.232 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.LocalTimeMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/LocalTimeMapping.class
07:05:21.232 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.time.calendar.LocalTime - null
07:05:21.233 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.SqlDateMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/SqlDateMapping.class
07:05:21.233 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.SqlTimeMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/SqlTimeMapping.class
07:05:21.233 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.SqlTimestampMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/SqlTimestampMapping.class
07:05:21.233 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.ArrayMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/ArrayMapping.class
07:05:21.234 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.MappingCallbacks - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/MappingCallbacks.class
07:05:21.234 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.AbstractContainerMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/AbstractContainerMapping.class
07:05:21.234 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.CollectionMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/CollectionMapping.class
07:05:21.235 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.BitSetMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/BitSetMapping.class
07:05:21.235 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.CurrencyMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/CurrencyMapping.class
07:05:21.235 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.MapMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/MapMapping.class
07:05:21.236 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.UUIDMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/UUIDMapping.class
07:05:21.236 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.OIDMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/OIDMapping.class
07:05:21.237 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.RDBMSPersistenceHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/RDBMSPersistenceHandler.class
07:05:21.237 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.AbstractPersistenceHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/AbstractPersistenceHandler.class
07:05:21.238 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.exceptions.DatastoreReadOnlyException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/exceptions/DatastoreReadOnlyException.class
07:05:21.238 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.Request - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/Request.class
07:05:21.239 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.InsertRequest - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/InsertRequest.class
07:05:21.239 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.FetchRequest - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/FetchRequest.class
07:05:21.240 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.UpdateRequest - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/UpdateRequest.class
07:05:21.241 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.DeleteRequest - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/DeleteRequest.class
07:05:21.241 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.LocateRequest - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/LocateRequest.class
07:05:21.242 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.flush.FlushOrdered - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/flush/FlushOrdered.class
07:05:21.242 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.NucleusOptimisticException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/NucleusOptimisticException.class
07:05:21.242 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.schema.RDBMSSchemaHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/schema/RDBMSSchemaHandler.class
07:05:21.243 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.StoreSchemaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/StoreSchemaData.class
07:05:21.244 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.SQLExpressionFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/SQLExpressionFactory.class
07:05:21.245 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLStatement - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLStatement.class
07:05:21.246 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLTable.class
07:05:21.246 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.SQLExpressionFactory$MethodKey - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/SQLExpressionFactory$MethodKey.class
07:05:21.251 pool-1-thread-1 DEBUG Connection: Created nontx data source using pooling type of BONECP
07:05:21.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/ConnectionFactoryImpl$ManagedConnectionImpl.class
07:05:21.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.AbstractManagedConnection - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/AbstractManagedConnection.class
07:05:21.253 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.TransactionUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/TransactionUtils.class
07:05:21.253 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.ConnectionProviderPriorityList - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/ConnectionProviderPriorityList.class
07:05:21.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.ConnectionProvider - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/ConnectionProvider.class
07:05:21.254 pool-1-thread-1 DEBUG BoneCPDataSource: JDBC URL = jdbc:derby:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8/metastore;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
07:05:21.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.BoneCPDataSource$FinalWrapper - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/BoneCPDataSource$FinalWrapper.class
07:05:21.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.BoneCP - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/BoneCP.class
07:05:21.256 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.ConnectionStrategy - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/ConnectionStrategy.class
07:05:21.256 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.Statistics - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/Statistics.class
07:05:21.257 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.StatisticsMBean - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/StatisticsMBean.class
07:05:21.257 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.atomic.AtomicLong - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/atomic/AtomicLong.class
07:05:21.257 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.cache.LoadingCache
07:05:21.258 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.hooks.AcquireFailConfig - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/hooks/AcquireFailConfig.class
07:05:21.258 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.mysql.jdbc.Driver
07:05:21.258 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.mysql.fabric.jdbc.FabricMySQLDriver - jar:file:/Users/lian/.ivy2/cache/mysql/mysql-connector-java/jars/mysql-connector-java-5.1.38.jar!/com/mysql/fabric/jdbc/FabricMySQLDriver.class
07:05:21.259 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.mysql.jdbc.NonRegisteringDriver
07:05:21.259 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.postgresql.Driver
07:05:21.259 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.remote.Driver - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/remote/Driver.class
07:05:21.259 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.UnregisteredDriver - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/UnregisteredDriver.class
07:05:21.260 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.Handler - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/Handler.class
07:05:21.260 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.remote.Service - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/remote/Service.class
07:05:21.261 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.Meta - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/Meta.class
07:05:21.261 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.UnregisteredDriver$JdbcVersion - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/UnregisteredDriver$JdbcVersion.class
07:05:21.261 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.PseudoColumnUsage - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/PseudoColumnUsage.class
07:05:21.261 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.UnregisteredDriver$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/UnregisteredDriver$1.class
07:05:21.262 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.NoSuchFieldError
07:05:21.262 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaJdbc41Factory - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaJdbc41Factory.class
07:05:21.262 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaFactory.class
07:05:21.262 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaConnection - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaConnection.class
07:05:21.263 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaJdbc41Factory$AvaticaJdbc41Connection - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaJdbc41Factory$AvaticaJdbc41Connection.class
07:05:21.263 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaDatabaseMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaDatabaseMetaData.class
07:05:21.264 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaJdbc41Factory$AvaticaJdbc41DatabaseMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaJdbc41Factory$AvaticaJdbc41DatabaseMetaData.class
07:05:21.264 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaStatement.class
07:05:21.265 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaJdbc41Factory$AvaticaJdbc41Statement - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaJdbc41Factory$AvaticaJdbc41Statement.class
07:05:21.265 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaPreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaPreparedStatement.class
07:05:21.265 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.ParameterMetaData - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/ParameterMetaData.class
07:05:21.266 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaJdbc41Factory$AvaticaJdbc41PreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaJdbc41Factory$AvaticaJdbc41PreparedStatement.class
07:05:21.267 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.DriverVersion - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/DriverVersion.class
07:05:21.267 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.HandlerImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/HandlerImpl.class
07:05:21.268 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.Driver - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/Driver.class
07:05:21.269 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.Driver$2 - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/Driver$2.class
07:05:21.269 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteJdbc41Factory - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteJdbc41Factory.class
07:05:21.269 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteFactory.class
07:05:21.270 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteJdbc41Factory$CalciteJdbc41Connection - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteJdbc41Factory$CalciteJdbc41Connection.class
07:05:21.270 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteConnectionImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteConnectionImpl.class
07:05:21.271 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteConnection - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteConnection.class
07:05:21.271 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.linq4j.QueryProvider - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-linq4j/jars/calcite-linq4j-1.2.0-incubating.jar!/org/apache/calcite/linq4j/QueryProvider.class
07:05:21.273 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteJdbc41Factory$CalciteJdbc41DatabaseMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteJdbc41Factory$CalciteJdbc41DatabaseMetaData.class
07:05:21.273 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteJdbc41Factory$CalciteJdbc41Statement - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteJdbc41Factory$CalciteJdbc41Statement.class
07:05:21.274 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteStatement.class
07:05:21.275 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteJdbc41Factory$CalciteJdbc41PreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteJdbc41Factory$CalciteJdbc41PreparedStatement.class
07:05:21.275 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalcitePreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalcitePreparedStatement.class
07:05:21.276 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.AvaticaResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/AvaticaResultSet.class
07:05:21.276 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.avatica.util.ArrayImpl$Factory - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar!/org/apache/calcite/avatica/util/ArrayImpl$Factory.class
07:05:21.277 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalciteResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalciteResultSet.class
07:05:21.278 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.Driver$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/Driver$1.class
07:05:21.278 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalcitePrepare - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalcitePrepare.class
07:05:21.278 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.linq4j.function.Function0 - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-linq4j/jars/calcite-linq4j-1.2.0-incubating.jar!/org/apache/calcite/linq4j/function/Function0.class
07:05:21.279 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.linq4j.function.Function - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-linq4j/jars/calcite-linq4j-1.2.0-incubating.jar!/org/apache/calcite/linq4j/function/Function.class
07:05:21.279 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalcitePrepare$2 - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalcitePrepare$2.class
07:05:21.279 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.calcite.jdbc.CalcitePrepare$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.calcite/calcite-core/jars/calcite-core-1.2.0-incubating.jar!/org/apache/calcite/jdbc/CalcitePrepare$1.class
07:05:21.280 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.h2.Driver - jar:file:/Users/lian/.ivy2/cache/com.h2database/h2/jars/h2-1.4.183.jar!/org/h2/Driver.class
07:05:21.280 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Blob - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Blob.class
07:05:21.280 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Clob - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Clob.class
07:05:21.281 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Savepoint - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Savepoint.class
07:05:21.281 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.security.DatabasePermission - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/security/DatabasePermission.class
07:05:21.283 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m2 - null
07:05:21.283 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m2_en - null
07:05:21.284 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m2_en_US - null
07:05:21.284 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.memory.LowMemory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/memory/LowMemory.class
07:05:21.285 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLPermission - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLPermission.class
07:05:21.285 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLClientInfoException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLClientInfoException.class
07:05:21.285 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.TransactionResourceImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/TransactionResourceImpl.class
07:05:21.286 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.IdUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/IdUtil.class
07:05:21.287 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedConnectionContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedConnectionContext.class
07:05:21.287 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.ConnectionContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/ConnectionContext.class
07:05:21.287 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.ref.SoftReference
07:05:21.287 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.BaseMonitor$ProviderEnumeration - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/BaseMonitor$ProviderEnumeration.class
07:05:21.288 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.io.DirStorageFactory4 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/io/DirStorageFactory4.class
07:05:21.288 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.io.DirStorageFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/io/DirStorageFactory.class
07:05:21.288 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.io.WritableStorageFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/io/WritableStorageFactory.class
07:05:21.289 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.io.BaseStorageFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/io/BaseStorageFactory.class
07:05:21.289 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService.class
07:05:21.290 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService$3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService$3.class
07:05:21.291 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService$2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService$2.class
07:05:21.291 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService$12 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService$12.class
07:05:21.291 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService$4 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService$4.class
07:05:21.292 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.io.DirFile4 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/io/DirFile4.class
07:05:21.292 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.io.DirFile - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/io/DirFile.class
07:05:21.293 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.RandomAccessFile - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/RandomAccessFile.class
07:05:21.293 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.nio.channels.OverlappingFileLockException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/nio/channels/OverlappingFileLockException.class
07:05:21.293 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.nio.channels.AsynchronousCloseException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/nio/channels/AsynchronousCloseException.class
07:05:21.293 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService$FileOperationHelper - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService$FileOperationHelper.class
07:05:21.294 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService$10 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService$10.class
07:05:21.295 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLDecimal - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLDecimal.class
07:05:21.295 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.NumberDataType - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/NumberDataType.class
07:05:21.296 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.ObjectInput - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/ObjectInput.class
07:05:21.296 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.IllegalAccessError
07:05:21.296 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.IncompatibleClassChangeError
07:05:21.297 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.cache.ClassSize - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/cache/ClassSize.class
07:05:21.298 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.cache.ClassSizeCatalog - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/cache/ClassSizeCatalog.class
07:05:21.298 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.cache.ClassSize$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/cache/ClassSize$1.class
07:05:21.298 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.TypeId - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/TypeId.class
07:05:21.299 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.BaseTypeIdImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/BaseTypeIdImpl.class
07:05:21.300 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.UserDefinedTypeIdImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/UserDefinedTypeIdImpl.class
07:05:21.300 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.DecimalTypeIdImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/DecimalTypeIdImpl.class
07:05:21.301 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.RegisteredFormatIds - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/RegisteredFormatIds.class
07:05:21.302 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.daemon.BasicDaemon - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/daemon/BasicDaemon.class
07:05:21.303 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.FileMonitor$4 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/FileMonitor$4.class
07:05:21.303 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.IllegalThreadStateException
07:05:21.304 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.daemon.SingleThreadDaemonFactory$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/daemon/SingleThreadDaemonFactory$1.class
07:05:21.304 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.LockingPolicy - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/LockingPolicy.class
07:05:21.305 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.ConcurrentLockSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/ConcurrentLockSet.class
07:05:21.305 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.locks.Latch - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/locks/Latch.class
07:05:21.306 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.Control - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/Control.class
07:05:21.306 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.Lock - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/Lock.class
07:05:21.306 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.ActiveLock - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/ActiveLock.class
07:05:21.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.NoLocking - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/NoLocking.class
07:05:21.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.RowLocking1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/RowLocking1.class
07:05:21.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.RowLocking2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/RowLocking2.class
07:05:21.308 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.RowLockingRR - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/RowLockingRR.class
07:05:21.308 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.RowLocking3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/RowLocking3.class
07:05:21.308 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.RowLocking2nohold - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/RowLocking2nohold.class
07:05:21.309 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.ContainerLocking2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/ContainerLocking2.class
07:05:21.309 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.ContainerLocking3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/ContainerLocking3.class
07:05:21.309 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.TransactionTable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/TransactionTable.class
07:05:21.310 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.TransactionTable$EntryVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/TransactionTable$EntryVisitor.class
07:05:21.310 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.TransactionInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/TransactionInfo.class
07:05:21.311 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BaseDataFileFactory$3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BaseDataFileFactory$3.class
07:05:21.311 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BaseDataFileFactory$2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BaseDataFileFactory$2.class
07:05:21.311 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BaseDataFileFactory$1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BaseDataFileFactory$1.class
07:05:21.312 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.ProtectionDomain - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/ProtectionDomain.class
07:05:21.312 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.CodeSource - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/CodeSource.class
07:05:21.312 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.io.DirRandomAccessFile - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/io/DirRandomAccessFile.class
07:05:21.313 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.FileDescriptor - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/FileDescriptor.class
07:05:21.315 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.nio.channels.FileChannel - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/nio/channels/FileChannel.class
07:05:21.319 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m13 - null
07:05:21.320 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m13_en - null
07:05:21.321 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m13_en_US - null
07:05:21.321 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.ConcurrentCache - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/ConcurrentCache.class
07:05:21.322 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.ReplacementPolicy - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/ReplacementPolicy.class
07:05:21.323 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.ClockPolicy - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/ClockPolicy.class
07:05:21.323 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.ClockPolicy$Holder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/ClockPolicy$Holder.class
07:05:21.324 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.ReplacementPolicy$Callback - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/ReplacementPolicy$Callback.class
07:05:21.324 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.BackgroundCleaner - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/BackgroundCleaner.class
07:05:21.325 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m16 - null
07:05:21.325 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m16_en - null
07:05:21.326 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m16_en_US - null
07:05:21.326 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.RFResource - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/RFResource.class
07:05:21.327 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.zip.CRC32 - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/zip/CRC32.class
07:05:21.327 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.ReuseFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/ReuseFactory.class
07:05:21.328 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m27 - null
07:05:21.329 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m27_en - null
07:05:21.329 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m27_en_US - null
07:05:21.330 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.DataOutputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/DataOutputStream.class
07:05:21.331 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.log.LogAccessFile - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/log/LogAccessFile.class
07:05:21.332 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.ArrayOutputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/ArrayOutputStream.class
07:05:21.332 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.Limit - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/Limit.class
07:05:21.333 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.log.LogAccessFileBuffer - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/log/LogAccessFileBuffer.class
07:05:21.333 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.log.ChecksumOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/log/ChecksumOperation.class
07:05:21.334 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.zip.Checksum - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/zip/Checksum.class
07:05:21.334 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatIdUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatIdUtil.class
07:05:21.334 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.log.LogRecord - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/log/LogRecord.class
07:05:21.335 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.CompressedNumber - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/CompressedNumber.class
07:05:21.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatIdOutputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatIdOutputStream.class
07:05:21.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.ObjectOutput - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/ObjectOutput.class
07:05:21.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.ErrorInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/ErrorInfo.class
07:05:21.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.daemon.ServiceRecord - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/daemon/ServiceRecord.class
07:05:21.337 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Math
07:05:21.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.ArrayBlockingQueue - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/ArrayBlockingQueue.class
07:05:21.339 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.LockSpace - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/LockSpace.class
07:05:21.340 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.XactId - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/XactId.class
07:05:21.340 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.XactContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/XactContext.class
07:05:21.341 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.TransactionTableEntry - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/TransactionTableEntry.class
07:05:21.342 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.RAMTransaction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/RAMTransaction.class
07:05:21.342 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.XATransactionController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/XATransactionController.class
07:05:21.342 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.TransactionManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/TransactionManager.class
07:05:21.343 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.GroupFetchScanController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/GroupFetchScanController.class
07:05:21.343 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.GenericScanController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/GenericScanController.class
07:05:21.343 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.RowCountable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/RowCountable.class
07:05:21.344 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.RowLocationRetRowSource - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/RowLocationRetRowSource.class
07:05:21.344 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.RowSource - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/RowSource.class
07:05:21.344 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.BackingStoreHashtable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/BackingStoreHashtable.class
07:05:21.345 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.BackingStoreHashTableFromScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/BackingStoreHashTableFromScan.class
07:05:21.345 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.ScanController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/ScanController.class
07:05:21.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.RAMTransactionContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/RAMTransactionContext.class
07:05:21.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.PropertyConglomerate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/PropertyConglomerate.class
07:05:21.347 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatableHashtable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatableHashtable.class
07:05:21.347 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.UTF - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/UTF.class
07:05:21.348 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.UserType - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/UserType.class
07:05:21.349 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.OpenConglomerate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/OpenConglomerate.class
07:05:21.349 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.heap.OpenHeap - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/heap/OpenHeap.class
07:05:21.349 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.ScanManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/ScanManager.class
07:05:21.349 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.StoreCostController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/StoreCostController.class
07:05:21.350 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.DynamicCompiledOpenConglomInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/DynamicCompiledOpenConglomInfo.class
07:05:21.350 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.ConglomerateController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/ConglomerateController.class
07:05:21.351 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.ConglomPropertyQueryable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/ConglomPropertyQueryable.class
07:05:21.351 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BaseContainerHandle - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BaseContainerHandle.class
07:05:21.352 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Observer - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Observer.class
07:05:21.352 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.Page - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/Page.class
07:05:21.352 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.RecordHandle - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/RecordHandle.class
07:05:21.353 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.ContainerLock - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/ContainerLock.class
07:05:21.353 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.ConcurrentLockSet$Entry - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/ConcurrentLockSet$Entry.class
07:05:21.354 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.cache.CacheEntry - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/cache/CacheEntry.class
07:05:21.354 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.RAFContainer4 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/RAFContainer4.class
07:05:21.355 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.RAFContainer - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/RAFContainer.class
07:05:21.356 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.InterruptDetectedException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/InterruptDetectedException.class
07:05:21.356 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.AllocationCache - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/AllocationCache.class
07:05:21.356 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.SpaceInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/SpaceInfo.class
07:05:21.360 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.nio.channels.ClosedByInterruptException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/nio/channels/ClosedByInterruptException.class
07:05:21.360 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.nio.channels.ClosedChannelException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/nio/channels/ClosedChannelException.class
07:05:21.361 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.ByteArray - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/ByteArray.class
07:05:21.362 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.AllocPage - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/AllocPage.class
07:05:21.363 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.StoredPage - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/StoredPage.class
07:05:21.364 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.CachedPage - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/CachedPage.class
07:05:21.364 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BasePage - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BasePage.class
07:05:21.366 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.LongColumnException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/LongColumnException.class
07:05:21.369 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.NoSpaceOnPage - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/NoSpaceOnPage.class
07:05:21.370 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.StoredRecordHeader - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/StoredRecordHeader.class
07:05:21.371 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.DynamicByteArrayOutputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/DynamicByteArrayOutputStream.class
07:05:21.372 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.ArrayInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/ArrayInputStream.class
07:05:21.372 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.LimitObjectInput - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/LimitObjectInput.class
07:05:21.373 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.ErrorObjectInput - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/ErrorObjectInput.class
07:05:21.374 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.ByteHolder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/ByteHolder.class
07:05:21.374 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.RememberBytesInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/RememberBytesInputStream.class
07:05:21.375 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.FilterInputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/FilterInputStream.class
07:05:21.375 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.OverflowInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/OverflowInputStream.class
07:05:21.376 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.Resetable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/Resetable.class
07:05:21.376 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.CloneableStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/CloneableStream.class
07:05:21.376 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.BufferedByteHolderInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/BufferedByteHolderInputStream.class
07:05:21.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.ByteHolderInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/ByteHolderInputStream.class
07:05:21.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatIdInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatIdInputStream.class
07:05:21.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.DataInputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/DataInputStream.class
07:05:21.379 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.PageTimeStamp - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/PageTimeStamp.class
07:05:21.381 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.RecordId - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/RecordId.class
07:05:21.382 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.PageKey - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/PageKey.class
07:05:21.382 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.nio.ByteBuffer - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/nio/ByteBuffer.class
07:05:21.383 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.locks.Condition - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/locks/Condition.class
07:05:21.384 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.DirectActions - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/DirectActions.class
07:05:21.384 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.UTFDataFormatException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/UTFDataFormatException.class
07:05:21.385 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.ObjectInputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/ObjectInputStream.class
07:05:21.385 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.ApplicationObjectInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/ApplicationObjectInputStream.class
07:05:21.385 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.StreamCorruptedException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/StreamCorruptedException.class
07:05:21.385 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.DirectAllocActions - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/DirectAllocActions.class
07:05:21.386 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.ContainerOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/ContainerOperation.class
07:05:21.386 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.Undoable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/Undoable.class
07:05:21.386 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.ContainerBasicOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/ContainerBasicOperation.class
07:05:21.387 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.Compensation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/Compensation.class
07:05:21.387 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.log.FileLogger - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/log/FileLogger.class
07:05:21.389 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.BeginXact - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/BeginXact.class
07:05:21.390 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.PageCreationArgs - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/PageCreationArgs.class
07:05:21.390 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatableInstanceGetter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatableInstanceGetter.class
07:05:21.390 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.ClassInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/ClassInfo.class
07:05:21.391 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.AllocExtent - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/AllocExtent.class
07:05:21.391 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatableBitSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatableBitSet.class
07:05:21.393 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.ConglomerateUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/ConglomerateUtil.class
07:05:21.394 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.StoredRecordHeader$OverflowInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/StoredRecordHeader$OverflowInfo.class
07:05:21.394 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.RawField - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/RawField.class
07:05:21.394 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.StoredFieldHeader - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/StoredFieldHeader.class
07:05:21.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.CacheableConglomerate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/CacheableConglomerate.class
07:05:21.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.CacheLock - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/CacheLock.class
07:05:21.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.locks.ShExLockable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/locks/ShExLockable.class
07:05:21.396 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.PC_XenaVersion - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/PC_XenaVersion.class
07:05:21.396 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.locks.ShExQual - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/locks/ShExQual.class
07:05:21.396 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.RowUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/RowUtil.class
07:05:21.397 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.FetchDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/FetchDescriptor.class
07:05:21.397 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.RowLocation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/RowLocation.class
07:05:21.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.OpenConglomerateScratchSpace - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/OpenConglomerateScratchSpace.class
07:05:21.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.locks.LockControl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/locks/LockControl.class
07:05:21.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.heap.HeapScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/heap/HeapScan.class
07:05:21.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.GenericScanController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/GenericScanController.class
07:05:21.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.GenericController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/GenericController.class
07:05:21.400 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.ScanInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/ScanInfo.class
07:05:21.401 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.RowPosition - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/RowPosition.class
07:05:21.401 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.UTFQualifier - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/UTFQualifier.class
07:05:21.402 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.heap.HeapController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/heap/HeapController.class
07:05:21.402 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.GenericConglomerateController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/GenericConglomerateController.class
07:05:21.403 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m28 - null
07:05:21.404 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m28_en - null
07:05:21.404 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m28_en_US - null
07:05:21.406 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.EndXact - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/EndXact.class
07:05:21.407 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.uuid.BasicUUIDGetter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/uuid/BasicUUIDGetter.class
07:05:21.408 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.TypeVariable
07:05:21.408 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.Type
07:05:21.409 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.UpdateLoader - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/UpdateLoader.class
07:05:21.409 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.ClassLoaderLock - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/ClassLoaderLock.class
07:05:21.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.JarLoader - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/JarLoader.class
07:05:21.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.SecureClassLoader - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/SecureClassLoader.class
07:05:21.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.DataTypeDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/DataTypeDescriptor.class
07:05:21.411 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.RowMultiSetImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/RowMultiSetImpl.class
07:05:21.411 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.TypeDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/TypeDescriptor.class
07:05:21.412 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.TypeDescriptorImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/TypeDescriptorImpl.class
07:05:21.413 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.DD_Version - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/DD_Version.class
07:05:21.415 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.TabInfoImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/TabInfoImpl.class
07:05:21.416 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.IndexInfoImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/IndexInfoImpl.class
07:05:21.416 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.DefaultInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/DefaultInfo.class
07:05:21.417 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.DataDescriptorGenerator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/DataDescriptorGenerator.class
07:05:21.418 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.ReferencedColumns - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/ReferencedColumns.class
07:05:21.419 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.GenericExecutionContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/GenericExecutionContext.class
07:05:21.420 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.StringUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/StringUtil.class
07:05:21.420 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ValueRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ValueRow.class
07:05:21.421 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLChar - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLChar.class
07:05:21.422 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.CounterOutputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/CounterOutputStream.class
07:05:21.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.DataTruncation - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/DataTruncation.class
07:05:21.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.StreamHeaderGenerator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/StreamHeaderGenerator.class
07:05:21.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.CharStreamHeaderGenerator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/CharStreamHeaderGenerator.class
07:05:21.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLLongint - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLLongint.class
07:05:21.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLVarchar - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLVarchar.class
07:05:21.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLBoolean - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLBoolean.class
07:05:21.427 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.IndexRowGenerator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/IndexRowGenerator.class
07:05:21.428 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.IndexDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/IndexDescriptor.class
07:05:21.428 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.IndexDescriptorImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/IndexDescriptorImpl.class
07:05:21.429 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.IndexRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/IndexRow.class
07:05:21.429 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.heap.HeapRowLocation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/heap/HeapRowLocation.class
07:05:21.430 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2ITableLocking3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2ITableLocking3.class
07:05:21.431 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2INoLocking - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2INoLocking.class
07:05:21.431 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BTreeLockingPolicy - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BTreeLockingPolicy.class
07:05:21.431 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IRowLocking3 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IRowLocking3.class
07:05:21.433 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.OpenBTree - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/OpenBTree.class
07:05:21.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.LeafControlRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/LeafControlRow.class
07:05:21.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.ControlRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/ControlRow.class
07:05:21.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.AuxObject - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/AuxObject.class
07:05:21.435 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.WaitError - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/WaitError.class
07:05:21.436 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BranchControlRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BranchControlRow.class
07:05:21.436 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.StorableFormatId - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/StorableFormatId.class
07:05:21.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IController.class
07:05:21.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BTreeController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BTreeController.class
07:05:21.441 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.LogicalUndo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/LogicalUndo.class
07:05:21.442 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IUndo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IUndo.class
07:05:21.442 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatableIntHolder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatableIntHolder.class
07:05:21.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.TemplateRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/TemplateRow.class
07:05:21.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.SearchParameters - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/SearchParameters.class
07:05:21.448 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLInteger - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLInteger.class
07:05:21.457 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.SystemColumn - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/SystemColumn.class
07:05:21.457 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SystemColumnImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SystemColumnImpl.class
07:05:21.458 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ConglomerateDescriptorList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ConglomerateDescriptorList.class
07:05:21.459 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ColumnDescriptorList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ColumnDescriptorList.class
07:05:21.459 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ConstraintDescriptorList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ConstraintDescriptorList.class
07:05:21.460 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.GenericDescriptorList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/GenericDescriptorList.class
07:05:21.468 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.CheckConstraintDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/CheckConstraintDescriptor.class
07:05:21.480 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.DependencyDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/DependencyDescriptor.class
07:05:21.487 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLLongvarchar - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLLongvarchar.class
07:05:21.495 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ViewDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ViewDescriptor.class
07:05:21.506 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BranchRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BranchRow.class
07:05:21.507 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IRowLockingRR - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IRowLockingRR.class
07:05:21.511 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.DataInputUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/DataInputUtil.class
07:05:21.512 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLTimestamp - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLTimestamp.class
07:05:21.519 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.FileInfoDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/FileInfoDescriptor.class
07:05:21.530 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.StatisticsDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/StatisticsDescriptor.class
07:05:21.554 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.RoleGrantDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/RoleGrantDescriptor.class
07:05:21.558 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.SequenceDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/SequenceDescriptor.class
07:05:21.568 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.RoutineAliasInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/RoutineAliasInfo.class
07:05:21.569 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.MethodAliasInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/MethodAliasInfo.class
07:05:21.570 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.NameTDCacheable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/NameTDCacheable.class
07:05:21.570 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.TDCacheable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/TDCacheable.class
07:05:21.571 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IForwardScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IForwardScan.class
07:05:21.571 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BTreeForwardScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BTreeForwardScan.class
07:05:21.571 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BTreeScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BTreeScan.class
07:05:21.573 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BTreeRowPosition - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BTreeRowPosition.class
07:05:21.575 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.TypesImplInstanceGetter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/TypesImplInstanceGetter.class
07:05:21.576 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.OIDTDCacheable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/OIDTDCacheable.class
07:05:21.577 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.ArrayUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/ArrayUtil.class
07:05:21.582 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.MemByteHolder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/MemByteHolder.class
07:05:21.606 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.OldRoutineType - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/OldRoutineType.class
07:05:21.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.security.MessageDigest - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/security/MessageDigest.class
07:05:21.611 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.depend.BasicDependencyManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/depend/BasicDependencyManager.class
07:05:21.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.depend.Dependency - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/depend/Dependency.class
07:05:21.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.depend.ProviderInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/depend/ProviderInfo.class
07:05:21.613 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.LanguageDbPropertySetter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/LanguageDbPropertySetter.class
07:05:21.613 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericParameterValueSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericParameterValueSet.class
07:05:21.614 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericParameter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericParameter.class
07:05:21.615 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.XactXAResourceManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/XactXAResourceManager.class
07:05:21.615 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.xa.XAResourceManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/xa/XAResourceManager.class
07:05:21.615 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.TransactionTable$UpdateTransactionCounter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/TransactionTable$UpdateTransactionCounter.class
07:05:21.624 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.daemon.IndexStatisticsDaemonImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/daemon/IndexStatisticsDaemonImpl.class
07:05:21.625 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.Statistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/Statistics.class
07:05:21.627 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService$6 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService$6.class
07:05:21.627 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m26 - null
07:05:21.628 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m26_en - null
07:05:21.628 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m26_en_US - null
07:05:21.629 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.monitor.StorageFactoryService$5 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/monitor/StorageFactoryService$5.class
07:05:21.629 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.BufferedWriter - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/BufferedWriter.class
07:05:21.629 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m22 - null
07:05:21.630 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m22_en - null
07:05:21.630 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m22_en_US - null
07:05:21.632 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.db.DatabaseContextImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/db/DatabaseContextImpl.class
07:05:21.632 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.db.DatabaseContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/db/DatabaseContext.class
07:05:21.632 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/conn/GenericLanguageConnectionContext.class
07:05:21.633 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.CompilerContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/CompilerContext.class
07:05:21.634 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CompilerContextImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CompilerContextImpl.class
07:05:21.634 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.conn.StatementContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/conn/StatementContext.class
07:05:21.635 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.conn.GenericStatementContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/conn/GenericStatementContext.class
07:05:21.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.conn.TempTableInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/conn/TempTableInfo.class
07:05:21.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.conn.SQLSessionContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/conn/SQLSessionContext.class
07:05:21.637 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.conn.Authorizer - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/conn/Authorizer.class
07:05:21.638 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.db.StoreClassFactoryContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/db/StoreClassFactoryContext.class
07:05:21.638 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.ClassFactoryContext - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/ClassFactoryContext.class
07:05:21.638 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.conn.GenericAuthorizer - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/conn/GenericAuthorizer.class
07:05:21.639 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.RowLock - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/RowLock.class
07:05:21.640 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.conn.SQLSessionContextImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/conn/SQLSessionContextImpl.class
07:05:21.640 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.InterruptStatus - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/InterruptStatus.class
07:05:21.641 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.error.SQLWarningFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/error/SQLWarningFactory.class
07:05:21.642 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m18 - null
07:05:21.642 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m18_en - null
07:05:21.643 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m18_en_US - null
07:05:21.644 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.base.FinalizableReferenceQueue
07:05:21.644 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.Executors - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/Executors.class
07:05:21.644 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.util.concurrent.MoreExecutors
07:05:21.644 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.ConnectionPartition - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/ConnectionPartition.class
07:05:21.645 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.CustomThreadFactory - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/CustomThreadFactory.class
07:05:21.645 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Thread$UncaughtExceptionHandler
07:05:21.645 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.DefaultConnectionStrategy - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/DefaultConnectionStrategy.class
07:05:21.645 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.AbstractConnectionStrategy - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/AbstractConnectionStrategy.class
07:05:21.646 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.LinkedBlockingQueue - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/LinkedBlockingQueue.class
07:05:21.646 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.ConnectionTesterThread - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/ConnectionTesterThread.class
07:05:21.647 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.ScheduledExecutorService - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/ScheduledExecutorService.class
07:05:21.647 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.PoolWatchThread - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/PoolWatchThread.class
07:05:21.647 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.concurrent.ExecutorService - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/ExecutorService.class
07:05:21.647 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: hive class: java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/locks/ReentrantReadWriteLock$ReadLock.class
07:05:21.650 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.ConnectionHandle - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/ConnectionHandle.class
07:05:21.652 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.StatementHandle - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/StatementHandle.class
07:05:21.653 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.CallableStatementHandle - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/CallableStatementHandle.class
07:05:21.653 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.PreparedStatementHandle - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/PreparedStatementHandle.class
07:05:21.655 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.IStatementCache - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/IStatementCache.class
07:05:21.655 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: shared class: com.google.common.collect.ImmutableSet
07:05:21.658 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: hive class: java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/concurrent/locks/ReentrantReadWriteLock$WriteLock.class
07:05:21.658 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.ConnectionPartition$1 - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/ConnectionPartition$1.class
07:05:21.658 BoneCP-pool-watch-thread DEBUG IsolatedClientLoader: shared class: com.google.common.base.FinalizableWeakReference
07:05:21.658 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@48e7961b" opened
07:05:21.661 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.adapter.DatastoreAdapterFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/adapter/DatastoreAdapterFactory.class
07:05:21.662 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.adapter.DatastoreAdapter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/adapter/DatastoreAdapter.class
07:05:21.662 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedDatabaseMetaData40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedDatabaseMetaData40.class
07:05:21.662 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedDatabaseMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedDatabaseMetaData.class
07:05:21.664 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.adapter.DerbyAdapter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/adapter/DerbyAdapter.class
07:05:21.665 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.adapter.BaseDatastoreAdapter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/adapter/BaseDatastoreAdapter.class
07:05:21.666 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.MappingManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/MappingManager.class
07:05:21.667 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.schema.SQLTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/schema/SQLTypeInfo.class
07:05:21.667 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.schema.DerbyTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/schema/DerbyTypeInfo.class
07:05:21.669 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.RDBMSMappingManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/RDBMSMappingManager.class
07:05:21.670 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.BitRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/BitRDBMSMapping.class
07:05:21.671 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.BooleanRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/BooleanRDBMSMapping.class
07:05:21.671 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.AbstractDatastoreMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/AbstractDatastoreMapping.class
07:05:21.671 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.DatastoreMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/DatastoreMapping.class
07:05:21.672 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/CharRDBMSMapping.class
07:05:21.673 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.RDBMSMappingManager$RDBMSTypeMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/RDBMSMappingManager$RDBMSTypeMapping.class
07:05:21.673 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Boolean (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:21.673 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Boolean (jdbc-type=BOOLEAN, sql-type=BOOLEAN, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BooleanRDBMSMapping, default=false)
07:05:21.673 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.TinyIntRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/TinyIntRDBMSMapping.class
07:05:21.674 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/SmallIntRDBMSMapping.class
07:05:21.674 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Boolean (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=false)
07:05:21.674 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/NumericRDBMSMapping.class
07:05:21.675 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Byte (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=false)
07:05:21.675 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Character (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=true)
07:05:21.675 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/IntegerRDBMSMapping.class
07:05:21.675 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Character (jdbc-type=INTEGER, sql-type=INTEGER, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:21.675 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Character (jdbc-type=INTEGER, sql-type=INT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:21.675 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/DoubleRDBMSMapping.class
07:05:21.676 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Double (jdbc-type=DOUBLE, sql-type=DOUBLE, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping, default=true)
07:05:21.676 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.DecimalRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/DecimalRDBMSMapping.class
07:05:21.676 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Double (jdbc-type=DECIMAL, sql-type=DECIMAL, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DecimalRDBMSMapping, default=false)
07:05:21.676 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.FloatRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/FloatRDBMSMapping.class
07:05:21.676 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Float (jdbc-type=FLOAT, sql-type=FLOAT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.FloatRDBMSMapping, default=true)
07:05:21.676 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Float (jdbc-type=DOUBLE, sql-type=DOUBLE, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping, default=false)
07:05:21.676 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.RealRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/RealRDBMSMapping.class
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Float (jdbc-type=REAL, sql-type=REAL, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.RealRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Float (jdbc-type=DECIMAL, sql-type=DECIMAL, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DecimalRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=INTEGER, sql-type=INTEGER, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=true)
07:05:21.677 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/BigIntRDBMSMapping.class
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=INTEGER, sql-type=INT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=true)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Integer (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Long (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=true)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Long (jdbc-type=INTEGER, sql-type=INT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Long (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Long (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Short (jdbc-type=SMALLINT, sql-type=SMALLINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping, default=true)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Short (jdbc-type=INTEGER, sql-type=INTEGER, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:21.677 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.Short (jdbc-type=INTEGER, sql-type=INT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:21.678 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/VarCharRDBMSMapping.class
07:05:21.678 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=true)
07:05:21.678 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:21.678 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:21.678 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.LongVarcharRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/LongVarcharRDBMSMapping.class
07:05:21.678 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=LONGVARCHAR, sql-type=LONGVARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.LongVarcharRDBMSMapping, default=false)
07:05:21.678 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.ClobRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/ClobRDBMSMapping.class
07:05:21.679 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=CLOB, sql-type=CLOB, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.ClobRDBMSMapping, default=false)
07:05:21.679 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.oracle.OracleClobRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/oracle/OracleClobRDBMSMapping.class
07:05:21.679 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.BlobRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/BlobRDBMSMapping.class
07:05:21.679 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.AbstractLargeBinaryRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/AbstractLargeBinaryRDBMSMapping.class
07:05:21.680 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=BLOB, sql-type=BLOB, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BlobRDBMSMapping, default=false)
07:05:21.680 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.oracle.OracleBlobRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/oracle/OracleBlobRDBMSMapping.class
07:05:21.680 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.DatalinkRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/DatalinkRDBMSMapping.class
07:05:21.681 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.SqlXmlRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/SqlXmlRDBMSMapping.class
07:05:21.681 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.oracle.XMLTypeRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/oracle/XMLTypeRDBMSMapping.class
07:05:21.681 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.NVarcharRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/NVarcharRDBMSMapping.class
07:05:21.682 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.NCharRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/NCharRDBMSMapping.class
07:05:21.682 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=NVARCHAR, sql-type=NVARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NVarcharRDBMSMapping, default=false)
07:05:21.682 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.lang.String (jdbc-type=NCHAR, sql-type=NCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NCharRDBMSMapping, default=false)
07:05:21.682 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.math.BigDecimal (jdbc-type=DECIMAL, sql-type=DECIMAL, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DecimalRDBMSMapping, default=true)
07:05:21.682 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.math.BigDecimal (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=false)
07:05:21.682 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.math.BigInteger (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=true)
07:05:21.682 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.DateRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/DateRDBMSMapping.class
07:05:21.682 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=DATE, sql-type=DATE, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DateRDBMSMapping, default=true)
07:05:21.683 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/TimestampRDBMSMapping.class
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=TIMESTAMP, sql-type=TIMESTAMP, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Date (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.TimeRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/TimeRDBMSMapping.class
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=TIME, sql-type=TIME, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimeRDBMSMapping, default=true)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=TIMESTAMP, sql-type=TIMESTAMP, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Time (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Timestamp (jdbc-type=TIMESTAMP, sql-type=TIMESTAMP, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping, default=true)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Timestamp (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.sql.Timestamp (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=TIMESTAMP, sql-type=TIMESTAMP, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimestampRDBMSMapping, default=true)
07:05:21.683 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=DATE, sql-type=DATE, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.DateRDBMSMapping, default=false)
07:05:21.684 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:21.684 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:21.684 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=false)
07:05:21.684 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.util.Date (jdbc-type=TIME, sql-type=TIME, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.TimeRDBMSMapping, default=false)
07:05:21.684 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.LongVarBinaryRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/LongVarBinaryRDBMSMapping.class
07:05:21.684 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.io.Serializable (jdbc-type=LONGVARBINARY, sql-type=LONGVARBINARY, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.LongVarBinaryRDBMSMapping, default=true)
07:05:21.684 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.io.Serializable (jdbc-type=BLOB, sql-type=BLOB, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BlobRDBMSMapping, default=false)
07:05:21.684 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.VarBinaryRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/VarBinaryRDBMSMapping.class
07:05:21.684 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.io.Serializable (jdbc-type=VARBINARY, sql-type=VARBINARY, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarBinaryRDBMSMapping, default=false)
07:05:21.684 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.TimesTenVarBinaryRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/TimesTenVarBinaryRDBMSMapping.class
07:05:21.685 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.BinaryStreamRDBMSMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/BinaryStreamRDBMSMapping.class
07:05:21.685 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type java.io.File (jdbc-type=LONGVARBINARY, sql-type=LONGVARBINARY, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BinaryStreamRDBMSMapping, default=true)
07:05:21.685 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=BIGINT, sql-type=BIGINT, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping, default=true)
07:05:21.685 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=INTEGER, sql-type=INTEGER, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping, default=false)
07:05:21.685 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=NUMERIC, sql-type=NUMERIC, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.NumericRDBMSMapping, default=false)
07:05:21.685 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=CHAR, sql-type=CHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping, default=false)
07:05:21.685 pool-1-thread-1 DEBUG Datastore: Adding RDBMS support for Java type org.datanucleus.identity.OID (jdbc-type=VARCHAR, sql-type=VARCHAR, datastore-mapping-type=org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping, default=false)
07:05:21.685 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.schema.RDBMSTypesInfo - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/schema/RDBMSTypesInfo.class
07:05:21.685 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.MapStoreSchemaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/MapStoreSchemaData.class
07:05:21.687 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.SPSNameCacheable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/SPSNameCacheable.class
07:05:21.688 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedPreparedStatement42 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedPreparedStatement42.class
07:05:21.688 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedPreparedStatement40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedPreparedStatement40.class
07:05:21.688 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedPreparedStatement30 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedPreparedStatement30.class
07:05:21.689 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedPreparedStatement20 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedPreparedStatement20.class
07:05:21.689 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedPreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedPreparedStatement.class
07:05:21.689 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.EnginePreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/EnginePreparedStatement.class
07:05:21.690 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.EngineStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/EngineStatement.class
07:05:21.690 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedStatement.class
07:05:21.693 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ExecPreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ExecPreparedStatement.class
07:05:21.693 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.EngineParameterMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/EngineParameterMetaData.class
07:05:21.693 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.RawToBinaryFormatStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/RawToBinaryFormatStream.class
07:05:21.694 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.LimitInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/LimitInputStream.class
07:05:21.694 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.ReaderToUTF8Stream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/ReaderToUTF8Stream.class
07:05:21.695 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericStatement.class
07:05:21.696 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericStorablePreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericStorablePreparedStatement.class
07:05:21.696 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.StorablePreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/StorablePreparedStatement.class
07:05:21.696 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericPreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericPreparedStatement.class
07:05:21.697 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.Visitable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/Visitable.class
07:05:21.698 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.conn.CachedStatement - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/conn/CachedStatement.class
07:05:21.698 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.Activation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/Activation.class
07:05:21.699 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericPreparedStatement$RowCountStatistics - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericPreparedStatement$RowCountStatistics.class
07:05:21.700 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.conn.GenericStatementContext$CancelQueryTask - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/conn/GenericStatementContext$CancelQueryTask.class
07:05:21.701 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ParserImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ParserImpl.class
07:05:21.701 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TokenMgrError - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TokenMgrError.class
07:05:21.702 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ParseException - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ParseException.class
07:05:21.702 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CharStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CharStream.class
07:05:21.702 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UCode_CharStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UCode_CharStream.class
07:05:21.703 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SQLParserTokenManager - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SQLParserTokenManager.class
07:05:21.704 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SQLParserConstants - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SQLParserConstants.class
07:05:21.713 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SQLParser - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SQLParser.class
07:05:21.716 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ResultSetNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ResultSetNode.class
07:05:21.717 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.QueryTreeNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/QueryTreeNode.class
07:05:21.717 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.FromTable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/FromTable.class
07:05:21.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.Optimizable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/Optimizable.class
07:05:21.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TableOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TableOperatorNode.class
07:05:21.719 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.JoinNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/JoinNode.class
07:05:21.720 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ValueNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ValueNode.class
07:05:21.720 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SubqueryNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SubqueryNode.class
07:05:21.721 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.NumericConstantNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/NumericConstantNode.class
07:05:21.721 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ConstantNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ConstantNode.class
07:05:21.722 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ParameterNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ParameterNode.class
07:05:21.722 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.NewInvocationNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/NewInvocationNode.class
07:05:21.723 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.MethodCallNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/MethodCallNode.class
07:05:21.723 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.JavaValueNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/JavaValueNode.class
07:05:21.724 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.JavaToSQLValueNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/JavaToSQLValueNode.class
07:05:21.725 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ColumnReference - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ColumnReference.class
07:05:21.726 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SelectNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SelectNode.class
07:05:21.728 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.StatementNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/StatementNode.class
07:05:21.729 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CursorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CursorNode.class
07:05:21.729 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.DMLStatementNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/DMLStatementNode.class
07:05:21.730 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TransactionStatementNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TransactionStatementNode.class
07:05:21.731 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CharConstantNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CharConstantNode.class
07:05:21.731 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TableName - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TableName.class
07:05:21.732 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SQLParser$LookaheadSuccess - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SQLParser$LookaheadSuccess.class
07:05:21.732 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SQLParser$JJCalls - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SQLParser$JJCalls.class
07:05:21.739 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ResultColumn - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ResultColumn.class
07:05:21.740 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TableElementNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TableElementNode.class
07:05:21.740 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ConstraintDefinitionNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ConstraintDefinitionNode.class
07:05:21.748 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.Token - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/Token.class
07:05:21.749 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ExecSPSNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ExecSPSNode.class
07:05:21.750 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.depend.BasicDependency - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/depend/BasicDependency.class
07:05:21.750 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.xact.SavePoint - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/xact/SavePoint.class
07:05:21.751 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ResultColumnList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ResultColumnList.class
07:05:21.752 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.QueryTreeNodeVector - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/QueryTreeNodeVector.class
07:05:21.753 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ExpressionClassBuilder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ExpressionClassBuilder.class
07:05:21.753 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.ExpressionClassBuilderInterface - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/ExpressionClassBuilderInterface.class
07:05:21.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ActivationClassBuilder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ActivationClassBuilder.class
07:05:21.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.VirtualColumnNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/VirtualColumnNode.class
07:05:21.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.IsNullNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/IsNullNode.class
07:05:21.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.RelationalOperator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/RelationalOperator.class
07:05:21.757 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UnaryComparisonOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UnaryComparisonOperatorNode.class
07:05:21.757 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UnaryOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UnaryOperatorNode.class
07:05:21.757 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OperatorNode.class
07:05:21.758 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BinaryRelationalOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BinaryRelationalOperatorNode.class
07:05:21.758 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BinaryComparisonOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BinaryComparisonOperatorNode.class
07:05:21.758 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BinaryOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BinaryOperatorNode.class
07:05:21.759 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.AndNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/AndNode.class
07:05:21.759 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BinaryLogicalOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BinaryLogicalOperatorNode.class
07:05:21.760 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CastNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CastNode.class
07:05:21.761 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TernaryOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TernaryOperatorNode.class
07:05:21.766 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UntypedNullConstantNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UntypedNullConstantNode.class
07:05:21.767 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.FromList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/FromList.class
07:05:21.767 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.OptimizableList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/OptimizableList.class
07:05:21.770 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BooleanConstantNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BooleanConstantNode.class
07:05:21.770 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.RowResultSetNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/RowResultSetNode.class
07:05:21.772 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.AccessPath - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/AccessPath.class
07:05:21.773 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.OptimizablePredicateList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/OptimizablePredicateList.class
07:05:21.774 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UnionNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UnionNode.class
07:05:21.775 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SetOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SetOperatorNode.class
07:05:21.776 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.OptimizablePredicate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/OptimizablePredicate.class
07:05:21.777 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OrderByList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OrderByList.class
07:05:21.778 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.RequiredRowOrdering - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/RequiredRowOrdering.class
07:05:21.778 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OrderedColumnList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OrderedColumnList.class
07:05:21.797 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OrNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OrNode.class
07:05:21.797 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.PredicateList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/PredicateList.class
07:05:21.798 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SubqueryList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SubqueryList.class
07:05:21.799 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.MaterializeSubqueryNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/MaterializeSubqueryNode.class
07:05:21.801 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.FromSubquery - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/FromSubquery.class
07:05:21.805 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.FromBaseTable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/FromBaseTable.class
07:05:21.808 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ValueNodeList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ValueNodeList.class
07:05:21.810 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.GroupByList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/GroupByList.class
07:05:21.812 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CollectNodesVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CollectNodesVisitor.class
07:05:21.812 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.WindowFunctionNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/WindowFunctionNode.class
07:05:21.813 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.WindowNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/WindowNode.class
07:05:21.813 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.WindowDefinitionNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/WindowDefinitionNode.class
07:05:21.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ExecCursorTableReference - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ExecCursorTableReference.class
07:05:21.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.Predicate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/Predicate.class
07:05:21.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.InListOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/InListOperatorNode.class
07:05:21.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BinaryListOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BinaryListOperatorNode.class
07:05:21.819 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.FromVTI - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/FromVTI.class
07:05:21.819 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.vti.VTIEnvironment - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/vti/VTIEnvironment.class
07:05:21.820 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.vti.Restriction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/vti/Restriction.class
07:05:21.820 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.vti.Restriction$ColumnQualifier - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/vti/Restriction$ColumnQualifier.class
07:05:21.821 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.vti.Restriction$AND - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/vti/Restriction$AND.class
07:05:21.821 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.vti.Restriction$OR - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/vti/Restriction$OR.class
07:05:21.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.AllResultColumn - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/AllResultColumn.class
07:05:21.825 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.HasNodeVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/HasNodeVisitor.class
07:05:21.827 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CharTypeCompiler - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CharTypeCompiler.class
07:05:21.828 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.NumericTypeCompiler - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/NumericTypeCompiler.class
07:05:21.829 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BooleanTypeCompiler - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BooleanTypeCompiler.class
07:05:21.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.JSQLType - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/JSQLType.class
07:05:21.835 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.util.JBitSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/util/JBitSet.class
07:05:21.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ProjectRestrictNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ProjectRestrictNode.class
07:05:21.837 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SingleChildResultSetNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SingleChildResultSetNode.class
07:05:21.838 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.MaterializeResultSetNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/MaterializeResultSetNode.class
07:05:21.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.RemapCRsVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/RemapCRsVisitor.class
07:05:21.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ConstantExpressionVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ConstantExpressionVisitor.class
07:05:21.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.HasCorrelatedCRsVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/HasCorrelatedCRsVisitor.class
07:05:21.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.JoinStrategy - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/JoinStrategy.class
07:05:21.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.NestedLoopJoinStrategy - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/NestedLoopJoinStrategy.class
07:05:21.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BaseJoinStrategy - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BaseJoinStrategy.class
07:05:21.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.HashJoinStrategy - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/HashJoinStrategy.class
07:05:21.843 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.Level2OptimizerImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/Level2OptimizerImpl.class
07:05:21.843 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OptimizerImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OptimizerImpl.class
07:05:21.844 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CostEstimateImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CostEstimateImpl.class
07:05:21.845 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.compile.RowOrdering - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/compile/RowOrdering.class
07:05:21.846 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.Level2CostEstimateImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/Level2CostEstimateImpl.class
07:05:21.846 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.RowOrderingImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/RowOrderingImpl.class
07:05:21.847 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ColumnOrdering - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ColumnOrdering.class
07:05:21.847 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.AccessPathImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/AccessPathImpl.class
07:05:21.854 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BaseTableNumbersVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BaseTableNumbersVisitor.class
07:05:21.857 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ScrollInsensitiveResultSetNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ScrollInsensitiveResultSetNode.class
07:05:21.858 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.BCClass - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/BCClass.class
07:05:21.858 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.GClass - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/GClass.class
07:05:21.859 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.compiler.MethodBuilder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/compiler/MethodBuilder.class
07:05:21.859 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.compiler.LocalField - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/compiler/LocalField.class
07:05:21.859 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.ClassHolder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/ClassHolder.class
07:05:21.860 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.ConstantPoolEntry - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/ConstantPoolEntry.class
07:05:21.860 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.CONSTANT_Utf8_info - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/CONSTANT_Utf8_info.class
07:05:21.861 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.CONSTANT_Index_info - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/CONSTANT_Index_info.class
07:05:21.861 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.CONSTANT_Float_info - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/CONSTANT_Float_info.class
07:05:21.861 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.CONSTANT_Double_info - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/CONSTANT_Double_info.class
07:05:21.862 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.CONSTANT_Integer_info - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/CONSTANT_Integer_info.class
07:05:21.862 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.CONSTANT_Long_info - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/CONSTANT_Long_info.class
07:05:21.862 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.Type - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/Type.class
07:05:21.863 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.MemberTable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/MemberTable.class
07:05:21.863 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.MemberTableHash - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/MemberTableHash.class
07:05:21.864 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.BCMethod - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/BCMethod.class
07:05:21.865 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.BCMethodDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/BCMethodDescriptor.class
07:05:21.866 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.ClassMember - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/ClassMember.class
07:05:21.866 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.CodeChunk - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/CodeChunk.class
07:05:21.867 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.ClassFormatOutput - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/ClassFormatOutput.class
07:05:21.868 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.Attributes - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/Attributes.class
07:05:21.868 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.classfile.AttributeEntry - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/classfile/AttributeEntry.class
07:05:21.869 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericColumnDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericColumnDescriptor.class
07:05:21.869 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericResultDescription - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericResultDescription.class
07:05:21.870 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.BCMethodCaller - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/BCMethodCaller.class
07:05:21.870 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.BCLocalField - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/BCLocalField.class
07:05:21.870 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ResultColumnList$ColumnMapping - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ResultColumnList$ColumnMapping.class
07:05:21.871 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.catalog.types.ReferencedColumnsDescriptorImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/catalog/types/ReferencedColumnsDescriptorImpl.class
07:05:21.871 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.HasVariantValueNodeVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/HasVariantValueNodeVisitor.class
07:05:21.885 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.bytecode.Conditional - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/bytecode/Conditional.class
07:05:21.887 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.AggregateNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/AggregateNode.class
07:05:21.889 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.CursorActivation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/CursorActivation.class
07:05:21.890 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.BaseActivation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/BaseActivation.class
07:05:21.890 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.CursorActivation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/CursorActivation.class
07:05:21.891 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.GeneratedByteCode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/GeneratedByteCode.class
07:05:21.891 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.loader.GeneratedMethod - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/loader/GeneratedMethod.class
07:05:21.891 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.DirectCall - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/DirectCall.class
07:05:21.893 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.LikeEscapeOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/LikeEscapeOperatorNode.class
07:05:21.895 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.StaticMethodCallNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/StaticMethodCallNode.class
07:05:21.899 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.RowChangerImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/RowChangerImpl.class
07:05:21.899 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.LoggableActions - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/LoggableActions.class
07:05:21.900 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.LoggableAllocActions - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/LoggableAllocActions.class
07:05:21.901 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.CompressSpacePageOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/CompressSpacePageOperation.class
07:05:21.901 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.PhysicalPageOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/PhysicalPageOperation.class
07:05:21.901 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.PageBasicOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/PageBasicOperation.class
07:05:21.902 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.RePreparable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/RePreparable.class
07:05:21.902 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.CompressSpacePageOperation10_2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/CompressSpacePageOperation10_2.class
07:05:21.902 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.IndexSetChanger - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/IndexSetChanger.class
07:05:21.903 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.IndexChanger - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/IndexChanger.class
07:05:21.903 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLDate - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLDate.class
07:05:21.905 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLTime - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLTime.class
07:05:21.906 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.RowUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/RowUtil.class
07:05:21.907 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.UpdateOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/UpdateOperation.class
07:05:21.908 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.InitPageOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/InitPageOperation.class
07:05:21.909 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.AllocPageOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/AllocPageOperation.class
07:05:21.909 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.InsertOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/InsertOperation.class
07:05:21.910 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.LogicalPageOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/LogicalPageOperation.class
07:05:21.910 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.raw.LogicalUndoable - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/raw/LogicalUndoable.class
07:05:21.911 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.CursorInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/CursorInfo.class
07:05:21.911 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.ObjectOutputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/ObjectOutputStream.class
07:05:21.917 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.ReclaimSpace - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/ReclaimSpace.class
07:05:21.919 derby.rawStoreDaemon DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.ReclaimSpaceHelper - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/ReclaimSpaceHelper.class
07:05:21.920 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.GenericActivationHolder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/GenericActivationHolder.class
07:05:21.921 derby.rawStoreDaemon DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.SetReservedSpaceOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/SetReservedSpaceOperation.class
07:05:21.921 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.NoPutResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/NoPutResultSet.class
07:05:21.922 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.UnionResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/UnionResultSet.class
07:05:21.922 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.CursorResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/CursorResultSet.class
07:05:21.922 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.NoPutResultSetImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/NoPutResultSetImpl.class
07:05:21.923 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/BasicNoPutResultSetImpl.class
07:05:21.923 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.RowResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/RowResultSet.class
07:05:21.927 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLSmallint - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLSmallint.class
07:05:21.929 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.GenericResultSetFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/GenericResultSetFactory.class
07:05:21.931 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.NormalizeResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/NormalizeResultSet.class
07:05:21.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.services.reflect.ReflectMethod - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/services/reflect/ReflectMethod.class
07:05:21.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ProjectRestrictResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ProjectRestrictResultSet.class
07:05:21.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.BinaryToRawStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/BinaryToRawStream.class
07:05:21.935 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.CloseFilterInputStream - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/CloseFilterInputStream.class
07:05:21.935 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.ReaderToAscii - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/ReaderToAscii.class
07:05:21.936 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.UTF8Reader - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/UTF8Reader.class
07:05:21.936 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.jdbc.CharacterStreamDescriptor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/jdbc/CharacterStreamDescriptor.class
07:05:21.939 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.schema.JDBCTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/schema/JDBCTypeInfo.class
07:05:21.942 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: sun.reflect.MethodAccessorImpl - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/sun/reflect/MethodAccessorImpl.class
07:05:21.943 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.JDBCUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/JDBCUtils.class
07:05:21.944 pool-1-thread-1 DEBUG Datastore: Removing RDBMS support for Java type java.lang.String (jdbc-type=NVARCHAR, sql-type=NVARCHAR)
07:05:21.944 pool-1-thread-1 DEBUG Datastore: Removing RDBMS support for Java type java.lang.String (jdbc-type=NCHAR, sql-type=NCHAR)
07:05:21.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.naming.DN2NamingFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/naming/DN2NamingFactory.class
07:05:21.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.naming.AbstractNamingFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/naming/AbstractNamingFactory.class
07:05:21.945 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.schema.naming.NamingCase - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/schema/naming/NamingCase.class
07:05:21.945 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.DNIdentifierFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/DNIdentifierFactory.class
07:05:21.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.AbstractIdentifierFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/AbstractIdentifierFactory.class
07:05:21.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.IdentifierFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/IdentifierFactory.class
07:05:21.947 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.DatastoreIdentifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/DatastoreIdentifier.class
07:05:21.947 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.TooManyForeignKeysException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/TooManyForeignKeysException.class
07:05:21.948 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.TooManyIndicesException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/TooManyIndicesException.class
07:05:21.948 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.IdentifierCase - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/IdentifierCase.class
07:05:21.949 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.SQLController - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/SQLController.class
07:05:21.951 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.jdbc.EmbedStatement40 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/jdbc/EmbedStatement40.class
07:05:21.952 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.DropAliasNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/DropAliasNode.class
07:05:21.952 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.DDLStatementNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/DDLStatementNode.class
07:05:21.953 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m47 - null
07:05:21.954 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m47_en - null
07:05:21.954 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m47_en_US - null
07:05:21.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.jolbox.bonecp.hooks.ConnectionState - jar:file:/Users/lian/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar!/com/jolbox/bonecp/hooks/ConnectionState.class
07:05:21.955 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.net.SocketException
07:05:21.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CreateAliasNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CreateAliasNode.class
07:05:21.957 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ConstantActionActivation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ConstantActionActivation.class
07:05:21.957 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.GenericConstantActionFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/GenericConstantActionFactory.class
07:05:21.957 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ConstantAction.class
07:05:21.958 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ConstraintConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ConstraintConstantAction.class
07:05:21.958 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DDLSingleTableConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DDLSingleTableConstantAction.class
07:05:21.958 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DDLConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DDLConstantAction.class
07:05:21.959 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DropConstraintConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DropConstraintConstantAction.class
07:05:21.959 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.CreateAliasConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/CreateAliasConstantAction.class
07:05:21.960 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.StatementTablePermission - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/StatementTablePermission.class
07:05:21.960 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.StatementPermission - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/StatementPermission.class
07:05:21.961 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.StatementRoutinePermission - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/StatementRoutinePermission.class
07:05:21.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.MiscResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/MiscResultSet.class
07:05:21.964 pool-1-thread-1 DEBUG Datastore: ======================= Datastore =========================
07:05:21.964 pool-1-thread-1 DEBUG Datastore: StoreManager : "rdbms" (org.datanucleus.store.rdbms.RDBMSStoreManager)
07:05:21.964 pool-1-thread-1 DEBUG Datastore: Datastore : read-write
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Schema Control : AutoCreate(Tables,Columns,Constraints), Validate(None)
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Query Languages : [JDOQL, JPQL, SQL, STOREDPROC]
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Queries : Timeout=0
07:05:21.965 pool-1-thread-1 DEBUG Datastore: ===========================================================
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Datastore Adapter : org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Datastore : name="Apache Derby" version="10.10.2.0 - (1582446)"
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Datastore Driver : name="Apache Derby Embedded JDBC Driver" version="10.10.2.0 - (1582446)"
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Primary Connection Factory : URL[jdbc:derby:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8/metastore;create=true]
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Secondary Connection Factory : URL[jdbc:derby:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8/metastore;create=true]
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Datastore Identifiers : factory="datanucleus1" case=UPPERCASE catalog= schema=APP
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Supported Identifier Cases : "MixedCase" UPPERCASE "MixedCase-Sensitive"
07:05:21.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.IdentifierType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/IdentifierType.class
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Supported Identifier Lengths (max) : Table=128 Column=128 Constraint=128 Index=128 Delimiter="
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Support for Identifiers in DDL : catalog=false schema=true
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Datastore : checkTableViewExistence, rdbmsConstraintCreateMode=DataNucleus, initialiseColumnInfo=ALL
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Support Statement Batching : yes (max-batch-size=50)
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Queries : Results direction=forward, type=forward-only, concurrency=read-only
07:05:21.965 pool-1-thread-1 DEBUG Datastore: Java-Types : string-default-length=255
07:05:21.965 pool-1-thread-1 DEBUG Datastore: JDBC-Types : VARCHAR, LONGVARCHAR, BINARY, BOOLEAN, VARBINARY, LONGVARBINARY, BIGINT, JAVA_OBJECT, [id=2009], CHAR, NUMERIC, DECIMAL, INTEGER, CLOB, SMALLINT, BLOB, FLOAT, REAL, DOUBLE, DATE, TIME, TIMESTAMP
07:05:21.965 pool-1-thread-1 DEBUG Datastore: ===========================================================
07:05:21.965 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@48e7961b" closed
07:05:21.965 pool-1-thread-1 DEBUG Datastore: StoreManager now created
07:05:21.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContextPool - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContextPool.class
07:05:21.966 pool-1-thread-1 DEBUG Persistence: Started pool of ExecutionContext (maxPool=20, reaperThread=false)
07:05:21.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.ObjectProviderFactoryImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/ObjectProviderFactoryImpl.class
07:05:21.967 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.ReferentialJDOStateManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/ReferentialJDOStateManager.class
07:05:21.967 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.JDOStateManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/JDOStateManager.class
07:05:21.968 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.ObjectProvider - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/ObjectProvider.class
07:05:21.969 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.AbstractStateManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/AbstractStateManager.class
07:05:21.970 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.ObjectProviderFactoryImpl$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/ObjectProviderFactoryImpl$1.class
07:05:21.970 pool-1-thread-1 DEBUG Persistence: ================= NucleusContext ===============
07:05:21.970 pool-1-thread-1 DEBUG Persistence: DataNucleus Context : Version "3.2.10" with JRE "1.8.0_31" on "Mac OS X"
07:05:21.970 pool-1-thread-1 DEBUG Persistence: Persistence API : JDO
07:05:21.970 pool-1-thread-1 DEBUG Persistence: Plugin Registry : org.datanucleus.plugin.NonManagedPluginRegistry
07:05:21.970 pool-1-thread-1 DEBUG Persistence: ClassLoading : datanucleus
07:05:21.970 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.TimeZone - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/TimeZone.class
07:05:21.970 pool-1-thread-1 DEBUG Persistence: Persistence : pm-multithreaded, nontransactional-read, nontransactional-write, reachability-at-commit, detach-all-on-commit, copy-on-attach, managed-relations(checked), deletion-policy=JDO2, serverTimeZone=America/Los_Angeles
07:05:21.970 pool-1-thread-1 DEBUG Persistence: AutoStart : mechanism=None, mode=checked
07:05:21.970 pool-1-thread-1 DEBUG Persistence: Transactions : type=RESOURCE_LOCAL, mode=datastore, isolation=read-committed
07:05:21.970 pool-1-thread-1 DEBUG Persistence: ValueGeneration : txn-isolation=read-committed connection=New
07:05:21.970 pool-1-thread-1 DEBUG Persistence: Cache : Level1 (soft), Level2 (none, mode=UNSPECIFIED), QueryResults (soft), Collections/Maps
07:05:21.970 pool-1-thread-1 DEBUG Persistence: ================================================
07:05:21.970 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDODataStoreCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDODataStoreCache.class
07:05:21.971 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.cache.NullLevel2Cache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/cache/NullLevel2Cache.class
07:05:21.971 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.cache.Level2Cache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/cache/Level2Cache.class
07:05:21.971 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.cache.AbstractLevel2Cache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/cache/AbstractLevel2Cache.class
07:05:21.972 pool-1-thread-1 DEBUG Cache: Level 2 Cache of type "none" initialised
07:05:22.030 pool-1-thread-1 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
07:05:22.031 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.datastore.JDOConnection - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/datastore/JDOConnection.class
07:05:22.031 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.FetchPlanState - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/FetchPlanState.class
07:05:22.031 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.DetachState - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/DetachState.class
07:05:22.032 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.Extent - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/Extent.class
07:05:22.033 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.TransactionActiveOnCloseException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/TransactionActiveOnCloseException.class
07:05:22.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.FetchPlan - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/FetchPlan.class
07:05:22.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.typesafe.TypesafeQuery - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/query/typesafe/TypesafeQuery.class
07:05:22.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.Transaction - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/Transaction.class
07:05:22.036 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContextThreadedImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContextThreadedImpl.class
07:05:22.036 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContextImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContextImpl.class
07:05:22.038 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.NucleusObjectNotFoundException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/NucleusObjectNotFoundException.class
07:05:22.039 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.TransactionNotActiveException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/TransactionNotActiveException.class
07:05:22.039 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.ClassNotPersistableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/ClassNotPersistableException.class
07:05:22.040 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.LockManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/LockManager.class
07:05:22.041 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.RelationshipManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/RelationshipManager.class
07:05:22.042 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.Transaction - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/Transaction.class
07:05:22.042 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContextImpl$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContextImpl$1.class
07:05:22.042 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.CommitStateTransitionException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/CommitStateTransitionException.class
07:05:22.043 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.RollbackStateTransitionException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/RollbackStateTransitionException.class
07:05:22.044 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.ClassNotDetachableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/ClassNotDetachableException.class
07:05:22.044 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.ObjectDetachedException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/ObjectDetachedException.class
07:05:22.045 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.properties.BasePropertyStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/properties/BasePropertyStore.class
07:05:22.045 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.enhancer.jdo.JDOImplementationCreator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/enhancer/jdo/JDOImplementationCreator.class
07:05:22.046 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.enhancer.EnhancerClassLoader - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/enhancer/EnhancerClassLoader.class
07:05:22.046 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.enhancer.ClassEnhancer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/enhancer/ClassEnhancer.class
07:05:22.047 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.FetchPlan - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/FetchPlan.class
07:05:22.048 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.TransactionImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/TransactionImpl.class
07:05:22.049 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.RollbackException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/RollbackException.class
07:05:22.049 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.HeuristicRollbackException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/HeuristicRollbackException.class
07:05:22.050 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.HeuristicMixedException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/HeuristicMixedException.class
07:05:22.051 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.TransactionActiveOnBeginException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/TransactionActiveOnBeginException.class
07:05:22.051 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.TransactionManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/TransactionManager.class
07:05:22.052 pool-1-thread-1 DEBUG Persistence: ExecutionContext "org.datanucleus.ExecutionContextThreadedImpl@4acf81d3" opened for datastore "org.datanucleus.store.rdbms.RDBMSStoreManager@6cb80e27" with txn="org.datanucleus.TransactionImpl@5d658283"
07:05:22.052 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.cache.SoftRefCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/cache/SoftRefCache.class
07:05:22.052 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.cache.Level1Cache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/cache/Level1Cache.class
07:05:22.052 pool-1-thread-1 DEBUG Cache: Level 1 Cache of type "soft" initialised
07:05:22.052 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOFetchPlan - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOFetchPlan.class
07:05:22.053 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOTransaction - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOTransaction.class
07:05:22.053 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.exceptions.TransactionActiveException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/exceptions/TransactionActiveException.class
07:05:22.054 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.exceptions.TransactionCommitingException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/exceptions/TransactionCommitingException.class
07:05:22.054 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionExpressionForMetastore.class
07:05:22.055 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.PartitionExpressionProxy - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/PartitionExpressionProxy.class
07:05:22.055 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.class
07:05:22.057 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$ApplyFunc - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$ApplyFunc.class
07:05:22.058 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$1.class
07:05:22.058 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$2.class
07:05:22.058 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$3 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$3.class
07:05:22.059 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$4 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$4.class
07:05:22.059 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$5 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$5.class
07:05:22.060 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$6 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$6.class
07:05:22.060 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$7 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$7.class
07:05:22.060 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$8 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$8.class
07:05:22.060 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$9 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$9.class
07:05:22.061 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$10 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$10.class
07:05:22.061 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.IExtrapolatePartStatus - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/IExtrapolatePartStatus.class
07:05:22.062 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreDirectSql$DB - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreDirectSql$DB.class
07:05:22.062 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.Transaction - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/Transaction.class
07:05:22.063 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.transaction.XidImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/transaction/XidImpl.class
07:05:22.063 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:22.063 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:22.064 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.federation.FederatedStoreManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/federation/FederatedStoreManager.class
07:05:22.064 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.ConnectionManagerImpl$2 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/ConnectionManagerImpl$2.class
07:05:22.065 pool-1-thread-1 DEBUG BoneCPDataSource: JDBC URL = jdbc:derby:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8/metastore;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
07:05:22.068 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@7c8ce967" opened with isolation level "read-committed" and auto-commit=false
07:05:22.068 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/ConnectionFactoryImpl$EmulatedXAResource.class
07:05:22.069 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@450871df, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:22.069 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@450871df is starting for transaction Xid=    with flags 0
07:05:22.069 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.connection.ConnectionManagerImpl$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/connection/ConnectionManagerImpl$1.class
07:05:22.070 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5568b29 [conn=com.jolbox.bonecp.ConnectionHandle@7c8ce967, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:22.070 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.RDBMSStoreManager$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/RDBMSStoreManager$1.class
07:05:22.071 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.NucleusConnectionImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/NucleusConnectionImpl.class
07:05:22.071 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOConnectionJDBCImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOConnectionJDBCImpl.class
07:05:22.072 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOConnectionImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOConnectionImpl.class
07:05:22.074 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m38 - null
07:05:22.074 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m38_en - null
07:05:22.075 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m38_en_US - null
07:05:22.075 pool-1-thread-1 DEBUG MetaStoreDirectSql: MySql check failed, assuming we are not on MySql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
07:05:22.075 pool-1-thread-1 DEBUG Transaction: Transaction rolling back for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:22.075 pool-1-thread-1 DEBUG Transaction: Rolling back [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@450871df]]
07:05:22.075 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@450871df is rolling back for transaction Xid=   
07:05:22.075 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@450871df rolled back connection for transaction Xid=   
07:05:22.075 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@7c8ce967" closed
07:05:22.076 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5568b29 [conn=com.jolbox.bonecp.ConnectionHandle@7c8ce967, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:22.076 pool-1-thread-1 DEBUG Transaction: Transaction rolled back in 1 ms
07:05:22.076 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:22.076 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:22.076 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5dab2384" opened with isolation level "read-committed" and auto-commit=false
07:05:22.076 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@141d9e47, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:22.076 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@141d9e47 is starting for transaction Xid=    with flags 0
07:05:22.076 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5bf48e9e [conn=com.jolbox.bonecp.ConnectionHandle@5dab2384, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:22.077 pool-1-thread-1 DEBUG MetaStoreDirectSql: Oracle check failed, assuming we are not on Oracle: Lexical error at line 1, column 22.  Encountered: "$" (36), after : "".
07:05:22.077 pool-1-thread-1 DEBUG Transaction: Transaction rolling back for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:22.077 pool-1-thread-1 DEBUG Transaction: Rolling back [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@141d9e47]]
07:05:22.077 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@141d9e47 is rolling back for transaction Xid=   
07:05:22.077 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@141d9e47 rolled back connection for transaction Xid=   
07:05:22.077 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5dab2384" closed
07:05:22.077 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@5bf48e9e [conn=com.jolbox.bonecp.ConnectionHandle@5dab2384, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:22.077 pool-1-thread-1 DEBUG Transaction: Transaction rolled back in 0 ms
07:05:22.077 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:22.077 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:22.077 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@27d0736e" opened with isolation level "read-committed" and auto-commit=false
07:05:22.077 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1721385a, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:22.077 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1721385a is starting for transaction Xid=    with flags 0
07:05:22.077 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@ef41c3b [conn=com.jolbox.bonecp.ConnectionHandle@27d0736e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:22.077 pool-1-thread-1 DEBUG MetaStoreDirectSql: MSSQL check failed, assuming we are not on MSSQL: Lexical error at line 1, column 8.  Encountered: "@" (64), after : "".
07:05:22.077 pool-1-thread-1 DEBUG Transaction: Transaction rolling back for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:22.077 pool-1-thread-1 DEBUG Transaction: Rolling back [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1721385a]]
07:05:22.077 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1721385a is rolling back for transaction Xid=   
07:05:22.078 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1721385a rolled back connection for transaction Xid=   
07:05:22.078 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@27d0736e" closed
07:05:22.078 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@ef41c3b [conn=com.jolbox.bonecp.ConnectionHandle@27d0736e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:22.078 pool-1-thread-1 DEBUG Transaction: Transaction rolled back in 1 ms
07:05:22.078 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:22.078 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:22.078 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@34e64b79" opened with isolation level "read-committed" and auto-commit=false
07:05:22.078 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a84058f, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:22.078 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a84058f is starting for transaction Xid=    with flags 0
07:05:22.078 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7478e33a [conn=com.jolbox.bonecp.ConnectionHandle@34e64b79, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:22.078 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.QueryManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/QueryManager.class
07:05:22.079 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.evaluator.memory.InvocationEvaluator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/evaluator/memory/InvocationEvaluator.class
07:05:22.080 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.cache.SoftQueryCompilationCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/cache/SoftQueryCompilationCache.class
07:05:22.080 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.cache.AbstractQueryCompilationCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/cache/AbstractQueryCompilationCache.class
07:05:22.080 pool-1-thread-1 DEBUG Cache: Query Cache of type "org.datanucleus.query.cache.SoftQueryCompilationCache" initialised
07:05:22.080 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.cache.SoftQueryDatastoreCompilationCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/cache/SoftQueryDatastoreCompilationCache.class
07:05:22.081 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.cache.AbstractQueryDatastoreCompilationCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/cache/AbstractQueryDatastoreCompilationCache.class
07:05:22.081 pool-1-thread-1 DEBUG Cache: Query Cache of type "org.datanucleus.store.query.cache.SoftQueryDatastoreCompilationCache" initialised
07:05:22.081 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.cache.SoftQueryResultsCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/cache/SoftQueryResultsCache.class
07:05:22.081 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.cache.AbstractQueryResultsCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/cache/AbstractQueryResultsCache.class
07:05:22.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.cache.QueryResultsCache - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/cache/QueryResultsCache.class
07:05:22.082 pool-1-thread-1 DEBUG Cache: Query Cache of type "org.datanucleus.store.query.cache.SoftQueryResultsCache" initialised
07:05:22.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.JDOQLQuery - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/JDOQLQuery.class
07:05:22.083 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.AbstractJDOQLQuery - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/AbstractJDOQLQuery.class
07:05:22.084 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.AbstractJavaQuery - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/AbstractJavaQuery.class
07:05:22.084 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.Query - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/Query.class
07:05:22.085 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContextListener - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContextListener.class
07:05:22.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.QueryInvalidParametersException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/QueryInvalidParametersException.class
07:05:22.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.NoQueryResultsException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/NoQueryResultsException.class
07:05:22.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.QueryNotUniqueException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/QueryNotUniqueException.class
07:05:22.087 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.TransactionNotReadableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/TransactionNotReadableException.class
07:05:22.087 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.NucleusUnsupportedOptionException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/NucleusUnsupportedOptionException.class
07:05:22.088 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.JavaQueryCompiler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/JavaQueryCompiler.class
07:05:22.088 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.symbol.SymbolResolver - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/symbol/SymbolResolver.class
07:05:22.089 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.JDOQLCompiler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/JDOQLCompiler.class
07:05:22.090 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.evaluator.JavaQueryEvaluator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/evaluator/JavaQueryEvaluator.class
07:05:22.090 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.evaluator.JDOQLEvaluator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/evaluator/JDOQLEvaluator.class
07:05:22.091 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.AbstractRDBMSQueryResult - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/AbstractRDBMSQueryResult.class
07:05:22.091 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.AbstractQueryResult - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/AbstractQueryResult.class
07:05:22.091 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.QueryResult - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/QueryResult.class
07:05:22.092 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ScrollableQueryResult - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ScrollableQueryResult.class
07:05:22.093 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ForwardQueryResult - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ForwardQueryResult.class
07:05:22.094 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.QueryInterruptedException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/QueryInterruptedException.class
07:05:22.094 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.QueryTimeoutException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/QueryTimeoutException.class
07:05:22.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.SQLExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/SQLExpression.class
07:05:22.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.BooleanExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/BooleanExpression.class
07:05:22.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.BooleanSubqueryExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/BooleanSubqueryExpression.class
07:05:22.097 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOQuery - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOQuery.class
07:05:22.098 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == ''"
07:05:22.098 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.QueryCompilerSyntaxException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/QueryCompilerSyntaxException.class
07:05:22.099 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.symbol.Symbol - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/symbol/Symbol.class
07:05:22.099 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.PrimaryExpressionIsClassLiteralException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/PrimaryExpressionIsClassLiteralException.class
07:05:22.099 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.PrimaryExpressionIsClassStaticFieldException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/PrimaryExpressionIsClassStaticFieldException.class
07:05:22.100 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.PrimaryExpressionIsVariableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/PrimaryExpressionIsVariableException.class
07:05:22.100 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.Parser - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/Parser.class
07:05:22.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.Expression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/Expression.class
07:05:22.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.InvokeExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/InvokeExpression.class
07:05:22.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.Imports - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/Imports.class
07:05:22.102 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.JDOQLParser - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/JDOQLParser.class
07:05:22.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.JDOQLParser$ParameterType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/JDOQLParser$ParameterType.class
07:05:22.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.symbol.SymbolTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/symbol/SymbolTable.class
07:05:22.105 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.symbol.PropertySymbol - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/symbol/PropertySymbol.class
07:05:22.105 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.Lexer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/Lexer.class
07:05:22.106 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.node.Node - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/node/Node.class
07:05:22.107 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.node.NodeType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/node/NodeType.class
07:05:22.108 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.ExpressionCompiler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/ExpressionCompiler.class
07:05:22.109 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.ClassExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/ClassExpression.class
07:05:22.109 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.OrderExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/OrderExpression.class
07:05:22.109 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.DyadicExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/DyadicExpression.class
07:05:22.110 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.ParameterExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/ParameterExpression.class
07:05:22.110 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.PrimaryExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/PrimaryExpression.class
07:05:22.111 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.VariableExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/VariableExpression.class
07:05:22.111 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.Literal - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/Literal.class
07:05:22.111 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.CreatorExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/CreatorExpression.class
07:05:22.112 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.ArrayExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/ArrayExpression.class
07:05:22.112 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.SubqueryExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/SubqueryExpression.class
07:05:22.112 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.CaseExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/CaseExpression.class
07:05:22.113 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.Expression$Operator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/Expression$Operator.class
07:05:22.113 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.Expression$DyadicOperator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/Expression$DyadicOperator.class
07:05:22.113 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.Expression$MonadicOperator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/Expression$MonadicOperator.class
07:05:22.114 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.PrimaryExpressionIsInvokeException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/PrimaryExpressionIsInvokeException.class
07:05:22.114 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.MetadataFileType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/MetadataFileType.class
07:05:22.115 pool-1-thread-1 DEBUG MetaData: MetaData of type "jdo" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /META-INF/package.jdo
07:05:22.116 pool-1-thread-1 DEBUG MetaData: MetaData of type "jdo" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /WEB-INF/package.jdo
07:05:22.117 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.xml.MetaDataParser - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/xml/MetaDataParser.class
07:05:22.117 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.helpers.DefaultHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/helpers/DefaultHandler.class
07:05:22.117 pool-1-thread-1 DEBUG MetaData: Parsing MetaData file "jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/package.jdo" using handler "jdo" (validation="false")
07:05:22.118 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.parsers.SAXParserFactory - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/parsers/SAXParserFactory.class
07:05:22.118 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.jaxp.SAXParserFactoryImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/jaxp/SAXParserFactoryImpl.class
07:05:22.118 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.xml.parsers.SAXParser - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/xml/parsers/SAXParser.class
07:05:22.119 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.jaxp.SAXParserImpl - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/jaxp/SAXParserImpl.class
07:05:22.119 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.xs.PSVIProvider - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/xs/PSVIProvider.class
07:05:22.120 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.XMLReader - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/XMLReader.class
07:05:22.120 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.Parser - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/Parser.class
07:05:22.120 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.DocumentHandler - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/DocumentHandler.class
07:05:22.120 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/jaxp/SAXParserImpl$JAXPSAXParser.class
07:05:22.121 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.SAXParser - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/SAXParser.class
07:05:22.121 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.AbstractSAXParser - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/AbstractSAXParser.class
07:05:22.122 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.AttributeList - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/AttributeList.class
07:05:22.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.AbstractSAXParser$AttributesProxy - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/AbstractSAXParser$AttributesProxy.class
07:05:22.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.ext.Attributes2 - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/ext/Attributes2.class
07:05:22.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.EntityResolverFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/EntityResolverFactory.class
07:05:22.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.xml.PluginEntityResolver - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/xml/PluginEntityResolver.class
07:05:22.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.AbstractXMLEntityResolver - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/AbstractXMLEntityResolver.class
07:05:22.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.ext.EntityResolver2 - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/ext/EntityResolver2.class
07:05:22.126 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.EntityResolverWrapper - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/EntityResolverWrapper.class
07:05:22.126 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.ExternalSubsetResolver - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/ExternalSubsetResolver.class
07:05:22.126 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.metadata.JDOMetaDataHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/metadata/JDOMetaDataHandler.class
07:05:22.127 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.xml.AbstractMetaDataHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/xml/AbstractMetaDataHandler.class
07:05:22.128 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.FetchPlanMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/FetchPlanMetaData.class
07:05:22.129 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.PackageMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/PackageMetaData.class
07:05:22.129 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.FileMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/FileMetaData.class
07:05:22.130 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.PrimaryKeyMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/PrimaryKeyMetaData.class
07:05:22.130 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ImplementsMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ImplementsMetaData.class
07:05:22.130 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.PropertyMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/PropertyMetaData.class
07:05:22.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.EmbeddedMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/EmbeddedMetaData.class
07:05:22.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.FetchGroupMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/FetchGroupMetaData.class
07:05:22.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.FetchGroupMemberMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/FetchGroupMemberMetaData.class
07:05:22.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.InheritanceMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/InheritanceMetaData.class
07:05:22.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.DiscriminatorMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/DiscriminatorMetaData.class
07:05:22.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.QueryMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/QueryMetaData.class
07:05:22.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.FieldMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/FieldMetaData.class
07:05:22.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ForeignKeyMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ForeignKeyMetaData.class
07:05:22.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.AbstractConstraintMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/AbstractConstraintMetaData.class
07:05:22.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.IndexMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/IndexMetaData.class
07:05:22.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.JoinMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/JoinMetaData.class
07:05:22.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.MapMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/MapMetaData.class
07:05:22.136 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ContainerMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ContainerMetaData.class
07:05:22.136 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ArrayMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ArrayMetaData.class
07:05:22.137 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.CollectionMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/CollectionMetaData.class
07:05:22.137 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.AbstractElementMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/AbstractElementMetaData.class
07:05:22.138 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.UniqueMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/UniqueMetaData.class
07:05:22.138 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.OrderMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/OrderMetaData.class
07:05:22.138 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.VersionMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/VersionMetaData.class
07:05:22.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ColumnMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ColumnMetaData.class
07:05:22.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ElementMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ElementMetaData.class
07:05:22.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.KeyMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/KeyMetaData.class
07:05:22.140 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ValueMetaData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ValueMetaData.class
07:05:22.141 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.util.ErrorHandlerWrapper - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/util/ErrorHandlerWrapper.class
07:05:22.141 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.parsers.AbstractSAXParser$LocatorProxy - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/parsers/AbstractSAXParser$LocatorProxy.class
07:05:22.141 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.xml.sax.ext.Locator2 - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/org/xml/sax/ext/Locator2.class
07:05:22.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.models.CMAny - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/models/CMAny.class
07:05:22.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.models.CMNode - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/models/CMNode.class
07:05:22.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.models.CMLeaf - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/models/CMLeaf.class
07:05:22.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.models.CMBinOp - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/models/CMBinOp.class
07:05:22.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.models.CMUniOp - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/models/CMUniOp.class
07:05:22.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.models.ContentModelValidator - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/models/ContentModelValidator.class
07:05:22.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.models.MixedContentModel - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/models/MixedContentModel.class
07:05:22.146 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.DTDGrammar$QNameHashtable - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/DTDGrammar$QNameHashtable.class
07:05:22.146 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.xerces.impl.dtd.XMLContentSpec - jar:file:/Users/lian/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar!/org/apache/xerces/impl/dtd/XMLContentSpec.class
07:05:22.146 pool-1-thread-1 DEBUG MetaData: XML Entity Public="-//Sun Microsystems, Inc.//DTD Java Data Objects Metadata 2.0//EN" System="http://java.sun.com/dtd/jdo_2_0.dtd" : using local source "/org/datanucleus/api/jdo/jdo_2_0.dtd"
07:05:22.152 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.MacroString$MacroHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/MacroString$MacroHandler.class
07:05:22.155 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.IdentityType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/IdentityType.class
07:05:22.156 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ClassPersistenceModifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ClassPersistenceModifier.class
07:05:22.157 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.IdentityStrategy - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/IdentityStrategy.class
07:05:22.160 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.InvalidMemberMetaDataException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/InvalidMemberMetaDataException.class
07:05:22.160 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.AccessibleObject
07:05:22.161 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.NullValue - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/NullValue.class
07:05:22.162 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.FieldPersistenceModifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/FieldPersistenceModifier.class
07:05:22.162 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.IndexedValue - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/IndexedValue.class
07:05:22.164 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.ContainerComponent - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/ContainerComponent.class
07:05:22.164 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.MetaDataUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/MetaDataUtils.class
07:05:22.183 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.MetaDataManager$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/MetaDataManager$1.class
07:05:22.184 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDatabase" : Populating Meta-Data
07:05:22.184 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.Modifier
07:05:22.184 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Package
07:05:22.185 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /META-INF/package.orm
07:05:22.186 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /WEB-INF/package.orm
07:05:22.186 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /package.orm
07:05:22.190 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/package.orm
07:05:22.191 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org.orm
07:05:22.191 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/package.orm
07:05:22.205 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache.orm
07:05:22.206 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/package.orm
07:05:22.207 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop.orm
07:05:22.208 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.209 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive.orm
07:05:22.210 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.210 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.210 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.211 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.211 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" NOT found at /org/apache/hadoop/hive/metastore/model/MDatabase.orm
07:05:22.211 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDatabase" not found
07:05:22.212 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDODetachedFieldAccessException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDODetachedFieldAccessException.class
07:05:22.213 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.InheritanceStrategy - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/InheritanceStrategy.class
07:05:22.214 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.RelationType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/RelationType.class
07:05:22.214 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.ParameterizedType
07:05:22.215 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MFieldSchema" : Populating Meta-Data
07:05:22.215 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /META-INF/package.orm
07:05:22.216 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /WEB-INF/package.orm
07:05:22.216 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /package.orm
07:05:22.221 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/package.orm
07:05:22.221 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org.orm
07:05:22.222 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/package.orm
07:05:22.225 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache.orm
07:05:22.226 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/package.orm
07:05:22.226 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop.orm
07:05:22.227 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.227 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive.orm
07:05:22.227 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.228 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.228 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.229 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.230 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" NOT found at /org/apache/hadoop/hive/metastore/model/MFieldSchema.orm
07:05:22.230 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFieldSchema" not found
07:05:22.230 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MType" : Populating Meta-Data
07:05:22.231 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /META-INF/package.orm
07:05:22.232 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /WEB-INF/package.orm
07:05:22.232 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /package.orm
07:05:22.237 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/package.orm
07:05:22.238 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org.orm
07:05:22.238 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/package.orm
07:05:22.242 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache.orm
07:05:22.243 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/package.orm
07:05:22.243 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop.orm
07:05:22.244 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.244 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive.orm
07:05:22.245 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.245 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.246 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.246 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.247 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" NOT found at /org/apache/hadoop/hive/metastore/model/MType.orm
07:05:22.247 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MType" not found
07:05:22.247 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.249 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.249 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTable" : Populating Meta-Data
07:05:22.250 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /META-INF/package.orm
07:05:22.250 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /WEB-INF/package.orm
07:05:22.251 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /package.orm
07:05:22.254 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/package.orm
07:05:22.254 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org.orm
07:05:22.255 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/package.orm
07:05:22.258 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache.orm
07:05:22.259 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/package.orm
07:05:22.259 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop.orm
07:05:22.260 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.260 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive.orm
07:05:22.261 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.261 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.261 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.262 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.263 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" NOT found at /org/apache/hadoop/hive/metastore/model/MTable.orm
07:05:22.263 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTable" not found
07:05:22.264 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.264 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.264 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.265 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.265 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.265 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" : Populating Meta-Data
07:05:22.266 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /META-INF/package.orm
07:05:22.266 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /WEB-INF/package.orm
07:05:22.267 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /package.orm
07:05:22.270 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/package.orm
07:05:22.270 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org.orm
07:05:22.271 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/package.orm
07:05:22.274 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache.orm
07:05:22.274 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/package.orm
07:05:22.275 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop.orm
07:05:22.275 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.275 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive.orm
07:05:22.276 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.276 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.277 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.277 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.278 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" NOT found at /org/apache/hadoop/hive/metastore/model/MSerDeInfo.orm
07:05:22.278 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" not found
07:05:22.278 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MOrder" : Populating Meta-Data
07:05:22.279 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /META-INF/package.orm
07:05:22.279 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /WEB-INF/package.orm
07:05:22.280 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /package.orm
07:05:22.283 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/package.orm
07:05:22.283 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org.orm
07:05:22.284 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/package.orm
07:05:22.287 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache.orm
07:05:22.287 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/package.orm
07:05:22.288 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop.orm
07:05:22.288 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.289 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive.orm
07:05:22.289 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.290 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.290 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.291 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.291 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" NOT found at /org/apache/hadoop/hive/metastore/model/MOrder.orm
07:05:22.292 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MOrder" not found
07:05:22.292 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" : Populating Meta-Data
07:05:22.292 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MColumnDescriptor - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MColumnDescriptor.class
07:05:22.293 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /META-INF/package.orm
07:05:22.294 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /WEB-INF/package.orm
07:05:22.294 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /package.orm
07:05:22.297 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/package.orm
07:05:22.298 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org.orm
07:05:22.298 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/package.orm
07:05:22.301 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache.orm
07:05:22.302 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/package.orm
07:05:22.302 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop.orm
07:05:22.303 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.303 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive.orm
07:05:22.304 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.304 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.305 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.305 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.305 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model/MColumnDescriptor.orm
07:05:22.305 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" not found
07:05:22.306 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.306 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MFieldSchema - null
07:05:22.307 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MStringList" : Populating Meta-Data
07:05:22.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MStringList - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MStringList.class
07:05:22.308 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /META-INF/package.orm
07:05:22.309 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /WEB-INF/package.orm
07:05:22.309 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /package.orm
07:05:22.313 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/package.orm
07:05:22.313 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org.orm
07:05:22.313 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/package.orm
07:05:22.317 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache.orm
07:05:22.317 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/package.orm
07:05:22.318 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop.orm
07:05:22.318 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.318 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive.orm
07:05:22.319 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.319 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.320 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.320 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.321 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" NOT found at /org/apache/hadoop/hive/metastore/model/MStringList.orm
07:05:22.321 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStringList" not found
07:05:22.321 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" : Populating Meta-Data
07:05:22.322 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /META-INF/package.orm
07:05:22.322 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /WEB-INF/package.orm
07:05:22.323 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /package.orm
07:05:22.326 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/package.orm
07:05:22.326 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org.orm
07:05:22.327 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/package.orm
07:05:22.330 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache.orm
07:05:22.330 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/package.orm
07:05:22.331 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop.orm
07:05:22.331 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.331 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive.orm
07:05:22.332 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.332 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.333 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.333 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.334 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" NOT found at /org/apache/hadoop/hive/metastore/model/MStorageDescriptor.orm
07:05:22.334 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" not found
07:05:22.335 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.338 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.338 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MStringList - null
07:05:22.338 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MOrder - null
07:05:22.338 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MOrder - null
07:05:22.339 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MOrder - null
07:05:22.339 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MOrder - null
07:05:22.339 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MOrder - null
07:05:22.339 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartition" : Populating Meta-Data
07:05:22.340 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /META-INF/package.orm
07:05:22.340 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /WEB-INF/package.orm
07:05:22.341 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /package.orm
07:05:22.344 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/package.orm
07:05:22.344 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org.orm
07:05:22.345 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/package.orm
07:05:22.348 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache.orm
07:05:22.348 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/package.orm
07:05:22.349 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop.orm
07:05:22.349 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.350 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive.orm
07:05:22.350 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.351 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.351 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.352 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.352 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" NOT found at /org/apache/hadoop/hive/metastore/model/MPartition.orm
07:05:22.352 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartition" not found
07:05:22.353 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MIndex" : Populating Meta-Data
07:05:22.353 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MIndex - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MIndex.class
07:05:22.354 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /META-INF/package.orm
07:05:22.355 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /WEB-INF/package.orm
07:05:22.355 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /package.orm
07:05:22.359 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/package.orm
07:05:22.360 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org.orm
07:05:22.360 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/package.orm
07:05:22.363 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache.orm
07:05:22.363 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/package.orm
07:05:22.364 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop.orm
07:05:22.364 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.365 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive.orm
07:05:22.365 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.366 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.366 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.367 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.367 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" NOT found at /org/apache/hadoop/hive/metastore/model/MIndex.orm
07:05:22.367 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MIndex" not found
07:05:22.368 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MRole" : Populating Meta-Data
07:05:22.368 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MRole - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MRole.class
07:05:22.369 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /META-INF/package.orm
07:05:22.370 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /WEB-INF/package.orm
07:05:22.370 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /package.orm
07:05:22.373 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/package.orm
07:05:22.373 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org.orm
07:05:22.374 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/package.orm
07:05:22.377 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache.orm
07:05:22.377 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/package.orm
07:05:22.378 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop.orm
07:05:22.378 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.378 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive.orm
07:05:22.379 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.379 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.380 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.380 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.381 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" NOT found at /org/apache/hadoop/hive/metastore/model/MRole.orm
07:05:22.381 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRole" not found
07:05:22.381 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MRoleMap" : Populating Meta-Data
07:05:22.381 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MRoleMap - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MRoleMap.class
07:05:22.382 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /META-INF/package.orm
07:05:22.383 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /WEB-INF/package.orm
07:05:22.383 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /package.orm
07:05:22.386 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/package.orm
07:05:22.387 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org.orm
07:05:22.387 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/package.orm
07:05:22.390 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache.orm
07:05:22.390 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/package.orm
07:05:22.391 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop.orm
07:05:22.391 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.392 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive.orm
07:05:22.392 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.393 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.393 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.394 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.394 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" NOT found at /org/apache/hadoop/hive/metastore/model/MRoleMap.orm
07:05:22.394 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MRoleMap" not found
07:05:22.395 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" : Populating Meta-Data
07:05:22.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MGlobalPrivilege - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MGlobalPrivilege.class
07:05:22.396 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /META-INF/package.orm
07:05:22.397 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /WEB-INF/package.orm
07:05:22.397 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /package.orm
07:05:22.402 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/package.orm
07:05:22.403 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org.orm
07:05:22.403 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/package.orm
07:05:22.407 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache.orm
07:05:22.408 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:22.408 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop.orm
07:05:22.409 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.409 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:22.410 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.410 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.411 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.411 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.412 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MGlobalPrivilege.orm
07:05:22.412 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" not found
07:05:22.412 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" : Populating Meta-Data
07:05:22.412 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MDBPrivilege - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MDBPrivilege.class
07:05:22.413 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /META-INF/package.orm
07:05:22.414 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /WEB-INF/package.orm
07:05:22.414 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /package.orm
07:05:22.417 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/package.orm
07:05:22.418 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org.orm
07:05:22.418 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/package.orm
07:05:22.421 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache.orm
07:05:22.421 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:22.422 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop.orm
07:05:22.422 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.423 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:22.423 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.423 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.424 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.424 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.425 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MDBPrivilege.orm
07:05:22.425 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" not found
07:05:22.425 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" : Populating Meta-Data
07:05:22.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MTablePrivilege - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MTablePrivilege.class
07:05:22.427 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /META-INF/package.orm
07:05:22.427 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /WEB-INF/package.orm
07:05:22.427 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /package.orm
07:05:22.430 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/package.orm
07:05:22.431 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org.orm
07:05:22.431 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/package.orm
07:05:22.434 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache.orm
07:05:22.434 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:22.435 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop.orm
07:05:22.435 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.436 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:22.436 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.437 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.437 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.438 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.438 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MTablePrivilege.orm
07:05:22.438 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" not found
07:05:22.439 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" : Populating Meta-Data
07:05:22.439 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MPartitionPrivilege - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MPartitionPrivilege.class
07:05:22.440 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /META-INF/package.orm
07:05:22.441 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /WEB-INF/package.orm
07:05:22.441 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /package.orm
07:05:22.444 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/package.orm
07:05:22.445 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org.orm
07:05:22.445 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/package.orm
07:05:22.449 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache.orm
07:05:22.450 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:22.450 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop.orm
07:05:22.451 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.451 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:22.452 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.452 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.453 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.453 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.454 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MPartitionPrivilege.orm
07:05:22.454 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" not found
07:05:22.455 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" : Populating Meta-Data
07:05:22.455 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MTableColumnPrivilege.class
07:05:22.456 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /META-INF/package.orm
07:05:22.457 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /WEB-INF/package.orm
07:05:22.457 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /package.orm
07:05:22.460 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/package.orm
07:05:22.460 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org.orm
07:05:22.461 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/package.orm
07:05:22.464 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache.orm
07:05:22.464 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:22.465 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop.orm
07:05:22.465 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.466 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:22.466 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.467 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.467 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.467 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.468 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MTableColumnPrivilege.orm
07:05:22.468 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" not found
07:05:22.469 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" : Populating Meta-Data
07:05:22.469 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MPartitionColumnPrivilege.class
07:05:22.470 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /META-INF/package.orm
07:05:22.470 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /WEB-INF/package.orm
07:05:22.471 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /package.orm
07:05:22.474 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/package.orm
07:05:22.475 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org.orm
07:05:22.475 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/package.orm
07:05:22.478 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache.orm
07:05:22.479 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/package.orm
07:05:22.479 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop.orm
07:05:22.480 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.480 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive.orm
07:05:22.481 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.481 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.482 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.482 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.483 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" NOT found at /org/apache/hadoop/hive/metastore/model/MPartitionColumnPrivilege.orm
07:05:22.483 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" not found
07:05:22.483 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" : Populating Meta-Data
07:05:22.483 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MPartitionEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MPartitionEvent.class
07:05:22.485 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /META-INF/package.orm
07:05:22.485 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /WEB-INF/package.orm
07:05:22.485 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /package.orm
07:05:22.488 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/package.orm
07:05:22.489 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org.orm
07:05:22.489 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/package.orm
07:05:22.492 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache.orm
07:05:22.493 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/package.orm
07:05:22.493 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop.orm
07:05:22.494 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.495 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive.orm
07:05:22.495 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.496 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.496 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.497 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.498 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" NOT found at /org/apache/hadoop/hive/metastore/model/MPartitionEvent.orm
07:05:22.498 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" not found
07:05:22.498 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MMasterKey" : Populating Meta-Data
07:05:22.498 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MMasterKey - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MMasterKey.class
07:05:22.500 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /META-INF/package.orm
07:05:22.500 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /WEB-INF/package.orm
07:05:22.500 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /package.orm
07:05:22.505 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/package.orm
07:05:22.505 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org.orm
07:05:22.506 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/package.orm
07:05:22.509 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache.orm
07:05:22.510 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/package.orm
07:05:22.510 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop.orm
07:05:22.511 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.511 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive.orm
07:05:22.512 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.512 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.513 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.513 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.514 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" NOT found at /org/apache/hadoop/hive/metastore/model/MMasterKey.orm
07:05:22.514 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MMasterKey" not found
07:05:22.514 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOClassNameConstants - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOClassNameConstants.class
07:05:22.515 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.identity.LongIdentity - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/identity/LongIdentity.class
07:05:22.515 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.identity.SingleFieldIdentity - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/identity/SingleFieldIdentity.class
07:05:22.516 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.identity.IntIdentity - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/identity/IntIdentity.class
07:05:22.516 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.identity.StringIdentity - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/identity/StringIdentity.class
07:05:22.516 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.identity.CharIdentity - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/identity/CharIdentity.class
07:05:22.516 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.identity.ByteIdentity - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/identity/ByteIdentity.class
07:05:22.517 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.identity.ObjectIdentity - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/identity/ObjectIdentity.class
07:05:22.517 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.identity.ShortIdentity - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/identity/ShortIdentity.class
07:05:22.517 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDelegationToken" : Populating Meta-Data
07:05:22.518 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MDelegationToken - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MDelegationToken.class
07:05:22.519 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /META-INF/package.orm
07:05:22.519 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /WEB-INF/package.orm
07:05:22.519 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /package.orm
07:05:22.523 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/package.orm
07:05:22.523 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org.orm
07:05:22.524 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/package.orm
07:05:22.527 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache.orm
07:05:22.527 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/package.orm
07:05:22.528 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop.orm
07:05:22.528 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.529 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive.orm
07:05:22.529 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.530 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.530 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.530 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.531 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" NOT found at /org/apache/hadoop/hive/metastore/model/MDelegationToken.orm
07:05:22.531 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MDelegationToken" not found
07:05:22.531 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" : Populating Meta-Data
07:05:22.532 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MTableColumnStatistics - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.class
07:05:22.533 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /META-INF/package.orm
07:05:22.533 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /WEB-INF/package.orm
07:05:22.534 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /package.orm
07:05:22.537 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/package.orm
07:05:22.538 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org.orm
07:05:22.538 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/package.orm
07:05:22.542 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache.orm
07:05:22.545 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/package.orm
07:05:22.546 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop.orm
07:05:22.546 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.547 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive.orm
07:05:22.547 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.548 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.548 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.549 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.549 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model/MTableColumnStatistics.orm
07:05:22.549 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" not found
07:05:22.550 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" : Populating Meta-Data
07:05:22.551 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.class
07:05:22.552 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /META-INF/package.orm
07:05:22.552 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /WEB-INF/package.orm
07:05:22.553 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /package.orm
07:05:22.556 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/package.orm
07:05:22.556 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org.orm
07:05:22.557 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/package.orm
07:05:22.560 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache.orm
07:05:22.561 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/package.orm
07:05:22.561 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop.orm
07:05:22.561 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.562 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive.orm
07:05:22.562 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.563 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.563 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.564 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.564 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" NOT found at /org/apache/hadoop/hive/metastore/model/MPartitionColumnStatistics.orm
07:05:22.564 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" not found
07:05:22.565 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MVersionTable" : Populating Meta-Data
07:05:22.565 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MVersionTable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MVersionTable.class
07:05:22.566 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /META-INF/package.orm
07:05:22.567 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /WEB-INF/package.orm
07:05:22.567 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /package.orm
07:05:22.571 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/package.orm
07:05:22.572 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org.orm
07:05:22.572 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/package.orm
07:05:22.576 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache.orm
07:05:22.576 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/package.orm
07:05:22.577 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop.orm
07:05:22.577 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.578 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive.orm
07:05:22.578 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.578 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.579 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.580 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.580 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" NOT found at /org/apache/hadoop/hive/metastore/model/MVersionTable.orm
07:05:22.580 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MVersionTable" not found
07:05:22.581 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MResourceUri" : Populating Meta-Data
07:05:22.581 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MResourceUri - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MResourceUri.class
07:05:22.583 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /META-INF/package.orm
07:05:22.584 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /WEB-INF/package.orm
07:05:22.584 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /package.orm
07:05:22.588 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/package.orm
07:05:22.588 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org.orm
07:05:22.588 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/package.orm
07:05:22.592 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache.orm
07:05:22.592 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/package.orm
07:05:22.593 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop.orm
07:05:22.593 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.594 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive.orm
07:05:22.594 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.594 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.595 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.595 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.596 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" NOT found at /org/apache/hadoop/hive/metastore/model/MResourceUri.orm
07:05:22.596 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MResourceUri" not found
07:05:22.596 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MFunction" : Populating Meta-Data
07:05:22.596 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MFunction - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MFunction.class
07:05:22.597 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /META-INF/package.orm
07:05:22.598 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /WEB-INF/package.orm
07:05:22.598 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /package.orm
07:05:22.601 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/package.orm
07:05:22.602 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org.orm
07:05:22.602 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/package.orm
07:05:22.605 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache.orm
07:05:22.606 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/package.orm
07:05:22.606 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop.orm
07:05:22.607 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.607 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive.orm
07:05:22.608 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.608 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.609 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.609 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.609 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" NOT found at /org/apache/hadoop/hive/metastore/model/MFunction.orm
07:05:22.609 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MFunction" not found
07:05:22.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MResourceUri - null
07:05:22.611 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MResourceUri - null
07:05:22.611 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MResourceUri - null
07:05:22.611 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MResourceUri - null
07:05:22.611 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: MResourceUri - null
07:05:22.612 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MNotificationLog" : Populating Meta-Data
07:05:22.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MNotificationLog - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MNotificationLog.class
07:05:22.613 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /META-INF/package.orm
07:05:22.614 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /WEB-INF/package.orm
07:05:22.614 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /package.orm
07:05:22.618 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/package.orm
07:05:22.618 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org.orm
07:05:22.619 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/package.orm
07:05:22.622 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache.orm
07:05:22.622 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/package.orm
07:05:22.622 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop.orm
07:05:22.623 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.623 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive.orm
07:05:22.624 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.625 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.625 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.625 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.626 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" NOT found at /org/apache/hadoop/hive/metastore/model/MNotificationLog.orm
07:05:22.626 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationLog" not found
07:05:22.626 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" : Populating Meta-Data
07:05:22.627 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.model.MNotificationNextId - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/model/MNotificationNextId.class
07:05:22.628 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /META-INF/package.orm
07:05:22.628 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /WEB-INF/package.orm
07:05:22.629 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /package.orm
07:05:22.632 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/package.orm
07:05:22.632 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org.orm
07:05:22.632 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/package.orm
07:05:22.636 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache.orm
07:05:22.636 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/package.orm
07:05:22.637 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop.orm
07:05:22.637 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/package.orm
07:05:22.638 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive.orm
07:05:22.638 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore/package.orm
07:05:22.639 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore.orm
07:05:22.639 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore/model/package.orm
07:05:22.640 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore/model.orm
07:05:22.640 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" NOT found at /org/apache/hadoop/hive/metastore/model/MNotificationNextId.orm
07:05:22.640 pool-1-thread-1 DEBUG MetaData: MetaData of type "orm" for class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" not found
07:05:22.640 pool-1-thread-1 DEBUG MetaData: MetaData of type "jdo" for class "org.apache.hadoop.hive.metastore.model.MDatabase" will use jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/package.jdo
07:05:22.641 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.MetaDataManager$2 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/MetaDataManager$2.class
07:05:22.641 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDatabase" : Initialising Meta-Data
07:05:22.641 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MFieldSchema" : Initialising Meta-Data
07:05:22.641 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MType" : Initialising Meta-Data
07:05:22.641 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTable" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MSerDeInfo" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MOrder" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MColumnDescriptor" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MStringList" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MStorageDescriptor" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartition" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MIndex" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MRole" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MRoleMap" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDBPrivilege" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTablePrivilege" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionPrivilege" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege" : Initialising Meta-Data
07:05:22.642 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionEvent" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MMasterKey" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MDelegationToken" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MTableColumnStatistics" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MVersionTable" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MResourceUri" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MFunction" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MNotificationLog" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG MetaData: Class "org.apache.hadoop.hive.metastore.model.MNotificationNextId" : Initialising Meta-Data
07:05:22.643 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.QueryCompilation - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/QueryCompilation.class
07:05:22.644 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.QueryUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/QueryUtils.class
07:05:22.645 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.JPQLCompiler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/JPQLCompiler.class
07:05:22.646 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 548 ms
07:05:22.646 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{name}  =  Literal{}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MDatabase]
07:05:22.646 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.SoftValueMap$SoftValueReference - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/SoftValueMap$SoftValueReference.class
07:05:22.647 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.RDBMSQueryCompilation - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/RDBMSQueryCompilation.class
07:05:22.648 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == ''" for datastore
07:05:22.648 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.StatementClassMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/StatementClassMapping.class
07:05:22.649 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.RDBMSQueryUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/RDBMSQueryUtils.class
07:05:22.649 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.StringLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/StringLiteral.class
07:05:22.650 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.SQLLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/SQLLiteral.class
07:05:22.650 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.StringExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/StringExpression.class
07:05:22.650 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.StatementGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/StatementGenerator.class
07:05:22.651 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/RDBMSStoreManager$ClassAdder.class
07:05:22.652 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.AbstractSchemaTransaction - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/AbstractSchemaTransaction.class
07:05:22.652 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.StoreData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/StoreData.class
07:05:22.653 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.RDBMSStoreData - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/RDBMSStoreData.class
07:05:22.653 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.DatastoreClass - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/DatastoreClass.class
07:05:22.654 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.TableIdentifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/TableIdentifier.class
07:05:22.655 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.DatastoreIdentifierImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/DatastoreIdentifierImpl.class
07:05:22.655 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.ClassTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/ClassTable.class
07:05:22.657 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.AbstractClassTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/AbstractClassTable.class
07:05:22.657 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.TableImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/TableImpl.class
07:05:22.658 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.AbstractTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/AbstractTable.class
07:05:22.659 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.MissingTableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/MissingTableException.class
07:05:22.659 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.exceptions.DatastoreValidationException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/exceptions/DatastoreValidationException.class
07:05:22.660 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.Column - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/Column.class
07:05:22.660 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.DuplicateColumnException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/DuplicateColumnException.class
07:05:22.661 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.WrongPrimaryKeyException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/WrongPrimaryKeyException.class
07:05:22.661 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.NotATableException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/NotATableException.class
07:05:22.662 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.UnexpectedColumnException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/UnexpectedColumnException.class
07:05:22.663 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.MissingColumnException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/MissingColumnException.class
07:05:22.664 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.VersionLongMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/VersionLongMapping.class
07:05:22.664 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.VersionMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/VersionMapping.class
07:05:22.664 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.VersionTimestampMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/VersionTimestampMapping.class
07:05:22.665 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.DiscriminatorMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/DiscriminatorMapping.class
07:05:22.666 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.PersistableMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/PersistableMapping.class
07:05:22.666 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.MultiMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/MultiMapping.class
07:05:22.667 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.ClassDefinitionException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/ClassDefinitionException.class
07:05:22.668 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.IndexMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/IndexMapping.class
07:05:22.668 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.NoSuchPersistentFieldException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/NoSuchPersistentFieldException.class
07:05:22.669 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MDatabase [Table : DBS, InheritanceStrategy : new-table]
07:05:22.670 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.exceptions.NotYetFlushedException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/exceptions/NotYetFlushedException.class
07:05:22.670 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.ColumnIdentifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/ColumnIdentifier.class
07:05:22.671 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.ColumnImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/ColumnImpl.class
07:05:22.672 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.IncompatibleDataTypeException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/IncompatibleDataTypeException.class
07:05:22.672 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.WrongPrecisionException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/WrongPrecisionException.class
07:05:22.672 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.WrongScaleException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/WrongScaleException.class
07:05:22.673 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.ColumnDefinitionException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/ColumnDefinitionException.class
07:05:22.673 pool-1-thread-1 DEBUG Schema: Column "DBS.DB_ID" added to internal representation of table.
07:05:22.674 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.DatastoreMappingFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/DatastoreMappingFactory.class
07:05:22.674 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.NullValueException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/NullValueException.class
07:05:22.675 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.exceptions.ReachableObjectNotCascadedException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/exceptions/ReachableObjectNotCascadedException.class
07:05:22.676 pool-1-thread-1 DEBUG Schema: Table DBS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MDatabase (inheritance strategy="new-table")
07:05:22.677 pool-1-thread-1 DEBUG Schema: Column "DBS."DESC"" added to internal representation of table.
07:05:22.677 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/datastore/CharRDBMSMapping$1.class
07:05:22.678 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.description] -> Column(s) [DBS."DESC"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.678 pool-1-thread-1 DEBUG Schema: Column "DBS.DB_LOCATION_URI" added to internal representation of table.
07:05:22.678 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.locationUri] -> Column(s) [DBS.DB_LOCATION_URI] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.678 pool-1-thread-1 DEBUG Schema: Column "DBS."NAME"" added to internal representation of table.
07:05:22.678 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.name] -> Column(s) [DBS."NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.678 pool-1-thread-1 DEBUG Schema: Column "DBS.OWNER_NAME" added to internal representation of table.
07:05:22.678 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.ownerName] -> Column(s) [DBS.OWNER_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.678 pool-1-thread-1 DEBUG Schema: Column "DBS.OWNER_TYPE" added to internal representation of table.
07:05:22.678 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.ownerType] -> Column(s) [DBS.OWNER_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.678 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.NoDatastoreMappingException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/NoDatastoreMappingException.class
07:05:22.679 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.SCOUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/SCOUtils.class
07:05:22.682 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.MapTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/MapTable.class
07:05:22.682 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.DatastoreMap - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/DatastoreMap.class
07:05:22.683 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.JoinTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/JoinTable.class
07:05:22.684 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MDatabase.parameters [Table : DATABASE_PARAMS]
07:05:22.684 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MDatabase.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:22.684 pool-1-thread-1 DEBUG Schema: Table/View DBS has been initialised
07:05:22.684 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.ColumnCreator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/ColumnCreator.class
07:05:22.685 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.ReferenceMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/ReferenceMapping.class
07:05:22.686 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.MultiPersistableMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/MultiPersistableMapping.class
07:05:22.687 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.CorrespondentColumnsMapper - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/CorrespondentColumnsMapper.class
07:05:22.688 pool-1-thread-1 DEBUG Schema: Column "DATABASE_PARAMS.DB_ID" added to internal representation of table.
07:05:22.688 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MDatabase.parameters] -> Column(s) [DATABASE_PARAMS.DB_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.688 pool-1-thread-1 DEBUG Schema: Column "DATABASE_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:22.688 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MDatabase.parameters] -> Column(s) [DATABASE_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.688 pool-1-thread-1 DEBUG Schema: Column "DATABASE_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:22.688 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MDatabase.parameters] -> Column(s) [DATABASE_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.689 pool-1-thread-1 DEBUG Schema: Table/View DATABASE_PARAMS has been initialised
07:05:22.689 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.ViewImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/ViewImpl.class
07:05:22.689 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@46738621" opened with isolation level "serializable" and auto-commit=false
07:05:22.689 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@46738621" with isolation "serializable"
07:05:22.692 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.NotNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/NotNode.class
07:05:22.692 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UnaryLogicalOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UnaryLogicalOperatorNode.class
07:05:22.695 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.ColumnOrdering - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/ColumnOrdering.class
07:05:22.696 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OrderByColumn - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OrderByColumn.class
07:05:22.696 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OrderedColumn - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OrderedColumn.class
07:05:22.698 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.BaseColumnNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/BaseColumnNode.class
07:05:22.699 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.ListIterator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/ListIterator.class
07:05:22.701 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SQLToJavaValueNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SQLToJavaValueNode.class
07:05:22.702 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.Like - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/Like.class
07:05:22.703 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.RuleBasedCollator - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/RuleBasedCollator.class
07:05:22.706 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IRowLocking1 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IRowLocking1.class
07:05:22.706 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IRowLocking2 - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IRowLocking2.class
07:05:22.708 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2ICostController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2ICostController.class
07:05:22.708 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BTreeCostController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BTreeCostController.class
07:05:22.709 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.PredicateList$PredicateWrapperList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/PredicateList$PredicateWrapperList.class
07:05:22.709 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.heap.HeapCostController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/heap/HeapCostController.class
07:05:22.709 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.conglomerate.GenericCostController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/conglomerate/GenericCostController.class
07:05:22.710 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ReferencedTablesVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ReferencedTablesVisitor.class
07:05:22.713 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.IndexColumnOrder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/IndexColumnOrder.class
07:05:22.714 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.IndexToBaseRowNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/IndexToBaseRowNode.class
07:05:22.714 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CurrentRowLocationNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CurrentRowLocationNode.class
07:05:22.716 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.OrderByNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/OrderByNode.class
07:05:22.717 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.FormatableArrayHolder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/FormatableArrayHolder.class
07:05:22.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ExecRowBuilder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ExecRowBuilder.class
07:05:22.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IStaticCompiledInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IStaticCompiledInfo.class
07:05:22.723 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.DDdependableFinder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/DDdependableFinder.class
07:05:22.724 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.catalog.DDColumnDependableFinder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/catalog/DDColumnDependableFinder.class
07:05:22.730 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: sun.reflect.SerializationConstructorAccessorImpl - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/sun/reflect/SerializationConstructorAccessorImpl.class
07:05:22.742 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.UpdateFieldOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/UpdateFieldOperation.class
07:05:22.743 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.CopyRowsOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/CopyRowsOperation.class
07:05:22.743 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.raw.data.PurgeOperation - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/raw/data/PurgeOperation.class
07:05:22.746 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.BulkTableScanResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/BulkTableScanResultSet.class
07:05:22.746 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.TableScanResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/TableScanResultSet.class
07:05:22.747 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ScanResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ScanResultSet.class
07:05:22.748 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.IndexRowToBaseRowResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/IndexRowToBaseRowResultSet.class
07:05:22.749 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.NestedLoopJoinResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/NestedLoopJoinResultSet.class
07:05:22.750 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.JoinResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/JoinResultSet.class
07:05:22.751 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.SortResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/SortResultSet.class
07:05:22.752 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.SortObserver - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/SortObserver.class
07:05:22.752 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.reflect.Array
07:05:22.753 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.BasicSortObserver - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/BasicSortObserver.class
07:05:22.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.Scan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/Scan.class
07:05:22.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.SortBufferScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/SortBufferScan.class
07:05:22.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.SortScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/SortScan.class
07:05:22.755 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.MergeScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/MergeScan.class
07:05:22.755 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.SortBufferRowSource - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/SortBufferRowSource.class
07:05:22.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.conglomerate.ScanControllerRowSource - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/conglomerate/ScanControllerRowSource.class
07:05:22.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.MergeScanRowSource - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/MergeScanRowSource.class
07:05:22.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.SortController - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/SortController.class
07:05:22.757 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.MergeInserter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/MergeInserter.class
07:05:22.757 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.store.access.SortInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/store/access/SortInfo.class
07:05:22.758 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.SortBuffer - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/SortBuffer.class
07:05:22.758 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.NodeAllocator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/NodeAllocator.class
07:05:22.759 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.Node - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/Node.class
07:05:22.759 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.sort.MergeSortInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/sort/MergeSortInfo.class
07:05:22.759 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m34 - null
07:05:22.760 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m34_en - null
07:05:22.760 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m34_en_US - null
07:05:22.761 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m25 - null
07:05:22.761 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m25_en - null
07:05:22.762 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.loc.m25_en_US - null
07:05:22.762 pool-1-thread-1 DEBUG Schema: Check of existence of DBS returned no table
07:05:22.762 pool-1-thread-1 DEBUG Schema: Creating table DBS
07:05:22.763 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.key.PrimaryKey - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/key/PrimaryKey.class
07:05:22.763 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.key.CandidateKey - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/key/CandidateKey.class
07:05:22.763 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.key.Key - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/key/Key.class
07:05:22.764 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.PrimaryKeyIdentifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/PrimaryKeyIdentifier.class
07:05:22.764 pool-1-thread-1 DEBUG Schema: CREATE TABLE DBS
(
    DB_ID BIGINT NOT NULL,
    "DESC" VARCHAR(4000),
    DB_LOCATION_URI VARCHAR(4000) NOT NULL,
    "NAME" VARCHAR(128),
    OWNER_NAME VARCHAR(128),
    OWNER_TYPE VARCHAR(10)
)
07:05:22.765 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TableElementList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TableElementList.class
07:05:22.765 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.IndexConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/IndexConstantAction.class
07:05:22.766 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DropIndexConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DropIndexConstantAction.class
07:05:22.766 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.CreateIndexConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/CreateIndexConstantAction.class
07:05:22.767 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ColumnDefinitionNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ColumnDefinitionNode.class
07:05:22.769 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CreateTableNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CreateTableNode.class
07:05:22.769 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.CreateConstraintConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/CreateConstraintConstantAction.class
07:05:22.770 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ModifyColumnNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ModifyColumnNode.class
07:05:22.771 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ColumnInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ColumnInfo.class
07:05:22.772 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.CreateTableConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/CreateTableConstantAction.class
07:05:22.775 pool-1-thread-1 DEBUG Schema: Execution Time = 11 ms
07:05:22.775 pool-1-thread-1 DEBUG Schema: ALTER TABLE DBS ADD CONSTRAINT DBS_PK PRIMARY KEY (DB_ID)
07:05:22.776 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.AlterTableNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/AlterTableNode.class
07:05:22.779 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.UniqueIndexSortObserver - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/UniqueIndexSortObserver.class
07:05:22.780 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.UniqueWithDuplicateNullsIndexSortObserver - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/UniqueWithDuplicateNullsIndexSortObserver.class
07:05:22.781 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.FKConstraintDefinitionNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/FKConstraintDefinitionNode.class
07:05:22.782 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.ConsInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/ConsInfo.class
07:05:22.782 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.AlterTableConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/AlterTableConstantAction.class
07:05:22.788 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.CardinalityCounter - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/CardinalityCounter.class
07:05:22.794 pool-1-thread-1 DEBUG Schema: Execution Time = 19 ms
07:05:22.794 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.services.io.InputStreamUtil - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/services/io/InputStreamUtil.class
07:05:22.797 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.ObjectStreamClass - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/ObjectStreamClass.class
07:05:22.797 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.798 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.798 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.799 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.800 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.800 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.800 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.801 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.801 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.801 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.802 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.802 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.802 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.802 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.804 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.804 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.805 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.805 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.806 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.806 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.807 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.807 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.807 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.807 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.808 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.808 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.808 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.809 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.813 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.813 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.816 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.816 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.816 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.818 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.819 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.819 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.820 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.820 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.820 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.821 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.821 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.822 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.822 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.822 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.824 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.826 pool-1-thread-1 DEBUG Schema: Check of existence of DATABASE_PARAMS returned no table
07:05:22.826 pool-1-thread-1 DEBUG Schema: Creating table DATABASE_PARAMS
07:05:22.826 pool-1-thread-1 DEBUG Schema: CREATE TABLE DATABASE_PARAMS
(
    DB_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(180) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:22.828 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:22.828 pool-1-thread-1 DEBUG Schema: ALTER TABLE DATABASE_PARAMS ADD CONSTRAINT DATABASE_PARAMS_PK PRIMARY KEY (DB_ID,PARAM_KEY)
07:05:22.832 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:22.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.833 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.833 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.835 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.835 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.837 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.837 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.842 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.843 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.846 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.EmbeddedPCMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/EmbeddedPCMapping.class
07:05:22.846 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.EmbeddedMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/EmbeddedMapping.class
07:05:22.847 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.TableUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/TableUtils.class
07:05:22.848 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.key.Index - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/key/Index.class
07:05:22.848 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.IndexIdentifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/IndexIdentifier.class
07:05:22.848 pool-1-thread-1 DEBUG Schema: Creating index "UNIQUE_DATABASE" in catalog "" schema ""
07:05:22.848 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX UNIQUE_DATABASE ON DBS ("NAME")
07:05:22.849 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CreateIndexNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CreateIndexNode.class
07:05:22.852 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:22.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.key.ForeignKey - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/key/ForeignKey.class
07:05:22.853 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.EmbeddedKeyPCMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/EmbeddedKeyPCMapping.class
07:05:22.854 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.EmbeddedValuePCMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/EmbeddedValuePCMapping.class
07:05:22.854 pool-1-thread-1 DEBUG Schema: Creating index "DATABASE_PARAMS_N49" in catalog "" schema ""
07:05:22.854 pool-1-thread-1 DEBUG Schema: CREATE INDEX DATABASE_PARAMS_N49 ON DATABASE_PARAMS (DB_ID)
07:05:22.857 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:22.857 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.identifier.ForeignKeyIdentifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/identifier/ForeignKeyIdentifier.class
07:05:22.857 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "DATABASE_PARAMS_FK1" in catalog "" schema ""
07:05:22.857 pool-1-thread-1 DEBUG Schema: ALTER TABLE DATABASE_PARAMS ADD CONSTRAINT DATABASE_PARAMS_FK1 FOREIGN KEY (DB_ID) REFERENCES DBS (DB_ID)
07:05:22.860 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ConstraintInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ConstraintInfo.class
07:05:22.861 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.DDUtils - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/DDUtils.class
07:05:22.865 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.DDLConstantAction$SettableBoolean - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/DDLConstantAction$SettableBoolean.class
07:05:22.865 pool-1-thread-1 DEBUG Schema: Execution Time = 8 ms
07:05:22.865 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@46738621"
07:05:22.866 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@46738621"
07:05:22.866 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@46738621" non enlisted to a transaction is being committed.
07:05:22.866 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@46738621" closed
07:05:22.866 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.UnionStatementGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/UnionStatementGenerator.class
07:05:22.866 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.AbstractStatementGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/AbstractStatementGenerator.class
07:05:22.867 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.NullLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/NullLiteral.class
07:05:22.869 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLTableAlphaNamer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLTableAlphaNamer.class
07:05:22.870 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLTableNamer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLTableNamer.class
07:05:22.871 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLTableGroup - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLTableGroup.class
07:05:22.871 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.IllegalExpressionOperationException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/IllegalExpressionOperationException.class
07:05:22.872 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.CharacterLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/CharacterLiteral.class
07:05:22.872 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.CharacterExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/CharacterExpression.class
07:05:22.873 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.BooleanLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/BooleanLiteral.class
07:05:22.874 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLText - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLText.class
07:05:22.874 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.AggregateNumericExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/AggregateNumericExpression.class
07:05:22.875 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.NumericExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/NumericExpression.class
07:05:22.875 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.AggregateTemporalExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/AggregateTemporalExpression.class
07:05:22.875 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.TemporalExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/TemporalExpression.class
07:05:22.876 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLStatementParameter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLStatementParameter.class
07:05:22.876 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.QueryToSQLMapper - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/QueryToSQLMapper.class
07:05:22.877 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.QueryGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/QueryGenerator.class
07:05:22.878 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.evaluator.AbstractExpressionEvaluator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/evaluator/AbstractExpressionEvaluator.class
07:05:22.878 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.expression.ExpressionEvaluator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/expression/ExpressionEvaluator.class
07:05:22.879 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.IntegerLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/IntegerLiteral.class
07:05:22.880 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.TemporalLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/TemporalLiteral.class
07:05:22.882 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.PersistableIdMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/PersistableIdMapping.class
07:05:22.882 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.ArrayExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/ArrayExpression.class
07:05:22.883 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.SubqueryExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/SubqueryExpression.class
07:05:22.883 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.TemporalSubqueryExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/TemporalSubqueryExpression.class
07:05:22.884 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.SubqueryExpressionComponent - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/SubqueryExpressionComponent.class
07:05:22.884 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.StringSubqueryExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/StringSubqueryExpression.class
07:05:22.884 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.NumericSubqueryExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/NumericSubqueryExpression.class
07:05:22.885 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.QueryToSQLMapper$SQLTableMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/QueryToSQLMapper$SQLTableMapping.class
07:05:22.886 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.compiler.CompilationComponent - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/compiler/CompilationComponent.class
07:05:22.886 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLStatementHelper - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLStatementHelper.class
07:05:22.888 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.SecondaryDatastoreClass - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/SecondaryDatastoreClass.class
07:05:22.888 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.SQLExpression$ColumnExpressionList - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/SQLExpression$ColumnExpressionList.class
07:05:22.888 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.ColumnExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/ColumnExpression.class
07:05:22.889 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.ParameterLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/ParameterLiteral.class
07:05:22.889 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.ExpressionUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/ExpressionUtils.class
07:05:22.890 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.ObjectExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/ObjectExpression.class
07:05:22.891 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.UnboundExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/UnboundExpression.class
07:05:22.892 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLJoin$JoinType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLJoin$JoinType.class
07:05:22.892 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.EnumExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/EnumExpression.class
07:05:22.892 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.DelegatedExpression - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/DelegatedExpression.class
07:05:22.893 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.FetchPlanForClass - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/FetchPlanForClass.class
07:05:22.894 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.StatementMappingIndex - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/StatementMappingIndex.class
07:05:22.895 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLColumn - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLColumn.class
07:05:22.896 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 247 ms
07:05:22.896 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == '' Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MDatabase' AS NUCLEUS_TYPE,A0."DESC",A0.DB_LOCATION_URI,A0."NAME",A0.OWNER_NAME,A0.OWNER_TYPE,A0.DB_ID FROM DBS A0 WHERE A0."NAME" = ''"
07:05:22.896 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7478e33a [conn=com.jolbox.bonecp.ConnectionHandle@34e64b79, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:22.896 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == ''" ...
07:05:22.902 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2eff4c7d"
07:05:22.902 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.ParamLoggingPreparedStatement - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/ParamLoggingPreparedStatement.class
07:05:22.904 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.ParamLoggingPreparedStatement$SubStatement - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/ParamLoggingPreparedStatement$SubStatement.class
07:05:22.904 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MDatabase' AS NUCLEUS_TYPE,A0."DESC",A0.DB_LOCATION_URI,A0."NAME",A0.OWNER_NAME,A0.OWNER_TYPE,A0.DB_ID FROM DBS A0 WHERE A0."NAME" = ''
07:05:22.904 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:22.905 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.PersistentClassROF - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/PersistentClassROF.class
07:05:22.906 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.FieldValues - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/FieldValues.class
07:05:22.908 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.JDOQLQuery$2 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/JDOQLQuery$2.class
07:05:22.908 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 12 ms
07:05:22.908 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MTableColumnStatistics WHERE dbName == ''"
07:05:22.908 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:22.908 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{dbName}  =  Literal{}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MTableColumnStatistics]
07:05:22.908 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MTableColumnStatistics WHERE dbName == ''" for datastore
07:05:22.909 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
07:05:22.909 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MColumnDescriptor [Table : CDS, InheritanceStrategy : new-table]
07:05:22.909 pool-1-thread-1 DEBUG Schema: Column "CDS.CD_ID" added to internal representation of table.
07:05:22.909 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MSerDeInfo [Table : SERDES, InheritanceStrategy : new-table]
07:05:22.909 pool-1-thread-1 DEBUG Schema: Column "SERDES.SERDE_ID" added to internal representation of table.
07:05:22.909 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MStringList [Table : SKEWED_STRING_LIST, InheritanceStrategy : new-table]
07:05:22.909 pool-1-thread-1 DEBUG Schema: Column "SKEWED_STRING_LIST.STRING_LIST_ID" added to internal representation of table.
07:05:22.909 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
07:05:22.909 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MStorageDescriptor [Table : SDS, InheritanceStrategy : new-table]
07:05:22.909 pool-1-thread-1 DEBUG Schema: Column "SDS.SD_ID" added to internal representation of table.
07:05:22.910 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MTable [Table : TBLS, InheritanceStrategy : new-table]
07:05:22.910 pool-1-thread-1 DEBUG Schema: Column "TBLS.TBL_ID" added to internal representation of table.
07:05:22.910 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics [Table : TAB_COL_STATS, InheritanceStrategy : new-table]
07:05:22.910 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.CS_ID" added to internal representation of table.
07:05:22.910 pool-1-thread-1 DEBUG Schema: Table CDS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MColumnDescriptor (inheritance strategy="new-table")
07:05:22.910 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.CollectionTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/CollectionTable.class
07:05:22.911 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.DatastoreElementContainer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/DatastoreElementContainer.class
07:05:22.911 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.ElementContainerTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/ElementContainerTable.class
07:05:22.913 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols [Table : COLUMNS_V2]
07:05:22.913 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:22.913 pool-1-thread-1 DEBUG Schema: Table/View CDS has been initialised
07:05:22.913 pool-1-thread-1 DEBUG Schema: Table TBLS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MTable (inheritance strategy="new-table")
07:05:22.913 pool-1-thread-1 DEBUG Schema: Column "TBLS.CREATE_TIME" added to internal representation of table.
07:05:22.913 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.createTime] -> Column(s) [TBLS.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.914 pool-1-thread-1 DEBUG Schema: Column "TBLS.DB_ID" added to internal representation of table.
07:05:22.914 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.database] -> Column(s) [TBLS.DB_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.914 pool-1-thread-1 DEBUG Schema: Column "TBLS.LAST_ACCESS_TIME" added to internal representation of table.
07:05:22.914 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.lastAccessTime] -> Column(s) [TBLS.LAST_ACCESS_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.914 pool-1-thread-1 DEBUG Schema: Column "TBLS.OWNER" added to internal representation of table.
07:05:22.914 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.owner] -> Column(s) [TBLS.OWNER] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.914 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.parameters [Table : TABLE_PARAMS]
07:05:22.914 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:22.914 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MTable.partitionKeys [Table : PARTITION_KEYS]
07:05:22.914 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.partitionKeys] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:22.914 pool-1-thread-1 DEBUG Schema: Column "TBLS.RETENTION" added to internal representation of table.
07:05:22.914 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.retention] -> Column(s) [TBLS.RETENTION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.915 pool-1-thread-1 DEBUG Schema: Column "TBLS.SD_ID" added to internal representation of table.
07:05:22.915 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.sd] -> Column(s) [TBLS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.915 pool-1-thread-1 DEBUG Schema: Column "TBLS.TBL_NAME" added to internal representation of table.
07:05:22.915 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.tableName] -> Column(s) [TBLS.TBL_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.915 pool-1-thread-1 DEBUG Schema: Column "TBLS.TBL_TYPE" added to internal representation of table.
07:05:22.915 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.tableType] -> Column(s) [TBLS.TBL_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.915 pool-1-thread-1 DEBUG Schema: Column "TBLS.VIEW_EXPANDED_TEXT" added to internal representation of table.
07:05:22.915 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.viewExpandedText] -> Column(s) [TBLS.VIEW_EXPANDED_TEXT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.LongVarcharRDBMSMapping)
07:05:22.915 pool-1-thread-1 DEBUG Schema: Column "TBLS.VIEW_ORIGINAL_TEXT" added to internal representation of table.
07:05:22.915 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTable.viewOriginalText] -> Column(s) [TBLS.VIEW_ORIGINAL_TEXT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.LongVarcharRDBMSMapping)
07:05:22.915 pool-1-thread-1 DEBUG Schema: Table/View TBLS has been initialised
07:05:22.915 pool-1-thread-1 DEBUG Schema: Table SERDES will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MSerDeInfo (inheritance strategy="new-table")
07:05:22.915 pool-1-thread-1 DEBUG Schema: Column "SERDES."NAME"" added to internal representation of table.
07:05:22.915 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.name] -> Column(s) [SERDES."NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.916 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters [Table : SERDE_PARAMS]
07:05:22.916 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:22.917 pool-1-thread-1 DEBUG Schema: Column "SERDES.SLIB" added to internal representation of table.
07:05:22.917 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.serializationLib] -> Column(s) [SERDES.SLIB] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.917 pool-1-thread-1 DEBUG Schema: Table/View SERDES has been initialised
07:05:22.917 pool-1-thread-1 DEBUG Schema: Table SKEWED_STRING_LIST will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MStringList (inheritance strategy="new-table")
07:05:22.917 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStringList.internalList [Table : SKEWED_STRING_LIST_VALUES]
07:05:22.917 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStringList.internalList] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:22.917 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_STRING_LIST has been initialised
07:05:22.917 pool-1-thread-1 DEBUG Schema: Table SDS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MStorageDescriptor (inheritance strategy="new-table")
07:05:22.917 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols [Table : BUCKETING_COLS]
07:05:22.917 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:22.917 pool-1-thread-1 DEBUG Schema: Column "SDS.CD_ID" added to internal representation of table.
07:05:22.917 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd] -> Column(s) [SDS.CD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.917 pool-1-thread-1 DEBUG Schema: Column "SDS.INPUT_FORMAT" added to internal representation of table.
07:05:22.917 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.inputFormat] -> Column(s) [SDS.INPUT_FORMAT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.917 pool-1-thread-1 DEBUG Schema: Column "SDS.IS_COMPRESSED" added to internal representation of table.
07:05:22.918 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.isCompressed] -> Column(s) [SDS.IS_COMPRESSED] using mapping of type "org.datanucleus.store.rdbms.mapping.java.BooleanMapping" (org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping)
07:05:22.918 pool-1-thread-1 DEBUG Schema: Column "SDS.IS_STOREDASSUBDIRECTORIES" added to internal representation of table.
07:05:22.918 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.isStoredAsSubDirectories] -> Column(s) [SDS.IS_STOREDASSUBDIRECTORIES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.BooleanMapping" (org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping)
07:05:22.918 pool-1-thread-1 DEBUG Schema: Column "SDS.LOCATION" added to internal representation of table.
07:05:22.918 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.location] -> Column(s) [SDS.LOCATION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.918 pool-1-thread-1 DEBUG Schema: Column "SDS.NUM_BUCKETS" added to internal representation of table.
07:05:22.918 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.numBuckets] -> Column(s) [SDS.NUM_BUCKETS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.918 pool-1-thread-1 DEBUG Schema: Column "SDS.OUTPUT_FORMAT" added to internal representation of table.
07:05:22.918 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.outputFormat] -> Column(s) [SDS.OUTPUT_FORMAT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.918 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters [Table : SD_PARAMS]
07:05:22.918 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:22.918 pool-1-thread-1 DEBUG Schema: Column "SDS.SERDE_ID" added to internal representation of table.
07:05:22.918 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo] -> Column(s) [SDS.SERDE_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.918 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames [Table : SKEWED_COL_NAMES]
07:05:22.918 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:22.919 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps [Table : SKEWED_COL_VALUE_LOC_MAP]
07:05:22.919 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:22.919 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues [Table : SKEWED_VALUES]
07:05:22.919 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:22.919 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols [Table : SORT_COLS]
07:05:22.919 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:22.919 pool-1-thread-1 DEBUG Schema: Table/View SDS has been initialised
07:05:22.919 pool-1-thread-1 DEBUG Schema: Table TAB_COL_STATS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MTableColumnStatistics (inheritance strategy="new-table")
07:05:22.919 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.AVG_COL_LEN" added to internal representation of table.
07:05:22.919 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.avgColLen] -> Column(s) [TAB_COL_STATS.AVG_COL_LEN] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:22.919 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS."COLUMN_NAME"" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.colName] -> Column(s) [TAB_COL_STATS."COLUMN_NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.COLUMN_TYPE" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.colType] -> Column(s) [TAB_COL_STATS.COLUMN_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.DB_NAME" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.dbName] -> Column(s) [TAB_COL_STATS.DB_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.BIG_DECIMAL_HIGH_VALUE" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.decimalHighValue] -> Column(s) [TAB_COL_STATS.BIG_DECIMAL_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.BIG_DECIMAL_LOW_VALUE" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.decimalLowValue] -> Column(s) [TAB_COL_STATS.BIG_DECIMAL_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.DOUBLE_HIGH_VALUE" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.doubleHighValue] -> Column(s) [TAB_COL_STATS.DOUBLE_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.DOUBLE_LOW_VALUE" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.doubleLowValue] -> Column(s) [TAB_COL_STATS.DOUBLE_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.LAST_ANALYZED" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.lastAnalyzed] -> Column(s) [TAB_COL_STATS.LAST_ANALYZED] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.LONG_HIGH_VALUE" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.longHighValue] -> Column(s) [TAB_COL_STATS.LONG_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.LONG_LOW_VALUE" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.longLowValue] -> Column(s) [TAB_COL_STATS.LONG_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.MAX_COL_LEN" added to internal representation of table.
07:05:22.920 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.maxColLen] -> Column(s) [TAB_COL_STATS.MAX_COL_LEN] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.920 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.NUM_DISTINCTS" added to internal representation of table.
07:05:22.921 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.numDVs] -> Column(s) [TAB_COL_STATS.NUM_DISTINCTS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.921 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.NUM_FALSES" added to internal representation of table.
07:05:22.921 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.numFalses] -> Column(s) [TAB_COL_STATS.NUM_FALSES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.921 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.NUM_NULLS" added to internal representation of table.
07:05:22.921 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.numNulls] -> Column(s) [TAB_COL_STATS.NUM_NULLS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.921 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.NUM_TRUES" added to internal representation of table.
07:05:22.921 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.numTrues] -> Column(s) [TAB_COL_STATS.NUM_TRUES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.921 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS.TBL_ID" added to internal representation of table.
07:05:22.921 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.table] -> Column(s) [TAB_COL_STATS.TBL_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.921 pool-1-thread-1 DEBUG Schema: Column "TAB_COL_STATS."TABLE_NAME"" added to internal representation of table.
07:05:22.921 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.tableName] -> Column(s) [TAB_COL_STATS."TABLE_NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.921 pool-1-thread-1 DEBUG Schema: Table/View TAB_COL_STATS has been initialised
07:05:22.921 pool-1-thread-1 DEBUG Schema: Column "SORT_COLS.SD_ID" added to internal representation of table.
07:05:22.921 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols] -> Column(s) [SORT_COLS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.921 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/EmbeddedElementPCMapping.class
07:05:22.922 pool-1-thread-1 DEBUG Schema: Column "SORT_COLS."COLUMN_NAME"" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "SORT_COLS."ORDER"" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols] -> Column(s) [SORT_COLS."COLUMN_NAME",SORT_COLS."ORDER"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "SORT_COLS.INTEGER_IDX" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols] -> Column(s) [SORT_COLS.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.923 pool-1-thread-1 DEBUG Schema: Table/View SORT_COLS has been initialised
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.TBL_ID" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.partitionKeys] -> Column(s) [PARTITION_KEYS.TBL_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.PKEY_COMMENT" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.PKEY_NAME" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.PKEY_TYPE" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.partitionKeys] -> Column(s) [PARTITION_KEYS.PKEY_COMMENT,PARTITION_KEYS.PKEY_NAME,PARTITION_KEYS.PKEY_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEYS.INTEGER_IDX" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.partitionKeys] -> Column(s) [PARTITION_KEYS.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.923 pool-1-thread-1 DEBUG Schema: Table/View PARTITION_KEYS has been initialised
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "SERDE_PARAMS.SERDE_ID" added to internal representation of table.
07:05:22.923 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters] -> Column(s) [SERDE_PARAMS.SERDE_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.923 pool-1-thread-1 DEBUG Schema: Column "SERDE_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:22.924 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters] -> Column(s) [SERDE_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.924 pool-1-thread-1 DEBUG Schema: Column "SERDE_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:22.924 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters] -> Column(s) [SERDE_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.924 pool-1-thread-1 DEBUG Schema: Table/View SERDE_PARAMS has been initialised
07:05:22.924 pool-1-thread-1 DEBUG Schema: Column "SKEWED_STRING_LIST_VALUES.STRING_LIST_ID" added to internal representation of table.
07:05:22.924 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStringList.internalList] -> Column(s) [SKEWED_STRING_LIST_VALUES.STRING_LIST_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.924 pool-1-thread-1 DEBUG Schema: Column "SKEWED_STRING_LIST_VALUES.STRING_LIST_VALUE" added to internal representation of table.
07:05:22.924 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStringList.internalList] -> Column(s) [SKEWED_STRING_LIST_VALUES.STRING_LIST_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.924 pool-1-thread-1 DEBUG Schema: Column "SKEWED_STRING_LIST_VALUES.INTEGER_IDX" added to internal representation of table.
07:05:22.924 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStringList.internalList] -> Column(s) [SKEWED_STRING_LIST_VALUES.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.924 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_STRING_LIST_VALUES has been initialised
07:05:22.924 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_VALUE_LOC_MAP.SD_ID" added to internal representation of table.
07:05:22.924 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps] -> Column(s) [SKEWED_COL_VALUE_LOC_MAP.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.924 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_VALUE_LOC_MAP.STRING_LIST_ID_KID" added to internal representation of table.
07:05:22.924 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps] -> Column(s) [SKEWED_COL_VALUE_LOC_MAP.STRING_LIST_ID_KID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.924 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_VALUE_LOC_MAP.LOCATION" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps] -> Column(s) [SKEWED_COL_VALUE_LOC_MAP.LOCATION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_COL_VALUE_LOC_MAP has been initialised
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "TABLE_PARAMS.TBL_ID" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.parameters] -> Column(s) [TABLE_PARAMS.TBL_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "TABLE_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.parameters] -> Column(s) [TABLE_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "TABLE_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MTable.parameters] -> Column(s) [TABLE_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Table/View TABLE_PARAMS has been initialised
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "BUCKETING_COLS.SD_ID" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols] -> Column(s) [BUCKETING_COLS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "BUCKETING_COLS.BUCKET_COL_NAME" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols] -> Column(s) [BUCKETING_COLS.BUCKET_COL_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "BUCKETING_COLS.INTEGER_IDX" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols] -> Column(s) [BUCKETING_COLS.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Table/View BUCKETING_COLS has been initialised
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "SD_PARAMS.SD_ID" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters] -> Column(s) [SD_PARAMS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "SD_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters] -> Column(s) [SD_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "SD_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters] -> Column(s) [SD_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Table/View SD_PARAMS has been initialised
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "SKEWED_VALUES.SD_ID_OID" added to internal representation of table.
07:05:22.925 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues] -> Column(s) [SKEWED_VALUES.SD_ID_OID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.925 pool-1-thread-1 DEBUG Schema: Column "SKEWED_VALUES.STRING_LIST_ID_EID" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues] -> Column(s) [SKEWED_VALUES.STRING_LIST_ID_EID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "SKEWED_VALUES.INTEGER_IDX" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues] -> Column(s) [SKEWED_VALUES.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.926 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_VALUES has been initialised
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_NAMES.SD_ID" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames] -> Column(s) [SKEWED_COL_NAMES.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_NAMES.SKEWED_COL_NAME" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames] -> Column(s) [SKEWED_COL_NAMES.SKEWED_COL_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "SKEWED_COL_NAMES.INTEGER_IDX" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames] -> Column(s) [SKEWED_COL_NAMES.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.926 pool-1-thread-1 DEBUG Schema: Table/View SKEWED_COL_NAMES has been initialised
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2.CD_ID" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols] -> Column(s) [COLUMNS_V2.CD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2.COMMENT" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2."COLUMN_NAME"" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2.TYPE_NAME" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols] -> Column(s) [COLUMNS_V2.COMMENT,COLUMNS_V2."COLUMN_NAME",COLUMNS_V2.TYPE_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:22.926 pool-1-thread-1 DEBUG Schema: Column "COLUMNS_V2.INTEGER_IDX" added to internal representation of table.
07:05:22.926 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols] -> Column(s) [COLUMNS_V2.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:22.926 pool-1-thread-1 DEBUG Schema: Table/View COLUMNS_V2 has been initialised
07:05:22.926 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@6d653aa7" opened with isolation level "serializable" and auto-commit=false
07:05:22.926 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@6d653aa7" with isolation "serializable"
07:05:22.930 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.931 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.931 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.931 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.933 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.933 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.935 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.935 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.937 pool-1-thread-1 DEBUG Schema: Check of existence of CDS returned no table
07:05:22.937 pool-1-thread-1 DEBUG Schema: Creating table CDS
07:05:22.937 pool-1-thread-1 DEBUG Schema: CREATE TABLE CDS
(
    CD_ID BIGINT NOT NULL
)
07:05:22.939 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:22.939 pool-1-thread-1 DEBUG Schema: ALTER TABLE CDS ADD CONSTRAINT CDS_PK PRIMARY KEY (CD_ID)
07:05:22.942 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:22.942 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.942 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.943 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.943 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.943 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.945 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.945 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.946 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.947 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.947 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.948 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.948 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.948 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.948 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.949 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.949 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.949 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.950 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.950 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.950 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.951 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.951 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.954 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.955 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.956 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.956 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.957 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.957 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.958 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.958 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.959 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.959 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.960 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.960 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.968 pool-1-thread-1 DEBUG Schema: Check of existence of TBLS returned no table
07:05:22.968 pool-1-thread-1 DEBUG Schema: Creating table TBLS
07:05:22.968 pool-1-thread-1 DEBUG Schema: CREATE TABLE TBLS
(
    TBL_ID BIGINT NOT NULL,
    CREATE_TIME INTEGER NOT NULL,
    DB_ID BIGINT,
    LAST_ACCESS_TIME INTEGER NOT NULL,
    OWNER VARCHAR(767),
    RETENTION INTEGER NOT NULL,
    SD_ID BIGINT,
    TBL_NAME VARCHAR(128),
    TBL_TYPE VARCHAR(128),
    VIEW_EXPANDED_TEXT LONG VARCHAR,
    VIEW_ORIGINAL_TEXT LONG VARCHAR
)
07:05:22.970 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:22.970 pool-1-thread-1 DEBUG Schema: ALTER TABLE TBLS ADD CONSTRAINT TBLS_PK PRIMARY KEY (TBL_ID)
07:05:22.974 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:22.974 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.975 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.975 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.975 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.975 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.976 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.976 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.976 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.976 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.977 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.977 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.977 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.977 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.978 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.979 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.979 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.979 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.979 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.980 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.980 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.980 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.981 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.981 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.981 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.981 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.982 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.982 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.982 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.986 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.986 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.986 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.987 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.987 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.987 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.990 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.991 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.991 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.991 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.992 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.992 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.992 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.993 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.993 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.993 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.994 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.994 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:22.994 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:22.994 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:22.997 pool-1-thread-1 DEBUG Schema: Check of existence of SERDES returned no table
07:05:22.998 pool-1-thread-1 DEBUG Schema: Creating table SERDES
07:05:22.998 pool-1-thread-1 DEBUG Schema: CREATE TABLE SERDES
(
    SERDE_ID BIGINT NOT NULL,
    "NAME" VARCHAR(128),
    SLIB VARCHAR(4000)
)
07:05:22.999 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:22.999 pool-1-thread-1 DEBUG Schema: ALTER TABLE SERDES ADD CONSTRAINT SERDES_PK PRIMARY KEY (SERDE_ID)
07:05:23.002 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.003 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.004 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.004 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.004 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.005 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.005 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.005 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.007 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.007 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.007 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.008 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.008 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.010 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.010 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.010 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.010 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.011 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.011 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.011 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.012 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.012 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.012 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.015 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.016 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.016 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.016 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.017 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.017 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.017 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.018 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.018 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.019 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.019 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.019 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.020 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.020 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.021 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.021 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.022 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.022 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.022 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.022 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.023 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.023 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.023 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.024 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.024 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.024 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.024 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.025 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.027 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_STRING_LIST returned no table
07:05:23.027 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_STRING_LIST
07:05:23.027 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_STRING_LIST
(
    STRING_LIST_ID BIGINT NOT NULL
)
07:05:23.029 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.029 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_STRING_LIST ADD CONSTRAINT SKEWED_STRING_LIST_PK PRIMARY KEY (STRING_LIST_ID)
07:05:23.032 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.032 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.032 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.033 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.033 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.033 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.036 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.036 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.037 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.037 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.038 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.038 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.039 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.039 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.040 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.040 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.041 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.041 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.042 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.042 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.043 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.043 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.047 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.048 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.048 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.049 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.049 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.050 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.050 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.050 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.051 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.051 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.052 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.052 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.053 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.053 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.055 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.055 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.055 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.056 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.056 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.057 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.057 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.057 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.058 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.058 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.058 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.059 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.059 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.059 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.062 pool-1-thread-1 DEBUG Schema: Check of existence of SDS returned no table
07:05:23.062 pool-1-thread-1 DEBUG Schema: Creating table SDS
07:05:23.063 pool-1-thread-1 DEBUG Schema: CREATE TABLE SDS
(
    SD_ID BIGINT NOT NULL,
    CD_ID BIGINT,
    INPUT_FORMAT VARCHAR(4000),
    IS_COMPRESSED CHAR(1) NOT NULL CHECK (IS_COMPRESSED IN ('Y','N')),
    IS_STOREDASSUBDIRECTORIES CHAR(1) NOT NULL CHECK (IS_STOREDASSUBDIRECTORIES IN ('Y','N')),
    LOCATION VARCHAR(4000),
    NUM_BUCKETS INTEGER NOT NULL,
    OUTPUT_FORMAT VARCHAR(4000),
    SERDE_ID BIGINT
)
07:05:23.064 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.depend.ProviderList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/depend/ProviderList.class
07:05:23.068 pool-1-thread-1 DEBUG Schema: Execution Time = 5 ms
07:05:23.068 pool-1-thread-1 DEBUG Schema: ALTER TABLE SDS ADD CONSTRAINT SDS_PK PRIMARY KEY (SD_ID)
07:05:23.073 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:23.073 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.074 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.074 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.075 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.075 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.076 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.077 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.077 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.078 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.078 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.079 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.079 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.079 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.080 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.081 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.083 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.084 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.084 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.085 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.085 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.087 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.087 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.088 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.088 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.093 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.093 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.094 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.094 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.097 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.097 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.097 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.098 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.099 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.099 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.100 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.102 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.102 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.102 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.103 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.103 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.105 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.105 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.110 pool-1-thread-1 DEBUG Schema: Check of existence of TAB_COL_STATS returned no table
07:05:23.110 pool-1-thread-1 DEBUG Schema: Creating table TAB_COL_STATS
07:05:23.110 pool-1-thread-1 DEBUG Schema: CREATE TABLE TAB_COL_STATS
(
    CS_ID BIGINT NOT NULL,
    AVG_COL_LEN DOUBLE,
    "COLUMN_NAME" VARCHAR(128) NOT NULL,
    COLUMN_TYPE VARCHAR(128) NOT NULL,
    DB_NAME VARCHAR(128) NOT NULL,
    BIG_DECIMAL_HIGH_VALUE VARCHAR(255),
    BIG_DECIMAL_LOW_VALUE VARCHAR(255),
    DOUBLE_HIGH_VALUE DOUBLE,
    DOUBLE_LOW_VALUE DOUBLE,
    LAST_ANALYZED BIGINT NOT NULL,
    LONG_HIGH_VALUE BIGINT,
    LONG_LOW_VALUE BIGINT,
    MAX_COL_LEN BIGINT,
    NUM_DISTINCTS BIGINT,
    NUM_FALSES BIGINT,
    NUM_NULLS BIGINT NOT NULL,
    NUM_TRUES BIGINT,
    TBL_ID BIGINT,
    "TABLE_NAME" VARCHAR(128) NOT NULL
)
07:05:23.112 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.types.SQLDouble - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/types/SQLDouble.class
07:05:23.118 pool-1-thread-1 DEBUG Schema: Execution Time = 8 ms
07:05:23.118 pool-1-thread-1 DEBUG Schema: ALTER TABLE TAB_COL_STATS ADD CONSTRAINT TAB_COL_STATS_PK PRIMARY KEY (CS_ID)
07:05:23.121 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.122 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.122 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.123 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.123 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.126 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.126 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.127 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.127 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.128 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.128 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.130 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.136 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.136 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.145 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.145 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.145 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.145 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.146 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.147 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.147 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.147 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.147 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.148 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.148 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.148 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.150 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.150 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.150 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.153 pool-1-thread-1 DEBUG Schema: Check of existence of SORT_COLS returned no table
07:05:23.153 pool-1-thread-1 DEBUG Schema: Creating table SORT_COLS
07:05:23.153 pool-1-thread-1 DEBUG Schema: CREATE TABLE SORT_COLS
(
    SD_ID BIGINT NOT NULL,
    "COLUMN_NAME" VARCHAR(128),
    "ORDER" INTEGER NOT NULL,
    INTEGER_IDX INTEGER NOT NULL
)
07:05:23.155 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.155 pool-1-thread-1 DEBUG Schema: ALTER TABLE SORT_COLS ADD CONSTRAINT SORT_COLS_PK PRIMARY KEY (SD_ID,INTEGER_IDX)
07:05:23.158 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.159 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.159 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.159 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.160 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.160 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.160 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.160 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.161 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.161 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.161 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.162 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.162 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.162 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.163 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.164 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.164 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.164 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.165 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.165 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.165 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.166 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.166 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.167 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.167 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.176 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.176 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.177 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.177 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.177 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.177 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.178 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.178 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.178 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.179 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.179 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.179 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.179 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.180 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.180 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.180 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.182 pool-1-thread-1 DEBUG Schema: Check of existence of PARTITION_KEYS returned no table
07:05:23.182 pool-1-thread-1 DEBUG Schema: Creating table PARTITION_KEYS
07:05:23.183 pool-1-thread-1 DEBUG Schema: CREATE TABLE PARTITION_KEYS
(
    TBL_ID BIGINT NOT NULL,
    PKEY_COMMENT VARCHAR(4000),
    PKEY_NAME VARCHAR(128) NOT NULL,
    PKEY_TYPE VARCHAR(767) NOT NULL,
    INTEGER_IDX INTEGER NOT NULL
)
07:05:23.184 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.184 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_KEYS ADD CONSTRAINT PARTITION_KEY_PK PRIMARY KEY (TBL_ID,PKEY_NAME)
07:05:23.187 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.187 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.187 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.188 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.188 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.188 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.188 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.191 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.191 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.192 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.192 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.193 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.193 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.193 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.194 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.194 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.195 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.195 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.195 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.196 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.196 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.196 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.199 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.199 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.200 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.200 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.200 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.200 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.201 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.201 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.201 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.204 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.204 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.204 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.207 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.207 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.207 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.209 pool-1-thread-1 DEBUG Schema: Check of existence of SERDE_PARAMS returned no table
07:05:23.209 pool-1-thread-1 DEBUG Schema: Creating table SERDE_PARAMS
07:05:23.209 pool-1-thread-1 DEBUG Schema: CREATE TABLE SERDE_PARAMS
(
    SERDE_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(256) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:23.211 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.211 pool-1-thread-1 DEBUG Schema: ALTER TABLE SERDE_PARAMS ADD CONSTRAINT SERDE_PARAMS_PK PRIMARY KEY (SERDE_ID,PARAM_KEY)
07:05:23.213 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.213 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.213 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.214 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.214 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.214 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.214 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.215 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.215 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.215 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.215 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.216 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.216 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.217 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.217 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.218 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.218 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.218 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.220 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.220 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.220 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.221 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.221 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.221 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.225 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.225 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.226 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.226 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.226 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.226 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.227 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.227 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.227 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.227 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.228 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.228 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.228 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.229 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.229 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.230 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.230 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.230 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.231 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.231 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.231 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.231 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.232 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.232 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.232 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.232 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.233 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.233 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.235 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_STRING_LIST_VALUES returned no table
07:05:23.235 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_STRING_LIST_VALUES
07:05:23.235 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_STRING_LIST_VALUES
(
    STRING_LIST_ID BIGINT NOT NULL,
    STRING_LIST_VALUE VARCHAR(255),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:23.237 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.237 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_STRING_LIST_VALUES ADD CONSTRAINT SKEWED_STRING_LIST_VALUES_PK PRIMARY KEY (STRING_LIST_ID,INTEGER_IDX)
07:05:23.239 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.239 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.239 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.240 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.240 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.240 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.241 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.241 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.241 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.242 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.242 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.242 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.242 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.243 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.243 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.244 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.244 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.244 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.245 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.245 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.245 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.246 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.246 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.247 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.247 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.247 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.253 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.253 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.253 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.253 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.255 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.255 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.255 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.256 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.256 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.257 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.257 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.257 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.257 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.258 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.258 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.258 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.258 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.259 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.259 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.260 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.260 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.263 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_COL_VALUE_LOC_MAP returned no table
07:05:23.263 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_COL_VALUE_LOC_MAP
07:05:23.263 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_COL_VALUE_LOC_MAP
(
    SD_ID BIGINT NOT NULL,
    STRING_LIST_ID_KID BIGINT NOT NULL,
    LOCATION VARCHAR(4000)
)
07:05:23.266 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.266 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_VALUE_LOC_MAP ADD CONSTRAINT SKEWED_COL_VALUE_LOC_MAP_PK PRIMARY KEY (SD_ID,STRING_LIST_ID_KID)
07:05:23.270 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:23.271 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.271 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.272 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.272 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.273 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.273 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.273 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.274 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.274 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.274 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.275 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.275 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.275 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.275 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.276 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.277 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.277 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.277 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.278 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.278 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.279 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.279 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.279 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.280 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.280 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.280 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.280 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.281 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.284 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.284 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.284 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.284 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.285 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.285 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.285 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.285 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.286 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.286 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.286 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.287 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.287 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.287 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.288 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.289 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.289 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.289 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.290 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.290 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.290 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.291 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.291 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.291 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.291 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.292 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.292 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.292 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.294 pool-1-thread-1 DEBUG Schema: Check of existence of TABLE_PARAMS returned no table
07:05:23.294 pool-1-thread-1 DEBUG Schema: Creating table TABLE_PARAMS
07:05:23.294 pool-1-thread-1 DEBUG Schema: CREATE TABLE TABLE_PARAMS
(
    TBL_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(256) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:23.296 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.296 pool-1-thread-1 DEBUG Schema: ALTER TABLE TABLE_PARAMS ADD CONSTRAINT TABLE_PARAMS_PK PRIMARY KEY (TBL_ID,PARAM_KEY)
07:05:23.298 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.299 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.299 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.300 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.300 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.300 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.301 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.301 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.302 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.302 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.302 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.303 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.303 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.304 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.304 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.305 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.305 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.305 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.306 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.306 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.306 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.307 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.308 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.308 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.308 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.309 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.312 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.312 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.313 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.313 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.313 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.314 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.314 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.314 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.315 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.315 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.315 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.315 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.316 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.316 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.317 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.317 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.317 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.317 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.318 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.318 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.318 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.318 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.319 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.319 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.319 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.320 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.320 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.320 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.323 pool-1-thread-1 DEBUG Schema: Check of existence of BUCKETING_COLS returned no table
07:05:23.323 pool-1-thread-1 DEBUG Schema: Creating table BUCKETING_COLS
07:05:23.323 pool-1-thread-1 DEBUG Schema: CREATE TABLE BUCKETING_COLS
(
    SD_ID BIGINT NOT NULL,
    BUCKET_COL_NAME VARCHAR(255),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:23.325 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.325 pool-1-thread-1 DEBUG Schema: ALTER TABLE BUCKETING_COLS ADD CONSTRAINT BUCKETING_COLS_PK PRIMARY KEY (SD_ID,INTEGER_IDX)
07:05:23.328 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.328 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.329 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.329 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.330 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.331 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.332 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.332 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.332 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.333 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.333 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.333 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.333 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.334 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.334 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.335 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.335 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.335 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.336 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.337 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.338 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.338 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.338 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.341 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.341 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.342 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.342 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.343 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.343 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.343 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.344 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.344 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.344 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.345 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.345 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.347 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.347 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.347 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.348 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.349 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.349 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.349 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.350 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.350 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.351 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.351 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.352 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.352 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.353 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.356 pool-1-thread-1 DEBUG Schema: Check of existence of SD_PARAMS returned no table
07:05:23.356 pool-1-thread-1 DEBUG Schema: Creating table SD_PARAMS
07:05:23.356 pool-1-thread-1 DEBUG Schema: CREATE TABLE SD_PARAMS
(
    SD_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(256) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:23.358 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.358 pool-1-thread-1 DEBUG Schema: ALTER TABLE SD_PARAMS ADD CONSTRAINT SD_PARAMS_PK PRIMARY KEY (SD_ID,PARAM_KEY)
07:05:23.360 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.361 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.361 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.362 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.362 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.362 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.362 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.363 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.363 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.363 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.364 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.364 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.365 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.365 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.365 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.366 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.366 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.366 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.367 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.367 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.367 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.368 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.368 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.368 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.368 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.369 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.369 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.369 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.369 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.377 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.378 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.378 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.378 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.379 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.379 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.379 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.379 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.380 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.380 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.380 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.380 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.381 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.382 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.382 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.382 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.382 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.383 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.383 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.383 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.383 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.384 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.384 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.384 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.385 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.385 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.387 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_VALUES returned no table
07:05:23.387 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_VALUES
07:05:23.387 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_VALUES
(
    SD_ID_OID BIGINT NOT NULL,
    STRING_LIST_ID_EID BIGINT,
    INTEGER_IDX INTEGER NOT NULL
)
07:05:23.389 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.389 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_VALUES ADD CONSTRAINT SKEWED_VALUES_PK PRIMARY KEY (SD_ID_OID,INTEGER_IDX)
07:05:23.393 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:23.394 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.394 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.396 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.396 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.396 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.396 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.397 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.397 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.397 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.397 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.399 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.400 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.400 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.400 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.401 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.401 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.401 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.401 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.402 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.404 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.405 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.405 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.405 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.406 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.406 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.406 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.406 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.407 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.407 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.407 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.407 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.408 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.408 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.409 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.409 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.409 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.410 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.411 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.411 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.411 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.411 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.412 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.412 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.412 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.415 pool-1-thread-1 DEBUG Schema: Check of existence of SKEWED_COL_NAMES returned no table
07:05:23.415 pool-1-thread-1 DEBUG Schema: Creating table SKEWED_COL_NAMES
07:05:23.415 pool-1-thread-1 DEBUG Schema: CREATE TABLE SKEWED_COL_NAMES
(
    SD_ID BIGINT NOT NULL,
    SKEWED_COL_NAME VARCHAR(255),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:23.419 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:23.419 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_NAMES ADD CONSTRAINT SKEWED_COL_NAMES_PK PRIMARY KEY (SD_ID,INTEGER_IDX)
07:05:23.421 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.422 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.422 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.422 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.423 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.424 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.425 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.426 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.431 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.431 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.432 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.432 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.433 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.433 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.433 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.434 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.435 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.435 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.435 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.438 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.438 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.438 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.439 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.439 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.439 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.439 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.440 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.441 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.441 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.441 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.442 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.442 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.442 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.443 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.444 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.444 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.444 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.444 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.445 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.445 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.445 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.447 pool-1-thread-1 DEBUG Schema: Check of existence of COLUMNS_V2 returned no table
07:05:23.447 pool-1-thread-1 DEBUG Schema: Creating table COLUMNS_V2
07:05:23.447 pool-1-thread-1 DEBUG Schema: CREATE TABLE COLUMNS_V2
(
    CD_ID BIGINT NOT NULL,
    COMMENT VARCHAR(256),
    "COLUMN_NAME" VARCHAR(128) NOT NULL,
    TYPE_NAME VARCHAR(4000) NOT NULL,
    INTEGER_IDX INTEGER NOT NULL
)
07:05:23.449 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.449 pool-1-thread-1 DEBUG Schema: ALTER TABLE COLUMNS_V2 ADD CONSTRAINT COLUMNS_PK PRIMARY KEY (CD_ID,"COLUMN_NAME")
07:05:23.452 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.452 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.452 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.453 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.453 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.453 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.454 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.454 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.454 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.454 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.455 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.455 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.455 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.455 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.456 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.456 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.457 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.457 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.457 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.458 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.463 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.463 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.464 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.464 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.464 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.464 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.465 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.465 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.465 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.468 pool-1-thread-1 DEBUG Schema: Creating index "TBLS_N50" in catalog "" schema ""
07:05:23.468 pool-1-thread-1 DEBUG Schema: CREATE INDEX TBLS_N50 ON TBLS (DB_ID)
07:05:23.470 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.470 pool-1-thread-1 DEBUG Schema: Creating index "TBLS_N49" in catalog "" schema ""
07:05:23.470 pool-1-thread-1 DEBUG Schema: CREATE INDEX TBLS_N49 ON TBLS (SD_ID)
07:05:23.472 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.472 pool-1-thread-1 DEBUG Schema: Creating index "UniqueTable" in catalog "" schema ""
07:05:23.472 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX UNIQUETABLE ON TBLS (TBL_NAME,DB_ID)
07:05:23.474 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.474 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "TBLS_FK1" in catalog "" schema ""
07:05:23.474 pool-1-thread-1 DEBUG Schema: ALTER TABLE TBLS ADD CONSTRAINT TBLS_FK1 FOREIGN KEY (DB_ID) REFERENCES DBS (DB_ID)
07:05:23.477 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.477 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "TBLS_FK2" in catalog "" schema ""
07:05:23.477 pool-1-thread-1 DEBUG Schema: ALTER TABLE TBLS ADD CONSTRAINT TBLS_FK2 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:23.480 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.481 pool-1-thread-1 DEBUG Schema: Creating index "SDS_N50" in catalog "" schema ""
07:05:23.481 pool-1-thread-1 DEBUG Schema: CREATE INDEX SDS_N50 ON SDS (SERDE_ID)
07:05:23.483 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.483 pool-1-thread-1 DEBUG Schema: Creating index "SDS_N49" in catalog "" schema ""
07:05:23.483 pool-1-thread-1 DEBUG Schema: CREATE INDEX SDS_N49 ON SDS (CD_ID)
07:05:23.484 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.484 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SDS_FK1" in catalog "" schema ""
07:05:23.484 pool-1-thread-1 DEBUG Schema: ALTER TABLE SDS ADD CONSTRAINT SDS_FK1 FOREIGN KEY (CD_ID) REFERENCES CDS (CD_ID)
07:05:23.487 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.488 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SDS_FK2" in catalog "" schema ""
07:05:23.488 pool-1-thread-1 DEBUG Schema: ALTER TABLE SDS ADD CONSTRAINT SDS_FK2 FOREIGN KEY (SERDE_ID) REFERENCES SERDES (SERDE_ID)
07:05:23.491 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.491 pool-1-thread-1 DEBUG Schema: Creating index "TAB_COL_STATS_N49" in catalog "" schema ""
07:05:23.491 pool-1-thread-1 DEBUG Schema: CREATE INDEX TAB_COL_STATS_N49 ON TAB_COL_STATS (TBL_ID)
07:05:23.492 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.493 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "TAB_COL_STATS_FK1" in catalog "" schema ""
07:05:23.493 pool-1-thread-1 DEBUG Schema: ALTER TABLE TAB_COL_STATS ADD CONSTRAINT TAB_COL_STATS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID)
07:05:23.496 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.496 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.SerialisedPCMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/SerialisedPCMapping.class
07:05:23.497 pool-1-thread-1 DEBUG Schema: Creating index "SORT_COLS_N49" in catalog "" schema ""
07:05:23.497 pool-1-thread-1 DEBUG Schema: CREATE INDEX SORT_COLS_N49 ON SORT_COLS (SD_ID)
07:05:23.498 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.498 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SORT_COLS_FK1" in catalog "" schema ""
07:05:23.498 pool-1-thread-1 DEBUG Schema: ALTER TABLE SORT_COLS ADD CONSTRAINT SORT_COLS_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:23.502 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:23.502 pool-1-thread-1 DEBUG Schema: Creating index "PARTITION_KEYS_N49" in catalog "" schema ""
07:05:23.502 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITION_KEYS_N49 ON PARTITION_KEYS (TBL_ID)
07:05:23.503 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.503 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITION_KEYS_FK1" in catalog "" schema ""
07:05:23.503 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_KEYS ADD CONSTRAINT PARTITION_KEYS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID)
07:05:23.506 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.507 pool-1-thread-1 DEBUG Schema: Creating index "SERDE_PARAMS_N49" in catalog "" schema ""
07:05:23.507 pool-1-thread-1 DEBUG Schema: CREATE INDEX SERDE_PARAMS_N49 ON SERDE_PARAMS (SERDE_ID)
07:05:23.509 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.509 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SERDE_PARAMS_FK1" in catalog "" schema ""
07:05:23.509 pool-1-thread-1 DEBUG Schema: ALTER TABLE SERDE_PARAMS ADD CONSTRAINT SERDE_PARAMS_FK1 FOREIGN KEY (SERDE_ID) REFERENCES SERDES (SERDE_ID)
07:05:23.511 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.511 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_STRING_LIST_VALUES_N49" in catalog "" schema ""
07:05:23.511 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_STRING_LIST_VALUES_N49 ON SKEWED_STRING_LIST_VALUES (STRING_LIST_ID)
07:05:23.513 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.513 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_STRING_LIST_VALUES_FK1" in catalog "" schema ""
07:05:23.513 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_STRING_LIST_VALUES ADD CONSTRAINT SKEWED_STRING_LIST_VALUES_FK1 FOREIGN KEY (STRING_LIST_ID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID)
07:05:23.515 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.515 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_COL_VALUE_LOC_MAP_N50" in catalog "" schema ""
07:05:23.515 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_COL_VALUE_LOC_MAP_N50 ON SKEWED_COL_VALUE_LOC_MAP (STRING_LIST_ID_KID)
07:05:23.517 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.517 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_COL_VALUE_LOC_MAP_N49" in catalog "" schema ""
07:05:23.517 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_COL_VALUE_LOC_MAP_N49 ON SKEWED_COL_VALUE_LOC_MAP (SD_ID)
07:05:23.518 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.518 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_COL_VALUE_LOC_MAP_FK1" in catalog "" schema ""
07:05:23.518 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_VALUE_LOC_MAP ADD CONSTRAINT SKEWED_COL_VALUE_LOC_MAP_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:23.521 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.521 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_COL_VALUE_LOC_MAP_FK2" in catalog "" schema ""
07:05:23.521 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_VALUE_LOC_MAP ADD CONSTRAINT SKEWED_COL_VALUE_LOC_MAP_FK2 FOREIGN KEY (STRING_LIST_ID_KID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID)
07:05:23.524 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.524 pool-1-thread-1 DEBUG Schema: Creating index "TABLE_PARAMS_N49" in catalog "" schema ""
07:05:23.524 pool-1-thread-1 DEBUG Schema: CREATE INDEX TABLE_PARAMS_N49 ON TABLE_PARAMS (TBL_ID)
07:05:23.525 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.525 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "TABLE_PARAMS_FK1" in catalog "" schema ""
07:05:23.525 pool-1-thread-1 DEBUG Schema: ALTER TABLE TABLE_PARAMS ADD CONSTRAINT TABLE_PARAMS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID)
07:05:23.528 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.528 pool-1-thread-1 DEBUG Schema: Creating index "BUCKETING_COLS_N49" in catalog "" schema ""
07:05:23.528 pool-1-thread-1 DEBUG Schema: CREATE INDEX BUCKETING_COLS_N49 ON BUCKETING_COLS (SD_ID)
07:05:23.530 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.530 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "BUCKETING_COLS_FK1" in catalog "" schema ""
07:05:23.530 pool-1-thread-1 DEBUG Schema: ALTER TABLE BUCKETING_COLS ADD CONSTRAINT BUCKETING_COLS_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:23.533 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.533 pool-1-thread-1 DEBUG Schema: Creating index "SD_PARAMS_N49" in catalog "" schema ""
07:05:23.533 pool-1-thread-1 DEBUG Schema: CREATE INDEX SD_PARAMS_N49 ON SD_PARAMS (SD_ID)
07:05:23.534 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.534 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SD_PARAMS_FK1" in catalog "" schema ""
07:05:23.534 pool-1-thread-1 DEBUG Schema: ALTER TABLE SD_PARAMS ADD CONSTRAINT SD_PARAMS_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:23.536 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.536 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_VALUES_N50" in catalog "" schema ""
07:05:23.536 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_VALUES_N50 ON SKEWED_VALUES (SD_ID_OID)
07:05:23.538 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.538 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_VALUES_N49" in catalog "" schema ""
07:05:23.538 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_VALUES_N49 ON SKEWED_VALUES (STRING_LIST_ID_EID)
07:05:23.539 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.539 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_VALUES_FK1" in catalog "" schema ""
07:05:23.539 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_VALUES ADD CONSTRAINT SKEWED_VALUES_FK1 FOREIGN KEY (SD_ID_OID) REFERENCES SDS (SD_ID)
07:05:23.542 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.542 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_VALUES_FK2" in catalog "" schema ""
07:05:23.542 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_VALUES ADD CONSTRAINT SKEWED_VALUES_FK2 FOREIGN KEY (STRING_LIST_ID_EID) REFERENCES SKEWED_STRING_LIST (STRING_LIST_ID)
07:05:23.545 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.545 pool-1-thread-1 DEBUG Schema: Creating index "SKEWED_COL_NAMES_N49" in catalog "" schema ""
07:05:23.545 pool-1-thread-1 DEBUG Schema: CREATE INDEX SKEWED_COL_NAMES_N49 ON SKEWED_COL_NAMES (SD_ID)
07:05:23.546 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.546 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "SKEWED_COL_NAMES_FK1" in catalog "" schema ""
07:05:23.546 pool-1-thread-1 DEBUG Schema: ALTER TABLE SKEWED_COL_NAMES ADD CONSTRAINT SKEWED_COL_NAMES_FK1 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:23.549 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.549 pool-1-thread-1 DEBUG Schema: Creating index "COLUMNS_V2_N49" in catalog "" schema ""
07:05:23.549 pool-1-thread-1 DEBUG Schema: CREATE INDEX COLUMNS_V2_N49 ON COLUMNS_V2 (CD_ID)
07:05:23.550 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.551 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "COLUMNS_V2_FK1" in catalog "" schema ""
07:05:23.551 pool-1-thread-1 DEBUG Schema: ALTER TABLE COLUMNS_V2 ADD CONSTRAINT COLUMNS_V2_FK1 FOREIGN KEY (CD_ID) REFERENCES CDS (CD_ID)
07:05:23.552 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.552 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@6d653aa7"
07:05:23.553 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@6d653aa7"
07:05:23.553 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@6d653aa7" non enlisted to a transaction is being committed.
07:05:23.553 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@6d653aa7" closed
07:05:23.554 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 646 ms
07:05:23.554 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MTableColumnStatistics WHERE dbName == '' Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MTableColumnStatistics' AS NUCLEUS_TYPE,A0.AVG_COL_LEN,A0."COLUMN_NAME",A0.COLUMN_TYPE,A0.DB_NAME,A0.BIG_DECIMAL_HIGH_VALUE,A0.BIG_DECIMAL_LOW_VALUE,A0.DOUBLE_HIGH_VALUE,A0.DOUBLE_LOW_VALUE,A0.LAST_ANALYZED,A0.LONG_HIGH_VALUE,A0.LONG_LOW_VALUE,A0.MAX_COL_LEN,A0.NUM_DISTINCTS,A0.NUM_FALSES,A0.NUM_NULLS,A0.NUM_TRUES,A0."TABLE_NAME",A0.CS_ID FROM TAB_COL_STATS A0 WHERE A0.DB_NAME = ''"
07:05:23.554 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7478e33a [conn=com.jolbox.bonecp.ConnectionHandle@34e64b79, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.554 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MTableColumnStatistics WHERE dbName == ''" ...
07:05:23.557 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.GenericQualifier - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/GenericQualifier.class
07:05:23.558 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@115c4461"
07:05:23.558 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MTableColumnStatistics' AS NUCLEUS_TYPE,A0.AVG_COL_LEN,A0."COLUMN_NAME",A0.COLUMN_TYPE,A0.DB_NAME,A0.BIG_DECIMAL_HIGH_VALUE,A0.BIG_DECIMAL_LOW_VALUE,A0.DOUBLE_HIGH_VALUE,A0.DOUBLE_LOW_VALUE,A0.LAST_ANALYZED,A0.LONG_HIGH_VALUE,A0.LONG_LOW_VALUE,A0.MAX_COL_LEN,A0.NUM_DISTINCTS,A0.NUM_FALSES,A0.NUM_NULLS,A0.NUM_TRUES,A0."TABLE_NAME",A0.CS_ID FROM TAB_COL_STATS A0 WHERE A0.DB_NAME = ''
07:05:23.559 pool-1-thread-1 DEBUG Retrieve: Execution Time = 1 ms
07:05:23.559 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 5 ms
07:05:23.559 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics WHERE dbName == ''"
07:05:23.559 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:23.559 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{dbName}  =  Literal{}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics]
07:05:23.559 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics WHERE dbName == ''" for datastore
07:05:23.559 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
07:05:23.559 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
07:05:23.559 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MPartition [Table : PARTITIONS, InheritanceStrategy : new-table]
07:05:23.559 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.PART_ID" added to internal representation of table.
07:05:23.559 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics [Table : PART_COL_STATS, InheritanceStrategy : new-table]
07:05:23.559 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.CS_ID" added to internal representation of table.
07:05:23.559 pool-1-thread-1 DEBUG Schema: Table PARTITIONS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MPartition (inheritance strategy="new-table")
07:05:23.559 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.CREATE_TIME" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.createTime] -> Column(s) [PARTITIONS.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.LAST_ACCESS_TIME" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.lastAccessTime] -> Column(s) [PARTITIONS.LAST_ACCESS_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.parameters [Table : PARTITION_PARAMS]
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.parameters] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.MapMapping" ()
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.PART_NAME" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.partitionName] -> Column(s) [PARTITIONS.PART_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.SD_ID" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.sd] -> Column(s) [PARTITIONS.SD_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PARTITIONS.TBL_ID" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.table] -> Column(s) [PARTITIONS.TBL_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MPartition.values [Table : PARTITION_KEY_VALS]
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartition.values] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:23.560 pool-1-thread-1 DEBUG Schema: Table/View PARTITIONS has been initialised
07:05:23.560 pool-1-thread-1 DEBUG Schema: Table PART_COL_STATS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics (inheritance strategy="new-table")
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.AVG_COL_LEN" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.avgColLen] -> Column(s) [PART_COL_STATS.AVG_COL_LEN] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS."COLUMN_NAME"" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.colName] -> Column(s) [PART_COL_STATS."COLUMN_NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.COLUMN_TYPE" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.colType] -> Column(s) [PART_COL_STATS.COLUMN_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.DB_NAME" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.dbName] -> Column(s) [PART_COL_STATS.DB_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.BIG_DECIMAL_HIGH_VALUE" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.decimalHighValue] -> Column(s) [PART_COL_STATS.BIG_DECIMAL_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.BIG_DECIMAL_LOW_VALUE" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.decimalLowValue] -> Column(s) [PART_COL_STATS.BIG_DECIMAL_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.DOUBLE_HIGH_VALUE" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.doubleHighValue] -> Column(s) [PART_COL_STATS.DOUBLE_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.DOUBLE_LOW_VALUE" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.doubleLowValue] -> Column(s) [PART_COL_STATS.DOUBLE_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.DoubleMapping" (org.datanucleus.store.rdbms.mapping.datastore.DoubleRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.LAST_ANALYZED" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.lastAnalyzed] -> Column(s) [PART_COL_STATS.LAST_ANALYZED] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.560 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.LONG_HIGH_VALUE" added to internal representation of table.
07:05:23.560 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.longHighValue] -> Column(s) [PART_COL_STATS.LONG_HIGH_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.LONG_LOW_VALUE" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.longLowValue] -> Column(s) [PART_COL_STATS.LONG_LOW_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.MAX_COL_LEN" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.maxColLen] -> Column(s) [PART_COL_STATS.MAX_COL_LEN] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.NUM_DISTINCTS" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.numDVs] -> Column(s) [PART_COL_STATS.NUM_DISTINCTS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.NUM_FALSES" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.numFalses] -> Column(s) [PART_COL_STATS.NUM_FALSES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.NUM_NULLS" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.numNulls] -> Column(s) [PART_COL_STATS.NUM_NULLS] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.NUM_TRUES" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.numTrues] -> Column(s) [PART_COL_STATS.NUM_TRUES] using mapping of type "org.datanucleus.store.rdbms.mapping.java.LongMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.PART_ID" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.partition] -> Column(s) [PART_COL_STATS.PART_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS.PARTITION_NAME" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.partitionName] -> Column(s) [PART_COL_STATS.PARTITION_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PART_COL_STATS."TABLE_NAME"" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.tableName] -> Column(s) [PART_COL_STATS."TABLE_NAME"] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Table/View PART_COL_STATS has been initialised
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PARTITION_PARAMS.PART_ID" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.parameters] -> Column(s) [PARTITION_PARAMS.PART_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PARTITION_PARAMS.PARAM_KEY" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.parameters] -> Column(s) [PARTITION_PARAMS.PARAM_KEY] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PARTITION_PARAMS.PARAM_VALUE" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.parameters] -> Column(s) [PARTITION_PARAMS.PARAM_VALUE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Table/View PARTITION_PARAMS has been initialised
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEY_VALS.PART_ID" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.values] -> Column(s) [PARTITION_KEY_VALS.PART_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEY_VALS.PART_KEY_VAL" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.values] -> Column(s) [PARTITION_KEY_VALS.PART_KEY_VAL] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Column "PARTITION_KEY_VALS.INTEGER_IDX" added to internal representation of table.
07:05:23.561 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MPartition.values] -> Column(s) [PARTITION_KEY_VALS.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:23.561 pool-1-thread-1 DEBUG Schema: Table/View PARTITION_KEY_VALS has been initialised
07:05:23.562 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@23b632c7" opened with isolation level "serializable" and auto-commit=false
07:05:23.562 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@23b632c7" with isolation "serializable"
07:05:23.562 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.562 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.563 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.563 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.563 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.564 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.564 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.564 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.565 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.565 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.565 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.565 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.566 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.566 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.567 pool-1-thread-1 DEBUG Schema: Check of existence of PARTITIONS returned no table
07:05:23.567 pool-1-thread-1 DEBUG Schema: Creating table PARTITIONS
07:05:23.567 pool-1-thread-1 DEBUG Schema: CREATE TABLE PARTITIONS
(
    PART_ID BIGINT NOT NULL,
    CREATE_TIME INTEGER NOT NULL,
    LAST_ACCESS_TIME INTEGER NOT NULL,
    PART_NAME VARCHAR(767),
    SD_ID BIGINT,
    TBL_ID BIGINT
)
07:05:23.571 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:23.571 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITIONS ADD CONSTRAINT PARTITIONS_PK PRIMARY KEY (PART_ID)
07:05:23.572 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.573 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.573 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.574 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.574 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.574 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.575 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.575 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.575 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.575 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.576 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.576 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.576 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.576 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.577 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.577 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.578 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.578 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.578 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.579 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.579 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.579 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.579 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.580 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.580 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.580 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.580 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.581 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.581 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.583 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.584 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.584 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.584 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.585 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.585 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.585 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.586 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.586 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.586 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.586 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.587 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.587 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.587 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.588 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.588 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.588 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.589 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.590 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.590 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.590 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.590 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.591 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.591 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.591 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.592 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.592 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.592 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.594 pool-1-thread-1 DEBUG Schema: Check of existence of PART_COL_STATS returned no table
07:05:23.595 pool-1-thread-1 DEBUG Schema: Creating table PART_COL_STATS
07:05:23.595 pool-1-thread-1 DEBUG Schema: CREATE TABLE PART_COL_STATS
(
    CS_ID BIGINT NOT NULL,
    AVG_COL_LEN DOUBLE,
    "COLUMN_NAME" VARCHAR(128) NOT NULL,
    COLUMN_TYPE VARCHAR(128) NOT NULL,
    DB_NAME VARCHAR(128) NOT NULL,
    BIG_DECIMAL_HIGH_VALUE VARCHAR(255),
    BIG_DECIMAL_LOW_VALUE VARCHAR(255),
    DOUBLE_HIGH_VALUE DOUBLE,
    DOUBLE_LOW_VALUE DOUBLE,
    LAST_ANALYZED BIGINT NOT NULL,
    LONG_HIGH_VALUE BIGINT,
    LONG_LOW_VALUE BIGINT,
    MAX_COL_LEN BIGINT,
    NUM_DISTINCTS BIGINT,
    NUM_FALSES BIGINT,
    NUM_NULLS BIGINT NOT NULL,
    NUM_TRUES BIGINT,
    PART_ID BIGINT,
    PARTITION_NAME VARCHAR(767) NOT NULL,
    "TABLE_NAME" VARCHAR(128) NOT NULL
)
07:05:23.604 pool-1-thread-1 DEBUG Schema: Execution Time = 9 ms
07:05:23.604 pool-1-thread-1 DEBUG Schema: ALTER TABLE PART_COL_STATS ADD CONSTRAINT PART_COL_STATS_PK PRIMARY KEY (CS_ID)
07:05:23.607 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.607 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.608 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.608 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.608 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.608 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.609 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.609 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.609 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.609 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.610 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.611 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.612 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.613 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.613 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.613 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.614 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.614 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.614 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.614 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.615 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.615 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.615 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.618 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.618 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.618 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.619 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.619 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.619 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.620 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.620 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.620 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.620 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.621 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.621 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.621 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.621 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.622 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.622 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.623 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.623 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.623 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.623 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.624 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.624 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.624 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.624 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.625 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.625 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.625 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.625 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.628 pool-1-thread-1 DEBUG Schema: Check of existence of PARTITION_PARAMS returned no table
07:05:23.628 pool-1-thread-1 DEBUG Schema: Creating table PARTITION_PARAMS
07:05:23.628 pool-1-thread-1 DEBUG Schema: CREATE TABLE PARTITION_PARAMS
(
    PART_ID BIGINT NOT NULL,
    PARAM_KEY VARCHAR(256) NOT NULL,
    PARAM_VALUE VARCHAR(4000)
)
07:05:23.629 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.629 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_PARAMS ADD CONSTRAINT PARTITION_PARAMS_PK PRIMARY KEY (PART_ID,PARAM_KEY)
07:05:23.630 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.631 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.631 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.631 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.631 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.632 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.632 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.632 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.633 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.633 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.633 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.633 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.634 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.634 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.634 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.635 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.635 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.635 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.635 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.636 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.637 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.637 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.637 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.637 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.638 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.638 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.640 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.641 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.641 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.641 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.642 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.642 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.642 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.642 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.643 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.643 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.643 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.643 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.644 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.644 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.644 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.645 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.645 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.645 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.646 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.646 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.646 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.646 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.647 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.647 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.647 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.647 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.648 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.648 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.650 pool-1-thread-1 DEBUG Schema: Check of existence of PARTITION_KEY_VALS returned no table
07:05:23.650 pool-1-thread-1 DEBUG Schema: Creating table PARTITION_KEY_VALS
07:05:23.650 pool-1-thread-1 DEBUG Schema: CREATE TABLE PARTITION_KEY_VALS
(
    PART_ID BIGINT NOT NULL,
    PART_KEY_VAL VARCHAR(255),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:23.651 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.651 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_KEY_VALS ADD CONSTRAINT PARTITION_KEY_VALS_PK PRIMARY KEY (PART_ID,INTEGER_IDX)
07:05:23.653 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.653 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.653 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.653 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.654 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.654 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.654 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.655 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.655 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.655 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.655 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.656 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.656 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.656 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.656 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.657 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.657 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.658 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.658 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.658 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.658 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.659 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.659 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.659 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.659 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.660 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.660 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.660 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.660 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.663 pool-1-thread-1 DEBUG Schema: Creating index "UniquePartition" in catalog "" schema ""
07:05:23.663 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX UNIQUEPARTITION ON PARTITIONS (PART_NAME,TBL_ID)
07:05:23.664 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.664 pool-1-thread-1 DEBUG Schema: Creating index "PARTITIONS_N50" in catalog "" schema ""
07:05:23.664 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITIONS_N50 ON PARTITIONS (SD_ID)
07:05:23.666 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.666 pool-1-thread-1 DEBUG Schema: Creating index "PARTITIONS_N49" in catalog "" schema ""
07:05:23.666 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITIONS_N49 ON PARTITIONS (TBL_ID)
07:05:23.668 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.668 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITIONS_FK1" in catalog "" schema ""
07:05:23.668 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITIONS ADD CONSTRAINT PARTITIONS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID)
07:05:23.671 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.671 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITIONS_FK2" in catalog "" schema ""
07:05:23.671 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITIONS ADD CONSTRAINT PARTITIONS_FK2 FOREIGN KEY (SD_ID) REFERENCES SDS (SD_ID)
07:05:23.675 pool-1-thread-1 DEBUG Schema: Execution Time = 4 ms
07:05:23.675 pool-1-thread-1 DEBUG Schema: Creating index "PART_COL_STATS_N49" in catalog "" schema ""
07:05:23.675 pool-1-thread-1 DEBUG Schema: CREATE INDEX PART_COL_STATS_N49 ON PART_COL_STATS (PART_ID)
07:05:23.677 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.677 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PART_COL_STATS_FK1" in catalog "" schema ""
07:05:23.677 pool-1-thread-1 DEBUG Schema: ALTER TABLE PART_COL_STATS ADD CONSTRAINT PART_COL_STATS_FK1 FOREIGN KEY (PART_ID) REFERENCES PARTITIONS (PART_ID)
07:05:23.680 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.680 pool-1-thread-1 DEBUG Schema: Creating index "PARTITION_PARAMS_N49" in catalog "" schema ""
07:05:23.680 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITION_PARAMS_N49 ON PARTITION_PARAMS (PART_ID)
07:05:23.682 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.682 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITION_PARAMS_FK1" in catalog "" schema ""
07:05:23.682 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_PARAMS ADD CONSTRAINT PARTITION_PARAMS_FK1 FOREIGN KEY (PART_ID) REFERENCES PARTITIONS (PART_ID)
07:05:23.684 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.684 pool-1-thread-1 DEBUG Schema: Creating index "PARTITION_KEY_VALS_N49" in catalog "" schema ""
07:05:23.684 pool-1-thread-1 DEBUG Schema: CREATE INDEX PARTITION_KEY_VALS_N49 ON PARTITION_KEY_VALS (PART_ID)
07:05:23.686 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.686 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "PARTITION_KEY_VALS_FK1" in catalog "" schema ""
07:05:23.686 pool-1-thread-1 DEBUG Schema: ALTER TABLE PARTITION_KEY_VALS ADD CONSTRAINT PARTITION_KEY_VALS_FK1 FOREIGN KEY (PART_ID) REFERENCES PARTITIONS (PART_ID)
07:05:23.689 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.689 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@23b632c7"
07:05:23.689 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@23b632c7"
07:05:23.689 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@23b632c7" non enlisted to a transaction is being committed.
07:05:23.689 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@23b632c7" closed
07:05:23.690 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 131 ms
07:05:23.690 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics WHERE dbName == '' Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics' AS NUCLEUS_TYPE,A0.AVG_COL_LEN,A0."COLUMN_NAME",A0.COLUMN_TYPE,A0.DB_NAME,A0.BIG_DECIMAL_HIGH_VALUE,A0.BIG_DECIMAL_LOW_VALUE,A0.DOUBLE_HIGH_VALUE,A0.DOUBLE_LOW_VALUE,A0.LAST_ANALYZED,A0.LONG_HIGH_VALUE,A0.LONG_LOW_VALUE,A0.MAX_COL_LEN,A0.NUM_DISTINCTS,A0.NUM_FALSES,A0.NUM_NULLS,A0.NUM_TRUES,A0.PARTITION_NAME,A0."TABLE_NAME",A0.CS_ID FROM PART_COL_STATS A0 WHERE A0.DB_NAME = ''"
07:05:23.690 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7478e33a [conn=com.jolbox.bonecp.ConnectionHandle@34e64b79, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.690 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics WHERE dbName == ''" ...
07:05:23.701 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@782e86ef"
07:05:23.701 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics' AS NUCLEUS_TYPE,A0.AVG_COL_LEN,A0."COLUMN_NAME",A0.COLUMN_TYPE,A0.DB_NAME,A0.BIG_DECIMAL_HIGH_VALUE,A0.BIG_DECIMAL_LOW_VALUE,A0.DOUBLE_HIGH_VALUE,A0.DOUBLE_LOW_VALUE,A0.LAST_ANALYZED,A0.LONG_HIGH_VALUE,A0.LONG_LOW_VALUE,A0.MAX_COL_LEN,A0.NUM_DISTINCTS,A0.NUM_FALSES,A0.NUM_NULLS,A0.NUM_TRUES,A0.PARTITION_NAME,A0."TABLE_NAME",A0.CS_ID FROM PART_COL_STATS A0 WHERE A0.DB_NAME = ''
07:05:23.701 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.701 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 11 ms
07:05:23.702 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.SQLQuery - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/SQLQuery.class
07:05:23.704 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.AbstractSQLQuery - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/AbstractSQLQuery.class
07:05:23.705 pool-1-thread-1 DEBUG Query: SQL Query : "select "DB_ID" from "DBS""
07:05:23.705 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7478e33a [conn=com.jolbox.bonecp.ConnectionHandle@34e64b79, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.708 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1efdcd3a"
07:05:23.708 pool-1-thread-1 DEBUG Native: select "DB_ID" from "DBS"
07:05:23.708 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.708 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF.class
07:05:23.709 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$2 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$2.class
07:05:23.710 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$ResultSetGetter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$ResultSetGetter.class
07:05:23.710 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$3 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$3.class
07:05:23.710 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$4 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$4.class
07:05:23.710 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$5 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$5.class
07:05:23.711 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$6 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$6.class
07:05:23.711 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$7 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$7.class
07:05:23.711 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$8 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$8.class
07:05:23.712 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$9 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$9.class
07:05:23.712 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$10 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$10.class
07:05:23.712 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$11 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$11.class
07:05:23.712 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$12 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$12.class
07:05:23.713 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$13 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$13.class
07:05:23.713 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$14 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$14.class
07:05:23.713 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.Array - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/Array.class
07:05:23.713 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$15 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$15.class
07:05:23.714 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.SQLQuery$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/SQLQuery$1.class
07:05:23.714 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:23.714 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:23.714 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.714 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:23.714 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:23.714 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a84058f]]
07:05:23.714 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a84058f is committing for transaction Xid=    with onePhase=true
07:05:23.714 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2a84058f committed connection for transaction Xid=    with onePhase=true
07:05:23.714 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@34e64b79" closed
07:05:23.714 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@7478e33a [conn=com.jolbox.bonecp.ConnectionHandle@34e64b79, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.714 pool-1-thread-1 DEBUG Transaction: Transaction committed in 0 ms
07:05:23.714 pool-1-thread-1 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
07:05:23.714 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.AggregateStatsCache - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/AggregateStatsCache.class
07:05:23.715 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.AggregateStatsCache$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/AggregateStatsCache$1.class
07:05:23.716 pool-1-thread-1 DEBUG ObjectStore: RawStore: org.apache.hadoop.hive.metastore.ObjectStore@73a76627, with PersistenceManager: org.datanucleus.api.jdo.JDOPersistenceManager@3f792c39 created in the thread with id: 11
07:05:23.716 pool-1-thread-1 INFO ObjectStore: Initialized ObjectStore
07:05:23.716 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.lang.ClassUtils - jar:file:/Users/lian/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.6.jar!/org/apache/commons/lang/ClassUtils.class
07:05:23.717 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.lang.NullArgumentException - jar:file:/Users/lian/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.6.jar!/org/apache/commons/lang/NullArgumentException.class
07:05:23.718 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.Void
07:05:23.718 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Type - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Type.class
07:05:23.719 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Table - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Table.class
07:05:23.719 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Index - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Index.class
07:05:23.720 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Database - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Database.class
07:05:23.721 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Function - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Function.class
07:05:23.722 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Partition - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Partition.class
07:05:23.722 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionEventType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionEventType.class
07:05:23.723 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEventRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEventRequest.class
07:05:23.723 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEventResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEventResponse.class
07:05:23.724 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CurrentNotificationEventId.class
07:05:23.724 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AggrStats - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AggrStats.class
07:05:23.724 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrivilegeBag - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrivilegeBag.class
07:05:23.725 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Role - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Role.class
07:05:23.726 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrincipalType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrincipalType.class
07:05:23.726 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ColumnStatistics - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ColumnStatistics.class
07:05:23.726 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.class
07:05:23.727 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrincipalPrivilegeSet.class
07:05:23.727 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEvent.class
07:05:23.736 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.meta_data.FieldValueMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/meta_data/FieldValueMetaData.class
07:05:23.736 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.meta_data.MapMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/meta_data/MapMetaData.class
07:05:23.737 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.meta_data.StructMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/meta_data/StructMetaData.class
07:05:23.737 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.meta_data.EnumMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/meta_data/EnumMetaData.class
07:05:23.738 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.TFieldIdEnum - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/TFieldIdEnum.class
07:05:23.738 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.protocol.TStruct - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/protocol/TStruct.class
07:05:23.739 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.protocol.TField - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/protocol/TField.class
07:05:23.739 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.scheme.StandardScheme - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/scheme/StandardScheme.class
07:05:23.739 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.scheme.IScheme - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/scheme/IScheme.class
07:05:23.740 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Database$DatabaseStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Database$DatabaseStandardSchemeFactory.class
07:05:23.740 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.scheme.SchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/scheme/SchemeFactory.class
07:05:23.740 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.scheme.TupleScheme - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/scheme/TupleScheme.class
07:05:23.740 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Database$DatabaseTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Database$DatabaseTupleSchemeFactory.class
07:05:23.741 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.EnumMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/EnumMap.class
07:05:23.741 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Database$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Database$_Fields.class
07:05:23.741 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.EnumSet - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/EnumSet.class
07:05:23.741 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.meta_data.FieldMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/meta_data/FieldMetaData.class
07:05:23.742 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.meta_data.ListMetaData - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/meta_data/ListMetaData.class
07:05:23.743 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Function$FunctionStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Function$FunctionStandardSchemeFactory.class
07:05:23.743 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Function$FunctionTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Function$FunctionTupleSchemeFactory.class
07:05:23.743 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Function$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Function$_Fields.class
07:05:23.744 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FunctionType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FunctionType.class
07:05:23.744 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ResourceUri - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ResourceUri.class
07:05:23.745 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrivilegeBag$PrivilegeBagStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrivilegeBag$PrivilegeBagStandardSchemeFactory.class
07:05:23.745 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrivilegeBag$PrivilegeBagTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrivilegeBag$PrivilegeBagTupleSchemeFactory.class
07:05:23.746 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrivilegeBag$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrivilegeBag$_Fields.class
07:05:23.746 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectPrivilege.class
07:05:23.748 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Type$TypeStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Type$TypeStandardSchemeFactory.class
07:05:23.748 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Type$TypeTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Type$TypeTupleSchemeFactory.class
07:05:23.748 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Type$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Type$_Fields.class
07:05:23.749 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FieldSchema - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FieldSchema.class
07:05:23.750 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Partition$PartitionStandardSchemeFactory.class
07:05:23.750 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Partition$PartitionTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Partition$PartitionTupleSchemeFactory.class
07:05:23.751 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Partition$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Partition$_Fields.class
07:05:23.751 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.StorageDescriptor - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/StorageDescriptor.class
07:05:23.752 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.protocol.TProtocolException - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/protocol/TProtocolException.class
07:05:23.753 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ColumnStatistics$ColumnStatisticsStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ColumnStatistics$ColumnStatisticsStandardSchemeFactory.class
07:05:23.753 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ColumnStatistics$ColumnStatisticsTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ColumnStatistics$ColumnStatisticsTupleSchemeFactory.class
07:05:23.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ColumnStatistics$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ColumnStatistics$_Fields.class
07:05:23.754 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ColumnStatisticsDesc.class
07:05:23.755 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ColumnStatisticsObj.class
07:05:23.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEventRequest$NotificationEventRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEventRequest$NotificationEventRequestStandardSchemeFactory.class
07:05:23.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEventRequest$NotificationEventRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEventRequest$NotificationEventRequestTupleSchemeFactory.class
07:05:23.756 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEventRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEventRequest$_Fields.class
07:05:23.757 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Role$RoleStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Role$RoleStandardSchemeFactory.class
07:05:23.758 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Role$RoleTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Role$RoleTupleSchemeFactory.class
07:05:23.758 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Role$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Role$_Fields.class
07:05:23.759 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Index$IndexStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Index$IndexStandardSchemeFactory.class
07:05:23.760 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Index$IndexTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Index$IndexTupleSchemeFactory.class
07:05:23.760 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Index$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Index$_Fields.class
07:05:23.761 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEvent$NotificationEventStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEvent$NotificationEventStandardSchemeFactory.class
07:05:23.762 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEvent$NotificationEventTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEvent$NotificationEventTupleSchemeFactory.class
07:05:23.762 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NotificationEvent$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NotificationEvent$_Fields.class
07:05:23.764 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Table$TableStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Table$TableStandardSchemeFactory.class
07:05:23.764 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Table$TableTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Table$TableTupleSchemeFactory.class
07:05:23.764 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Table$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Table$_Fields.class
07:05:23.765 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.Deadline - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/Deadline.class
07:05:23.766 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.Deadline$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/Deadline$1.class
07:05:23.766 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.DeadlineException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/DeadlineException.class
07:05:23.767 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.MetaException$MetaExceptionStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/MetaException$MetaExceptionStandardSchemeFactory.class
07:05:23.767 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.MetaException$MetaExceptionTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/MetaException$MetaExceptionTupleSchemeFactory.class
07:05:23.767 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.MetaException$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/MetaException$_Fields.class
07:05:23.768 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.768 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:23.768 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.StackTraceElement
07:05:23.768 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6717)
07:05:23.768 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable"
07:05:23.768 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:23.768 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MVersionTable]
07:05:23.768 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable" for datastore
07:05:23.769 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MVersionTable [Table : VERSION, InheritanceStrategy : new-table]
07:05:23.769 pool-1-thread-1 DEBUG Schema: Column "VERSION.VER_ID" added to internal representation of table.
07:05:23.769 pool-1-thread-1 DEBUG Schema: Table VERSION will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MVersionTable (inheritance strategy="new-table")
07:05:23.769 pool-1-thread-1 DEBUG Schema: Column "VERSION.SCHEMA_VERSION" added to internal representation of table.
07:05:23.769 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MVersionTable.schemaVersion] -> Column(s) [VERSION.SCHEMA_VERSION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.769 pool-1-thread-1 DEBUG Schema: Column "VERSION.VERSION_COMMENT" added to internal representation of table.
07:05:23.769 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MVersionTable.versionComment] -> Column(s) [VERSION.VERSION_COMMENT] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.769 pool-1-thread-1 DEBUG Schema: Table/View VERSION has been initialised
07:05:23.769 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@122b1f21" opened with isolation level "serializable" and auto-commit=false
07:05:23.769 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@122b1f21" with isolation "serializable"
07:05:23.770 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.770 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.770 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.770 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.771 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.771 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.771 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.772 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.772 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.772 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.772 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.773 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.773 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.773 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.774 pool-1-thread-1 DEBUG Schema: Check of existence of VERSION returned no table
07:05:23.774 pool-1-thread-1 DEBUG Schema: Creating table VERSION
07:05:23.774 pool-1-thread-1 DEBUG Schema: CREATE TABLE VERSION
(
    VER_ID BIGINT NOT NULL,
    SCHEMA_VERSION VARCHAR(127) NOT NULL,
    VERSION_COMMENT VARCHAR(255) NOT NULL
)
07:05:23.776 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.776 pool-1-thread-1 DEBUG Schema: ALTER TABLE VERSION ADD CONSTRAINT VERSION_PK PRIMARY KEY (VER_ID)
07:05:23.778 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.778 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.779 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.779 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.779 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.780 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.780 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.780 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.781 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.781 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.781 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.781 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.782 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.782 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.782 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.783 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.783 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.784 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.784 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.785 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.785 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.785 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.786 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.786 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.787 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.787 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.788 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.788 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.789 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.792 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@122b1f21"
07:05:23.792 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@122b1f21"
07:05:23.792 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@122b1f21" non enlisted to a transaction is being committed.
07:05:23.792 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@122b1f21" closed
07:05:23.793 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 25 ms
07:05:23.793 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MVersionTable' AS NUCLEUS_TYPE,A0.SCHEMA_VERSION,A0.VERSION_COMMENT,A0.VER_ID FROM VERSION A0"
07:05:23.793 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@2c4685ba" opened with isolation level "read-committed" and auto-commit=false
07:05:23.793 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4242d16d, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.793 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4242d16d is starting for transaction Xid=    with flags 0
07:05:23.793 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@eda0aa0 [conn=com.jolbox.bonecp.ConnectionHandle@2c4685ba, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.793 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable" ...
07:05:23.795 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@3c3aeb62"
07:05:23.795 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MVersionTable' AS NUCLEUS_TYPE,A0.SCHEMA_VERSION,A0.VERSION_COMMENT,A0.VER_ID FROM VERSION A0
07:05:23.795 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.795 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 2 ms
07:05:23.795 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ForwardQueryResult$QueryResultIterator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ForwardQueryResult$QueryResultIterator.class
07:05:23.796 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.query.AbstractQueryResultIterator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/query/AbstractQueryResultIterator.class
07:05:23.797 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6731)
07:05:23.797 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:23.797 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:23.797 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.797 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:23.797 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:23.797 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4242d16d]]
07:05:23.797 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4242d16d is committing for transaction Xid=    with onePhase=true
07:05:23.797 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4242d16d committed connection for transaction Xid=    with onePhase=true
07:05:23.797 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@2c4685ba" closed
07:05:23.797 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@eda0aa0 [conn=com.jolbox.bonecp.ConnectionHandle@2c4685ba, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.798 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:23.798 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NoSuchObjectException$NoSuchObjectExceptionStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NoSuchObjectException$NoSuchObjectExceptionStandardSchemeFactory.class
07:05:23.799 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NoSuchObjectException$NoSuchObjectExceptionTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NoSuchObjectException$NoSuchObjectExceptionTupleSchemeFactory.class
07:05:23.799 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NoSuchObjectException$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NoSuchObjectException$_Fields.class
07:05:23.800 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfo.class
07:05:23.800 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.io.FileReader - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/io/FileReader.class
07:05:23.801 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.collect.ImmutableMap
07:05:23.801 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hive.common.util.HiveVersionInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/common/util/HiveVersionInfo.class
07:05:23.801 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hive.common.HiveVersionAnnotation - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/common/HiveVersionAnnotation.class
07:05:23.801 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.annotation.Annotation
07:05:23.802 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hive.common.package-info - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/common/package-info.class
07:05:23.802 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.annotation.Retention
07:05:23.802 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.annotation.RetentionPolicy
07:05:23.802 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: java.lang.annotation.Target
07:05:23.803 pool-1-thread-1 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
07:05:23.803 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.803 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:23.803 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6717)
07:05:23.803 pool-1-thread-1 DEBUG Query: Query "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTableFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:23.803 pool-1-thread-1 DEBUG Query: Query "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTableFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:23.803 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@368ba71f" opened with isolation level "read-committed" and auto-commit=false
07:05:23.803 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@17380601, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.803 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@17380601 is starting for transaction Xid=    with flags 0
07:05:23.803 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@d12a035 [conn=com.jolbox.bonecp.ConnectionHandle@368ba71f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.803 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MVersionTable" ...
07:05:23.803 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@66ca4dd7"
07:05:23.803 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MVersionTable' AS NUCLEUS_TYPE,A0.SCHEMA_VERSION,A0.VERSION_COMMENT,A0.VER_ID FROM VERSION A0
07:05:23.804 pool-1-thread-1 DEBUG Retrieve: Execution Time = 1 ms
07:05:23.804 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 1 ms
07:05:23.804 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6731)
07:05:23.804 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:23.804 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:23.804 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.804 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:23.804 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:23.804 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@17380601]]
07:05:23.804 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@17380601 is committing for transaction Xid=    with onePhase=true
07:05:23.804 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@17380601 committed connection for transaction Xid=    with onePhase=true
07:05:23.804 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@368ba71f" closed
07:05:23.804 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@d12a035 [conn=com.jolbox.bonecp.ConnectionHandle@368ba71f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.804 pool-1-thread-1 DEBUG Transaction: Transaction committed in 0 ms
07:05:23.804 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.spi.RegisterClassEvent - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/spi/RegisterClassEvent.class
07:05:23.804 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.EventObject - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/EventObject.class
07:05:23.805 pool-1-thread-1 DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MVersionTable
07:05:23.805 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.805 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:23.805 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6772)
07:05:23.805 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContextImpl$ThreadContextInfo - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContextImpl$ThreadContextInfo.class
07:05:23.805 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337"
07:05:23.807 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.AbstractFetchDepthFieldManager$EndOfFetchPlanGraphException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/AbstractFetchDepthFieldManager$EndOfFetchPlanGraphException.class
07:05:23.810 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.SingleTypeFieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/SingleTypeFieldManager.class
07:05:23.810 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.JDOStateManager$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/JDOStateManager$1.class
07:05:23.811 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.ActivityState - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/ActivityState.class
07:05:23.811 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.LifeCycleStateFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/LifeCycleStateFactory.class
07:05:23.811 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.LifeCycleState - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/LifeCycleState.class
07:05:23.812 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.Hollow - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/Hollow.class
07:05:23.812 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.IllegalStateTransitionException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/IllegalStateTransitionException.class
07:05:23.812 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.PersistentClean - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/PersistentClean.class
07:05:23.813 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.PersistentDirty - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/PersistentDirty.class
07:05:23.813 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.PersistentNew - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/PersistentNew.class
07:05:23.813 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.PersistentNewDeleted - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/PersistentNewDeleted.class
07:05:23.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.PersistentDeleted - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/PersistentDeleted.class
07:05:23.814 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.PersistentNontransactional - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/PersistentNontransactional.class
07:05:23.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.TransientClean - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/TransientClean.class
07:05:23.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.TransientDirty - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/TransientDirty.class
07:05:23.815 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.PersistentNontransactionalDirty - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/PersistentNontransactionalDirty.class
07:05:23.816 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.DetachedClean - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/DetachedClean.class
07:05:23.816 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.state.DetachedDirty - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/state/DetachedDirty.class
07:05:23.816 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.JDOStateManager$2 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/JDOStateManager$2.class
07:05:23.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.valuegenerator.ValueGenerationManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/valuegenerator/ValueGenerationManager.class
07:05:23.817 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.valuegenerator.ValueGenerationException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/valuegenerator/ValueGenerationException.class
07:05:23.818 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.valuegenerator.TableGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/valuegenerator/TableGenerator.class
07:05:23.818 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.valuegenerator.AbstractRDBMSGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/valuegenerator/AbstractRDBMSGenerator.class
07:05:23.819 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.valuegenerator.AbstractDatastoreGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/valuegenerator/AbstractDatastoreGenerator.class
07:05:23.819 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.valuegenerator.AbstractGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/valuegenerator/AbstractGenerator.class
07:05:23.819 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.valuegenerator.ValueGenerator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/valuegenerator/ValueGenerator.class
07:05:23.819 pool-1-thread-1 DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MVersionTable"
07:05:23.820 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.RDBMSStoreManager$2 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/RDBMSStoreManager$2.class
07:05:23.821 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.valuegenerator.SequenceTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/valuegenerator/SequenceTable.class
07:05:23.822 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:23.822 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:23.822 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@56ce31f4" opened with isolation level "read-committed" and auto-commit=false
07:05:23.822 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.823 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.824 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.824 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.824 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.824 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.825 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.825 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.825 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.826 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.826 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.827 pool-1-thread-1 DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned no table
07:05:23.827 pool-1-thread-1 DEBUG Schema: Creating table APP.SEQUENCE_TABLE
07:05:23.827 pool-1-thread-1 DEBUG Schema: CREATE TABLE APP.SEQUENCE_TABLE
(
    SEQUENCE_NAME VARCHAR(255) NOT NULL,
    NEXT_VAL BIGINT NOT NULL
)
07:05:23.828 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.828 pool-1-thread-1 DEBUG Schema: ALTER TABLE APP.SEQUENCE_TABLE ADD CONSTRAINT SEQUENCE_TABLE_PK PRIMARY KEY (SEQUENCE_NAME)
07:05:23.831 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:23.831 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.831 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.832 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.833 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.833 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.833 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.834 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.835 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.835 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.836 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.837 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.837 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.838 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.838 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.839 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.840 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.841 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.847 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.CursorTableReference - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/CursorTableReference.class
07:05:23.848 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1b808861"
07:05:23.848 pool-1-thread-1 DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MVersionTable'> FOR UPDATE
07:05:23.848 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.849 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.MaxMinAggregateDefinition - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/MaxMinAggregateDefinition.class
07:05:23.849 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.AggregateDefinition - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/AggregateDefinition.class
07:05:23.851 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UserAggregateDefinition - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UserAggregateDefinition.class
07:05:23.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SumAvgAggregateDefinition - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SumAvgAggregateDefinition.class
07:05:23.852 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.CountAggregateDefinition - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/CountAggregateDefinition.class
07:05:23.853 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.MaxMinAggregator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/MaxMinAggregator.class
07:05:23.853 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.OrderableAggregator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/OrderableAggregator.class
07:05:23.854 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.SystemAggregator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/SystemAggregator.class
07:05:23.855 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.ExecAggregator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/ExecAggregator.class
07:05:23.856 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.VerifyAggregateExpressionsVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/VerifyAggregateExpressionsVisitor.class
07:05:23.857 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.GroupByNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/GroupByNode.class
07:05:23.860 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.AggregatorInfoList - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/AggregatorInfoList.class
07:05:23.860 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.ReplaceAggregatesWithCRVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/ReplaceAggregatesWithCRVisitor.class
07:05:23.861 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.UserTypeConstantNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/UserTypeConstantNode.class
07:05:23.862 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.AggregatorInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/AggregatorInfo.class
07:05:23.863 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@e5e6416"
07:05:23.863 pool-1-thread-1 DEBUG Native: SELECT MAX(VER_ID) FROM VERSION
07:05:23.864 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.LastIndexKeyResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/LastIndexKeyResultSet.class
07:05:23.864 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ScalarAggregateResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ScalarAggregateResultSet.class
07:05:23.865 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.GenericAggregateResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/GenericAggregateResultSet.class
07:05:23.865 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.IndexValueRow - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/IndexValueRow.class
07:05:23.867 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.GenericAggregator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/GenericAggregator.class
07:05:23.868 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.index.B2IMaxScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/index/B2IMaxScan.class
07:05:23.868 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.store.access.btree.BTreeMaxScan - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/store/access/btree/BTreeMaxScan.class
07:05:23.870 pool-1-thread-1 DEBUG Retrieve: Execution Time = 7 ms
07:05:23.871 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.InsertNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/InsertNode.class
07:05:23.871 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.DMLModStatementNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/DMLModStatementNode.class
07:05:23.873 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.TestConstraintNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/TestConstraintNode.class
07:05:23.875 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.NormalizeResultSetNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/NormalizeResultSetNode.class
07:05:23.877 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.dictionary.IndexLister - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/dictionary/IndexLister.class
07:05:23.877 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.HasTableFunctionVisitor - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/HasTableFunctionVisitor.class
07:05:23.878 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.InsertConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/InsertConstantAction.class
07:05:23.879 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.WriteCursorConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/WriteCursorConstantAction.class
07:05:23.880 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6c155cb4" for connection "com.jolbox.bonecp.ConnectionHandle@56ce31f4"
07:05:23.880 pool-1-thread-1 DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MVersionTable'>,<6>)
07:05:23.881 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.iapi.sql.execute.TemporaryRowHolder - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/iapi/sql/execute/TemporaryRowHolder.class
07:05:23.883 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.UpdatableVTIConstantAction - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/UpdatableVTIConstantAction.class
07:05:23.884 pool-1-thread-1 DEBUG Persist: Execution Time = 4 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6c155cb4"
07:05:23.884 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6c155cb4"
07:05:23.884 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7e9cb97b"
07:05:23.884 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6e6ba547"
07:05:23.884 pool-1-thread-1 DEBUG ValueGeneration: Reserved a block of 5 values
07:05:23.884 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.valuegenerator.ValueGenerationBlock - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/valuegenerator/ValueGenerationBlock.class
07:05:23.885 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@56ce31f4" non enlisted to a transaction is being committed.
07:05:23.885 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@56ce31f4" closed
07:05:23.885 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.valuegenerator.ValueGeneration - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/valuegenerator/ValueGeneration.class
07:05:23.885 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MVersionTable (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:23.885 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.identity.OIDFactory - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/identity/OIDFactory.class
07:05:23.886 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.identity.IdentityUtils - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/identity/IdentityUtils.class
07:05:23.887 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") added to Level 1 cache (loadedFlags="[YY]")
07:05:23.887 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") enlisted in transactional cache
07:05:23.887 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.JDOCallbackHandler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/JDOCallbackHandler.class
07:05:23.887 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.JDOUserCallbackException - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/JDOUserCallbackException.class
07:05:23.888 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.listener.InstanceLifecycleEvent - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/listener/InstanceLifecycleEvent.class
07:05:23.888 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.api.jdo.FieldInstanceLifecycleEvent - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar!/org/datanucleus/api/jdo/FieldInstanceLifecycleEvent.class
07:05:23.889 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.IdentityHashMap - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/IdentityHashMap.class
07:05:23.889 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.listener.StoreCallback - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/listener/StoreCallback.class
07:05:23.889 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.ClassView - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/ClassView.class
07:05:23.890 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.RequestIdentifier - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/RequestIdentifier.class
07:05:23.890 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.RequestType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/RequestType.class
07:05:23.891 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.MappingConsumer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/MappingConsumer.class
07:05:23.892 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.request.InsertRequest$InsertMappingConsumer - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/request/InsertRequest$InsertMappingConsumer.class
07:05:23.893 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.table.SecondaryTable - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/table/SecondaryTable.class
07:05:23.893 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337" being inserted into table "VERSION"
07:05:23.893 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@e9fa135" opened with isolation level "read-committed" and auto-commit=false
07:05:23.894 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4917459e, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.894 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4917459e is starting for transaction Xid=    with flags 0
07:05:23.894 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b20c460 [conn=com.jolbox.bonecp.ConnectionHandle@e9fa135, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.895 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@698f367c" for connection "com.jolbox.bonecp.ConnectionHandle@e9fa135"
07:05:23.895 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO VERSION (VER_ID,SCHEMA_VERSION,VERSION_COMMENT) VALUES (?,?,?)" has been made batchable
07:05:23.895 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.SQLController$ConnectionStatementState - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/SQLController$ConnectionStatementState.class
07:05:23.896 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.SQLController$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/SQLController$1.class
07:05:23.896 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.fieldmanager.ParameterSetter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/fieldmanager/ParameterSetter.class
07:05:23.896 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.AbstractFieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/AbstractFieldManager.class
07:05:23.897 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO VERSION (VER_ID,SCHEMA_VERSION,VERSION_COMMENT) VALUES (?,?,?)" for processing (batch size = 1)
07:05:23.897 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6774)
07:05:23.898 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO VERSION (VER_ID,SCHEMA_VERSION,VERSION_COMMENT) VALUES (<1>,<'1.2.0'>,<'Set by MetaStore lian@192.168.1.102'>)]
07:05:23.898 pool-1-thread-1 DEBUG Datastore: Execution Time = 1 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@698f367c"
07:05:23.898 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:23.898 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:23.898 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.898 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:23.898 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable"
07:05:23.898 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") taken from Level 1 cache (loadedFlags="[YY]") [cache size = 1]
07:05:23.898 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.identity.SCOID - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/identity/SCOID.class
07:05:23.898 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:23.899 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.ReachabilityFieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/ReachabilityFieldManager.class
07:05:23.899 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:23.899 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.899 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:23.899 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.LoadFieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/LoadFieldManager.class
07:05:23.900 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.AbstractFetchDepthFieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/AbstractFetchDepthFieldManager.class
07:05:23.900 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.SingleValueFieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/SingleValueFieldManager.class
07:05:23.901 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4917459e]]
07:05:23.901 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4917459e is committing for transaction Xid=    with onePhase=true
07:05:23.901 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4917459e committed connection for transaction Xid=    with onePhase=true
07:05:23.901 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@e9fa135" closed
07:05:23.901 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b20c460 [conn=com.jolbox.bonecp.ConnectionHandle@e9fa135, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.901 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337" (depth=0)
07:05:23.901 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.listener.DetachCallback - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/listener/DetachCallback.class
07:05:23.901 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.DetachFieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/DetachFieldManager.class
07:05:23.902 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:23.902 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337" (id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable") being evicted from transactional cache
07:05:23.902 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MVersionTable@22fc7337, lifecycle=DETACHED_CLEAN]
07:05:23.902 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MVersionTable" being removed from Level 1 cache [current cache size = 1]
07:05:23.902 pool-1-thread-1 DEBUG Transaction: Transaction committed in 5 ms
07:05:23.902 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.ObjectStore$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/ObjectStore$1.class
07:05:23.902 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.ObjectStore$GetDbHelper - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/ObjectStore$GetDbHelper.class
07:05:23.903 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.ObjectStore$GetHelper - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/ObjectStore$GetHelper.class
07:05:23.903 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hive.common.util.HiveStringUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/common/util/HiveStringUtils.class
07:05:23.904 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.NumberFormat - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/NumberFormat.class
07:05:23.905 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.text.DecimalFormat - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/text/DecimalFormat.class
07:05:23.905 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.collect.Interners
07:05:23.905 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.905 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:23.905 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
07:05:23.905 pool-1-thread-1 DEBUG Query: SQL Query : "select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = ?"
07:05:23.905 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1552f4f8" opened with isolation level "read-committed" and auto-commit=false
07:05:23.905 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7e378e66, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.905 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7e378e66 is starting for transaction Xid=    with flags 0
07:05:23.905 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2d981ce5 [conn=com.jolbox.bonecp.ConnectionHandle@1552f4f8, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.909 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@e2efc59"
07:05:23.909 pool-1-thread-1 DEBUG Native: select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = <'default'>
07:05:23.909 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.909 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.ResultClassROF$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/ResultClassROF$1.class
07:05:23.910 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.FetchGroupManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/FetchGroupManager.class
07:05:23.910 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
07:05:23.910 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:23.910 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:23.910 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.910 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:23.910 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:23.910 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7e378e66]]
07:05:23.910 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7e378e66 is committing for transaction Xid=    with onePhase=true
07:05:23.910 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7e378e66 committed connection for transaction Xid=    with onePhase=true
07:05:23.910 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1552f4f8" closed
07:05:23.910 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2d981ce5 [conn=com.jolbox.bonecp.ConnectionHandle@1552f4f8, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.910 pool-1-thread-1 DEBUG Transaction: Transaction committed in 0 ms
07:05:23.910 pool-1-thread-1 DEBUG ObjectStore: db details for db default retrieved using SQL in 5.545446ms
07:05:23.910 pool-1-thread-1 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
07:05:23.911 pool-1-thread-1 DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MDatabase
07:05:23.911 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   	, enlisted resources=[]]
07:05:23.911 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:23.911 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:520)
07:05:23.911 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654"
07:05:23.911 pool-1-thread-1 DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:23.911 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:23.911 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:23.911 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4c717823" opened with isolation level "read-committed" and auto-commit=false
07:05:23.912 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.912 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.912 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.913 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.913 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.913 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.914 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.914 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.914 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.914 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.915 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.915 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.915 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.915 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.917 pool-1-thread-1 DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:23.920 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@160c732b"
07:05:23.920 pool-1-thread-1 DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MDatabase'> FOR UPDATE
07:05:23.921 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.923 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@9bc7c7"
07:05:23.923 pool-1-thread-1 DEBUG Native: SELECT MAX(DB_ID) FROM DBS
07:05:23.923 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.924 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4ca8295f" for connection "com.jolbox.bonecp.ConnectionHandle@4c717823"
07:05:23.924 pool-1-thread-1 DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MDatabase'>,<6>)
07:05:23.925 pool-1-thread-1 DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4ca8295f"
07:05:23.925 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4ca8295f"
07:05:23.925 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2b042d28"
07:05:23.925 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1e3c2de2"
07:05:23.925 pool-1-thread-1 DEBUG ValueGeneration: Reserved a block of 5 values
07:05:23.925 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4c717823" non enlisted to a transaction is being committed.
07:05:23.925 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4c717823" closed
07:05:23.925 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MDatabase (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:23.925 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") added to Level 1 cache (loadedFlags="[YYYYYY]")
07:05:23.925 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") enlisted in transactional cache
07:05:23.925 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" being inserted into table "DBS"
07:05:23.925 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@2b7f6965" opened with isolation level "read-committed" and auto-commit=false
07:05:23.925 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@54d312d5, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   	, enlisted resources=[]]
07:05:23.925 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@54d312d5 is starting for transaction Xid=   	 with flags 0
07:05:23.925 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@662e557f [conn=com.jolbox.bonecp.ConnectionHandle@2b7f6965, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.929 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@11c72977" for connection "com.jolbox.bonecp.ConnectionHandle@2b7f6965"
07:05:23.929 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO DBS (DB_ID,OWNER_NAME,"DESC",OWNER_TYPE,"NAME",DB_LOCATION_URI) VALUES (?,?,?,?,?,?)" has been made batchable
07:05:23.929 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO DBS (DB_ID,OWNER_NAME,"DESC",OWNER_TYPE,"NAME",DB_LOCATION_URI) VALUES (?,?,?,?,?,?)" for processing (batch size = 1)
07:05:23.929 pool-1-thread-1 DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MDatabase.parameters"
07:05:23.929 pool-1-thread-1 DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MDatabase.parameters"
07:05:23.930 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.flush.Operation - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/flush/Operation.class
07:05:23.931 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.JoinMapStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/JoinMapStore.class
07:05:23.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.AbstractMapStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/AbstractMapStore.class
07:05:23.932 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.BaseContainerStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/BaseContainerStore.class
07:05:23.934 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.exceptions.MappedDatastoreException - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/exceptions/MappedDatastoreException.class
07:05:23.936 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.BackingStoreHelper - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/BackingStoreHelper.class
07:05:23.938 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:23.939 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:522)
07:05:23.939 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO DBS (DB_ID,OWNER_NAME,"DESC",OWNER_TYPE,"NAME",DB_LOCATION_URI) VALUES (<1>,<'public'>,<'Default Hive database'>,<'ROLE'>,<'default'>,<'file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5'>)]
07:05:23.940 pool-1-thread-1 DEBUG Datastore: Execution Time = 1 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@11c72977"
07:05:23.940 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:23.940 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:23.940 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.940 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:23.940 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:23.940 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") taken from Level 1 cache (loadedFlags="[YYYYYY]") [cache size = 1]
07:05:23.940 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:23.940 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:23.940 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.940 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:23.940 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   	, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@54d312d5]]
07:05:23.940 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@54d312d5 is committing for transaction Xid=   	 with onePhase=true
07:05:23.940 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@54d312d5 committed connection for transaction Xid=   	 with onePhase=true
07:05:23.940 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@2b7f6965" closed
07:05:23.940 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@662e557f [conn=com.jolbox.bonecp.ConnectionHandle@2b7f6965, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.940 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" (depth=0)
07:05:23.940 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" field "parameters" loading contents to SCO wrapper from the datastore
07:05:23.941 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.MapEntrySetStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/MapEntrySetStore.class
07:05:23.943 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.ObjectLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/ObjectLiteral.class
07:05:23.943 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.NullMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/NullMapping.class
07:05:23.944 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.StatementParameterMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/StatementParameterMapping.class
07:05:23.944 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@56d623fc [conn=null, commitOnRelease=true, closeOnRelease=true, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:nontx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@6fd2ae3f]
07:05:23.944 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@3364764" opened with isolation level "read-committed" and auto-commit=false
07:05:23.950 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4ff804ac"
07:05:23.950 pool-1-thread-1 DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM DATABASE_PARAMS A0 WHERE A0.DB_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:23.950 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.950 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.MapEntrySetStore$1 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/MapEntrySetStore$1.class
07:05:23.951 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.MapEntrySetStore$SetIterator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/MapEntrySetStore$SetIterator.class
07:05:23.952 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7aa5a91f"
07:05:23.952 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@3364764" non enlisted to a transaction is being committed.
07:05:23.952 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@3364764" closed
07:05:23.952 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@56d623fc [conn=com.jolbox.bonecp.ConnectionHandle@3364764, commitOnRelease=true, closeOnRelease=true, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:nontx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@6fd2ae3f]
07:05:23.952 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:23.952 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:23.952 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") being evicted from transactional cache
07:05:23.952 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MDatabase@70ab0654, lifecycle=DETACHED_CLEAN]
07:05:23.952 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.fieldmanager.UnsetOwnerFieldManager - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/fieldmanager/UnsetOwnerFieldManager.class
07:05:23.952 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" being removed from Level 1 cache [current cache size = 1]
07:05:23.953 pool-1-thread-1 DEBUG Transaction: Transaction committed in 14 ms
07:05:23.953 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   
, enlisted resources=[]]
07:05:23.953 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:23.953 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3220)
07:05:23.953 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
07:05:23.953 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.JDOQLQueryHelper - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/JDOQLQueryHelper.class
07:05:23.954 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1"
07:05:23.955 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 1 ms
07:05:23.955 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{roleName}  =  ParameterExpression{t1}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MRole, t1 type=java.lang.String]
07:05:23.955 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1" for datastore
07:05:23.955 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MRole [Table : ROLES, InheritanceStrategy : new-table]
07:05:23.955 pool-1-thread-1 DEBUG Schema: Column "ROLES.ROLE_ID" added to internal representation of table.
07:05:23.956 pool-1-thread-1 DEBUG Schema: Table ROLES will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MRole (inheritance strategy="new-table")
07:05:23.956 pool-1-thread-1 DEBUG Schema: Column "ROLES.CREATE_TIME" added to internal representation of table.
07:05:23.956 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MRole.createTime] -> Column(s) [ROLES.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:23.956 pool-1-thread-1 DEBUG Schema: Column "ROLES.OWNER_NAME" added to internal representation of table.
07:05:23.956 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MRole.ownerName] -> Column(s) [ROLES.OWNER_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.956 pool-1-thread-1 DEBUG Schema: Column "ROLES.ROLE_NAME" added to internal representation of table.
07:05:23.956 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MRole.roleName] -> Column(s) [ROLES.ROLE_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:23.956 pool-1-thread-1 DEBUG Schema: Table/View ROLES has been initialised
07:05:23.956 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@31014a10" opened with isolation level "serializable" and auto-commit=false
07:05:23.956 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@31014a10" with isolation "serializable"
07:05:23.957 pool-1-thread-1 DEBUG Schema: Check of existence of ROLES returned no table
07:05:23.957 pool-1-thread-1 DEBUG Schema: Creating table ROLES
07:05:23.957 pool-1-thread-1 DEBUG Schema: CREATE TABLE ROLES
(
    ROLE_ID BIGINT NOT NULL,
    CREATE_TIME INTEGER NOT NULL,
    OWNER_NAME VARCHAR(128),
    ROLE_NAME VARCHAR(128)
)
07:05:23.958 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:23.958 pool-1-thread-1 DEBUG Schema: ALTER TABLE ROLES ADD CONSTRAINT ROLES_PK PRIMARY KEY (ROLE_ID)
07:05:23.960 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.961 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.961 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.962 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.963 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.964 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.965 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.966 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.967 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.967 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.967 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.968 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.969 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.969 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.970 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.970 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.971 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.976 pool-1-thread-1 DEBUG Schema: Creating index "RoleEntityINDEX" in catalog "" schema ""
07:05:23.976 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX ROLEENTITYINDEX ON ROLES (ROLE_NAME)
07:05:23.978 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:23.978 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@31014a10"
07:05:23.978 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@31014a10"
07:05:23.978 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@31014a10" non enlisted to a transaction is being committed.
07:05:23.978 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@31014a10" closed
07:05:23.979 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 24 ms
07:05:23.979 pool-1-thread-1 DEBUG Query: SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1 Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MRole' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.OWNER_NAME,A0.ROLE_NAME,A0.ROLE_ID FROM ROLES A0 WHERE A0.ROLE_NAME = ?"
07:05:23.979 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1c6e9232" opened with isolation level "read-committed" and auto-commit=false
07:05:23.979 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@599f590a, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   
, enlisted resources=[]]
07:05:23.979 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@599f590a is starting for transaction Xid=   
 with flags 0
07:05:23.979 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2378d4b8 [conn=com.jolbox.bonecp.ConnectionHandle@1c6e9232, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.979 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1" ...
07:05:23.983 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@3a1e1fa2"
07:05:23.984 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.MappingHelper - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/MappingHelper.class
07:05:23.985 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MRole' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.OWNER_NAME,A0.ROLE_NAME,A0.ROLE_ID FROM ROLES A0 WHERE A0.ROLE_NAME = <'admin'>
07:05:23.985 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.985 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 6 ms
07:05:23.985 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
07:05:23.985 pool-1-thread-1 DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MRole
07:05:23.985 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313"
07:05:23.985 pool-1-thread-1 DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MRole"
07:05:23.986 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:23.986 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:23.986 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@73c462ef" opened with isolation level "read-committed" and auto-commit=false
07:05:23.986 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.986 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.987 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.987 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.988 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.989 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.990 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.990 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.990 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:23.991 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:23.991 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:23.992 pool-1-thread-1 DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:23.992 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1222dab4"
07:05:23.992 pool-1-thread-1 DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MRole'> FOR UPDATE
07:05:23.992 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.994 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1ed61494"
07:05:23.994 pool-1-thread-1 DEBUG Native: SELECT MAX(ROLE_ID) FROM ROLES
07:05:23.995 pool-1-thread-1 DEBUG Retrieve: Execution Time = 1 ms
07:05:23.995 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@64d0b7e" for connection "com.jolbox.bonecp.ConnectionHandle@73c462ef"
07:05:23.995 pool-1-thread-1 DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MRole'>,<6>)
07:05:23.995 pool-1-thread-1 DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@64d0b7e"
07:05:23.995 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@64d0b7e"
07:05:23.995 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@212ac12a"
07:05:23.995 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6e6c1871"
07:05:23.995 pool-1-thread-1 DEBUG ValueGeneration: Reserved a block of 5 values
07:05:23.995 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@73c462ef" non enlisted to a transaction is being committed.
07:05:23.995 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@73c462ef" closed
07:05:23.995 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MRole (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:23.995 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") added to Level 1 cache (loadedFlags="[YYY]")
07:05:23.995 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") enlisted in transactional cache
07:05:23.996 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313" being inserted into table "ROLES"
07:05:23.996 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2378d4b8 [conn=com.jolbox.bonecp.ConnectionHandle@1c6e9232, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.997 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@f7b0fc6" for connection "com.jolbox.bonecp.ConnectionHandle@1c6e9232"
07:05:23.997 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO ROLES (ROLE_ID,OWNER_NAME,CREATE_TIME,ROLE_NAME) VALUES (?,?,?,?)" has been made batchable
07:05:23.997 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO ROLES (ROLE_ID,OWNER_NAME,CREATE_TIME,ROLE_NAME) VALUES (?,?,?,?)" for processing (batch size = 1)
07:05:23.997 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3229)
07:05:23.997 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO ROLES (ROLE_ID,OWNER_NAME,CREATE_TIME,ROLE_NAME) VALUES (<1>,<'admin'>,<1461247523>,<'admin'>)]
07:05:23.998 pool-1-thread-1 DEBUG Datastore: Execution Time = 1 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@f7b0fc6"
07:05:23.998 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:23.998 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:23.998 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.998 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:23.998 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MRole"
07:05:23.998 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") taken from Level 1 cache (loadedFlags="[YYY]") [cache size = 1]
07:05:23.998 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:23.998 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:23.998 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:23.998 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:23.998 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   
, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@599f590a]]
07:05:23.998 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@599f590a is committing for transaction Xid=   
 with onePhase=true
07:05:23.998 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@599f590a committed connection for transaction Xid=   
 with onePhase=true
07:05:23.998 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1c6e9232" closed
07:05:23.998 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2378d4b8 [conn=com.jolbox.bonecp.ConnectionHandle@1c6e9232, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.998 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313" (depth=0)
07:05:23.998 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:23.998 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@7bdf3313" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") being evicted from transactional cache
07:05:23.998 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MRole@7bdf3313 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MRole@7bdf3313, lifecycle=DETACHED_CLEAN]
07:05:23.998 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MRole" being removed from Level 1 cache [current cache size = 1]
07:05:23.998 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:23.998 pool-1-thread-1 INFO HiveMetaStore: Added admin role in metastore
07:05:23.998 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.998 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:23.999 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3220)
07:05:23.999 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
07:05:23.999 pool-1-thread-1 DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1FetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:23.999 pool-1-thread-1 DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1FetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:23.999 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5a61dd53" opened with isolation level "read-committed" and auto-commit=false
07:05:23.999 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@733a056b, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:23.999 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@733a056b is starting for transaction Xid=    with flags 0
07:05:23.999 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@73bbdf1b [conn=com.jolbox.bonecp.ConnectionHandle@5a61dd53, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:23.999 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1" ...
07:05:23.999 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@27e40f7a"
07:05:23.999 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MRole' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.OWNER_NAME,A0.ROLE_NAME,A0.ROLE_ID FROM ROLES A0 WHERE A0.ROLE_NAME = <'public'>
07:05:23.999 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:23.999 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:23.999 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
07:05:23.999 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78"
07:05:23.999 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MRole (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=2
07:05:23.999 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") added to Level 1 cache (loadedFlags="[YYY]")
07:05:23.999 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") enlisted in transactional cache
07:05:24.000 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78" being inserted into table "ROLES"
07:05:24.000 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@73bbdf1b [conn=com.jolbox.bonecp.ConnectionHandle@5a61dd53, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.000 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1e907c1b" for connection "com.jolbox.bonecp.ConnectionHandle@5a61dd53"
07:05:24.000 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO ROLES (ROLE_ID,OWNER_NAME,CREATE_TIME,ROLE_NAME) VALUES (?,?,?,?)" has been made batchable
07:05:24.000 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO ROLES (ROLE_ID,OWNER_NAME,CREATE_TIME,ROLE_NAME) VALUES (?,?,?,?)" for processing (batch size = 1)
07:05:24.000 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3229)
07:05:24.000 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO ROLES (ROLE_ID,OWNER_NAME,CREATE_TIME,ROLE_NAME) VALUES (<2>,<'public'>,<1461247523>,<'public'>)]
07:05:24.000 pool-1-thread-1 DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1e907c1b"
07:05:24.000 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:24.000 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:24.000 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:24.000 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:24.000 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "2[OID]org.apache.hadoop.hive.metastore.model.MRole"
07:05:24.000 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") taken from Level 1 cache (loadedFlags="[YYY]") [cache size = 1]
07:05:24.000 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:24.000 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:24.000 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:24.000 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:24.000 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@733a056b]]
07:05:24.000 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@733a056b is committing for transaction Xid=    with onePhase=true
07:05:24.000 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@733a056b committed connection for transaction Xid=    with onePhase=true
07:05:24.000 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5a61dd53" closed
07:05:24.000 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@73bbdf1b [conn=com.jolbox.bonecp.ConnectionHandle@5a61dd53, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.000 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78" (depth=0)
07:05:24.000 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:24.000 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@33fe2e78" (id="2[OID]org.apache.hadoop.hive.metastore.model.MRole") being evicted from transactional cache
07:05:24.000 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MRole@33fe2e78 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MRole@33fe2e78, lifecycle=DETACHED_CLEAN]
07:05:24.000 pool-1-thread-1 DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MRole" being removed from Level 1 cache [current cache size = 1]
07:05:24.000 pool-1-thread-1 DEBUG Transaction: Transaction committed in 0 ms
07:05:24.000 pool-1-thread-1 INFO HiveMetaStore: Added public role in metastore
07:05:24.001 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege$HiveObjectPrivilegeStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectPrivilege$HiveObjectPrivilegeStandardSchemeFactory.class
07:05:24.002 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege$HiveObjectPrivilegeTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectPrivilege$HiveObjectPrivilegeTupleSchemeFactory.class
07:05:24.002 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectPrivilege$_Fields.class
07:05:24.003 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectRef - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectRef.class
07:05:24.004 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrivilegeGrantInfo.class
07:05:24.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectRef$HiveObjectRefStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectRef$HiveObjectRefStandardSchemeFactory.class
07:05:24.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectRef$HiveObjectRefTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectRef$HiveObjectRefTupleSchemeFactory.class
07:05:24.006 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectRef$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectRef$_Fields.class
07:05:24.007 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HiveObjectType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HiveObjectType.class
07:05:24.008 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo$PrivilegeGrantInfoStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrivilegeGrantInfo$PrivilegeGrantInfoStandardSchemeFactory.class
07:05:24.008 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo$PrivilegeGrantInfoTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrivilegeGrantInfo$PrivilegeGrantInfoTupleSchemeFactory.class
07:05:24.009 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrivilegeGrantInfo$_Fields.class
07:05:24.010 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.EncodingUtils - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/EncodingUtils.class
07:05:24.010 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:24.010 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:24.010 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:3912)
07:05:24.011 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3527)
07:05:24.011 pool-1-thread-1 DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1FetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:24.011 pool-1-thread-1 DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1FetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:24.011 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5a2c5496" opened with isolation level "read-committed" and auto-commit=false
07:05:24.011 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6a6af06b, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:24.011 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6a6af06b is starting for transaction Xid=    with flags 0
07:05:24.011 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@20dc39dc [conn=com.jolbox.bonecp.ConnectionHandle@5a2c5496, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.011 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MRole WHERE roleName == t1 PARAMETERS java.lang.String t1" ...
07:05:24.011 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@186c41b5"
07:05:24.011 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MRole' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.OWNER_NAME,A0.ROLE_NAME,A0.ROLE_ID FROM ROLES A0 WHERE A0.ROLE_NAME = <'admin'>
07:05:24.011 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:24.011 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:24.012 pool-1-thread-1 DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MRole"
07:05:24.012 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.PersistentClassROF$2 - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/PersistentClassROF$2.class
07:05:24.012 pool-1-thread-1 DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MRole" not found in Level 1 cache [cache size = 0]
07:05:24.013 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.identity.DatastoreUniqueOID - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/identity/DatastoreUniqueOID.class
07:05:24.013 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContextImpl$ClassDetailsForId - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContextImpl$ClassDetailsForId.class
07:05:24.014 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") added to Level 1 cache (loadedFlags="[NNN]")
07:05:24.014 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.fieldmanager.ResultSetGetter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/fieldmanager/ResultSetGetter.class
07:05:24.015 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.jdo.listener.LoadCallback - jar:file:/Users/lian/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar!/javax/jdo/listener/LoadCallback.class
07:05:24.015 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:24.015 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") enlisted in transactional cache
07:05:24.015 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMRole(ObjectStore.java:3533)
07:05:24.016 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalGlobalGrants(ObjectStore.java:4397)
07:05:24.016 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MGlobalPrivilege WHERE principalName == t1 && principalType == t2 PARAMETERS java.lang.String t1, java.lang.String t2"
07:05:24.016 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:24.016 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{DyadicExpression{PrimaryExpression{principalName}  =  ParameterExpression{t1}}  AND  DyadicExpression{PrimaryExpression{principalType}  =  ParameterExpression{t2}}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MGlobalPrivilege, t1 type=java.lang.String, t2 type=java.lang.String]
07:05:24.016 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT FROM org.apache.hadoop.hive.metastore.model.MGlobalPrivilege WHERE principalName == t1 && principalType == t2 PARAMETERS java.lang.String t1, java.lang.String t2" for datastore
07:05:24.016 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege [Table : GLOBAL_PRIVS, InheritanceStrategy : new-table]
07:05:24.016 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.USER_GRANT_ID" added to internal representation of table.
07:05:24.017 pool-1-thread-1 DEBUG Schema: Table GLOBAL_PRIVS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MGlobalPrivilege (inheritance strategy="new-table")
07:05:24.017 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.CREATE_TIME" added to internal representation of table.
07:05:24.017 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.createTime] -> Column(s) [GLOBAL_PRIVS.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:24.017 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.GRANT_OPTION" added to internal representation of table.
07:05:24.018 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.grantOption] -> Column(s) [GLOBAL_PRIVS.GRANT_OPTION] using mapping of type "org.datanucleus.store.rdbms.mapping.java.BooleanMapping" (org.datanucleus.store.rdbms.mapping.datastore.SmallIntRDBMSMapping)
07:05:24.018 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.GRANTOR" added to internal representation of table.
07:05:24.018 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.grantor] -> Column(s) [GLOBAL_PRIVS.GRANTOR] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.018 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.GRANTOR_TYPE" added to internal representation of table.
07:05:24.018 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.grantorType] -> Column(s) [GLOBAL_PRIVS.GRANTOR_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.018 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.PRINCIPAL_NAME" added to internal representation of table.
07:05:24.018 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.principalName] -> Column(s) [GLOBAL_PRIVS.PRINCIPAL_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.018 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.PRINCIPAL_TYPE" added to internal representation of table.
07:05:24.018 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.principalType] -> Column(s) [GLOBAL_PRIVS.PRINCIPAL_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.018 pool-1-thread-1 DEBUG Schema: Column "GLOBAL_PRIVS.USER_PRIV" added to internal representation of table.
07:05:24.018 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.privilege] -> Column(s) [GLOBAL_PRIVS.USER_PRIV] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.018 pool-1-thread-1 DEBUG Schema: Table/View GLOBAL_PRIVS has been initialised
07:05:24.018 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5d3813da" opened with isolation level "serializable" and auto-commit=false
07:05:24.018 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@5d3813da" with isolation "serializable"
07:05:24.019 pool-1-thread-1 DEBUG Schema: Check of existence of GLOBAL_PRIVS returned no table
07:05:24.019 pool-1-thread-1 DEBUG Schema: Creating table GLOBAL_PRIVS
07:05:24.019 pool-1-thread-1 DEBUG Schema: CREATE TABLE GLOBAL_PRIVS
(
    USER_GRANT_ID BIGINT NOT NULL,
    CREATE_TIME INTEGER NOT NULL,
    GRANT_OPTION SMALLINT NOT NULL CHECK (GRANT_OPTION IN (0,1)),
    GRANTOR VARCHAR(128),
    GRANTOR_TYPE VARCHAR(128),
    PRINCIPAL_NAME VARCHAR(128),
    PRINCIPAL_TYPE VARCHAR(128),
    USER_PRIV VARCHAR(128)
)
07:05:24.022 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:24.022 pool-1-thread-1 DEBUG Schema: ALTER TABLE GLOBAL_PRIVS ADD CONSTRAINT GLOBAL_PRIVS_PK PRIMARY KEY (USER_GRANT_ID)
07:05:24.027 pool-1-thread-1 DEBUG Schema: Execution Time = 5 ms
07:05:24.028 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.028 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.029 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.029 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.030 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.030 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.031 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.031 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.032 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.032 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.033 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.033 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.034 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.035 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.036 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.036 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.037 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.037 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.038 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.038 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.038 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.039 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.039 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.040 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.040 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.040 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.044 pool-1-thread-1 DEBUG Schema: Creating index "GlobalPrivilegeIndex" in catalog "" schema ""
07:05:24.044 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX GLOBALPRIVILEGEINDEX ON GLOBAL_PRIVS (PRINCIPAL_NAME,PRINCIPAL_TYPE,USER_PRIV,GRANTOR,GRANTOR_TYPE)
07:05:24.046 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:24.046 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@5d3813da"
07:05:24.046 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@5d3813da"
07:05:24.046 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5d3813da" non enlisted to a transaction is being committed.
07:05:24.046 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5d3813da" closed
07:05:24.047 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 31 ms
07:05:24.047 pool-1-thread-1 DEBUG Query: SELECT FROM org.apache.hadoop.hive.metastore.model.MGlobalPrivilege WHERE principalName == t1 && principalType == t2 PARAMETERS java.lang.String t1, java.lang.String t2 Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.GRANT_OPTION,A0.GRANTOR,A0.GRANTOR_TYPE,A0.PRINCIPAL_NAME,A0.PRINCIPAL_TYPE,A0.USER_PRIV,A0.USER_GRANT_ID FROM GLOBAL_PRIVS A0 WHERE A0.PRINCIPAL_NAME = ? AND A0.PRINCIPAL_TYPE = ?"
07:05:24.047 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@20dc39dc [conn=com.jolbox.bonecp.ConnectionHandle@5a2c5496, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.047 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT FROM org.apache.hadoop.hive.metastore.model.MGlobalPrivilege WHERE principalName == t1 && principalType == t2 PARAMETERS java.lang.String t1, java.lang.String t2" ...
07:05:24.053 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6f77a3ca"
07:05:24.053 pool-1-thread-1 DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.GRANT_OPTION,A0.GRANTOR,A0.GRANTOR_TYPE,A0.PRINCIPAL_NAME,A0.PRINCIPAL_TYPE,A0.USER_PRIV,A0.USER_GRANT_ID FROM GLOBAL_PRIVS A0 WHERE A0.PRINCIPAL_NAME = <'admin'> AND A0.PRINCIPAL_TYPE = <'ROLE'>
07:05:24.055 pool-1-thread-1 DEBUG Retrieve: Execution Time = 2 ms
07:05:24.055 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 8 ms
07:05:24.056 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalGlobalGrants(ObjectStore.java:4407)
07:05:24.056 pool-1-thread-1 DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MGlobalPrivilege
07:05:24.057 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.PersistenceBatchType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/PersistenceBatchType.class
07:05:24.057 pool-1-thread-1 DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e"
07:05:24.058 pool-1-thread-1 DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege"
07:05:24.058 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:24.058 pool-1-thread-1 DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:24.058 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@428311b8" opened with isolation level "read-committed" and auto-commit=false
07:05:24.059 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.060 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.060 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.061 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.061 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.062 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.062 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.063 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.063 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.064 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.064 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.065 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.065 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.066 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.071 pool-1-thread-1 DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:24.071 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@684b57bc"
07:05:24.071 pool-1-thread-1 DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege'> FOR UPDATE
07:05:24.071 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:24.075 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4698ea61"
07:05:24.075 pool-1-thread-1 DEBUG Native: SELECT MAX(USER_GRANT_ID) FROM GLOBAL_PRIVS
07:05:24.075 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:24.075 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5666f815" for connection "com.jolbox.bonecp.ConnectionHandle@428311b8"
07:05:24.075 pool-1-thread-1 DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege'>,<6>)
07:05:24.076 pool-1-thread-1 DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5666f815"
07:05:24.076 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5666f815"
07:05:24.076 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@790a95d9"
07:05:24.076 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@23277040"
07:05:24.076 pool-1-thread-1 DEBUG ValueGeneration: Reserved a block of 5 values
07:05:24.076 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@428311b8" non enlisted to a transaction is being committed.
07:05:24.076 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@428311b8" closed
07:05:24.076 pool-1-thread-1 DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:24.076 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") added to Level 1 cache (loadedFlags="[YYYYYYY]")
07:05:24.076 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") enlisted in transactional cache
07:05:24.077 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e" being inserted into table "GLOBAL_PRIVS"
07:05:24.077 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@20dc39dc [conn=com.jolbox.bonecp.ConnectionHandle@5a2c5496, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.080 pool-1-thread-1 DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2d60f664" for connection "com.jolbox.bonecp.ConnectionHandle@5a2c5496"
07:05:24.080 pool-1-thread-1 DEBUG Persist: The requested statement "INSERT INTO GLOBAL_PRIVS (USER_GRANT_ID,GRANT_OPTION,USER_PRIV,GRANTOR,PRINCIPAL_NAME,CREATE_TIME,PRINCIPAL_TYPE,GRANTOR_TYPE) VALUES (?,?,?,?,?,?,?,?)" has been made batchable
07:05:24.080 pool-1-thread-1 DEBUG Persist: Batch has been added to statement "INSERT INTO GLOBAL_PRIVS (USER_GRANT_ID,GRANT_OPTION,USER_PRIV,GRANTOR,PRINCIPAL_NAME,CREATE_TIME,PRINCIPAL_TYPE,GRANTOR_TYPE) VALUES (?,?,?,?,?,?,?,?)" for processing (batch size = 1)
07:05:24.080 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:4112)
07:05:24.080 pool-1-thread-1 DEBUG Native: BATCH [INSERT INTO GLOBAL_PRIVS (USER_GRANT_ID,GRANT_OPTION,USER_PRIV,GRANTOR,PRINCIPAL_NAME,CREATE_TIME,PRINCIPAL_TYPE,GRANTOR_TYPE) VALUES (<1>,<1>,<'All'>,<'admin'>,<'admin'>,<1461247524>,<'ROLE'>,<'ROLE'>)]
07:05:24.080 pool-1-thread-1 DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2d60f664"
07:05:24.080 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:24.080 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:24.080 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:24.080 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:24.080 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege"
07:05:24.080 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") taken from Level 1 cache (loadedFlags="[YYYYYYY]") [cache size = 2]
07:05:24.081 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:24.081 pool-1-thread-1 DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MRole"
07:05:24.081 pool-1-thread-1 DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") taken from Level 1 cache (loadedFlags="[YYY]") [cache size = 2]
07:05:24.081 pool-1-thread-1 DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:24.081 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:24.081 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:24.081 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:24.081 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6a6af06b]]
07:05:24.081 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6a6af06b is committing for transaction Xid=    with onePhase=true
07:05:24.081 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@6a6af06b committed connection for transaction Xid=    with onePhase=true
07:05:24.081 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5a2c5496" closed
07:05:24.081 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@20dc39dc [conn=com.jolbox.bonecp.ConnectionHandle@5a2c5496, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.081 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e" (depth=0)
07:05:24.081 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:24.081 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege") being evicted from transactional cache
07:05:24.081 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e from StateManager[pc=org.apache.hadoop.hive.metastore.model.MGlobalPrivilege@5921129e, lifecycle=DETACHED_CLEAN]
07:05:24.081 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MGlobalPrivilege" being removed from Level 1 cache [current cache size = 2]
07:05:24.081 pool-1-thread-1 DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c" (depth=0)
07:05:24.081 pool-1-thread-1 DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:24.081 pool-1-thread-1 DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MRole") being evicted from transactional cache
07:05:24.081 pool-1-thread-1 DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c from StateManager[pc=org.apache.hadoop.hive.metastore.model.MRole@4a7fa05c, lifecycle=DETACHED_CLEAN]
07:05:24.081 pool-1-thread-1 DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MRole" being removed from Level 1 cache [current cache size = 1]
07:05:24.081 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:24.081 pool-1-thread-1 INFO HiveMetaStore: No user is added in admin role, since config is empty
07:05:24.081 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStorePreEventListener - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStorePreEventListener.class
07:05:24.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreEventListener - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreEventListener.class
07:05:24.082 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.SessionPropertiesListener - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/SessionPropertiesListener.class
07:05:24.083 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreEndFunctionListener.class
07:05:24.083 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.LockRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/LockRequest.class
07:05:24.084 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.LockResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/LockResponse.class
07:05:24.085 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NoSuchTxnException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NoSuchTxnException.class
07:05:24.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.TxnAbortedException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/TxnAbortedException.class
07:05:24.086 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CompactionRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CompactionRequest.class
07:05:24.087 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.UnlockRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/UnlockRequest.class
07:05:24.087 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.NoSuchLockException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/NoSuchLockException.class
07:05:24.089 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.TxnOpenException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/TxnOpenException.class
07:05:24.089 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.EnvironmentContext - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/EnvironmentContext.class
07:05:24.090 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatRequest.class
07:05:24.091 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetPrincipalsInRoleRequest.class
07:05:24.091 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetPrincipalsInRoleResponse.class
07:05:24.092 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddPartitionsRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddPartitionsRequest.class
07:05:24.093 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddPartitionsResult - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddPartitionsResult.class
07:05:24.094 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.DropPartitionsRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/DropPartitionsRequest.class
07:05:24.094 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.DropPartitionsResult - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/DropPartitionsResult.class
07:05:24.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsByExprRequest.class
07:05:24.095 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsByExprResult - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsByExprResult.class
07:05:24.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SetPartitionsStatsRequest.class
07:05:24.096 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.TableStatsRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/TableStatsRequest.class
07:05:24.097 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.TableStatsResult - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/TableStatsResult.class
07:05:24.097 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsStatsRequest.class
07:05:24.098 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsStatsResult - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsStatsResult.class
07:05:24.099 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokeRoleRequest.class
07:05:24.100 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokeRoleResponse.class
07:05:24.100 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokePrivilegeRequest.class
07:05:24.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokePrivilegeResponse.class
07:05:24.101 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetOpenTxnsResponse.class
07:05:24.102 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.OpenTxnRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/OpenTxnRequest.class
07:05:24.102 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.OpenTxnsResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/OpenTxnsResponse.class
07:05:24.103 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AbortTxnRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AbortTxnRequest.class
07:05:24.103 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CommitTxnRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CommitTxnRequest.class
07:05:24.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetOpenTxnsInfoResponse.class
07:05:24.104 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CheckLockRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CheckLockRequest.class
07:05:24.105 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowLocksRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowLocksRequest.class
07:05:24.105 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowLocksResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowLocksResponse.class
07:05:24.106 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatTxnRangeRequest.class
07:05:24.106 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatTxnRangeResponse.class
07:05:24.107 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowCompactRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowCompactRequest.class
07:05:24.107 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowCompactResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowCompactResponse.class
07:05:24.108 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddDynamicPartitions - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddDynamicPartitions.class
07:05:24.108 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FireEventRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FireEventRequest.class
07:05:24.109 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FireEventResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FireEventResponse.class
07:05:24.109 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetRoleGrantsForPrincipalRequest.class
07:05:24.110 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetRoleGrantsForPrincipalResponse.class
07:05:24.110 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: com.facebook.fb303.fb_status - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libfb303/jars/libfb303-0.9.2.jar!/com/facebook/fb303/fb_status.class
07:05:24.119 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.EnvironmentContext$EnvironmentContextStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/EnvironmentContext$EnvironmentContextStandardSchemeFactory.class
07:05:24.119 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.EnvironmentContext$EnvironmentContextTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/EnvironmentContext$EnvironmentContextTupleSchemeFactory.class
07:05:24.120 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.EnvironmentContext$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/EnvironmentContext$_Fields.class
07:05:24.120 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest$HeartbeatTxnRangeRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatTxnRangeRequest$HeartbeatTxnRangeRequestStandardSchemeFactory.class
07:05:24.121 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest$HeartbeatTxnRangeRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatTxnRangeRequest$HeartbeatTxnRangeRequestTupleSchemeFactory.class
07:05:24.121 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatTxnRangeRequest$_Fields.class
07:05:24.122 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddPartitionsRequest$AddPartitionsRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddPartitionsRequest$AddPartitionsRequestStandardSchemeFactory.class
07:05:24.122 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddPartitionsRequest$AddPartitionsRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddPartitionsRequest$AddPartitionsRequestTupleSchemeFactory.class
07:05:24.123 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddPartitionsRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddPartitionsRequest$_Fields.class
07:05:24.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest$GetRoleGrantsForPrincipalRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetRoleGrantsForPrincipalRequest$GetRoleGrantsForPrincipalRequestStandardSchemeFactory.class
07:05:24.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest$GetRoleGrantsForPrincipalRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetRoleGrantsForPrincipalRequest$GetRoleGrantsForPrincipalRequestTupleSchemeFactory.class
07:05:24.124 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetRoleGrantsForPrincipalRequest$_Fields.class
07:05:24.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CheckLockRequest$CheckLockRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CheckLockRequest$CheckLockRequestStandardSchemeFactory.class
07:05:24.125 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CheckLockRequest$CheckLockRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CheckLockRequest$CheckLockRequestTupleSchemeFactory.class
07:05:24.126 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CheckLockRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CheckLockRequest$_Fields.class
07:05:24.127 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.TableStatsRequest$TableStatsRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/TableStatsRequest$TableStatsRequestStandardSchemeFactory.class
07:05:24.127 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.TableStatsRequest$TableStatsRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/TableStatsRequest$TableStatsRequestTupleSchemeFactory.class
07:05:24.127 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.TableStatsRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/TableStatsRequest$_Fields.class
07:05:24.128 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CommitTxnRequest$CommitTxnRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CommitTxnRequest$CommitTxnRequestStandardSchemeFactory.class
07:05:24.128 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CommitTxnRequest$CommitTxnRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CommitTxnRequest$CommitTxnRequestTupleSchemeFactory.class
07:05:24.129 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CommitTxnRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CommitTxnRequest$_Fields.class
07:05:24.129 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest$PartitionsByExprRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsByExprRequest$PartitionsByExprRequestStandardSchemeFactory.class
07:05:24.130 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest$PartitionsByExprRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsByExprRequest$PartitionsByExprRequestTupleSchemeFactory.class
07:05:24.130 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsByExprRequest$_Fields.class
07:05:24.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddDynamicPartitions$AddDynamicPartitionsStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddDynamicPartitions$AddDynamicPartitionsStandardSchemeFactory.class
07:05:24.131 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddDynamicPartitions$AddDynamicPartitionsTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddDynamicPartitions$AddDynamicPartitionsTupleSchemeFactory.class
07:05:24.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AddDynamicPartitions$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AddDynamicPartitions$_Fields.class
07:05:24.132 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowLocksRequest$ShowLocksRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowLocksRequest$ShowLocksRequestStandardSchemeFactory.class
07:05:24.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowLocksRequest$ShowLocksRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowLocksRequest$ShowLocksRequestTupleSchemeFactory.class
07:05:24.133 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowLocksRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowLocksRequest$_Fields.class
07:05:24.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.DropPartitionsRequest$DropPartitionsRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/DropPartitionsRequest$DropPartitionsRequestStandardSchemeFactory.class
07:05:24.134 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.DropPartitionsRequest$DropPartitionsRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/DropPartitionsRequest$DropPartitionsRequestTupleSchemeFactory.class
07:05:24.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.DropPartitionsRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/DropPartitionsRequest$_Fields.class
07:05:24.135 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.RequestPartsSpec - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/RequestPartsSpec.class
07:05:24.136 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.thrift.TUnion - jar:file:/Users/lian/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.2.jar!/org/apache/thrift/TUnion.class
07:05:24.137 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FireEventRequest$FireEventRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FireEventRequest$FireEventRequestStandardSchemeFactory.class
07:05:24.137 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FireEventRequest$FireEventRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FireEventRequest$FireEventRequestTupleSchemeFactory.class
07:05:24.137 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FireEventRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FireEventRequest$_Fields.class
07:05:24.138 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FireEventRequestData - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FireEventRequestData.class
07:05:24.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.OpenTxnRequest$OpenTxnRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/OpenTxnRequest$OpenTxnRequestStandardSchemeFactory.class
07:05:24.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.OpenTxnRequest$OpenTxnRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/OpenTxnRequest$OpenTxnRequestTupleSchemeFactory.class
07:05:24.139 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.OpenTxnRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/OpenTxnRequest$_Fields.class
07:05:24.140 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest$PartitionsStatsRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsStatsRequest$PartitionsStatsRequestStandardSchemeFactory.class
07:05:24.141 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest$PartitionsStatsRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsStatsRequest$PartitionsStatsRequestTupleSchemeFactory.class
07:05:24.141 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PartitionsStatsRequest$_Fields.class
07:05:24.142 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest$GrantRevokeRoleRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokeRoleRequest$GrantRevokeRoleRequestStandardSchemeFactory.class
07:05:24.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest$GrantRevokeRoleRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokeRoleRequest$GrantRevokeRoleRequestTupleSchemeFactory.class
07:05:24.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokeRoleRequest$_Fields.class
07:05:24.143 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokeType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokeType.class
07:05:24.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest$SetPartitionsStatsRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SetPartitionsStatsRequest$SetPartitionsStatsRequestStandardSchemeFactory.class
07:05:24.144 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest$SetPartitionsStatsRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SetPartitionsStatsRequest$SetPartitionsStatsRequestTupleSchemeFactory.class
07:05:24.145 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SetPartitionsStatsRequest$_Fields.class
07:05:24.146 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest$GrantRevokePrivilegeRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokePrivilegeRequest$GrantRevokePrivilegeRequestStandardSchemeFactory.class
07:05:24.146 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest$GrantRevokePrivilegeRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokePrivilegeRequest$GrantRevokePrivilegeRequestTupleSchemeFactory.class
07:05:24.146 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GrantRevokePrivilegeRequest$_Fields.class
07:05:24.147 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest$GetPrincipalsInRoleRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetPrincipalsInRoleRequest$GetPrincipalsInRoleRequestStandardSchemeFactory.class
07:05:24.147 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest$GetPrincipalsInRoleRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetPrincipalsInRoleRequest$GetPrincipalsInRoleRequestTupleSchemeFactory.class
07:05:24.148 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/GetPrincipalsInRoleRequest$_Fields.class
07:05:24.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CompactionRequest$CompactionRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CompactionRequest$CompactionRequestStandardSchemeFactory.class
07:05:24.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CompactionRequest$CompactionRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CompactionRequest$CompactionRequestTupleSchemeFactory.class
07:05:24.149 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CompactionRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CompactionRequest$_Fields.class
07:05:24.150 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.CompactionType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/CompactionType.class
07:05:24.150 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.LockRequest$LockRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/LockRequest$LockRequestStandardSchemeFactory.class
07:05:24.151 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.LockRequest$LockRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/LockRequest$LockRequestTupleSchemeFactory.class
07:05:24.151 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.LockRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/LockRequest$_Fields.class
07:05:24.152 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.LockComponent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/LockComponent.class
07:05:24.152 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.UnlockRequest$UnlockRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/UnlockRequest$UnlockRequestStandardSchemeFactory.class
07:05:24.153 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.UnlockRequest$UnlockRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/UnlockRequest$UnlockRequestTupleSchemeFactory.class
07:05:24.153 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.UnlockRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/UnlockRequest$_Fields.class
07:05:24.154 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AbortTxnRequest$AbortTxnRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AbortTxnRequest$AbortTxnRequestStandardSchemeFactory.class
07:05:24.154 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AbortTxnRequest$AbortTxnRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AbortTxnRequest$AbortTxnRequestTupleSchemeFactory.class
07:05:24.154 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AbortTxnRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AbortTxnRequest$_Fields.class
07:05:24.155 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowCompactRequest$ShowCompactRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowCompactRequest$ShowCompactRequestStandardSchemeFactory.class
07:05:24.155 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowCompactRequest$ShowCompactRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowCompactRequest$ShowCompactRequestTupleSchemeFactory.class
07:05:24.156 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.ShowCompactRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/ShowCompactRequest$_Fields.class
07:05:24.157 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatRequest$HeartbeatRequestStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatRequest$HeartbeatRequestStandardSchemeFactory.class
07:05:24.157 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatRequest$HeartbeatRequestTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatRequest$HeartbeatRequestTupleSchemeFactory.class
07:05:24.157 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.HeartbeatRequest$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/HeartbeatRequest$_Fields.class
07:05:24.158 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.PartitionDropOptions - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/PartitionDropOptions.class
07:05:24.158 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.ValidTxnList - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/ValidTxnList.class
07:05:24.159 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.IMetaStoreClient$NotificationFilter - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/IMetaStoreClient$NotificationFilter.class
07:05:24.167 pool-1-thread-1 INFO HiveMetaStore: 0: get_all_databases
07:05:24.167 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.Utils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/Utils.class
07:05:24.167 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.security.token.TokenSelector
07:05:24.167 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.security.auth.login.Configuration - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/security/auth/login/Configuration.class
07:05:24.167 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.shims.Utils$JaasConfiguration - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/shims/Utils$JaasConfiguration.class
07:05:24.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.Formatter - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/Formatter.class
07:05:24.168 pool-1-thread-1 INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_all_databases
07:05:24.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.metrics.Metrics - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/metrics/Metrics.class
07:05:24.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.MalformedObjectNameException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/MalformedObjectNameException.class
07:05:24.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.metrics.MetricsMBean - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/metrics/MetricsMBean.class
07:05:24.168 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.DynamicMBean - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/DynamicMBean.class
07:05:24.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.metrics.Metrics$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/metrics/Metrics$1.class
07:05:24.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/metrics/MetricsMBeanImpl.class
07:05:24.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.ReflectionException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/ReflectionException.class
07:05:24.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.AttributeNotFoundException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/AttributeNotFoundException.class
07:05:24.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.MBeanException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/MBeanException.class
07:05:24.169 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.InvalidAttributeValueException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/InvalidAttributeValueException.class
07:05:24.170 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: javax.management.MBeanOperationInfo - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/javax/management/MBeanOperationInfo.class
07:05:24.170 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:24.170 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:24.170 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getDatabases(ObjectStore.java:677)
07:05:24.170 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.JDOQLSingleStringParser - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/JDOQLSingleStringParser.class
07:05:24.170 pool-1-thread-1 DEBUG Query: JDOQL Single-String with "select name from org.apache.hadoop.hive.metastore.model.MDatabase where ( name.matches("(?i)..*"))"
07:05:24.170 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.JDOQLSingleStringParser$Compiler - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/JDOQLSingleStringParser$Compiler.class
07:05:24.171 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.query.JDOQLSingleStringParser$Parser - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/query/JDOQLSingleStringParser$Parser.class
07:05:24.172 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT name FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE ( name.matches("(?i)..*")) ORDER BY name ascending"
07:05:24.172 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:24.172 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [result:PrimaryExpression{name}]
  [filter:InvokeExpression{[PrimaryExpression{name}].matches(Literal{(?i)..*})}]
  [ordering:OrderExpression{PrimaryExpression{name} ascendingnull}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MDatabase]
07:05:24.172 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT name FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE ( name.matches("(?i)..*")) ORDER BY name ascending" for datastore
07:05:24.172 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.query.StatementResultMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/query/StatementResultMapping.class
07:05:24.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.method.StringMatchesDerbyMethod - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/method/StringMatchesDerbyMethod.class
07:05:24.173 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.method.StringMatchesMethod - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/method/StringMatchesMethod.class
07:05:24.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.method.AbstractSQLMethod - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/method/AbstractSQLMethod.class
07:05:24.174 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.method.SQLMethod - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/method/SQLMethod.class
07:05:24.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.util.RegularExpressionConverter - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/util/RegularExpressionConverter.class
07:05:24.175 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.method.StringToLowerMethod - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/method/StringToLowerMethod.class
07:05:24.176 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.method.SimpleStringMethod - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/method/SimpleStringMethod.class
07:05:24.176 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 4 ms
07:05:24.176 pool-1-thread-1 DEBUG Query: SELECT name FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE ( name.matches("(?i)..*")) ORDER BY name ascending Query compiled to datastore query "SELECT A0."NAME" AS NUCORDER0 FROM DBS A0 WHERE LOWER(A0."NAME") LIKE '_%' ESCAPE '\' ORDER BY NUCORDER0"
07:05:24.176 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5de22678" opened with isolation level "read-committed" and auto-commit=false
07:05:24.176 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@10405242, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:24.176 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@10405242 is starting for transaction Xid=    with flags 0
07:05:24.176 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3e6b068b [conn=com.jolbox.bonecp.ConnectionHandle@5de22678, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.177 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT name FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE ( name.matches("(?i)..*")) ORDER BY name ascending" ...
07:05:24.177 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.SimpleStringOperatorNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/SimpleStringOperatorNode.class
07:05:24.179 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@36e365f7"
07:05:24.179 pool-1-thread-1 DEBUG Native: SELECT A0."NAME" AS NUCORDER0 FROM DBS A0 WHERE LOWER(A0."NAME") LIKE '_%' ESCAPE '\' ORDER BY NUCORDER0
07:05:24.179 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:24.180 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 3 ms
07:05:24.180 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getDatabases(ObjectStore.java:701)
07:05:24.180 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:24.180 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:24.180 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:24.180 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:24.180 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:24.180 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@10405242]]
07:05:24.180 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@10405242 is committing for transaction Xid=    with onePhase=true
07:05:24.180 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@10405242 committed connection for transaction Xid=    with onePhase=true
07:05:24.180 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5de22678" closed
07:05:24.180 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3e6b068b [conn=com.jolbox.bonecp.ConnectionHandle@5de22678, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.180 pool-1-thread-1 DEBUG Transaction: Transaction committed in 0 ms
07:05:24.180 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.MetaStoreEndFunctionContext - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/MetaStoreEndFunctionContext.class
07:05:24.181 pool-1-thread-1 INFO HiveMetaStore: 0: get_functions: db=default pat=*
07:05:24.181 pool-1-thread-1 INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*
07:05:24.181 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:24.181 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:24.181 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getFunctions(ObjectStore.java:7013)
07:05:24.181 pool-1-thread-1 DEBUG Query: JDOQL Single-String with "select functionName from org.apache.hadoop.hive.metastore.model.MFunction where database.name == dbName && ( functionName.matches("(?i).*"))"
07:05:24.181 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT functionName FROM org.apache.hadoop.hive.metastore.model.MFunction WHERE database.name == dbName && ( functionName.matches("(?i).*")) PARAMETERS java.lang.String dbName ORDER BY functionName ascending"
07:05:24.181 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time = 0 ms
07:05:24.181 pool-1-thread-1 DEBUG Query: QueryCompilation:
  [result:PrimaryExpression{functionName}]
  [filter:DyadicExpression{DyadicExpression{PrimaryExpression{database.name}  =  ParameterExpression{dbName}}  AND  InvokeExpression{[PrimaryExpression{functionName}].matches(Literal{(?i).*})}}]
  [ordering:OrderExpression{PrimaryExpression{functionName} ascendingnull}]
  [symbols: dbName type=java.lang.String, this type=org.apache.hadoop.hive.metastore.model.MFunction]
07:05:24.181 pool-1-thread-1 DEBUG Query: JDOQL Query : Compiling "SELECT functionName FROM org.apache.hadoop.hive.metastore.model.MFunction WHERE database.name == dbName && ( functionName.matches("(?i).*")) PARAMETERS java.lang.String dbName ORDER BY functionName ascending" for datastore
07:05:24.182 pool-1-thread-1 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
07:05:24.182 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Class : org.apache.hadoop.hive.metastore.model.MFunction [Table : FUNCS, InheritanceStrategy : new-table]
07:05:24.182 pool-1-thread-1 DEBUG Schema: Column "FUNCS.FUNC_ID" added to internal representation of table.
07:05:24.182 pool-1-thread-1 DEBUG Schema: Table FUNCS will manage the persistence of the fields for class org.apache.hadoop.hive.metastore.model.MFunction (inheritance strategy="new-table")
07:05:24.182 pool-1-thread-1 DEBUG Schema: Column "FUNCS.CLASS_NAME" added to internal representation of table.
07:05:24.182 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.className] -> Column(s) [FUNCS.CLASS_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.182 pool-1-thread-1 DEBUG Schema: Column "FUNCS.CREATE_TIME" added to internal representation of table.
07:05:24.182 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.createTime] -> Column(s) [FUNCS.CREATE_TIME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:24.182 pool-1-thread-1 DEBUG Schema: Column "FUNCS.DB_ID" added to internal representation of table.
07:05:24.182 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.database] -> Column(s) [FUNCS.DB_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:24.182 pool-1-thread-1 DEBUG Schema: Column "FUNCS.FUNC_NAME" added to internal representation of table.
07:05:24.182 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.functionName] -> Column(s) [FUNCS.FUNC_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.182 pool-1-thread-1 DEBUG Schema: Column "FUNCS.FUNC_TYPE" added to internal representation of table.
07:05:24.182 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.functionType] -> Column(s) [FUNCS.FUNC_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:24.182 pool-1-thread-1 DEBUG Schema: Column "FUNCS.OWNER_NAME" added to internal representation of table.
07:05:24.182 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.ownerName] -> Column(s) [FUNCS.OWNER_NAME] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.182 pool-1-thread-1 DEBUG Schema: Column "FUNCS.OWNER_TYPE" added to internal representation of table.
07:05:24.182 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.ownerType] -> Column(s) [FUNCS.OWNER_TYPE] using mapping of type "org.datanucleus.store.rdbms.mapping.java.StringMapping" (org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.182 pool-1-thread-1 DEBUG Persistence: Managing Persistence of Field : org.apache.hadoop.hive.metastore.model.MFunction.resourceUris [Table : FUNC_RU]
07:05:24.182 pool-1-thread-1 DEBUG Schema: Field [org.apache.hadoop.hive.metastore.model.MFunction.resourceUris] -> Column(s) [[none]] using mapping of type "org.datanucleus.store.rdbms.mapping.java.CollectionMapping" ()
07:05:24.182 pool-1-thread-1 DEBUG Schema: Table/View FUNCS has been initialised
07:05:24.183 pool-1-thread-1 DEBUG Schema: Column "FUNC_RU.FUNC_ID" added to internal representation of table.
07:05:24.183 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MFunction.resourceUris] -> Column(s) [FUNC_RU.FUNC_ID] using mapping of type "org.datanucleus.store.rdbms.mapping.java.PersistableMapping" (org.datanucleus.store.rdbms.mapping.datastore.BigIntRDBMSMapping)
07:05:24.183 pool-1-thread-1 DEBUG Schema: Column "FUNC_RU.RESOURCE_TYPE" added to internal representation of table.
07:05:24.183 pool-1-thread-1 DEBUG Schema: Column "FUNC_RU.RESOURCE_URI" added to internal representation of table.
07:05:24.183 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MFunction.resourceUris] -> Column(s) [FUNC_RU.RESOURCE_TYPE,FUNC_RU.RESOURCE_URI] using mapping of type "org.datanucleus.store.rdbms.mapping.java.EmbeddedElementPCMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping,org.datanucleus.store.rdbms.mapping.datastore.VarCharRDBMSMapping)
07:05:24.183 pool-1-thread-1 DEBUG Schema: Column "FUNC_RU.INTEGER_IDX" added to internal representation of table.
07:05:24.183 pool-1-thread-1 DEBUG Datastore: Field [org.apache.hadoop.hive.metastore.model.MFunction.resourceUris] -> Column(s) [FUNC_RU.INTEGER_IDX] using mapping of type "org.datanucleus.store.rdbms.mapping.java.IntegerMapping" (org.datanucleus.store.rdbms.mapping.datastore.IntegerRDBMSMapping)
07:05:24.183 pool-1-thread-1 DEBUG Schema: Table/View FUNC_RU has been initialised
07:05:24.183 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@312b2ae2" opened with isolation level "serializable" and auto-commit=false
07:05:24.183 pool-1-thread-1 DEBUG Schema: Schema Transaction started with connection "com.jolbox.bonecp.ConnectionHandle@312b2ae2" with isolation "serializable"
07:05:24.183 pool-1-thread-1 DEBUG Schema: Check of existence of FUNCS returned no table
07:05:24.183 pool-1-thread-1 DEBUG Schema: Creating table FUNCS
07:05:24.183 pool-1-thread-1 DEBUG Schema: CREATE TABLE FUNCS
(
    FUNC_ID BIGINT NOT NULL,
    CLASS_NAME VARCHAR(4000),
    CREATE_TIME INTEGER NOT NULL,
    DB_ID BIGINT,
    FUNC_NAME VARCHAR(128),
    FUNC_TYPE INTEGER NOT NULL,
    OWNER_NAME VARCHAR(128),
    OWNER_TYPE VARCHAR(10)
)
07:05:24.185 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:24.185 pool-1-thread-1 DEBUG Schema: ALTER TABLE FUNCS ADD CONSTRAINT FUNCS_PK PRIMARY KEY (FUNC_ID)
07:05:24.188 pool-1-thread-1 DEBUG Schema: Execution Time = 3 ms
07:05:24.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.189 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.190 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.191 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.191 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.191 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.192 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.192 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.192 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.192 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.193 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.193 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.193 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.194 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.194 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.194 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.194 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.195 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.195 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.195 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.195 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.196 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.196 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.196 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.200 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.201 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.201 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.201 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.202 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.203 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.204 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.204 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.205 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.206 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.207 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.207 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.207 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.207 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.208 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.208 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.210 pool-1-thread-1 DEBUG Schema: Check of existence of FUNC_RU returned no table
07:05:24.210 pool-1-thread-1 DEBUG Schema: Creating table FUNC_RU
07:05:24.210 pool-1-thread-1 DEBUG Schema: CREATE TABLE FUNC_RU
(
    FUNC_ID BIGINT NOT NULL,
    RESOURCE_TYPE INTEGER NOT NULL,
    RESOURCE_URI VARCHAR(4000),
    INTEGER_IDX INTEGER NOT NULL
)
07:05:24.212 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:24.212 pool-1-thread-1 DEBUG Schema: ALTER TABLE FUNC_RU ADD CONSTRAINT FUNC_RU_PK PRIMARY KEY (FUNC_ID,INTEGER_IDX)
07:05:24.214 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:24.214 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.215 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.215 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.215 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.216 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.216 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.216 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.216 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.217 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.217 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.217 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.217 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.217 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.218 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.219 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.220 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.220 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.220 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.221 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.221 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.221 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:24.222 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [I - null
07:05:24.223 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:24.225 pool-1-thread-1 DEBUG Schema: Creating index "FUNCS_N49" in catalog "" schema ""
07:05:24.225 pool-1-thread-1 DEBUG Schema: CREATE INDEX FUNCS_N49 ON FUNCS (DB_ID)
07:05:24.227 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:24.227 pool-1-thread-1 DEBUG Schema: Creating index "UniqueFunction" in catalog "" schema ""
07:05:24.227 pool-1-thread-1 DEBUG Schema: CREATE UNIQUE INDEX UNIQUEFUNCTION ON FUNCS (FUNC_NAME,DB_ID)
07:05:24.228 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:24.228 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "FUNCS_FK1" in catalog "" schema ""
07:05:24.228 pool-1-thread-1 DEBUG Schema: ALTER TABLE FUNCS ADD CONSTRAINT FUNCS_FK1 FOREIGN KEY (DB_ID) REFERENCES DBS (DB_ID)
07:05:24.230 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:24.230 pool-1-thread-1 DEBUG Schema: Creating index "FUNC_RU_N49" in catalog "" schema ""
07:05:24.230 pool-1-thread-1 DEBUG Schema: CREATE INDEX FUNC_RU_N49 ON FUNC_RU (FUNC_ID)
07:05:24.231 pool-1-thread-1 DEBUG Schema: Execution Time = 1 ms
07:05:24.231 pool-1-thread-1 DEBUG Schema: Creating foreign key constraint : "FUNC_RU_FK1" in catalog "" schema ""
07:05:24.231 pool-1-thread-1 DEBUG Schema: ALTER TABLE FUNC_RU ADD CONSTRAINT FUNC_RU_FK1 FOREIGN KEY (FUNC_ID) REFERENCES FUNCS (FUNC_ID)
07:05:24.233 pool-1-thread-1 DEBUG Schema: Execution Time = 2 ms
07:05:24.233 pool-1-thread-1 DEBUG Schema: Schema Transaction committing with connection "com.jolbox.bonecp.ConnectionHandle@312b2ae2"
07:05:24.233 pool-1-thread-1 DEBUG Schema: Schema Transaction closing with connection "com.jolbox.bonecp.ConnectionHandle@312b2ae2"
07:05:24.233 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@312b2ae2" non enlisted to a transaction is being committed.
07:05:24.234 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@312b2ae2" closed
07:05:24.234 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.ByteLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/ByteLiteral.class
07:05:24.234 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.expression.FloatingPointLiteral - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/expression/FloatingPointLiteral.class
07:05:24.235 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.sql.SQLJoin - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/sql/SQLJoin.class
07:05:24.236 pool-1-thread-1 DEBUG Query: JDOQL Query : Compile Time for datastore = 55 ms
07:05:24.236 pool-1-thread-1 DEBUG Query: SELECT functionName FROM org.apache.hadoop.hive.metastore.model.MFunction WHERE database.name == dbName && ( functionName.matches("(?i).*")) PARAMETERS java.lang.String dbName ORDER BY functionName ascending Query compiled to datastore query "SELECT A0.FUNC_NAME AS NUCORDER0 FROM FUNCS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE B0."NAME" = ? AND LOWER(A0.FUNC_NAME) LIKE '%' ESCAPE '\' ORDER BY NUCORDER0"
07:05:24.236 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5c265a48" opened with isolation level "read-committed" and auto-commit=false
07:05:24.236 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@25b309bc, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:24.236 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@25b309bc is starting for transaction Xid=    with flags 0
07:05:24.236 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@77fddd1f [conn=com.jolbox.bonecp.ConnectionHandle@5c265a48, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.236 pool-1-thread-1 DEBUG Query: JDOQL Query : Executing "SELECT functionName FROM org.apache.hadoop.hive.metastore.model.MFunction WHERE database.name == dbName && ( functionName.matches("(?i).*")) PARAMETERS java.lang.String dbName ORDER BY functionName ascending" ...
07:05:24.236 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.compile.HalfOuterJoinNode - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/compile/HalfOuterJoinNode.class
07:05:24.242 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6931e586"
07:05:24.242 pool-1-thread-1 DEBUG Native: SELECT A0.FUNC_NAME AS NUCORDER0 FROM FUNCS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE B0."NAME" = <'default'> AND LOWER(A0.FUNC_NAME) LIKE '%' ESCAPE '\' ORDER BY NUCORDER0
07:05:24.243 pool-1-thread-1 DEBUG Retrieve: Execution Time = 1 ms
07:05:24.243 pool-1-thread-1 DEBUG Query: JDOQL Query : Execution Time = 7 ms
07:05:24.243 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getFunctions(ObjectStore.java:7041)
07:05:24.243 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:24.243 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:24.243 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:24.243 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:24.243 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:24.243 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@25b309bc]]
07:05:24.243 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@25b309bc is committing for transaction Xid=    with onePhase=true
07:05:24.243 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@25b309bc committed connection for transaction Xid=    with onePhase=true
07:05:24.243 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5c265a48" closed
07:05:24.243 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@77fddd1f [conn=com.jolbox.bonecp.ConnectionHandle@5c265a48, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:24.243 pool-1-thread-1 DEBUG Transaction: Transaction committed in 0 ms
07:05:24.244 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.permission.FsPermission
07:05:24.244 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities.class
07:05:24.247 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$1.class
07:05:24.247 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$3 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$3.class
07:05:24.247 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$4 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$4.class
07:05:24.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$5 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$5.class
07:05:24.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hive.com.esotericsoftware.kryo.Serializer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/com/esotericsoftware/kryo/Serializer.class
07:05:24.248 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/com/esotericsoftware/kryo/serializers/FieldSerializer.class
07:05:24.249 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.HiveOutputFormat - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/HiveOutputFormat.class
07:05:24.249 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.OutputFormat
07:05:24.250 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.ReduceWork - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/ReduceWork.class
07:05:24.250 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.zip.DeflaterOutputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/zip/DeflaterOutputStream.class
07:05:24.251 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.FSDataOutputStream
07:05:24.251 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.util.zip.InflaterInputStream - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/util/zip/InflaterInputStream.class
07:05:24.251 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.FSDataInputStream
07:05:24.251 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$DatePersistenceDelegate - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$DatePersistenceDelegate.class
07:05:24.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$TimestampPersistenceDelegate - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$TimestampPersistenceDelegate.class
07:05:24.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$MapDelegate - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$MapDelegate.class
07:05:24.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$ListDelegate - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$ListDelegate.class
07:05:24.252 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$CommonTokenDelegate - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$CommonTokenDelegate.class
07:05:24.253 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$PathDelegate - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$PathDelegate.class
07:05:24.253 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.compress.CompressionOutputStream
07:05:24.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.HiveInterruptCallback - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/HiveInterruptCallback.class
07:05:24.254 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: java.sql.SQLTransientException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/sql/SQLTransientException.class
07:05:24.254 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.Writable
07:05:24.255 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$ReduceField - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$ReduceField.class
07:05:24.255 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$SqlDateSerializer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$SqlDateSerializer.class
07:05:24.256 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$TimestampSerializer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$TimestampSerializer.class
07:05:24.256 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$PathSerializer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$PathSerializer.class
07:05:24.256 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.objenesis.strategy.InstantiatorStrategy - jar:file:/Users/lian/.ivy2/cache/org.objenesis/objenesis/jars/objenesis-2.1.jar!/org/objenesis/strategy/InstantiatorStrategy.class
07:05:24.257 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.Utilities$CommonTokenSerializer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/Utilities$CommonTokenSerializer.class
07:05:24.257 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.PlanUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/PlanUtils.class
07:05:24.258 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.TableDesc - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/TableDesc.class
07:05:24.259 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.TextInputFormat
07:05:24.260 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/IgnoreKeyTextOutputFormat.class
07:05:24.260 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.TextOutputFormat
07:05:24.261 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/MetadataTypedColumnsetSerDe.class
07:05:24.262 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.AbstractSerDe - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/AbstractSerDe.class
07:05:24.262 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.SerDe - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/SerDe.class
07:05:24.262 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.Deserializer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/Deserializer.class
07:05:24.262 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.Serializer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/Serializer.class
07:05:24.263 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.HiveFileFormatUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.class
07:05:24.263 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.util.Progressable
07:05:24.264 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/HiveIgnoreKeyTextOutputFormat.class
07:05:24.264 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.SequenceFileOutputFormat
07:05:24.265 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/HiveSequenceFileOutputFormat.class
07:05:24.265 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.SequenceFileInputFormat
07:05:24.265 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/SequenceFileInputFormatChecker.class
07:05:24.265 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.InputFormatChecker - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/InputFormatChecker.class
07:05:24.266 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.RCFileInputFormat - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/RCFileInputFormat.class
07:05:24.266 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.FileInputFormat
07:05:24.266 pool-1-thread-1 DEBUG Utilities: Create dirs file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba with permission rwx-wx-wx recursive true
07:05:24.271 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.IOUtils
07:05:24.272 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.FileStatus
07:05:24.277 pool-1-thread-1 DEBUG SessionState: HDFS root scratch dir: file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba with schema file, permission: rwx-wx-wx
07:05:24.281 pool-1-thread-1 INFO SessionState: Created HDFS directory: file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba/lian
07:05:24.285 pool-1-thread-1 INFO SessionState: Created local directory: /Users/lian/local/src/spark/workspace-d/target/tmp/13b870e3-e2a1-47d8-8b39-2580b37ee1dc_resources
07:05:24.290 pool-1-thread-1 INFO SessionState: Created HDFS directory: file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba/lian/13b870e3-e2a1-47d8-8b39-2580b37ee1dc
07:05:24.294 pool-1-thread-1 INFO SessionState: Created local directory: /Users/lian/local/src/spark/workspace-d/target/tmp/lian/13b870e3-e2a1-47d8-8b39-2580b37ee1dc
07:05:24.299 pool-1-thread-1 INFO SessionState: Created HDFS directory: file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba/lian/13b870e3-e2a1-47d8-8b39-2580b37ee1dc/_tmp_space.db
07:05:24.301 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.reflect.ClassTag$
07:05:24.301 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.BoxesRunTime
07:05:24.301 pool-1-thread-1 DEBUG IsolatedClientLoader: Initializing the logger to avoid disaster...
07:05:24.301 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.cli.CliSessionState - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-cli/jars/hive-cli-1.2.1.spark2.jar!/org/apache/hadoop/hive/cli/CliSessionState.class
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.cookie.is.httponly=true
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: nfs3.mountd.port=4242
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.healthchecker.script.timeout=600000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.input.dir.recursive=false
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.orc.compute.splits.num.threads=10
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.classloader.system.classes=java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop.
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3n.awsSecretAccessKey=lkJm1rO2asxYC6VQtuSma3SlElf3Ywiz9r7n8XgU
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.sortmerge.join.to.mapjoin=false
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.reduce.groupby.enabled=true
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.max.partition.factor=2.0
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.framework.name=local
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.script.maxerrsize=100000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.txns=1000000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: tfile.fs.output.buffer.size=262144
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.job.task.listener.thread-count=30
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3a.secret.key=lkJm1rO2asxYC6VQtuSma3SlElf3Ywiz9r7n8XgU
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.local.dir.minspacekill=0
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.support.concurrency=false
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.block.size=67108864
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.repl.task.factory=org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.retries.max=0
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.hdfs.configuration.version=1
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.bytes-per-checksum=512
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.buffer.dir=${hadoop.tmp.dir}/s3
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.acl-view-job=
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.typecheck.on.insert=true
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.loadedjobs.cache.size=5
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.persist.jobstatus.hours=1
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.hybridgrace.memcheckfrequency=1024
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.unlock.numretries=10
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.handler.count=10
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.copyfile.maxsize=33554432
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.split.minsize=1
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.plan.serialization.format=kryo
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.failed.volumes.tolerated=0
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.amliveliness-monitor.interval-ms=1000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.client.thread-count=50
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.seqfile.compress.blocksize=1000000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.http.threads=40
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.explain.dependency.append.tasktype=false
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.retrycache.expirytime.millis=600000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.backup.address=0.0.0.0:50100
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hwi.listen.host=0.0.0.0
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.replication=3
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.jobhistory.block.size=3145728
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.profile.maps=0-2
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.reliable=false
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.global.init.file.location=${env:HIVE_CONF_DIR}
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.io.rcfile.record.buffer.size=4194304
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.retiredjobs.cache.size=1000
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.ppd=true
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.am.max-attempts=2
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.print.current.db=false
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.trash.checkpoint.interval=0
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.check.period=60
07:05:24.345 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.compress.intermediate=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.rdbms.datastoreAdapterClassName=org.datanucleus.store.rdbms.adapter.DerbyAdapter
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.container-monitor.interval-ms=3000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hashtable.loadfactor=0.75
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.test.mode.samplefreq=32
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.jetty.logs.serve.aliases=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.groupby.checkinterval=100000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.max-completed-applications=10000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.skip.proc.count.autoincr=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.identifierFactory=datanucleus1
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.decode.partition.name=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hbase.generatehfiles=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.fallback-to-simple-auth-allowed=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.localize.resource.wait.interval=5000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.file.max.footer=100
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.fs-limits.max-component-length=0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.DetachAllOnCommit=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.constant.propagation=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.health-monitor.check-interval.ms=1000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.prompt=hive
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.check.crossproducts=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.connect.timeout=1000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.explain.user=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.tasktracker.maxblacklists=4
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapred.child.java.opts=-Xmx200m
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.common.configuration.version=0.23.0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.remote-app-log-dir-suffix=logs
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.concatenate.check.index=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.blocksize=67108864
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.convert.join.bucket.mapjoin.tez=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hmshandler.retry.interval=2000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.binary.record.max.length=1000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.profile.reduces=0-2
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.gather.num.threads=10
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.address=0.0.0.0:50010
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.https.server.keystore.resource=ssl-server.xml
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.job.debug.timeout=30000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.script.allow.partial.consumption=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.memory.pool=0.5
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.rpc.threads=8
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.row.index.stride=10000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.submit.local.task.via.child=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.NonTransactionalRead=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.aggr.hash.min.reduction=0.5
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.test.authz.sstd.hs2.mode=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.timeout=600000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.entity.separator=@
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.insert.into.external.tables=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.connect.max-wait.ms=900000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.defaultFS=file:///
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.transactionIsolation=read-committed
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.compression.codec.bzip2.library=system-native
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.audit.loggers=default
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.warehouse.subdir.inherit.perms=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.sample.seednumber=0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.split.strategy=HYBRID
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.default.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.block.access.key.update.interval=600
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.authentication=NONE
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.block.padding=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.full=0.9
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.returnpath.hiveop=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.max.objects=0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: stream.stderr.reporter.prefix=reporter:
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.address=0.0.0.0:10020
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.address=${yarn.nodemanager.hostname}:0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.dbconnectionstring=jdbc:derby:;databaseName=TempStatsStore;create=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.smallfiles.avgsize=16000000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.am.max-attempts=2
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.rcfile.use.explicit.header=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.orm.retrieveMapNullsAsEmptyStrings=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.clean.extra.nodes=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hbase.snapshot.restoredir=/tmp
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authorization.sqlstd.confwhitelist=hive\.auto\..*|hive\.cbo\..*|hive\.convert\..*|hive\.exec\.dynamic\.partition.*|hive\.exec\..*\.dynamic\.partitions\..*|hive\.exec\.compress\..*|hive\.exec\.infer\..*|hive\.exec\.mode.local\..*|hive\.exec\.orc\..*|hive\.exec\.parallel.*|hive\.explain\..*|hive\.fetch.task\..*|hive\.groupby\..*|hive\.hbase\..*|hive\.index\..*|hive\.index\..*|hive\.intermediate\..*|hive\.join\..*|hive\.limit\..*|hive\.log\..*|hive\.mapjoin\..*|hive\.merge\..*|hive\.optimize\..*|hive\.orc\..*|hive\.outerjoin\..*|hive\.parquet\..*|hive\.ppd\..*|hive\.prewarm\..*|hive\.server2\.proxy\.user|hive\.skewjoin\..*|hive\.smbjoin\..*|hive\.stats\..*|hive\.tez\..*|hive\.vectorized\..*|mapred\.map\..*|mapred\.reduce\..*|mapred\.output\.compression\.codec|mapred\.job\.queuename|mapred\.output\.compression\.type|mapred\.min\.split\.size|mapreduce\.job\.reduce\.slowstart\.completedmaps|mapreduce\.job\.queuename|mapreduce\.job\.tags|mapreduce\.input\.fileinputformat\.split\.minsize|mapreduce\.map\..*|mapreduce\.reduce\..*|mapreduce\.output\.fileoutputformat\.compress\.codec|mapreduce\.output\.fileoutputformat\.compress\.type|tez\.am\..*|tez\.task\..*|tez\.runtime\..*|tez.queue.name|hive\.exec\.reducers\.bytes\.per\.reducer|hive\.client\.stats\.counters|hive\.exec\.default\.partition\.name|hive\.exec\.drop\.ignorenonexistent|hive\.counters\.group\.name|hive\.default\.fileformat\.managed|hive\.enforce\.bucketing|hive\.enforce\.bucketmapjoin|hive\.enforce\.sorting|hive\.enforce\.sortmergebucketmapjoin|hive\.cache\.expr\.evaluation|hive\.hashtable\.loadfactor|hive\.hashtable\.initialCapacity|hive\.ignore\.mapjoin\.hint|hive\.limit\.row\.max\.size|hive\.mapred\.mode|hive\.map\.aggr|hive\.compute\.query\.using\.stats|hive\.exec\.rowoffset|hive\.variable\.substitute|hive\.variable\.substitute\.depth|hive\.autogen\.columnalias\.prefix\.includefuncname|hive\.autogen\.columnalias\.prefix\.label|hive\.exec\.check\.crossproducts|hive\.compat|hive\.exec\.concatenate\.check\.index|hive\.display\.partition\.cols\.separately|hive\.error\.on\.empty\.partition|hive\.execution\.engine|hive\.exim\.uri\.scheme\.whitelist|hive\.file\.max\.footer|hive\.mapred\.supports\.subdirectories|hive\.insert\.into\.multilevel\.dirs|hive\.localize\.resource\.num\.wait\.attempts|hive\.multi\.insert\.move\.tasks\.share\.dependencies|hive\.support\.quoted\.identifiers|hive\.resultset\.use\.unique\.column\.names|hive\.analyze\.stmt\.collect\.partlevel\.stats|hive\.server2\.logging\.operation\.level|hive\.support\.sql11\.reserved\.keywords|hive\.exec\.job\.debug\.capture\.stacktraces|hive\.exec\.job\.debug\.timeout|hive\.exec\.max\.created\.files|hive\.exec\.reducers\.max|hive\.reorder\.nway\.joins|hive\.output\.file\.extension|hive\.exec\.show\.job\.failure\.debug\.info|hive\.exec\.tasklog\.debug\.timeout
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.reducers.bytes.per.reducer=256000000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.start.cleanup.scratchdir=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.drop.cache.behind.reads=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.auto.reducer.parallelism=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.size=10000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.cache.level2=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.try.direct.sql=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.bucket.cache.size=100
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateColumns=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.block.write.replace-datanode-on-failure.enable=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.bytes-per-checksum=512
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.resource.memory-mb=8192
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.local.fs.read=4.0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.sampling.orderby=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.heartbeat.interval=3
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.joblist.cache.size=20000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hmshandler.force.reload.conf=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.ftp.host=0.0.0.0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.ha.tail-edits.period=60
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.dynamic.partition=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hwi.war.file=${env:HWI_WAR_FILE}
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.resultset.use.unique.column.names=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.expire.trackers.interval=600000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.default.fileformat.managed=none
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.ha.fencing.ssh.connect-timeout=30000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.log-aggregation-enable=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.markreset.buffer.percent=0.0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapper.cannot.span.multiple.partitions=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.future.timeout=60
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.edits.noeditlogchannelflush=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.support.append=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.io.sort.factor=10
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.outofband.heartbeat=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.failover-controller.new-active.rpc-timeout.ms=60000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.datestring.cache.size=200000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.acl-modify-job=
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.https-address=0.0.0.0:50470
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.ppd.remove.duplicatefilters=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.job.committer.commit-window=10000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.container-manager.thread-count=20
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.integral.jdo.pushdown=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.zookeeper.session-timeout.ms=5000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: tfile.io.chunk.size=1048576
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.speculative.slowtaskthreshold=1.0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.initiator.on=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.directory.search.timeout=10000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.ha.automatic-failover.enabled=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.warehouse.dir=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.decommission.interval=30
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.local-cache.max-files-per-directory=8192
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.direct.sql.batch.size=0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.handler.count=10
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.client.drop.partitions.using.expressions=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.safemode.threshold-pct=0.999f
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.batch.retrieve.table.partition.max=1000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.sort.spill.percent=0.80
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.metadataonly=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.sync.behind.writes=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.ttl=600
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.stale.datanode.interval=30000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.ifile.readahead=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.hybridgrace.minwbsize=524288
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.max.dynamic.partitions.pernode=100
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.orc.splits.include.file.footer=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.smbjoin.cache.rows=10000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.display.partition.cols.separately=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.groupby.sorted=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.resourcemanager.connect.wait.secs=900
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.dynamic.partition.pruning=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.enabled=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.command-opts=-Xmx1024m
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.outerjoin.supports.filters=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.logging.operation.enabled=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.cluster.local.dir=${hadoop.tmp.dir}/mapred/local
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.mapfile.bloom.error.rate=0.005
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.bucketmapjoin=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.replication=3
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.uid.cache.secs=14400
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.maxtaskfailures.per.tracker=3
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.enforce.bucketmapjoin=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.skip.checksum.errors=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.server.max.message.size=104857600
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.directoryscan.interval=21600
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: nfs3.server.port=2049
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.dbclass=fs
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.write.stale.datanode.ratio=0.5f
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.hdfs.write=10.0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.user.install.directory=hdfs:///user/
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.connection.max.retries=3
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.downloaded.resources.dir=/Users/lian/local/src/spark/workspace-d/target/tmp/${hive.session.id}_resources
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.taskcache.levels=2
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.http.policy=HTTP_ONLY
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.cpu.vcores=-1
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.token.validity=36000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.writer.wait=5000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.shuffle.max.connections=0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.enforce.sortmergebucketmapjoin=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.thrift.compact.protocol.enabled=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.dictionary.key.size.threshold=0.8
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.mode.local.auto.inputbytes.max=134217728
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.move.thread-count=3
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.admin.client.thread-count=1
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.persist.jobstatus.active=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.sparkfiles=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.txn.max.open.batch=1000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.uris=
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.expression.proxy=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.io.rcfile.tolerate.corruptions=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.shuffle.port=13562
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.hybridgrace.hashtable=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.job.debug.capture.stacktraces=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.log.explain.output=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.health-checker.interval-ms=600000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.report.address=127.0.0.1:0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateTables=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.in.test=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.seqfile.lazydecompress=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.blocksize=67108864
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.smb.number.waves=0.5
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.backup.http-address=0.0.0.0:50105
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.groupby.sorted.testmode=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.distinct.rewrite=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.rpc.max.size=52428800
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.delete.debug-delay-sec=0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.filter.hook=org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.http.address=0.0.0.0:50030
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.enable.doAs=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.pmem-check-enabled=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server.read.socket.timeout=10
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.tez.sessions.per.default.queue=1
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.groups.cache.secs=300
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.autoupdate=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.variance=0.01
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.server.tcpnodelay=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.authorization=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.complete.cancel.delegation.tokens=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.http.policy=HTTP_ONLY
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.dns.interface=default
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.replication=3
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.ssl=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.rdbms.useLegacyNativeValueStrategy=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.max.worker.threads=500
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.dynamic.partition.pruning.max.event.size=1048576
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.key.prefix.reserve.length=24
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.server.max.threads=1000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.secret.bits=256
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.insert.into.multilevel.dirs=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.reducers.max=1009
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.parallel.thread.number=8
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.query.max.table.partition=-1
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.bin.path=/Users/lian/local/opt/hadoop/bin/hadoop
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.zookeeper.parent-znode=/hadoop-ha
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.safemode.extension=30000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.transport.mode=binary
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.merge.percent=0.66
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.blocksize=134217728
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.schema.verification=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.localize.resource.num.wait.attempts=5
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.idle.operation.timeout=432000000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.autogen.columnalias.prefix.label=_c
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.admin.acl=*
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compat=0.12
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.compression.strategy=SPEED
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.jdbc.timeout=30
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.skip.maxgroups=0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.connect.timeout=180000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.shuffle.ssl.file.buffer.size=65536
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.tez.initialize.default.sessions=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.journalnode.http-address=0.0.0.0:8480
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.enabled=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.transform.escape.input=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.end-notification.max.attempts=5
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.jobname.length=50
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.cleaner.run.interval=5000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.edits.dir=${dfs.namenode.name.dir}
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.health-monitor.connect-retry-interval.ms=1000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.batch.retrieve.max=300
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.keytab=/etc/krb5.keytab
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.support.allow.format=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.connect.retries=3
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.event.db.listener.timetolive=86400
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.authorization.storage.checks=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.async.exec.keepalive.time=10
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.input.buffer.percent=0.70
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.replication=3
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.try.direct.sql.ddl=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.rowoffset=false
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.long.polling.timeout=5000
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.connection.retries=0
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.allow.user.substitution=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.querylog.enable.plan.progress=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.ppd.recognizetransivity=true
07:05:24.346 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.tmp.dir=/tmp/hadoop-${user.name}
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.maps=2
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.index.compact.query.max.size=10737418240
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.secondary.http-address=0.0.0.0:50090
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.end-notification.max.retry.interval=5000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.log-aggregation.retain-check-interval-seconds=-1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.resource-tracker.client.thread-count=50
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.collect.rawdatasize=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.ipc.serializer.type=protocolbuffers
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.hybridgrace.minnumpartitions=16
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.test.mode=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.seqfile.local.dir=${hadoop.tmp.dir}/io/local
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.list.num.entries=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.client.submit.file.replication=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.minicluster.fixed.ports=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.print.header=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.counters.group.name=HIVE
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.data.dir.perm=700
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.session.history.enabled=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/name
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.rpc.sasl.mechanisms=DIGEST-MD5
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.namespace=hive_zookeeper_namespace
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.idlethreshold=4000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lazysimple.extended_boolean_literal=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.input.buffer.percent=0.0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cluster.delegation.token.store.zookeeper.znode=/hivedelegation
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.ftp.host.port=21
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.num.checkpoints.retained=2
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.dml.events=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.client-write-packet-size=65536
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.async.exec.wait.queue.size=100
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.localtask.max.memory.usage=0.9
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.file-block-storage-locations.timeout=60
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.kill.max=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.session.silent=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.speculative=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.temporary.table.storage=default
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compute.splits.in.am=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.disk-health-checker.interval-ms=120000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.archive.enabled=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.connection.maxidletime=10000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.io.sort.mb=100
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.client.thread-count=5
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hashtable.key.count.adjustment=1.0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.max-retries=3
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.in.tez.test=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.connectionPoolingType=BONECP
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.delegation.token.max-lifetime=604800000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.process-kill-wait.ms=2000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.num.extra.edits.retained=1000000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.hdfs-servers=${fs.defaultFS}
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.ignore.mapjoin.hint=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.df.interval=60000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.groupby.mapaggr.checkinterval=100000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.sleepTimeSeconds=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.file.buffer.size=65536
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.tasklog.debug.timeout=20000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.check.memory.rows=100000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.collect.scancols=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.groupby=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.work.around.non.threadsafe.getpwuid=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.permissions.superusergroup=supergroup
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.search.attr.member=member
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.dns.interface=default
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.groupby.maxentries=1000000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fetch.task.conversion.threshold=1073741824
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.permissions.enabled=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.connect.retry-interval.ms=30000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.ssl.protocol.blacklist=SSLv2,SSLv3
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.scheduler.minimum-allocation-mb=1024
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.read.timeout=180000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.https.address=0.0.0.0:50475
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.mode.local.auto.input.files.max=4
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.support.sql11.reserved.keywords=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.abortedtxn.threshold=1000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.execute.setugi=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.querylog.plan.progress.interval=60000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.io.rcfile.column.number.conf=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.require.client.cert=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.kerberos.kinit.command=kinit
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.fuse.connection.timeout=300
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.worker.keepalive.time=60
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.log.level=INFO
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.hdfs.read=1.5
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.ubertask.enable=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.skewjoin.mapjoin.min.split=33554432
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.rcfile.block.level=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.prewarm.enabled=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.vmem-pmem-ratio=2.1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.metastore.authorization.auth.reads=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.rpc.protection=authentication
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.health-monitor.rpc-timeout.ms=45000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.stream-buffer-size=4096
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.remote-app-log-dir=/tmp/logs
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.check.interval=300
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.enabled=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3a.access.key=AKIAJTMRDVCTJGAZZECQ
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.instrumentation.requires.admin=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.delete.thread-count=4
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.txn.timeout=300
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.balance.bandwidthPerSec=1048576
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.udtf.auto.progress=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.name.dir.restore=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.sleep.max.millis=15000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.blocksize=67108864
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.script.trust=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.sasl.enabled=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionUserName=APP
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.scratch.dir.permission=700
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.map.index.interval=128
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.login.timeout=20
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.counters.max=120
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.event.clean.freq=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.reducededuplication.min.reducer=4
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.move.interval-ms=180000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authorization.task.factory=org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.operator.truncate.env=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.orcfile.stripe.level=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.parquet.timestamp.skip.conversion=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server.tcp.keepalive=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.fetch.thread-count=4
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.client.thread-count=50
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.querylog.location=/Users/lian/local/src/spark/workspace-d/target/tmp/lian
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.mode.local.auto=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.hostname.verifier=DEFAULT
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.classloader=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.enforce.sorting=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.connect.timeout=20000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.stream-buffer-size=4096
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.nm.liveness-monitor.interval-ms=1000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nm.liveness-monitor.expiry-interval-ms=600000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.bytes-per-checksum=512
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.client.connect.retry.delay=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.pushdown.memory.usage=-1.0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.path=cliservice
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.aggr.hash.percentmemory=0.5
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.fs-limits.max-directory-items=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.delta.pct.threshold=0.1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.smalltable.filesize=25000000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.delegation.token.renew-interval=86400000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.address=local
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.end-notification.retry.interval=1000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: parquet.memory.pool.ratio=0.5
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.delta.num.threshold=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.command.whitelist=set,reset,dfs,add,list,delete,reload,compile
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.bytes-per-checksum=512
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.period=3600
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hadoop.supports.splittable.combineinputformat=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.new.job.grouping.set.cardinality=30
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.block.padding.tolerance=0.05
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: net.topology.script.number.args=100
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.merge.progress.records=10000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.skewjoin.key=100000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.optimize.limit.file=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cache.expr.evaluation=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.deserialization.factor=1.0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.groupby=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.client.max-nodemanagers-proxies=500
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.session.check.interval=21600000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.index.compact.query.max.entries=10000000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.optimized.hashtable.wbsize=10485760
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.skip.maxrecords=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.decommission.nodes.per.interval=5
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.show.job.failure.debug.info=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.thrift.framed.transport.enabled=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.port=10001
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.handler.count=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.type=simple
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.taskscheduler=org.apache.hadoop.mapred.JobQueueTaskScheduler
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.jvm.numtasks=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.dynamic.partition.pruning.max.data.size=104857600
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.mapredfiles=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.userlog.limit.kb=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.monitor.enable=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3n.block.size=67108864
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.connect.max.retries=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.sampling.orderby.percent=0.1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.orc.cache.stripe.details.size=10000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.split.minsize.per.rack=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.http-address=0.0.0.0:50070
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.clean.until=0.8
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.aggr.hash.force.flush.memory.threshold=0.9
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hwi.listen.port=9999
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.logging.operation.level=EXECUTION
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.infer.bucket.sort.num.buckets.power.two=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.jobhistory.lru.cache.size=5
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.directoryscan.threads=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.fs-limits.max-blocks-per-file=1048576
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.sampling.orderby.number=1000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lock.mapred.only.operation=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.local.dir.minspacestart=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.cpu.vcores=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metadata.move.exported.metadata.to.trash=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.filter.compact.maxsize=-1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.exec.inplace.progress=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.client.port=2181
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.log.every.n.records=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.idle.session.check.operation=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.listbucketing=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.ppd.storage=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.parallel=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.native.enabled=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.ha.log-roll.period=120
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.sleep.base.millis=500
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hashtable.initialCapacity=100000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.groupby.skewindata=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.util.hash.type=murmur
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.native.multikey.only.enabled=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.storeManagerType=rdbms
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.accesstime.precision=3600000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.buffer.size=262144
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.retry-delay.max.ms=60000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.job.monitor.timeout=60
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.event.expiry.duration=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.port=10000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.worker.timeout=86400
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.ndv.error=20.0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.enforce.bucketing=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.map.fair.scheduler.queue=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.connection.basesleeptime=1000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.error.on.empty.partition=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.cleaner.interval-ms=86400000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.errors.ignore=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.output.fileoutputformat.compress=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.entity.capture.transform=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.server.conf=ssl-server.xml
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.skip.corrupt.data=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.https.keystore.resource=ssl-client.xml
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.client.completion.pollinterval=5000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.min.worker.threads=5
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.resource.cpu-vcores=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.acl.enable=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.speculative.speculativecap=0.1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.drop.ignorenonexistent=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.map.tasks.maximum=2
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.profile=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.webhdfs.enabled=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.client.socket.lifetime=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.local.fs.write=4.0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.cpu=0.000001
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.local.mem=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.awsSecretAccessKey=lkJm1rO2asxYC6VQtuSma3SlElf3Ywiz9r7n8XgU
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.zookeeper.namespace=hiveserver2
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.client.thread-count=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: tfile.fs.input.buffer.size=262144
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hbase.wal.enabled=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.client.progressmonitor.pollinterval=1000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.seqfile.sorter.recordlimit=1000000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.blockreport.initialDelay=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.automatic.close=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.replication.min=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.sortmerge.join=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lock.numretries=100
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: stream.stderr.reporter.enabled=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.hostname=0.0.0.0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.stream-buffer-size=4096
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.optimize.fetch.max=50000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.map.num.entries=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.join.noconditionaltask=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.replication.work.multiplier.per.iteration=2
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.ubertask.maxmaps=9
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.avoid.write.stale.datanode=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.files.preserve.failedtasks=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.async.exec.shutdown.timeout=10
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.reduce.enabled=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.failover-controller.graceful-fence.connection.retries=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.max.dynamic.partitions=1000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.multi.insert.move.tasks.share.dependencies=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.worker.keepalive.time=60
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.join.cache.size=25000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.cpu.vcores=1
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compactor.worker.threads=0
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.client.resolve.remote.symlinks=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.index.compact.binary.search=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.restart.recover=false
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.server.tcp.keepalive=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapjoin.optimized.hashtable=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.fetch.partition.stats=true
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.reduce.tasks.maximum=2
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.split.maxsize=256000000
07:05:24.347 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.filter=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.reduce.tasks.speculative.execution=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.join.factor=1.1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.max.start.attempts=30
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: net.topology.impl=org.apache.hadoop.net.NetworkTopology
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.map.index.skip=0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.remove.identity.project=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.safemode.min.datanodes=0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.userlog.retain.hours=24
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.scheduler.maximum-allocation-vcores=32
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.log-aggregation.compression-type=none
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.enable.retrycache=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.maxattempts=4
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.committer.setup.cleanup.needed=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.readahead.bytes=4193404
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.heartbeats.in.second=100
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.conf.validation=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.submitviachild=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.extended=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.token.tracking.ids.enabled=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.analyze.stmt.collect.partlevel.stats=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.autoStartMechanismMode=checked
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.tmp.dir=./tmp
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.enable=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.delegation.key.update-interval=86400000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.server.min.threads=200
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.rcfile.use.sync.cache=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.file-block-storage-locations.num-threads=10
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.healthchecker.interval=60000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.block.size=268435456
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.simple.anonymous.allowed=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fetch.task.conversion=more
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.autogen.columnalias.prefix.includefuncname=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.rework.mapredwork=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.avoid.read.stale.datanode=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.health-checker.script.timeout-ms=1200000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.cookie.auth.enabled=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.compute.query.using.stats=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.logging.operation.log.location=/Users/lian/local/src/spark/workspace-d/target/tmp/lian/operation_logs
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.log.level=INFO
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.cookie.max.age=86400
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.output.fileoutputformat.compress.type=RECORD
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.table.type.mapping=CLASSIC
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.log.level=INFO
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.ifile.readahead.bytes=4194304
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.blockreport.intervalMsec=21600000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.cluster.acls.enabled=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.test.mode.prefix=test_
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.zerocopy=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.stream-buffer-size=4096
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.speculative=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.jdbcdriver=org.apache.derby.jdbc.EmbeddedDriver
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.size.per.task=256000000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.use.datanode.hostname=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.stagingdir=.hive-staging
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.min.partition.factor=0.25
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.fs-limits.min-block-size=1048576
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.linux-container-executor.cgroups.mount=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.prewarm.numcontainers=10
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.reduce.slowstart.completedmaps=0.05
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.local.scratchdir=/Users/lian/local/src/spark/workspace-d/target/tmp/lian
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping.ldap.search.attr.group.name=cn
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.skewjoin=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.invalidate.work.pct.per.iteration=0.32f
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cli.pretty.output.num.cols=-1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.index.compact.file.ignore.hdfs=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.replication.max=512
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.hostname=0.0.0.0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fileformat.check=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.task.skip.start.attempts=2
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.io.rcfile.record.interval=2147483647
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.heartbeat.interval=1000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.reader.wait=1000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.cache.level2.type=none
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.tcpnodelay=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.application-tokens.master-key-rolling-interval-secs=86400
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.optimize.enable=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.maxRetries=4
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exim.strict.repl.tables=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.fixedDatastore=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.default.chunk.view.size=32768
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8/metastore;create=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.client.connect.max.retries.on.timeouts=45
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.split.metainfo.maxsize=10000000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.am.liveness-monitor.expiry-interval-ms=600000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.progress.timeout=0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3.awsAccessKeyId=AKIAJTMRDVCTJGAZZECQ
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.image.compress=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.application.classpath=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,$HADOOP_COMMON_HOME/share/hadoop/common/lib/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,$HADOOP_YARN_HOME/share/hadoop/yarn/*,$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.http.address=0.0.0.0:50060
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.fetch.column.stats=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.fuse.timer.period=5
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.stats.ndv.densityfunction=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.max.idle.time=1800000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.hdfs-blocks-metadata.enabled=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.lock.sleep.between.retries=60
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.max.transfer.threads=4096
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.bucketingsorting=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.client.output.filter=FAILED
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.support.dynamic.service.discovery=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stageid.rearrange=none
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.client-write-packet-size=65536
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.admin.user.env=LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.key.prefix.max.length=150
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: file.replication=1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.max.variable.length=100
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.resourcemanager.connect.retry_interval.secs=30
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.supports.subdirectories=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.log.retain-seconds=10800
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.plugin.pluginRegistryBundleCheck=LOG
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.collect.tablekeys=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.max.created.files=100000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.keytab=/etc/krb5.keytab
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.merge.inmem.threshold=1000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.https.need-auth=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.minmax.enabled=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.shuffle.ssl.enabled=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.orc.row.index.stride.dictionary.check=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.join.emit.interval=1000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.block.access.token.enable=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.max.partitions=10000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.groupby.orderby.position.alias=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.skewjoin.mapjoin.map.tasks=10000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.groupby.flush.percent=0.1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.ConnectionPassword=xxx
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.multigroupby.singlereducer=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.validateConstraints=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.https.enable=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.log-aggregation.retain-seconds=-1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.retries.wait=3000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.zookeeper.session.timeout=1200000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.replication.considerLoad=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.max-age-ms=604800000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.retrycache.heap.percent=0.03f
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.webapp.address=0.0.0.0:19888
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.join.noconditionaltask.size=10000000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.journalnode.rpc-address=0.0.0.0:8485
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.reorder.nway.joins=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.s3n.awsAccessKeyId=AKIAJTMRDVCTJGAZZECQ
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.parallelcopies=5
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.trash.interval=0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.replication.interval=3
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.mapred.mode=nonstrict
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.client.max-retries=3
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.speculative.slownodethreshold=1.0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.authentication=simple
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.reducededuplication=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.du.reserved=0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.union.remove=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.stripe.size=67108864
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.am.resource.mb=1536
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.hmshandler.retry.attempts=10
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.mapfile.bloom.size=1048576
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.resource.cpu-vcores=8
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.reduces=-1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.scheduler.minimum-allocation-vcores=1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.http.address=0.0.0.0:50075
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.ssl.client.conf=ssl-client.xml
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.int.timestamp.conversion.in.seconds=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.variable.substitute=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.queuename=default
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.health-monitor.sleep-after-disconnect.ms=1000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.bytes-per-checksum=512
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.spark.client.server.connect.timeout=90000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.fetch.task.aggr=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.skewjoin.compiletime=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.join.use.nonstaged=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.encrypt.data.transfer=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.default.compress=ZLIB
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.staticuser.user=dr.who
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.variable.substitute.depth=40
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.default.fileformat=TextFile
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.shuffle.memory.limit.percent=0.25
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3.client-write-packet-size=65536
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.container.size=-1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.output.compress=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.zookeeper.acl=world:anyone:rwcda
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.committer.task.cleanup.needed=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.use.SSL=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.schema.verification.record.version=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cbo.costmodel.network=150.0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.reduce.skip.proc.count.autoincr=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.scheduler.maximum-allocation-mb=8192
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: datanucleus.autoCreateSchema=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: s3native.blocksize=67108864
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.max.message.size=104857600
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.query.result.fileformat=TextFile
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.failure.retries=1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.sort.dynamic.partition=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobhistory.cleaner.enable=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.idle.session.timeout=604800000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.counters.pull.interval=1000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.use.datanode.hostname=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.stream-buffer-size=4096
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.disallow.incompatible.col.type.changes=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.input.fileinputformat.split.minsize.per.node=1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.null.scan=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.client.nodemanager-client-async.thread-pool-max-size=500
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.map.maxattempts=4
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.async.exec.threads=100
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.drop.cache.behind.writes=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.dns.nameserver=default
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.scratchdir=file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.support.quoted.identifiers=column
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.sleep-delay-before-sigkill.ms=250
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.end-notification.retry.attempts=0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.dynamic.partition.mode=strict
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exim.uri.scheme.whitelist=hdfs,pfile
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.atomic=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.limit.row.max.size=100000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.max.attempts=15
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.tasktracker.indexcache.mb=10
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.localizer.cache.target-size-mb=10240
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: javax.jdo.option.Multithreaded=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.compress.output=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.bucketmapjoin.sortedmerge=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.index.filter.compact.minsize=5368709120
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.aggregate.stats.cache.fpp=0.01
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.http.cookie.is.secure=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ftp.client-write-packet-size=65536
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.auto.convert.join=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.jobtracker.maxtasks.perjob=-1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.block.access.token.lifetime=600
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.max.extra.edits.segments.retained=10000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.sasl.qop=auth
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.optimize.correlation=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.rpc.query.plan=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.image.transfer.bandwidthPerSec=0
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: io.native.lib.available=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.tez.exec.print.summary=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ipc.server.listen.queue.size=128
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.metastore.client.socket.timeout=600
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.client.app-submission.poll-interval=1000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.debug.localtask=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.tezfiles=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: map.sort.class=org.apache.hadoop.util.QuickSort
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.script.auto.progress=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: fs.permissions.umask-mode=022
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.ipc.address=0.0.0.0:50020
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.vmem-check-enabled=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.server2.thrift.exponential.backoff.slot.length=100
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.app.mapreduce.client-am.ipc.max-retries=3
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.merge.mapfiles=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.block.write.retries=3
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: mapreduce.job.ubertask.maxreduces=1
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.orc.encoding.strategy=SPEED
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.namenode.logging.level=info
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.security.authorization.enabled=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: ha.failover-controller.cli-check.rpc-timeout.ms=20000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client-write-packet-size=65536
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.dns.nameserver=default
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.stats.autogather=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.map.aggr=true
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.image.transfer.timeout=600000
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: yarn.resourcemanager.recovery.enabled=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.exec.infer.bucket.sort=false
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: hive.execution.engine=mr
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240
07:05:24.348 pool-1-thread-1 DEBUG HiveClientImpl: Hive Config: dfs.client.failover.connection.retries.on.timeouts=0
07:05:24.348 pool-1-thread-1 DEBUG SessionState: SessionState user: null
07:05:24.355 pool-1-thread-1 DEBUG SessionState: HDFS root scratch dir: file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba with schema file, permission: rwx-wx-wx
07:05:24.359 pool-1-thread-1 INFO SessionState: Created local directory: /Users/lian/local/src/spark/workspace-d/target/tmp/8c061f50-3efa-413f-9814-068176e61696_resources
07:05:24.363 pool-1-thread-1 INFO SessionState: Created HDFS directory: file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba/lian/8c061f50-3efa-413f-9814-068176e61696
07:05:24.368 pool-1-thread-1 INFO SessionState: Created local directory: /Users/lian/local/src/spark/workspace-d/target/tmp/lian/8c061f50-3efa-413f-9814-068176e61696
07:05:24.373 pool-1-thread-1 INFO SessionState: Created HDFS directory: file:/Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba/lian/8c061f50-3efa-413f-9814-068176e61696/_tmp_space.db
07:05:24.381 pool-1-thread-1 DEBUG HiveClientImpl: Running hiveql 'SET spark.sql.test='
07:05:24.382 pool-1-thread-1 DEBUG HiveClientImpl: Changing config: SET spark.sql.test=
07:05:24.387 pool-1-thread-1 DEBUG VariableSubstitution: Substitution is on:
07:05:24.388 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1 - 1148512049
07:05:24.388 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.execution.QueryExecutionException
07:05:24.388 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1 - 1372735855
07:05:24.388 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.util.MutableURLClassLoader
07:05:24.388 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.IntRef
07:05:24.388 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.ObjectRef
07:05:24.389 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/HiveAuthenticationProvider.class
07:05:24.389 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.SessionState$AuthorizationMode - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/SessionState$AuthorizationMode.class
07:05:24.389 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/authorization/HiveAuthorizationProvider.class
07:05:24.390 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveAuthorizer.class
07:05:24.390 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/lockmgr/HiveTxnManager.class
07:05:24.390 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.lockmgr.LockException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/lockmgr/LockException.class
07:05:24.390 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.SessionState$ResourceType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/SessionState$ResourceType.class
07:05:24.391 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.plan.HiveOperation - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/plan/HiveOperation.class
07:05:24.391 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/session/CreateTableAutomaticGrant.class
07:05:24.392 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.log.PerfLogger - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/log/PerfLogger.class
07:05:24.392 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.tez.TezSessionState - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.class
07:05:24.392 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.spark.session.SparkSession - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/spark/session/SparkSession.class
07:05:24.393 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim$$anonfun$findStaticMethod$1 - 716051658
07:05:24.393 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1$$anonfun$apply$11 - 1688867844
07:05:24.393 pool-1-thread-1 DEBUG HiveClientImpl: Running hiveql 'SET spark.sql.test='
07:05:24.393 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1$$anonfun$apply$12 - 889452928
07:05:24.393 pool-1-thread-1 DEBUG HiveClientImpl: Changing config: SET spark.sql.test=
07:05:24.394 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.processors.CommandProcessorFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.class
07:05:24.394 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.processors.CommandProcessor - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/processors/CommandProcessor.class
07:05:24.394 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.processors.HiveCommand - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/processors/HiveCommand.class
07:05:24.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.processors.CommandProcessorFactory$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory$1.class
07:05:24.395 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.processors.SetProcessor - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/processors/SetProcessor.class
07:05:24.396 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.Driver - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/Driver.class
07:05:24.397 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.collection.mutable.StringBuilder
07:05:24.397 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.collection.Seq$
07:05:24.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.processors.CommandProcessorResponse - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/processors/CommandProcessorResponse.class
07:05:24.398 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.parse.VariableSubstitution - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/parse/VariableSubstitution.class
07:05:24.398 pool-1-thread-1 DEBUG VariableSubstitution: Substitution is on:
07:05:24.400 pool-1-thread-1 DEBUG HiveClientImpl: Running hiveql 'SET spark.sql.hive.metastore.barrierPrefixes=org.apache.spark.sql.hive.execution.PairSerDe'
07:05:24.400 pool-1-thread-1 DEBUG HiveClientImpl: Changing config: SET spark.sql.hive.metastore.barrierPrefixes=org.apache.spark.sql.hive.execution.PairSerDe
07:05:24.401 pool-1-thread-1 DEBUG VariableSubstitution: Substitution is on: org.apache.spark.sql.hive.execution.PairSerDe
07:05:24.401 pool-1-thread-1 DEBUG HiveClientImpl: Running hiveql 'SET spark.sql.hive.metastore.barrierPrefixes=org.apache.spark.sql.hive.execution.PairSerDe'
07:05:24.401 pool-1-thread-1 DEBUG HiveClientImpl: Changing config: SET spark.sql.hive.metastore.barrierPrefixes=org.apache.spark.sql.hive.execution.PairSerDe
07:05:24.401 pool-1-thread-1 DEBUG VariableSubstitution: Substitution is on: org.apache.spark.sql.hive.execution.PairSerDe
07:05:24.401 pool-1-thread-1 DEBUG TestHive: create HiveContext
07:05:24.401 pool-1-thread-1 DEBUG HiveClientImpl: Running hiveql 'SET hive.support.sql11.reserved.keywords=false'
07:05:24.401 pool-1-thread-1 DEBUG HiveClientImpl: Changing config: SET hive.support.sql11.reserved.keywords=false
07:05:24.401 pool-1-thread-1 DEBUG VariableSubstitution: Substitution is on: false
07:05:24.401 pool-1-thread-1 DEBUG HiveClientImpl: Running hiveql 'SET hive.support.sql11.reserved.keywords=false'
07:05:24.401 pool-1-thread-1 DEBUG HiveClientImpl: Changing config: SET hive.support.sql11.reserved.keywords=false
07:05:24.401 pool-1-thread-1 DEBUG VariableSubstitution: Substitution is on: false
07:05:25.031 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!input[0, scala.Tuple3]._1 AS _1#0   input[0, scala.Tuple3]._1
!+- input[0, scala.Tuple3]._1        +- input[0, scala.Tuple3]
!   +- input[0, scala.Tuple3]

07:05:25.031 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!input[0, scala.Tuple3]._2 AS _2#1   input[0, scala.Tuple3]._2
!+- input[0, scala.Tuple3]._2        +- input[0, scala.Tuple3]
!   +- input[0, scala.Tuple3]

07:05:25.032 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true) AS _3#2   staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true)
!+- staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true)        +- input[0, scala.Tuple3]._3
!   +- input[0, scala.Tuple3]._3                                                                                                    +- input[0, scala.Tuple3]
!      +- input[0, scala.Tuple3]

07:05:25.051 pool-1-thread-1 DEBUG GenerateUnsafeProjection: code for input[0, scala.Tuple3]._1,input[0, scala.Tuple3]._2,staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, scala.Tuple3]._1 */
/* 032 */     /* input[0, scala.Tuple3] */
/* 033 */     scala.Tuple3 value1 = (scala.Tuple3)i.get(0, null);
/* 034 */
/* 035 */     int value = false ? -1 : (int) ((java.lang.Integer)value1._1()).intValue();
/* 036 */     rowWriter.write(0, value);
/* 037 */
/* 038 */     /* input[0, scala.Tuple3]._2 */
/* 039 */     /* input[0, scala.Tuple3] */
/* 040 */     scala.Tuple3 value3 = (scala.Tuple3)i.get(0, null);
/* 041 */
/* 042 */     int value2 = false ? -1 : (int) ((java.lang.Integer)value3._2()).intValue();
/* 043 */     rowWriter.write(1, value2);
/* 044 */
/* 045 */     /* staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true) */
/* 046 */     /* input[0, scala.Tuple3]._3 */
/* 047 */     /* input[0, scala.Tuple3] */
/* 048 */     scala.Tuple3 value6 = (scala.Tuple3)i.get(0, null);
/* 049 */
/* 050 */     java.lang.String value5 = false ? null : (java.lang.String) value6._3();
/* 051 */     boolean isNull5 = value5 == null;
/* 052 */
/* 053 */     boolean isNull4 = !!(isNull5);
/* 054 */     UTF8String value4 = null;
/* 055 */
/* 056 */     if (!(isNull5)) {
/* 057 */       value4 = org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 058 */       isNull4 = value4 == null;
/* 059 */     }
/* 060 */     if (isNull4) {
/* 061 */       rowWriter.setNullAt(2);
/* 062 */     } else {
/* 063 */       rowWriter.write(2, value4);
/* 064 */     }
/* 065 */     result.setTotalSize(holder.totalSize());
/* 066 */     return result;
/* 067 */   }
/* 068 */ }
/* 069 */

07:05:25.091 pool-1-thread-1 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, scala.Tuple3]._1 */
/* 032 */     /* input[0, scala.Tuple3] */
/* 033 */     scala.Tuple3 value1 = (scala.Tuple3)i.get(0, null);
/* 034 */
/* 035 */     int value = false ? -1 : (int) ((java.lang.Integer)value1._1()).intValue();
/* 036 */     rowWriter.write(0, value);
/* 037 */
/* 038 */     /* input[0, scala.Tuple3]._2 */
/* 039 */     /* input[0, scala.Tuple3] */
/* 040 */     scala.Tuple3 value3 = (scala.Tuple3)i.get(0, null);
/* 041 */
/* 042 */     int value2 = false ? -1 : (int) ((java.lang.Integer)value3._2()).intValue();
/* 043 */     rowWriter.write(1, value2);
/* 044 */
/* 045 */     /* staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true) */
/* 046 */     /* input[0, scala.Tuple3]._3 */
/* 047 */     /* input[0, scala.Tuple3] */
/* 048 */     scala.Tuple3 value6 = (scala.Tuple3)i.get(0, null);
/* 049 */
/* 050 */     java.lang.String value5 = false ? null : (java.lang.String) value6._3();
/* 051 */     boolean isNull5 = value5 == null;
/* 052 */
/* 053 */     boolean isNull4 = !!(isNull5);
/* 054 */     UTF8String value4 = null;
/* 055 */
/* 056 */     if (!(isNull5)) {
/* 057 */       value4 = org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 058 */       isNull4 = value4 == null;
/* 059 */     }
/* 060 */     if (isNull4) {
/* 061 */       rowWriter.setNullAt(2);
/* 062 */     } else {
/* 063 */       rowWriter.write(2, value4);
/* 064 */     }
/* 065 */     result.setTotalSize(holder.totalSize());
/* 066 */     return result;
/* 067 */   }
/* 068 */ }
/* 069 */

07:05:25.206 pool-1-thread-1 INFO CodeGenerator: Code generated in 149.433436 ms
07:05:25.326 pool-1-thread-1 DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1 - -854372546
07:05:25.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.runtime.AbstractFunction0$mcV$sp
07:05:25.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.catalyst.catalog.CatalogDatabase
07:05:25.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.collection.JavaConverters$
07:05:25.327 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: scala.collection.convert.Decorators$AsJava
07:05:25.328 pool-1-thread-1 INFO HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:, parameters:{})
07:05:25.328 pool-1-thread-1 INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:, parameters:{})
07:05:25.328 pool-1-thread-1 DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:25.329 pool-1-thread-1 DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:25.329 pool-1-thread-1 DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
07:05:25.329 pool-1-thread-1 DEBUG Query: SQL Query : "select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = ?"
07:05:25.329 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@53d955c" opened with isolation level "read-committed" and auto-commit=false
07:05:25.329 pool-1-thread-1 DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3294f0f7, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:25.329 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3294f0f7 is starting for transaction Xid=    with flags 0
07:05:25.329 pool-1-thread-1 DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3cb6f960 [conn=com.jolbox.bonecp.ConnectionHandle@53d955c, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:25.332 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5e6347cc"
07:05:25.332 pool-1-thread-1 DEBUG Native: select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = <'default'>
07:05:25.333 pool-1-thread-1 DEBUG Retrieve: Execution Time = 1 ms
07:05:25.333 pool-1-thread-1 DEBUG Query: SQL Query : "select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = ?  AND "PARAM_KEY" IS NOT NULL"
07:05:25.333 pool-1-thread-1 DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3cb6f960 [conn=com.jolbox.bonecp.ConnectionHandle@53d955c, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:25.336 pool-1-thread-1 DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1fa9bae4"
07:05:25.336 pool-1-thread-1 DEBUG Native: select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = <1>  AND "PARAM_KEY" IS NOT NULL
07:05:25.336 pool-1-thread-1 DEBUG Retrieve: Execution Time = 0 ms
07:05:25.336 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.base.Predicates
07:05:25.338 pool-1-thread-1 DEBUG IsolatedClientLoader: shared class: com.google.common.collect.Maps
07:05:25.344 pool-1-thread-1 DEBUG MetaStoreDirectSql: getDatabase: directsql returning db default locn[file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5] desc [Default Hive database] owner [public] ownertype [ROLE]
07:05:25.344 pool-1-thread-1 DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
07:05:25.344 pool-1-thread-1 DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:25.344 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:25.344 pool-1-thread-1 DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:25.344 pool-1-thread-1 DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:25.344 pool-1-thread-1 DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:25.344 pool-1-thread-1 DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3294f0f7]]
07:05:25.344 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3294f0f7 is committing for transaction Xid=    with onePhase=true
07:05:25.345 pool-1-thread-1 DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3294f0f7 committed connection for transaction Xid=    with onePhase=true
07:05:25.345 pool-1-thread-1 DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@53d955c" closed
07:05:25.345 pool-1-thread-1 DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3cb6f960 [conn=com.jolbox.bonecp.ConnectionHandle@53d955c, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:25.345 pool-1-thread-1 DEBUG Transaction: Transaction committed in 1 ms
07:05:25.345 pool-1-thread-1 DEBUG ObjectStore: db details for db default retrieved using SQL in 16.20965ms
07:05:25.345 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AlreadyExistsException$AlreadyExistsExceptionStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AlreadyExistsException$AlreadyExistsExceptionStandardSchemeFactory.class
07:05:25.345 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AlreadyExistsException$AlreadyExistsExceptionTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AlreadyExistsException$AlreadyExistsExceptionTupleSchemeFactory.class
07:05:25.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.AlreadyExistsException$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/AlreadyExistsException$_Fields.class
07:05:25.346 pool-1-thread-1 DEBUG IsolatedClientLoader: hive class: org.apache.commons.lang.exception.ExceptionUtils - jar:file:/Users/lian/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.6.jar!/org/apache/commons/lang/exception/ExceptionUtils.class
07:05:25.359 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.475 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(newInstance(class scala.Tuple3), _1#3, _2#4, _3#5) AS #6]   Project [newInstance(class scala.Tuple3) AS #6]
 +- LocalRelation [_1#3,_2#4,_3#5]                                                            +- LocalRelation [_1#3,_2#4,_3#5]

07:05:25.489 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.497 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.509 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#7, j#8, k#9) AS #10]   Project [createexternalrow(if (isnull(i#7)) null else i#7, if (isnull(j#8)) null else j#8, if (isnull(k#9)) null else k#9.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #10]
 +- LocalRelation [i#7,j#8,k#9]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#7,j#8,k#9]

07:05:25.533 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!input[0, scala.Tuple3]._1 AS _1#11   input[0, scala.Tuple3]._1
!+- input[0, scala.Tuple3]._1         +- input[0, scala.Tuple3]
!   +- input[0, scala.Tuple3]

07:05:25.534 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._2, true) AS _2#12   staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._2, true)
!+- staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._2, true)         +- input[0, scala.Tuple3]._2
!   +- input[0, scala.Tuple3]._2                                                                                                     +- input[0, scala.Tuple3]
!      +- input[0, scala.Tuple3]

07:05:25.534 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!input[0, scala.Tuple3]._3 AS _3#13   input[0, scala.Tuple3]._3
!+- input[0, scala.Tuple3]._3         +- input[0, scala.Tuple3]
!   +- input[0, scala.Tuple3]

07:05:25.537 pool-1-thread-1 DEBUG GenerateUnsafeProjection: code for input[0, scala.Tuple3]._1,staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._2, true),input[0, scala.Tuple3]._3:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, scala.Tuple3]._1 */
/* 032 */     /* input[0, scala.Tuple3] */
/* 033 */     scala.Tuple3 value1 = (scala.Tuple3)i.get(0, null);
/* 034 */
/* 035 */     int value = false ? -1 : (int) ((java.lang.Integer)value1._1()).intValue();
/* 036 */     rowWriter.write(0, value);
/* 037 */
/* 038 */     /* staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._2, true) */
/* 039 */     /* input[0, scala.Tuple3]._2 */
/* 040 */     /* input[0, scala.Tuple3] */
/* 041 */     scala.Tuple3 value4 = (scala.Tuple3)i.get(0, null);
/* 042 */
/* 043 */     java.lang.String value3 = false ? null : (java.lang.String) value4._2();
/* 044 */     boolean isNull3 = value3 == null;
/* 045 */
/* 046 */     boolean isNull2 = !!(isNull3);
/* 047 */     UTF8String value2 = null;
/* 048 */
/* 049 */     if (!(isNull3)) {
/* 050 */       value2 = org.apache.spark.unsafe.types.UTF8String.fromString(value3);
/* 051 */       isNull2 = value2 == null;
/* 052 */     }
/* 053 */     if (isNull2) {
/* 054 */       rowWriter.setNullAt(1);
/* 055 */     } else {
/* 056 */       rowWriter.write(1, value2);
/* 057 */     }
/* 058 */
/* 059 */     /* input[0, scala.Tuple3]._3 */
/* 060 */     /* input[0, scala.Tuple3] */
/* 061 */     scala.Tuple3 value6 = (scala.Tuple3)i.get(0, null);
/* 062 */
/* 063 */     int value5 = false ? -1 : (int) ((java.lang.Integer)value6._3()).intValue();
/* 064 */     rowWriter.write(2, value5);
/* 065 */     result.setTotalSize(holder.totalSize());
/* 066 */     return result;
/* 067 */   }
/* 068 */ }
/* 069 */

07:05:25.539 pool-1-thread-1 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, scala.Tuple3]._1 */
/* 032 */     /* input[0, scala.Tuple3] */
/* 033 */     scala.Tuple3 value1 = (scala.Tuple3)i.get(0, null);
/* 034 */
/* 035 */     int value = false ? -1 : (int) ((java.lang.Integer)value1._1()).intValue();
/* 036 */     rowWriter.write(0, value);
/* 037 */
/* 038 */     /* staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._2, true) */
/* 039 */     /* input[0, scala.Tuple3]._2 */
/* 040 */     /* input[0, scala.Tuple3] */
/* 041 */     scala.Tuple3 value4 = (scala.Tuple3)i.get(0, null);
/* 042 */
/* 043 */     java.lang.String value3 = false ? null : (java.lang.String) value4._2();
/* 044 */     boolean isNull3 = value3 == null;
/* 045 */
/* 046 */     boolean isNull2 = !!(isNull3);
/* 047 */     UTF8String value2 = null;
/* 048 */
/* 049 */     if (!(isNull3)) {
/* 050 */       value2 = org.apache.spark.unsafe.types.UTF8String.fromString(value3);
/* 051 */       isNull2 = value2 == null;
/* 052 */     }
/* 053 */     if (isNull2) {
/* 054 */       rowWriter.setNullAt(1);
/* 055 */     } else {
/* 056 */       rowWriter.write(1, value2);
/* 057 */     }
/* 058 */
/* 059 */     /* input[0, scala.Tuple3]._3 */
/* 060 */     /* input[0, scala.Tuple3] */
/* 061 */     scala.Tuple3 value6 = (scala.Tuple3)i.get(0, null);
/* 062 */
/* 063 */     int value5 = false ? -1 : (int) ((java.lang.Integer)value6._3()).intValue();
/* 064 */     rowWriter.write(2, value5);
/* 065 */     result.setTotalSize(holder.totalSize());
/* 066 */     return result;
/* 067 */   }
/* 068 */ }
/* 069 */

07:05:25.551 pool-1-thread-1 INFO CodeGenerator: Code generated in 13.803611 ms
07:05:25.554 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.564 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(newInstance(class scala.Tuple3), _1#14, _2#15, _3#16) AS #17]   Project [newInstance(class scala.Tuple3) AS #17]
 +- LocalRelation [_1#14,_2#15,_3#16]                                                             +- LocalRelation [_1#14,_2#15,_3#16]

07:05:25.565 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.567 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.576 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, string])) null else input[1, string].toString, if (isnull(input[2, int])) null else input[2, int], StructField(i,IntegerType,false), StructField(j,StringType,true), StructField(k,IntegerType,false)), i#18, j#19, k#20) AS #21]   Project [createexternalrow(if (isnull(i#18)) null else i#18, if (isnull(j#19)) null else j#19.toString, if (isnull(k#20)) null else k#20, StructField(i,IntegerType,false), StructField(j,StringType,true), StructField(k,IntegerType,false)) AS #21]
 +- LocalRelation [i#18,j#19,k#20]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#18,j#19,k#20]

07:05:25.624 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!input[0, scala.Tuple3]._1 AS _1#22   input[0, scala.Tuple3]._1
!+- input[0, scala.Tuple3]._1         +- input[0, scala.Tuple3]
!   +- input[0, scala.Tuple3]

07:05:25.624 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!input[0, scala.Tuple3]._2 AS _2#23   input[0, scala.Tuple3]._2
!+- input[0, scala.Tuple3]._2         +- input[0, scala.Tuple3]
!   +- input[0, scala.Tuple3]

07:05:25.624 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true) AS _3#24   staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true)
!+- staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true)         +- input[0, scala.Tuple3]._3
!   +- input[0, scala.Tuple3]._3                                                                                                     +- input[0, scala.Tuple3]
!      +- input[0, scala.Tuple3]

07:05:25.628 pool-1-thread-1 DEBUG GenerateUnsafeProjection: code for input[0, scala.Tuple3]._1,input[0, scala.Tuple3]._2,staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, scala.Tuple3]._1 */
/* 032 */     /* input[0, scala.Tuple3] */
/* 033 */     scala.Tuple3 value1 = (scala.Tuple3)i.get(0, null);
/* 034 */
/* 035 */     int value = false ? -1 : (int) ((java.lang.Integer)value1._1()).intValue();
/* 036 */     rowWriter.write(0, value);
/* 037 */
/* 038 */     /* input[0, scala.Tuple3]._2 */
/* 039 */     /* input[0, scala.Tuple3] */
/* 040 */     scala.Tuple3 value3 = (scala.Tuple3)i.get(0, null);
/* 041 */
/* 042 */     int value2 = false ? -1 : (int) ((java.lang.Integer)value3._2()).intValue();
/* 043 */     rowWriter.write(1, value2);
/* 044 */
/* 045 */     /* staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true) */
/* 046 */     /* input[0, scala.Tuple3]._3 */
/* 047 */     /* input[0, scala.Tuple3] */
/* 048 */     scala.Tuple3 value6 = (scala.Tuple3)i.get(0, null);
/* 049 */
/* 050 */     java.lang.String value5 = false ? null : (java.lang.String) value6._3();
/* 051 */     boolean isNull5 = value5 == null;
/* 052 */
/* 053 */     boolean isNull4 = !!(isNull5);
/* 054 */     UTF8String value4 = null;
/* 055 */
/* 056 */     if (!(isNull5)) {
/* 057 */       value4 = org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 058 */       isNull4 = value4 == null;
/* 059 */     }
/* 060 */     if (isNull4) {
/* 061 */       rowWriter.setNullAt(2);
/* 062 */     } else {
/* 063 */       rowWriter.write(2, value4);
/* 064 */     }
/* 065 */     result.setTotalSize(holder.totalSize());
/* 066 */     return result;
/* 067 */   }
/* 068 */ }
/* 069 */

07:05:25.628 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.635 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(newInstance(class scala.Tuple3), _1#25, _2#26, _3#27) AS #28]   Project [newInstance(class scala.Tuple3) AS #28]
 +- LocalRelation [_1#25,_2#26,_3#27]                                                             +- LocalRelation [_1#25,_2#26,_3#27]

07:05:25.636 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.639 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.648 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#29, j#30, k#31) AS #32]   Project [createexternalrow(if (isnull(i#29)) null else i#29, if (isnull(j#30)) null else j#30, if (isnull(k#31)) null else k#31.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #32]
 +- LocalRelation [i#29,j#30,k#31]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#29,j#30,k#31]

07:05:25.650 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.657 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#29, j#30, k#31) AS #33]   Project [createexternalrow(if (isnull(i#29)) null else i#29, if (isnull(j#30)) null else j#30, if (isnull(k#31)) null else k#31.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #33]
 +- LocalRelation [i#29,j#30,k#31]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#29,j#30,k#31]

07:05:25.674 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!input[0, scala.Tuple3]._1 AS _1#34   input[0, scala.Tuple3]._1
!+- input[0, scala.Tuple3]._1         +- input[0, scala.Tuple3]
!   +- input[0, scala.Tuple3]

07:05:25.674 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!input[0, scala.Tuple3]._2 AS _2#35   input[0, scala.Tuple3]._2
!+- input[0, scala.Tuple3]._2         +- input[0, scala.Tuple3]
!   +- input[0, scala.Tuple3]

07:05:25.675 pool-1-thread-1 DEBUG package$ExpressionCanonicalizer:
=== Result of Batch CleanExpressions ===
!staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true) AS _3#36   staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true)
!+- staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true)         +- input[0, scala.Tuple3]._3
!   +- input[0, scala.Tuple3]._3                                                                                                     +- input[0, scala.Tuple3]
!      +- input[0, scala.Tuple3]

07:05:25.677 pool-1-thread-1 DEBUG GenerateUnsafeProjection: code for input[0, scala.Tuple3]._1,input[0, scala.Tuple3]._2,staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, scala.Tuple3]._1 */
/* 032 */     /* input[0, scala.Tuple3] */
/* 033 */     scala.Tuple3 value1 = (scala.Tuple3)i.get(0, null);
/* 034 */
/* 035 */     int value = false ? -1 : (int) ((java.lang.Integer)value1._1()).intValue();
/* 036 */     rowWriter.write(0, value);
/* 037 */
/* 038 */     /* input[0, scala.Tuple3]._2 */
/* 039 */     /* input[0, scala.Tuple3] */
/* 040 */     scala.Tuple3 value3 = (scala.Tuple3)i.get(0, null);
/* 041 */
/* 042 */     int value2 = false ? -1 : (int) ((java.lang.Integer)value3._2()).intValue();
/* 043 */     rowWriter.write(1, value2);
/* 044 */
/* 045 */     /* staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, scala.Tuple3]._3, true) */
/* 046 */     /* input[0, scala.Tuple3]._3 */
/* 047 */     /* input[0, scala.Tuple3] */
/* 048 */     scala.Tuple3 value6 = (scala.Tuple3)i.get(0, null);
/* 049 */
/* 050 */     java.lang.String value5 = false ? null : (java.lang.String) value6._3();
/* 051 */     boolean isNull5 = value5 == null;
/* 052 */
/* 053 */     boolean isNull4 = !!(isNull5);
/* 054 */     UTF8String value4 = null;
/* 055 */
/* 056 */     if (!(isNull5)) {
/* 057 */       value4 = org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 058 */       isNull4 = value4 == null;
/* 059 */     }
/* 060 */     if (isNull4) {
/* 061 */       rowWriter.setNullAt(2);
/* 062 */     } else {
/* 063 */       rowWriter.write(2, value4);
/* 064 */     }
/* 065 */     result.setTotalSize(holder.totalSize());
/* 066 */     return result;
/* 067 */   }
/* 068 */ }
/* 069 */

07:05:25.677 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.683 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(newInstance(class scala.Tuple3), _1#37, _2#38, _3#39) AS #40]   Project [newInstance(class scala.Tuple3) AS #40]
 +- LocalRelation [_1#37,_2#38,_3#39]                                                             +- LocalRelation [_1#37,_2#38,_3#39]

07:05:25.684 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.686 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.691 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#41, j#42, k#43) AS #44]   Project [createexternalrow(if (isnull(i#41)) null else i#41, if (isnull(j#42)) null else j#42, if (isnull(k#43)) null else k#43.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #44]
 +- LocalRelation [i#41,j#42,k#43]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#41,j#42,k#43]

07:05:25.691 pool-1-thread-1 DEBUG TestHive: Query references test tables:
07:05:25.696 pool-1-thread-1 DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#41, j#42, k#43) AS #45]   Project [createexternalrow(if (isnull(i#41)) null else i#41, if (isnull(j#42)) null else j#42, if (isnull(k#43)) null else k#43.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #45]
 +- LocalRelation [i#41,j#42,k#43]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#41,j#42,k#43]

07:05:25.758 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO BucketedReadSuite:

===== TEST OUTPUT FOR o.a.s.sql.sources.BucketedReadSuite: 'a' =====

07:05:25.767 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#29, j#30, k#31) AS #46]   Project [createexternalrow(if (isnull(i#29)) null else i#29, if (isnull(j#30)) null else j#30, if (isnull(k#31)) null else k#31.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #46]
 +- LocalRelation [i#29,j#30,k#31]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#29,j#30,k#31]

07:05:25.823 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveSqlParser: Parsing command: t1
07:05:25.936 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1 - 464393226
07:05:25.936 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$4 - 900462401
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t1
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.Option$
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t1
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t1
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:25.937 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Compiling "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db"
07:05:25.938 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Compile Time = 1 ms
07:05:25.938 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{DyadicExpression{PrimaryExpression{tableName}  =  ParameterExpression{table}}  AND  DyadicExpression{PrimaryExpression{database.name}  =  ParameterExpression{db}}}]
  [symbols: this type=org.apache.hadoop.hive.metastore.model.MTable, table type=java.lang.String, db type=java.lang.String]
07:05:25.938 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Compiling "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" for datastore
07:05:25.939 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Compile Time for datastore = 1 ms
07:05:25.939 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db Query compiled to datastore query "SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = ? AND B0."NAME" = ?"
07:05:25.939 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@75bbb501" opened with isolation level "read-committed" and auto-commit=false
07:05:25.939 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@640cfa4a, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:25.939 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@640cfa4a is starting for transaction Xid=    with flags 0
07:05:25.939 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@63ee1e8f [conn=com.jolbox.bonecp.ConnectionHandle@75bbb501, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:25.939 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:25.946 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@243d2ce3"
07:05:25.946 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t1'> AND B0."NAME" = <'default'>
07:05:25.946 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:25.946 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 7 ms
07:05:25.946 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:25.946 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@640cfa4a]]
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@640cfa4a is committing for transaction Xid=    with onePhase=true
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@640cfa4a committed connection for transaction Xid=    with onePhase=true
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@75bbb501" closed
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@63ee1e8f [conn=com.jolbox.bonecp.ConnectionHandle@75bbb501, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 0 ms
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$5 - -2096711960
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.collection.GenTraversableOnce
07:05:25.947 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.Option
07:05:25.953 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:25.959 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t1
07:05:25.959 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t1
07:05:25.959 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t1
07:05:25.959 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:25.959 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:25.959 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@9c9eaba" opened with isolation level "read-committed" and auto-commit=false
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@497ae084, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@497ae084 is starting for transaction Xid=    with flags 0
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@66f07b02 [conn=com.jolbox.bonecp.ConnectionHandle@9c9eaba, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@202ad15c"
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t1'> AND B0."NAME" = <'default'>
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:25.960 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@497ae084]]
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@497ae084 is committing for transaction Xid=    with onePhase=true
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@497ae084 committed connection for transaction Xid=    with onePhase=true
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@9c9eaba" closed
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@66f07b02 [conn=com.jolbox.bonecp.ConnectionHandle@9c9eaba, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:25.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 0 ms
07:05:25.985 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SparkOptimizer:
=== Result of Batch Finish Analysis ===
 CreateTableUsingAsSelect `t1`, parquet, false, [Ljava.lang.String;@163a620b, Some(BucketSpec(8,ArrayBuffer(i),List())), Overwrite, Map()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    CreateTableUsingAsSelect `t1`, parquet, false, [Ljava.lang.String;@163a620b, Some(BucketSpec(8,ArrayBuffer(i),List())), Overwrite, Map()
!+- SubqueryAlias df1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        +- Project [_1#25 AS i#29,_2#26 AS j#30,_3#27 AS k#31]
!   +- Project [_1#25 AS i#29,_2#26 AS j#30,_3#27 AS k#31]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      +- LocalRelation [_1#25,_2#26,_3#27], [[0,0,0,2000000001,30],[0,1,1,2000000001,31],[0,2,2,2000000001,32],[0,3,3,2000000001,33],[0,4,4,2000000001,34],[0,0,5,2000000001,35],[0,1,6,2000000001,36],[0,2,7,2000000001,37],[0,3,8,2000000001,38],[0,4,9,2000000001,39],[0,0,a,2000000002,3031],[0,1,b,2000000002,3131],[0,2,c,2000000002,3231],[0,3,0,2000000002,3331],[0,4,1,2000000002,3431],[0,0,2,2000000002,3531],[0,1,3,2000000002,3631],[0,2,4,2000000002,3731],[0,3,5,2000000002,3831],[0,4,6,2000000002,3931],[0,0,7,2000000002,3032],[0,1,8,2000000002,3132],[0,2,9,2000000002,3232],[0,3,a,2000000002,3332],[0,4,b,2000000002,3432],[0,0,c,2000000002,3532],[0,1,0,2000000002,3632],[0,2,1,2000000002,3732],[0,3,2,2000000002,3832],[0,4,3,2000000002,3932],[0,0,4,2000000002,3033],[0,1,5,2000000002,3133],[0,2,6,2000000002,3233],[0,3,7,2000000002,3333],[0,4,8,2000000002,3433],[0,0,9,2000000002,3533],[0,1,a,2000000002,3633],[0,2,b,2000000002,3733],[0,3,c,2000000002,3833],[0,4,0,2000000002,3933],[0,0,1,2000000002,3034],[0,1,2,2000000002,3134],[0,2,3,2000000002,3234],[0,3,4,2000000002,3334],[0,4,5,2000000002,3434],[0,0,6,2000000002,3534],[0,1,7,2000000002,3634],[0,2,8,2000000002,3734],[0,3,9,2000000002,3834],[0,4,a,2000000002,3934]]
!      +- LocalRelation [_1#25,_2#26,_3#27], [[0,0,0,2000000001,30],[0,1,1,2000000001,31],[0,2,2,2000000001,32],[0,3,3,2000000001,33],[0,4,4,2000000001,34],[0,0,5,2000000001,35],[0,1,6,2000000001,36],[0,2,7,2000000001,37],[0,3,8,2000000001,38],[0,4,9,2000000001,39],[0,0,a,2000000002,3031],[0,1,b,2000000002,3131],[0,2,c,2000000002,3231],[0,3,0,2000000002,3331],[0,4,1,2000000002,3431],[0,0,2,2000000002,3531],[0,1,3,2000000002,3631],[0,2,4,2000000002,3731],[0,3,5,2000000002,3831],[0,4,6,2000000002,3931],[0,0,7,2000000002,3032],[0,1,8,2000000002,3132],[0,2,9,2000000002,3232],[0,3,a,2000000002,3332],[0,4,b,2000000002,3432],[0,0,c,2000000002,3532],[0,1,0,2000000002,3632],[0,2,1,2000000002,3732],[0,3,2,2000000002,3832],[0,4,3,2000000002,3932],[0,0,4,2000000002,3033],[0,1,5,2000000002,3133],[0,2,6,2000000002,3233],[0,3,7,2000000002,3333],[0,4,8,2000000002,3433],[0,0,9,2000000002,3533],[0,1,a,2000000002,3633],[0,2,b,2000000002,3733],[0,3,c,2000000002,3833],[0,4,0,2000000002,3933],[0,0,1,2000000002,3034],[0,1,2,2000000002,3134],[0,2,3,2000000002,3234],[0,3,4,2000000002,3334],[0,4,5,2000000002,3434],[0,0,6,2000000002,3534],[0,1,7,2000000002,3634],[0,2,8,2000000002,3734],[0,3,9,2000000002,3834],[0,4,a,2000000002,3934]]

07:05:26.009 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SparkOptimizer:
=== Result of Batch LocalRelation ===
 CreateTableUsingAsSelect `t1`, parquet, false, [Ljava.lang.String;@163a620b, Some(BucketSpec(8,ArrayBuffer(i),List())), Overwrite, Map()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    CreateTableUsingAsSelect `t1`, parquet, false, [Ljava.lang.String;@163a620b, Some(BucketSpec(8,ArrayBuffer(i),List())), Overwrite, Map()
!+- SubqueryAlias df1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        +- LocalRelation [i#29,j#30,k#31], [[0,0,0],[1,1,1],[2,2,2],[3,3,3],[4,4,4],[0,5,5],[1,6,6],[2,7,7],[3,8,8],[4,9,9],[0,10,10],[1,11,11],[2,12,12],[3,0,13],[4,1,14],[0,2,15],[1,3,16],[2,4,17],[3,5,18],[4,6,19],[0,7,20],[1,8,21],[2,9,22],[3,10,23],[4,11,24],[0,12,25],[1,0,26],[2,1,27],[3,2,28],[4,3,29],[0,4,30],[1,5,31],[2,6,32],[3,7,33],[4,8,34],[0,9,35],[1,10,36],[2,11,37],[3,12,38],[4,0,39],[0,1,40],[1,2,41],[2,3,42],[3,4,43],[4,5,44],[0,6,45],[1,7,46],[2,8,47],[3,9,48],[4,10,49]]
!   +- Project [_1#25 AS i#29,_2#26 AS j#30,_3#27 AS k#31]
!      +- LocalRelation [_1#25,_2#26,_3#27], [[0,0,0,2000000001,30],[0,1,1,2000000001,31],[0,2,2,2000000001,32],[0,3,3,2000000001,33],[0,4,4,2000000001,34],[0,0,5,2000000001,35],[0,1,6,2000000001,36],[0,2,7,2000000001,37],[0,3,8,2000000001,38],[0,4,9,2000000001,39],[0,0,a,2000000002,3031],[0,1,b,2000000002,3131],[0,2,c,2000000002,3231],[0,3,0,2000000002,3331],[0,4,1,2000000002,3431],[0,0,2,2000000002,3531],[0,1,3,2000000002,3631],[0,2,4,2000000002,3731],[0,3,5,2000000002,3831],[0,4,6,2000000002,3931],[0,0,7,2000000002,3032],[0,1,8,2000000002,3132],[0,2,9,2000000002,3232],[0,3,a,2000000002,3332],[0,4,b,2000000002,3432],[0,0,c,2000000002,3532],[0,1,0,2000000002,3632],[0,2,1,2000000002,3732],[0,3,2,2000000002,3832],[0,4,3,2000000002,3932],[0,0,4,2000000002,3033],[0,1,5,2000000002,3133],[0,2,6,2000000002,3233],[0,3,7,2000000002,3333],[0,4,8,2000000002,3433],[0,0,9,2000000002,3533],[0,1,a,2000000002,3633],[0,2,b,2000000002,3733],[0,3,c,2000000002,3833],[0,4,0,2000000002,3933],[0,0,1,2000000002,3034],[0,1,2,2000000002,3134],[0,2,3,2000000002,3234],[0,3,4,2000000002,3334],[0,4,5,2000000002,3434],[0,0,6,2000000002,3534],[0,1,7,2000000002,3634],[0,2,8,2000000002,3734],[0,3,9,2000000002,3834],[0,4,a,2000000002,3934]]

07:05:26.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getDatabaseOption$1 - -1470907755
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_database: default
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_database: default
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SQL Query : "select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = ?"
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@11826d34" opened with isolation level "read-committed" and auto-commit=false
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@326e7c77, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@326e7c77 is starting for transaction Xid=    with flags 0
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@4d209f75 [conn=com.jolbox.bonecp.ConnectionHandle@11826d34, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4fc9b028"
07:05:26.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = <'default'>
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SQL Query : "select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = ?  AND "PARAM_KEY" IS NOT NULL"
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@4d209f75 [conn=com.jolbox.bonecp.ConnectionHandle@11826d34, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@16238053"
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = <1>  AND "PARAM_KEY" IS NOT NULL
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaStoreDirectSql: getDatabase: directsql returning db default locn[file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5] desc [Default Hive database] owner [public] ownertype [ROLE]
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@326e7c77]]
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@326e7c77 is committing for transaction Xid=    with onePhase=true
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@326e7c77 committed connection for transaction Xid=    with onePhase=true
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@11826d34" closed
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@4d209f75 [conn=com.jolbox.bonecp.ConnectionHandle@11826d34, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 0 ms
07:05:26.149 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: db details for db default retrieved using SQL in 1.671134ms
07:05:26.150 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.PreEventContext$PreEventType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/PreEventContext$PreEventType.class
07:05:26.150 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getDatabaseOption$1$$anonfun$apply$3 - 839580977
07:05:26.150 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.collection.convert.Decorators$AsScala
07:05:26.150 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.collection.TraversableOnce
07:05:26.151 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t1
07:05:26.151 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t1
07:05:26.151 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t1
07:05:26.151 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:26.151 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:26.151 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:26.151 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:26.151 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@6d8d8aaf" opened with isolation level "read-committed" and auto-commit=false
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7864854, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7864854 is starting for transaction Xid=    with flags 0
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@153bc032 [conn=com.jolbox.bonecp.ConnectionHandle@6d8d8aaf, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@14e2e8c8"
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t1'> AND B0."NAME" = <'default'>
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7864854]]
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7864854 is committing for transaction Xid=    with onePhase=true
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7864854 committed connection for transaction Xid=    with onePhase=true
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@6d8d8aaf" closed
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@153bc032 [conn=com.jolbox.bonecp.ConnectionHandle@6d8d8aaf, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 0 ms
07:05:26.152 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:26.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:26.157 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#29, j#30, k#31) AS #47]   Project [createexternalrow(if (isnull(i#29)) null else i#29, if (isnull(j#30)) null else j#30, if (isnull(k#31)) null else k#31.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #47]
 +- LocalRelation [i#29,j#30,k#31]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#29,j#30,k#31]

07:05:26.184 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:26.227 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:26.227 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:26.234 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#29, j#30, k#31) AS #48]   Project [createexternalrow(if (isnull(i#29)) null else i#29, if (isnull(j#30)) null else j#30, if (isnull(k#31)) null else k#31.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #48]
 +- LocalRelation [i#29,j#30,k#31]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#29,j#30,k#31]

07:05:26.255 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     int value1 = i.getInt(1);
/* 037 */     rowWriter.write(1, value1);
/* 038 */
/* 039 */     /* input[2, string] */
/* 040 */     boolean isNull2 = i.isNullAt(2);
/* 041 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 042 */     if (isNull2) {
/* 043 */       rowWriter.setNullAt(2);
/* 044 */     } else {
/* 045 */       rowWriter.write(2, value2);
/* 046 */     }
/* 047 */     result.setTotalSize(holder.totalSize());
/* 048 */     return result;
/* 049 */   }
/* 050 */ }
/* 051 */

07:05:26.256 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     int value1 = i.getInt(1);
/* 037 */     rowWriter.write(1, value1);
/* 038 */
/* 039 */     /* input[2, string] */
/* 040 */     boolean isNull2 = i.isNullAt(2);
/* 041 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 042 */     if (isNull2) {
/* 043 */       rowWriter.setNullAt(2);
/* 044 */     } else {
/* 045 */       rowWriter.write(2, value2);
/* 046 */     }
/* 047 */     result.setTotalSize(holder.totalSize());
/* 048 */     return result;
/* 049 */   }
/* 050 */ }
/* 051 */

07:05:26.262 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO CodeGenerator: Code generated in 6.972836 ms
07:05:26.285 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
07:05:26.286 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
07:05:26.286 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
07:05:26.286 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
07:05:26.286 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
07:05:26.294 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DefaultSource: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
07:05:26.320 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DynamicPartitionWriterContainer: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
07:05:26.343 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1) +++
07:05:26.357 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:26.357 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1.serialVersionUID
07:05:26.358 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.sql.execution.metric.LongSQLMetric org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1.numOutputRows$1
07:05:26.358 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:26.359 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1.apply(java.lang.Object)
07:05:26.359 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final org.apache.spark.sql.catalyst.InternalRow org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1.apply(org.apache.spark.sql.catalyst.InternalRow)
07:05:26.359 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:26.360 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:26.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:26.363 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:26.369 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:26.369 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:26.371 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1) is now cleaned +++
07:05:26.396 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1) +++
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1.serialVersionUID
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.sql.execution.datasources.BaseWriterContainer org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1.writerContainer$1
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(java.lang.Object,java.lang.Object)
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final void org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:26.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1) is now cleaned +++
07:05:26.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO SparkContext: Starting job: apply at Transformer.scala:22
07:05:26.462 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (apply at Transformer.scala:22) with 1 output partitions
07:05:26.463 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (apply at Transformer.scala:22)
07:05:26.463 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
07:05:26.465 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
07:05:26.469 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 0)
07:05:26.472 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List()
07:05:26.474 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at apply at Transformer.scala:22), which has no missing parents
07:05:26.474 dag-scheduler-event-loop DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)
07:05:26.475 dag-scheduler-event-loop DEBUG DAGScheduler: partitionsToCompute Vector(0)
07:05:26.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ParallelCollectionPartition@691 of MapPartitionsRDD[1] at apply at Transformer.scala:22:
07:05:26.479 dag-scheduler-event-loop DEBUG ParallelCollectionRDD: Preferred locations for org.apache.spark.rdd.ParallelCollectionPartition@691 of ParallelCollectionRDD[0] at apply at Transformer.scala:22:
07:05:26.480 dag-scheduler-event-loop DEBUG DAGScheduler: taskIdToLocations Map(0 -> List())
07:05:26.542 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 52.1 KB, free 2.2 GB)
07:05:26.543 dag-scheduler-event-loop DEBUG BlockManager: Put block broadcast_0 locally took  25 ms
07:05:26.545 dag-scheduler-event-loop DEBUG BlockManager: Putting block broadcast_0 without replication took  27 ms
07:05:26.567 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.3 KB, free 2.2 GB)
07:05:26.569 dispatcher-event-loop-4 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.102:52176 (size: 19.3 KB, free: 2.2 GB)
07:05:26.570 dag-scheduler-event-loop DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
07:05:26.570 dag-scheduler-event-loop DEBUG BlockManager: Told master about block broadcast_0_piece0
07:05:26.571 dag-scheduler-event-loop DEBUG BlockManager: Put block broadcast_0_piece0 locally took  6 ms
07:05:26.571 dag-scheduler-event-loop DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took  6 ms
07:05:26.572 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
07:05:26.578 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at apply at Transformer.scala:22)
07:05:26.578 dag-scheduler-event-loop DEBUG DAGScheduler: New pending partitions: Set(0)
07:05:26.579 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
07:05:26.585 dag-scheduler-event-loop DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0
07:05:26.589 dag-scheduler-event-loop DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
07:05:26.602 dispatcher-event-loop-5 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
07:05:26.602 dispatcher-event-loop-5 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
07:05:26.606 dispatcher-event-loop-5 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:26.625 dispatcher-event-loop-5 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 8409 bytes)
07:05:26.632 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
07:05:26.659 Executor task launch worker-0 DEBUG Executor: Task 0's epoch is 0
07:05:26.661 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_0
07:05:26.663 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:26.673 Executor task launch worker-0 INFO deprecation: mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
07:05:26.675 Executor task launch worker-0 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
07:05:26.675 Executor task launch worker-0 INFO deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
07:05:26.676 Executor task launch worker-0 INFO deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
07:05:26.680 Executor task launch worker-0 INFO DynamicPartitionWriterContainer: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
07:05:26.684 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     int value2 = i.getInt(0);
/* 032 */     value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 033 */
/* 034 */     int value = -1;
/* 035 */
/* 036 */     int r = value1 % 8;
/* 037 */     if (r < 0) {
/* 038 */       value = (r + 8) % 8;
/* 039 */     } else {
/* 040 */       value = r;
/* 041 */     }
/* 042 */     rowWriter.write(0, value);
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:26.685 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     int value2 = i.getInt(0);
/* 032 */     value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 033 */
/* 034 */     int value = -1;
/* 035 */
/* 036 */     int r = value1 % 8;
/* 037 */     if (r < 0) {
/* 038 */       value = (r + 8) % 8;
/* 039 */     } else {
/* 040 */       value = r;
/* 041 */     }
/* 042 */     rowWriter.write(0, value);
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:26.695 Executor task launch worker-0 INFO CodeGenerator: Code generated in 11.320376 ms
07:05:26.697 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     int value1 = i.getInt(1);
/* 037 */     rowWriter.write(1, value1);
/* 038 */
/* 039 */     /* input[2, string] */
/* 040 */     boolean isNull2 = i.isNullAt(2);
/* 041 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 042 */     if (isNull2) {
/* 043 */       rowWriter.setNullAt(2);
/* 044 */     } else {
/* 045 */       rowWriter.write(2, value2);
/* 046 */     }
/* 047 */     result.setTotalSize(holder.totalSize());
/* 048 */     return result;
/* 049 */   }
/* 050 */ }
/* 051 */

07:05:26.698 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for concat():
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* concat() */
/* 032 */     boolean isNull = false;
/* 033 */     UTF8String value = UTF8String.concat();
/* 034 */     if (value == null) {
/* 035 */       isNull = true;
/* 036 */     }
/* 037 */     if (isNull) {
/* 038 */       rowWriter.setNullAt(0);
/* 039 */     } else {
/* 040 */       rowWriter.write(0, value);
/* 041 */     }
/* 042 */     result.setTotalSize(holder.totalSize());
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:26.699 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* concat() */
/* 032 */     boolean isNull = false;
/* 033 */     UTF8String value = UTF8String.concat();
/* 034 */     if (value == null) {
/* 035 */       isNull = true;
/* 036 */     }
/* 037 */     if (isNull) {
/* 038 */       rowWriter.setNullAt(0);
/* 039 */     } else {
/* 040 */       rowWriter.write(0, value);
/* 041 */     }
/* 042 */     result.setTotalSize(holder.totalSize());
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:26.707 Executor task launch worker-0 INFO CodeGenerator: Code generated in 8.228955 ms
07:05:26.713 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:26.713 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:26.729 Executor task launch worker-0 INFO CodeGenerator: Code generated in 15.623596 ms
07:05:26.733 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:26.734 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:26.745 Executor task launch worker-0 INFO CodeGenerator: Code generated in 11.491881 ms
07:05:26.755 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 0 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@40bfd2f9
07:05:26.757 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 0 acquire 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@40bfd2f9
07:05:26.764 Executor task launch worker-0 INFO DynamicPartitionWriterContainer: Sorting complete. Writing out partition files one at a time.
07:05:26.768 Executor task launch worker-0 DEBUG DynamicPartitionWriterContainer: Writing partition: [0,3]
07:05:26.801 Executor task launch worker-0 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "j",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "k",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 i;
  required int32 j;
  optional binary k (UTF8);
}


07:05:26.828 Executor task launch worker-0 INFO CodecPool: Got brand-new compressor [.snappy]
07:05:27.003 Executor task launch worker-0 DEBUG DynamicPartitionWriterContainer: Writing partition: [0,6]
07:05:27.006 Executor task launch worker-0 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "j",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "k",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 i;
  required int32 j;
  optional binary k (UTF8);
}


07:05:27.013 Executor task launch worker-0 INFO CodecPool: Got brand-new compressor [.snappy]
07:05:27.016 Executor task launch worker-0 DEBUG DynamicPartitionWriterContainer: Writing partition: [0,7]
07:05:27.017 Executor task launch worker-0 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "j",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "k",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 i;
  required int32 j;
  optional binary k (UTF8);
}


07:05:27.024 Executor task launch worker-0 INFO CodecPool: Got brand-new compressor [.snappy]
07:05:27.026 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 0 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@40bfd2f9
07:05:27.028 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 0 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@40bfd2f9
07:05:27.039 dispatcher-event-loop-7 DEBUG OutputCommitCoordinator: Authorizing attemptNumber=0 to commit for stage=0, partition=0
07:05:27.042 Executor task launch worker-0 INFO FileOutputCommitter: Saved output of task 'attempt_201604210705_0000_m_000000_0' to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/_temporary/0/task_201604210705_0000_m_000000
07:05:27.042 Executor task launch worker-0 INFO SparkHadoopMapRedUtil: attempt_201604210705_0000_m_000000_0: Committed
07:05:27.069 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2746 bytes result sent to driver
07:05:27.071 dispatcher-event-loop-0 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0, runningTasks: 0
07:05:27.071 dispatcher-event-loop-0 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:27.073 dispatcher-event-loop-0 INFO TaskSetManager: No tasks dequeued
07:05:27.074 dispatcher-event-loop-0 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
07:05:27.074 dispatcher-event-loop-0 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = ANY
 - allowedLocality = ANY

07:05:27.074 dispatcher-event-loop-0 INFO TaskSetManager: No tasks dequeued
07:05:27.077 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 471 ms on localhost (1/1)
07:05:27.079 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
07:05:27.085 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (apply at Transformer.scala:22) finished in 0.492 s
07:05:27.090 dag-scheduler-event-loop DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0
07:05:27.092 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DAGScheduler: Job 0 finished: apply at Transformer.scala:22, took 0.657495 s
07:05:27.096 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/_temporary/0/task_201604210705_0000_m_000000; isDirectory=true; modification_time=1461247527000; access_time=0; owner=; group=; permission=rwxrwxrwx; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1
07:05:27.098 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/_temporary/0/task_201604210705_0000_m_000000/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet; isDirectory=false; length=815; replication=1; blocksize=33554432; modification_time=1461247527000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet
07:05:27.098 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/_temporary/0/task_201604210705_0000_m_000000/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet; isDirectory=false; length=820; replication=1; blocksize=33554432; modification_time=1461247527000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet
07:05:27.099 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/_temporary/0/task_201604210705_0000_m_000000/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet; isDirectory=false; length=727; replication=1; blocksize=33554432; modification_time=1461247527000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet
07:05:27.143 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DynamicPartitionWriterContainer: Job job_201604210705_0000 committed.
07:05:27.163 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HDFSFileCatalog: Listing file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1 on driver
07:05:27.208 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HDFSFileCatalog: Listing file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1 on driver
07:05:27.242 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetastoreCatalog: Persisting data source relation `t1` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1.
07:05:27.243 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1 - 749795855
07:05:27.243 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.Table - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/Table.class
07:05:27.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.Table$ValidationFailureSemanticException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/Table$ValidationFailureSemanticException.class
07:05:27.245 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.catalyst.catalog.CatalogTable
07:05:27.245 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.catalyst.TableIdentifier
07:05:27.246 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/StorageDescriptor$StorageDescriptorStandardSchemeFactory.class
07:05:27.247 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/StorageDescriptor$StorageDescriptorTupleSchemeFactory.class
07:05:27.247 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.StorageDescriptor$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/StorageDescriptor$_Fields.class
07:05:27.248 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SerDeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SerDeInfo.class
07:05:27.248 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.Order - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/Order.class
07:05:27.249 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SkewedInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SkewedInfo.class
07:05:27.250 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SerDeInfo$SerDeInfoStandardSchemeFactory.class
07:05:27.250 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SerDeInfo$SerDeInfoTupleSchemeFactory.class
07:05:27.250 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SerDeInfo$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SerDeInfo$_Fields.class
07:05:27.251 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SkewedInfo$SkewedInfoStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SkewedInfo$SkewedInfoStandardSchemeFactory.class
07:05:27.252 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SkewedInfo$SkewedInfoTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SkewedInfo$SkewedInfoTupleSchemeFactory.class
07:05:27.252 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.SkewedInfo$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/SkewedInfo$_Fields.class
07:05:27.252 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.TableType - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/TableType.class
07:05:27.253 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.HiveUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/HiveUtils.class
07:05:27.254 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/HadoopDefaultAuthenticator.class
07:05:27.264 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Groups: Returning fetched groups for 'lian'
07:05:27.264 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Groups: Returning cached groups for 'lian'
07:05:27.264 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/authorization/DefaultHiveAuthorizationProvider.class
07:05:27.265 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/authorization/BitSetCheckedAuthorizationProvider.class
07:05:27.265 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/authorization/HiveAuthorizationProviderBase.class
07:05:27.266 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.AuthorizationException - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/AuthorizationException.class
07:05:27.266 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase$HiveProxy - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/security/authorization/HiveAuthorizationProviderBase$HiveProxy.class
07:05:27.267 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SessionState: Session is using authorization class class org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider
07:05:27.267 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.catalyst.catalog.CatalogTableType$
07:05:27.267 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$12 - 1040834774
07:05:27.267 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.catalyst.catalog.CatalogColumn
07:05:27.268 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FieldSchema$FieldSchemaStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FieldSchema$FieldSchemaStandardSchemeFactory.class
07:05:27.268 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FieldSchema$FieldSchemaTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FieldSchema$FieldSchemaTupleSchemeFactory.class
07:05:27.269 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.FieldSchema$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/FieldSchema$_Fields.class
07:05:27.269 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.collection.TraversableLike
07:05:27.269 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$13 - -73222634
07:05:27.270 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat
07:05:27.270 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$1 - 634292453
07:05:27.270 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.StructField - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/StructField.class
07:05:27.270 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.ProtectMode - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/ProtectMode.class
07:05:27.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.metadata.HiveStorageHandler - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.class
07:05:27.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$TableSpec - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer$TableSpec.class
07:05:27.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$2 - -1217976911
07:05:27.272 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.spark.util.Utils$
07:05:27.272 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.class
07:05:27.272 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.RecordReader
07:05:27.272 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$3 - 283539608
07:05:27.273 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$4 - -1400167328
07:05:27.273 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/MapredParquetOutputFormat.class
07:05:27.273 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.FileOutputFormat
07:05:27.274 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: parquet.hadoop.api.WriteSupport - jar:file:/Users/lian/.ivy2/cache/com.twitter/parquet-hadoop-bundle/jars/parquet-hadoop-bundle-1.6.0.jar!/parquet/hadoop/api/WriteSupport.class
07:05:27.277 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriteSupport.class
07:05:27.277 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/exec/FileSinkOperator$RecordWriter.class
07:05:27.278 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapreduce.OutputFormat
07:05:27.278 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: parquet.hadoop.ParquetOutputFormat - jar:file:/Users/lian/.ivy2/cache/com.twitter/parquet-hadoop-bundle/jars/parquet-hadoop-bundle-1.6.0.jar!/parquet/hadoop/ParquetOutputFormat.class
07:05:27.278 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
07:05:27.278 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$5 - -80902127
07:05:27.279 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$6 - 359136259
07:05:27.279 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$7 - 671034974
07:05:27.279 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$8 - -305952492
07:05:27.279 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$9 - 473736356
07:05:27.279 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$10 - 956736155
07:05:27.280 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$org$apache$spark$sql$hive$client$HiveClientImpl$$toHiveTable$11 - 1799834194
07:05:27.280 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.commons.lang3.StringUtils - jar:file:/Users/lian/.ivy2/cache/org.apache.commons/commons-lang3/jars/commons-lang3-3.3.2.jar!/org/apache/commons/lang3/StringUtils.class
07:05:27.285 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.class
07:05:27.285 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspector.class
07:05:27.285 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.Text
07:05:27.286 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: parquet.hadoop.ParquetWriter - jar:file:/Users/lian/.ivy2/cache/com.twitter/parquet-hadoop-bundle/jars/parquet-hadoop-bundle-1.6.0.jar!/parquet/hadoop/ParquetWriter.class
07:05:27.286 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: parquet.hadoop.metadata.CompressionCodecName - jar:file:/Users/lian/.ivy2/cache/com.twitter/parquet-hadoop-bundle/jars/parquet-hadoop-bundle-1.6.0.jar!/parquet/hadoop/metadata/CompressionCodecName.class
07:05:27.287 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: parquet.hadoop.codec.CompressionCodecNotSupportedException - jar:file:/Users/lian/.ivy2/cache/com.twitter/parquet-hadoop-bundle/jars/parquet-hadoop-bundle-1.6.0.jar!/parquet/hadoop/codec/CompressionCodecNotSupportedException.class
07:05:27.287 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: parquet.format.CompressionCodec - jar:file:/Users/lian/.ivy2/cache/com.twitter/parquet-hadoop-bundle/jars/parquet-hadoop-bundle-1.6.0.jar!/parquet/format/CompressionCodec.class
07:05:27.287 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: parquet.org.apache.thrift.TEnum - jar:file:/Users/lian/.ivy2/cache/org.apache.parquet/parquet-format/jars/parquet-format-2.3.0-incubating.jar!/parquet/org/apache/thrift/TEnum.class
07:05:27.288 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: parquet.column.ParquetProperties$WriterVersion - jar:file:/Users/lian/.ivy2/cache/com.twitter/parquet-hadoop-bundle/jars/parquet-hadoop-bundle-1.6.0.jar!/parquet/column/ParquetProperties$WriterVersion.class
07:05:27.288 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hive.common.util.ReflectionUtil - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hive/common/util/ReflectionUtil.class
07:05:27.289 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.mapred.JobConfigurable
07:05:27.289 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: com.google.common.cache.Cache
07:05:27.290 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.ParquetHiveRecord - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/ParquetHiveRecord.class
07:05:27.290 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.SerDeStats - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/SerDeStats.class
07:05:27.291 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.SerDeUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/SerDeUtils.class
07:05:27.292 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.class
07:05:27.293 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.TypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.class
07:05:27.293 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.class
07:05:27.294 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils$TypeInfoParser.class
07:05:27.295 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/VarcharTypeInfo.class
07:05:27.295 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/BaseCharTypeInfo.class
07:05:27.295 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/CharTypeInfo.class
07:05:27.296 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/DecimalTypeInfo.class
07:05:27.296 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser$Token - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils$TypeInfoParser$Token.class
07:05:27.296 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.class
07:05:27.297 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: java.nio.charset.CharacterCodingException - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/java/nio/charset/CharacterCodingException.class
07:05:27.298 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils$PrimitiveTypeEntry - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils$PrimitiveTypeEntry.class
07:05:27.298 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector$PrimitiveCategory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector$PrimitiveCategory.class
07:05:27.299 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.BytesWritable
07:05:27.299 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.BooleanWritable
07:05:27.299 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.IntWritable
07:05:27.299 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.LongWritable
07:05:27.299 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.FloatWritable
07:05:27.299 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.NullWritable
07:05:27.299 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.DoubleWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/DoubleWritable.class
07:05:27.300 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.DoubleWritable
07:05:27.300 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.ByteWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/ByteWritable.class
07:05:27.300 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.ByteWritable
07:05:27.300 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.ShortWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/ShortWritable.class
07:05:27.300 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.io.WritableComparable
07:05:27.300 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.DateWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/DateWritable.class
07:05:27.301 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.TimestampWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/TimestampWritable.class
07:05:27.301 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.type.HiveIntervalYearMonth - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/type/HiveIntervalYearMonth.class
07:05:27.302 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.HiveIntervalYearMonthWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/HiveIntervalYearMonthWritable.class
07:05:27.302 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.type.HiveIntervalDayTime - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/type/HiveIntervalDayTime.class
07:05:27.302 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.HiveIntervalDayTimeWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/HiveIntervalDayTimeWritable.class
07:05:27.303 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.type.HiveDecimal - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/type/HiveDecimal.class
07:05:27.303 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.HiveDecimalWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.class
07:05:27.304 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.type.HiveVarchar - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/type/HiveVarchar.class
07:05:27.304 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.type.HiveBaseChar - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/type/HiveBaseChar.class
07:05:27.304 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/HiveVarcharWritable.class
07:05:27.305 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/HiveBaseCharWritable.class
07:05:27.305 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.type.HiveChar - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/type/HiveChar.class
07:05:27.305 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.io.HiveCharWritable - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/io/HiveCharWritable.class
07:05:27.306 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils$1.class
07:05:27.306 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector$Category - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspector$Category.class
07:05:27.306 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.class
07:05:27.307 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.class
07:05:27.307 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/UnionTypeInfo.class
07:05:27.308 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.class
07:05:27.308 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.class
07:05:27.308 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/BaseCharUtils.class
07:05:27.309 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/typeinfo/HiveDecimalUtils.class
07:05:27.309 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.class
07:05:27.310 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.SettableStructObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/SettableStructObjectInspector.class
07:05:27.310 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/StructObjectInspector.class
07:05:27.311 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector$StructFieldImpl - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector$StructFieldImpl.class
07:05:27.311 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorFactory.class
07:05:27.312 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/AbstractPrimitiveWritableObjectInspector.class
07:05:27.312 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/AbstractPrimitiveObjectInspector.class
07:05:27.313 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.class
07:05:27.313 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveCharObjectInspector.class
07:05:27.313 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveCharObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableHiveCharObjectInspector.class
07:05:27.313 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveCharObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/HiveCharObjectInspector.class
07:05:27.314 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveVarcharObjectInspector.class
07:05:27.314 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveVarcharObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableHiveVarcharObjectInspector.class
07:05:27.314 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveVarcharObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/HiveVarcharObjectInspector.class
07:05:27.315 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveDecimalObjectInspector.class
07:05:27.315 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveDecimalObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableHiveDecimalObjectInspector.class
07:05:27.315 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/HiveDecimalObjectInspector.class
07:05:27.316 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/ConstantObjectInspector.class
07:05:27.316 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/AbstractPrimitiveJavaObjectInspector.class
07:05:27.316 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveCharObjectInspector.class
07:05:27.316 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveVarcharObjectInspector.class
07:05:27.317 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveDecimalObjectInspector.class
07:05:27.317 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBooleanObjectInspector.class
07:05:27.317 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBooleanObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableBooleanObjectInspector.class
07:05:27.318 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/BooleanObjectInspector.class
07:05:27.318 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableByteObjectInspector.class
07:05:27.318 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableByteObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableByteObjectInspector.class
07:05:27.318 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/ByteObjectInspector.class
07:05:27.319 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableShortObjectInspector.class
07:05:27.319 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableShortObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableShortObjectInspector.class
07:05:27.319 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/ShortObjectInspector.class
07:05:27.319 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableIntObjectInspector.class
07:05:27.320 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableIntObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableIntObjectInspector.class
07:05:27.320 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/IntObjectInspector.class
07:05:27.320 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableLongObjectInspector.class
07:05:27.321 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableLongObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableLongObjectInspector.class
07:05:27.321 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/LongObjectInspector.class
07:05:27.321 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableFloatObjectInspector.class
07:05:27.321 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableFloatObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableFloatObjectInspector.class
07:05:27.322 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/FloatObjectInspector.class
07:05:27.322 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableDoubleObjectInspector.class
07:05:27.322 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableDoubleObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableDoubleObjectInspector.class
07:05:27.322 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/DoubleObjectInspector.class
07:05:27.323 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableStringObjectInspector.class
07:05:27.323 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableStringObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableStringObjectInspector.class
07:05:27.323 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/StringObjectInspector.class
07:05:27.324 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableVoidObjectInspector.class
07:05:27.324 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.VoidObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/VoidObjectInspector.class
07:05:27.324 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableDateObjectInspector.class
07:05:27.324 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableDateObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableDateObjectInspector.class
07:05:27.325 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.DateObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/DateObjectInspector.class
07:05:27.325 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableTimestampObjectInspector.class
07:05:27.325 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableTimestampObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableTimestampObjectInspector.class
07:05:27.325 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/TimestampObjectInspector.class
07:05:27.326 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveIntervalYearMonthObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveIntervalYearMonthObjectInspector.class
07:05:27.326 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveIntervalYearMonthObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableHiveIntervalYearMonthObjectInspector.class
07:05:27.326 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveIntervalYearMonthObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/HiveIntervalYearMonthObjectInspector.class
07:05:27.327 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveIntervalDayTimeObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveIntervalDayTimeObjectInspector.class
07:05:27.327 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveIntervalDayTimeObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableHiveIntervalDayTimeObjectInspector.class
07:05:27.327 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveIntervalDayTimeObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/HiveIntervalDayTimeObjectInspector.class
07:05:27.328 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBinaryObjectInspector.class
07:05:27.328 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBinaryObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableBinaryObjectInspector.class
07:05:27.328 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/BinaryObjectInspector.class
07:05:27.329 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBooleanObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBooleanObjectInspector.class
07:05:27.329 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaByteObjectInspector.class
07:05:27.330 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaShortObjectInspector.class
07:05:27.330 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaIntObjectInspector.class
07:05:27.330 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaLongObjectInspector.class
07:05:27.330 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaFloatObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaFloatObjectInspector.class
07:05:27.331 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDoubleObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaDoubleObjectInspector.class
07:05:27.331 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaStringObjectInspector.class
07:05:27.332 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaVoidObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaVoidObjectInspector.class
07:05:27.332 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaDateObjectInspector.class
07:05:27.332 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaTimestampObjectInspector.class
07:05:27.333 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveIntervalYearMonthObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveIntervalYearMonthObjectInspector.class
07:05:27.333 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveIntervalDayTimeObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveIntervalDayTimeObjectInspector.class
07:05:27.333 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBinaryObjectInspector.class
07:05:27.334 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetPrimitiveInspectorFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetPrimitiveInspectorFactory.class
07:05:27.334 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetByteInspector.class
07:05:27.335 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetShortInspector.class
07:05:27.335 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/serde/primitive/ParquetStringInspector.class
07:05:27.336 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe$LAST_OPERATION - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe$LAST_OPERATION.class
07:05:27.337 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet$PrincipalPrivilegeSetStandardSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrincipalPrivilegeSet$PrincipalPrivilegeSetStandardSchemeFactory.class
07:05:27.337 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet$PrincipalPrivilegeSetTupleSchemeFactory - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrincipalPrivilegeSet$PrincipalPrivilegeSetTupleSchemeFactory.class
07:05:27.337 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet$_Fields - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/api/PrincipalPrivilegeSet$_Fields.class
07:05:27.338 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: create_table: Table(tableName:t1, dbName:default, owner:lian, createTime:1461247527, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:i, type:int, comment:null), FieldSchema(name:j, type:int, comment:null), FieldSchema(name:k, type:string, comment:null)], location:file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.bucketCol.0=i, spark.sql.sources.schema.numBuckets=8, EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"i","type":"integer","nullable":true,"metadata":{}},{"name":"j","type":"integer","nullable":true,"metadata":{}},{"name":"k","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.schema.numBucketCols=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
07:05:27.338 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=create_table: Table(tableName:t1, dbName:default, owner:lian, createTime:1461247527, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:i, type:int, comment:null), FieldSchema(name:j, type:int, comment:null), FieldSchema(name:k, type:string, comment:null)], location:file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.bucketCol.0=i, spark.sql.sources.schema.numBuckets=8, EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"i","type":"integer","nullable":true,"metadata":{}},{"name":"j","type":"integer","nullable":true,"metadata":{}},{"name":"k","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.schema.numBucketCols=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
07:05:27.338 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.338 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.339 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
07:05:27.339 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
07:05:27.340 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SQL Query : "select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = ?"
07:05:27.340 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@27e61c0" opened with isolation level "read-committed" and auto-commit=false
07:05:27.340 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1770084b, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.340 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1770084b is starting for transaction Xid=    with flags 0
07:05:27.340 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.340 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@37144ba0"
07:05:27.340 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = <'default'>
07:05:27.340 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SQL Query : "select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = ?  AND "PARAM_KEY" IS NOT NULL"
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@31aecbd8"
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = <1>  AND "PARAM_KEY" IS NOT NULL
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaStoreDirectSql: getDatabase: directsql returning db default locn[file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5] desc [Default Hive database] owner [public] ownertype [ROLE]
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: db details for db default retrieved using SQL in 2.454797ms
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.341 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 3, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@584f3dbc"
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t1'> AND B0."NAME" = <'default'>
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 2, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.342 pool-1-thread-1-ScalaTest-running-BucketedReadSuite WARN HiveMetaStore: Location: file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1 specified for non-external table:t1
07:05:27.343 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.HiveStatsUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/HiveStatsUtils.class
07:05:27.343 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.FileUtils - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/FileUtils.class
07:05:27.344 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.FileUtils$1 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/FileUtils$1.class
07:05:27.345 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.FileUtils$2 - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/FileUtils$2.class
07:05:27.346 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.common.StatsSetupConst - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar!/org/apache/hadoop/hive/common/StatsSetupConst.class
07:05:27.346 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:812)
07:05:27.346 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 3, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:535)
07:05:27.346 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Compiling "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == dbname PARAMETERS java.lang.String dbname"
07:05:27.347 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Compile Time = 1 ms
07:05:27.347 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: QueryCompilation:
  [filter:DyadicExpression{PrimaryExpression{name}  =  ParameterExpression{dbname}}]
  [symbols: dbname type=java.lang.String, this type=org.apache.hadoop.hive.metastore.model.MDatabase]
07:05:27.347 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Compiling "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == dbname PARAMETERS java.lang.String dbname" for datastore
07:05:27.347 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Compile Time for datastore = 0 ms
07:05:27.347 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == dbname PARAMETERS java.lang.String dbname Query compiled to datastore query "SELECT 'org.apache.hadoop.hive.metastore.model.MDatabase' AS NUCLEUS_TYPE,A0."DESC",A0.DB_LOCATION_URI,A0."NAME",A0.OWNER_NAME,A0.OWNER_TYPE,A0.DB_ID FROM DBS A0 WHERE A0."NAME" = ?"
07:05:27.347 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.347 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == dbname PARAMETERS java.lang.String dbname" ...
07:05:27.357 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4b195f5d"
07:05:27.357 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MDatabase' AS NUCLEUS_TYPE,A0."DESC",A0.DB_LOCATION_URI,A0."NAME",A0.OWNER_NAME,A0.OWNER_TYPE,A0.DB_ID FROM DBS A0 WHERE A0."NAME" = <'default'>
07:05:27.358 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.358 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 11 ms
07:05:27.358 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:27.358 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" not found in Level 1 cache [cache size = 0]
07:05:27.358 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") added to Level 1 cache (loadedFlags="[NNNNNN]")
07:05:27.360 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.360 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.360 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.360 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@621fb489"
07:05:27.360 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM DATABASE_PARAMS A0 WHERE A0.DB_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:27.360 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3cb49d4b"
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") enlisted in transactional cache
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 2, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:542)
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MColumnDescriptor
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MSerDeInfo
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MStorageDescriptor
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MTable
07:05:27.361 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaData: Listener found initialisation for persistable class org.apache.hadoop.hive.metastore.model.MFieldSchema
07:05:27.362 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MTable@2310cda8"
07:05:27.362 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MTable"
07:05:27.362 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:27.362 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:27.362 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@340a23ea" opened with isolation level "read-committed" and auto-commit=false
07:05:27.363 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:27.363 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:27.363 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [I - null
07:05:27.364 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:27.365 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [I - null
07:05:27.365 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:27.366 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:27.367 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [I - null
07:05:27.367 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:27.367 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [I - null
07:05:27.368 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:27.368 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Z - null
07:05:27.368 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [I - null
07:05:27.368 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: [Ljava.lang.Object; - null
07:05:27.370 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: sun.reflect.MethodAccessorImpl - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/sun/reflect/MethodAccessorImpl.class
07:05:27.370 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:27.370 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@217c2b5e"
07:05:27.370 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MTable'> FOR UPDATE
07:05:27.370 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.372 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5ad720eb"
07:05:27.372 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT MAX(TBL_ID) FROM TBLS
07:05:27.372 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.372 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2df42414" for connection "com.jolbox.bonecp.ConnectionHandle@340a23ea"
07:05:27.372 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MTable'>,<6>)
07:05:27.372 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2df42414"
07:05:27.372 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2df42414"
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@312bc5c0"
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@15cdc3dc"
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Reserved a block of 5 values
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@340a23ea" non enlisted to a transaction is being committed.
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@340a23ea" closed
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MTable (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") added to Level 1 cache (loadedFlags="[YYYYYYYYYYYY]")
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") enlisted in transactional cache
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" being inserted into table "TBLS"
07:05:27.373 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.374 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.FKInfo - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/FKInfo.class
07:05:27.376 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4998429f" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.376 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.SerialisedReferenceMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/SerialisedReferenceMapping.class
07:05:27.376 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MTable.sd" is being persisted for "cascade-persist".
07:05:27.376 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23"
07:05:27.376 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MStorageDescriptor"
07:05:27.376 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:27.376 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:27.377 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@40d5e26c" opened with isolation level "read-committed" and auto-commit=false
07:05:27.377 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:27.377 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@415968aa"
07:05:27.377 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MStorageDescriptor'> FOR UPDATE
07:05:27.377 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.379 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@65d0ba7b"
07:05:27.379 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT MAX(SD_ID) FROM SDS
07:05:27.379 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5e0c6b16" for connection "com.jolbox.bonecp.ConnectionHandle@40d5e26c"
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MStorageDescriptor'>,<6>)
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5e0c6b16"
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5e0c6b16"
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@638fb387"
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6c0b685"
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Reserved a block of 5 values
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@40d5e26c" non enlisted to a transaction is being committed.
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@40d5e26c" closed
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") added to Level 1 cache (loadedFlags="[YYYYYYYYYYYYYY]")
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") enlisted in transactional cache
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" being inserted into table "SDS"
07:05:27.380 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.382 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@42469249" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.382 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo" is being persisted for "cascade-persist".
07:05:27.382 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb"
07:05:27.382 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MSerDeInfo"
07:05:27.383 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:27.383 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:27.383 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@75e0e79b" opened with isolation level "read-committed" and auto-commit=false
07:05:27.383 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:27.383 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5ee39674"
07:05:27.383 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MSerDeInfo'> FOR UPDATE
07:05:27.383 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.387 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@20751178"
07:05:27.387 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT MAX(SERDE_ID) FROM SERDES
07:05:27.387 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.387 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7111f2dd" for connection "com.jolbox.bonecp.ConnectionHandle@75e0e79b"
07:05:27.387 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MSerDeInfo'>,<6>)
07:05:27.387 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7111f2dd"
07:05:27.387 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7111f2dd"
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@75481bd9"
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@64e6152b"
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Reserved a block of 5 values
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@75e0e79b" non enlisted to a transaction is being committed.
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@75e0e79b" closed
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MSerDeInfo (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") added to Level 1 cache (loadedFlags="[YYY]")
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") enlisted in transactional cache
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" being inserted into table "SERDES"
07:05:27.388 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.389 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6f168d96" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.389 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: The requested statement "INSERT INTO SERDES (SERDE_ID,SLIB,"NAME") VALUES (?,?,?)" has been made batchable
07:05:27.389 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Batch has been added to statement "INSERT INTO SERDES (SERDE_ID,SLIB,"NAME") VALUES (?,?,?)" for processing (batch size = 1)
07:05:27.389 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters"
07:05:27.389 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters"
07:05:27.389 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters" is being persisted for "cascade-persist".
07:05:27.390 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.390 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: BATCH [INSERT INTO SERDES (SERDE_ID,SLIB,"NAME") VALUES (<1>,<'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'>,<null>)]
07:05:27.391 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Execution Time = 1 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6f168d96"
07:05:27.394 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6539e1e3"
07:05:27.394 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM SERDE_PARAMS A0 WHERE A0.SERDE_ID = <1> AND A0.PARAM_KEY = <'path'>
07:05:27.394 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.394 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@44b6371"
07:05:27.395 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.395 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@327c26dd"
07:05:27.395 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM SERDE_PARAMS A0 WHERE A0.SERDE_ID = <1> AND A0.PARAM_KEY = <'serialization.format'>
07:05:27.395 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.395 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@27a7e84"
07:05:27.395 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.396 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@331cb0fb" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.396 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO SERDE_PARAMS (PARAM_VALUE,SERDE_ID,PARAM_KEY) VALUES (<'file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1'>,<1>,<'path'>)
07:05:27.396 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.RISetChecker - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/RISetChecker.class
07:05:27.397 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ForeignKeyRIChecker - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ForeignKeyRIChecker.class
07:05:27.397 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.GenericRIChecker - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/GenericRIChecker.class
07:05:27.397 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.ReferencedKeyRIChecker - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/ReferencedKeyRIChecker.class
07:05:27.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 2 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@331cb0fb"
07:05:27.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@331cb0fb"
07:05:27.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@392ac6c6" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO SERDE_PARAMS (PARAM_VALUE,SERDE_ID,PARAM_KEY) VALUES (<'1'>,<1>,<'serialization.format'>)
07:05:27.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@392ac6c6"
07:05:27.398 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@392ac6c6"
07:05:27.399 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.HashMap" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.399 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" field "parameters" is having its SCO wrapper initialised with a container with 2 values
07:05:27.399 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.399 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.399 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd" is being persisted for "cascade-persist".
07:05:27.399 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4"
07:05:27.400 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Creating ValueGenerator instance of "org.datanucleus.store.rdbms.valuegenerator.TableGenerator" for "org.apache.hadoop.hive.metastore.model.MColumnDescriptor"
07:05:27.400 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Column "APP.SEQUENCE_TABLE.SEQUENCE_NAME" added to internal representation of table.
07:05:27.400 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Column "APP.SEQUENCE_TABLE.NEXT_VAL" added to internal representation of table.
07:05:27.400 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5bce6caf" opened with isolation level "read-committed" and auto-commit=false
07:05:27.400 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Schema: Check of existence of APP.SEQUENCE_TABLE returned table type of TABLE
07:05:27.400 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1af26e9"
07:05:27.400 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT NEXT_VAL FROM APP.SEQUENCE_TABLE WHERE SEQUENCE_NAME=<'org.apache.hadoop.hive.metastore.model.MColumnDescriptor'> FOR UPDATE
07:05:27.400 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@109325b3"
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT MAX(CD_ID) FROM CDS
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6fbb3ce2" for connection "com.jolbox.bonecp.ConnectionHandle@5bce6caf"
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO APP.SEQUENCE_TABLE (SEQUENCE_NAME,NEXT_VAL) VALUES (<'org.apache.hadoop.hive.metastore.model.MColumnDescriptor'>,<6>)
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6fbb3ce2"
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6fbb3ce2"
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2692a65d"
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3401ee0d"
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Reserved a block of 5 values
07:05:27.403 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5bce6caf" non enlisted to a transaction is being committed.
07:05:27.404 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5bce6caf" closed
07:05:27.404 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=1
07:05:27.404 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") added to Level 1 cache (loadedFlags="[Y]")
07:05:27.404 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") enlisted in transactional cache
07:05:27.404 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" being inserted into table "CDS"
07:05:27.404 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.405 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@60bbb546" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.405 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO CDS (CD_ID) VALUES (<1>)
07:05:27.405 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@60bbb546"
07:05:27.405 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols"
07:05:27.405 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@60bbb546"
07:05:27.406 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols"
07:05:27.406 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols" is being persisted for "cascade-persist".
07:05:27.406 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.JoinListStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/JoinListStore.class
07:05:27.407 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.AbstractListStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/AbstractListStore.class
07:05:27.408 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.AbstractCollectionStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/AbstractCollectionStore.class
07:05:27.409 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.ElementContainerStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/ElementContainerStore.class
07:05:27.412 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.413 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4596374e" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.413 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: The requested statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " has been made batchable
07:05:27.414 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Batch has been added to statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " for processing (batch size = 1)
07:05:27.414 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: BATCH [INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (<1>,<null>,<'i'>,<'int'>,<0>) ]
07:05:27.414 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4596374e"
07:05:27.414 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4596374e"
07:05:27.414 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6d0bea3d" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.414 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: The requested statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " has been made batchable
07:05:27.414 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Batch has been added to statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " for processing (batch size = 1)
07:05:27.414 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: BATCH [INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (<1>,<null>,<'j'>,<'int'>,<1>) ]
07:05:27.415 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Execution Time = 1 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6d0bea3d"
07:05:27.415 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6d0bea3d"
07:05:27.415 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@71616d67" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.415 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: The requested statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " has been made batchable
07:05:27.415 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Batch has been added to statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " for processing (batch size = 1)
07:05:27.415 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: BATCH [INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (<1>,<null>,<'k'>,<'string'>,<2>) ]
07:05:27.415 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@71616d67"
07:05:27.415 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@71616d67"
07:05:27.417 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" field "cols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.418 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" field "cols" is having its SCO wrapper initialised with a container with 3 values
07:05:27.418 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO SDS (SD_ID,CD_ID,IS_STOREDASSUBDIRECTORIES,IS_COMPRESSED,SERDE_ID,LOCATION,NUM_BUCKETS,INPUT_FORMAT,OUTPUT_FORMAT) VALUES (<1>,<1>,<'N'>,<'N'>,<1>,<'file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1'>,<-1>,<'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'>,<'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'>)
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@42469249"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@42469249"
07:05:27.419 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd"
07:05:27.420 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols"
07:05:27.420 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols" is being persisted for "cascade-persist".
07:05:27.420 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "sortCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.420 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "sortCols" is having its SCO wrapper initialised with a container with 0 values
07:05:27.420 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps"
07:05:27.420 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps" is being persisted for "cascade-persist".
07:05:27.421 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "skewedColValueLocationMaps" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.HashMap" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.421 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "skewedColValueLocationMaps" is having its SCO wrapper initialised with a container with 0 values
07:05:27.421 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols"
07:05:27.421 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols" is being persisted for "cascade-persist".
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "bucketCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "bucketCols" is having its SCO wrapper initialised with a container with 0 values
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo"
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters"
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters" is being persisted for "cascade-persist".
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.HashMap" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "parameters" is having its SCO wrapper initialised with a container with 0 values
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues"
07:05:27.423 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues" is being persisted for "cascade-persist".
07:05:27.424 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.ElementContainerStore$ElementInfo - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/ElementContainerStore$ElementInfo.class
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "skewedColValues" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "skewedColValues" is having its SCO wrapper initialised with a container with 0 values
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames"
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames" is being persisted for "cascade-persist".
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "skewedColNames" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" field "skewedColNames" is having its SCO wrapper initialised with a container with 0 values
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TBLS (TBL_ID,LAST_ACCESS_TIME,DB_ID,SD_ID,TBL_TYPE,OWNER,TBL_NAME,VIEW_ORIGINAL_TEXT,VIEW_EXPANDED_TEXT,CREATE_TIME,RETENTION) VALUES (<1>,<0>,<1>,<1>,<'MANAGED_TABLE'>,<'lian'>,<'t1'>,<null>,<null>,<1461247527>,<0>)
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4998429f"
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MTable.parameters"
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MTable.database"
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MTable.sd"
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MTable.partitionKeys"
07:05:27.425 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4998429f"
07:05:27.426 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MTable.parameters"
07:05:27.426 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MTable.parameters" is being persisted for "cascade-persist".
07:05:27.426 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.429 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@275bb42c"
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'spark.sql.sources.schema.bucketCol.0'>
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@40459da5"
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@11dd0620"
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'numRows'>
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7141b198"
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1d85b2a0"
07:05:27.430 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'rawDataSize'>
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@66187f75"
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1a29df5f"
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'transient_lastDdlTime'>
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4ca65994"
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@696064a2"
07:05:27.431 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'spark.sql.sources.schema.numBuckets'>
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5c7fdd7c"
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@226b559e"
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'totalSize'>
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@494d4cd6"
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6bfbfcbe"
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'EXTERNAL'>
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@78eb1ddf"
07:05:27.432 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5b63d5e8"
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'spark.sql.sources.schema.part.0'>
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@58a54976"
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@7800a258"
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'COLUMN_STATS_ACCURATE'>
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1c80a7de"
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6e1ca3c1"
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'numFiles'>
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.433 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@61a9a8af"
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@788e5d13"
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'spark.sql.sources.schema.numParts'>
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7e88792e"
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@52c85a80"
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'spark.sql.sources.schema.numBucketCols'>
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@14b41fc8"
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@29485579"
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY = <'spark.sql.sources.provider'>
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4607a626"
07:05:27.434 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.436 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@44c8d0b3" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.436 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'-1'>,<1>,<'numRows'>)
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@44c8d0b3"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@44c8d0b3"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@45ae9415" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'-1'>,<1>,<'rawDataSize'>)
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@45ae9415"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@45ae9415"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@76915fb6" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'3'>,<1>,<'numFiles'>)
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@76915fb6"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@76915fb6"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@e597325" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'1461247527'>,<1>,<'transient_lastDdlTime'>)
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@e597325"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@e597325"
07:05:27.437 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e720a52" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'i'>,<1>,<'spark.sql.sources.schema.bucketCol.0'>)
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e720a52"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e720a52"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4e4ad09b" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'8'>,<1>,<'spark.sql.sources.schema.numBuckets'>)
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4e4ad09b"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4e4ad09b"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@15dc9468" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'1'>,<1>,<'spark.sql.sources.schema.numParts'>)
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@15dc9468"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@15dc9468"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7023b6e5" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.438 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'1'>,<1>,<'spark.sql.sources.schema.numBucketCols'>)
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7023b6e5"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7023b6e5"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@19faed15" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'FALSE'>,<1>,<'EXTERNAL'>)
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@19faed15"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@19faed15"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@11f08f3f" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'{"type":"struct","fields":[{"name":"i","type":"integer","nullable":true,"metadata":{}},{"name":"j","type":"integer","nullable":true,"metadata":{}},{"name":"k","type":"string","nullable":true,"metadata":{}}]}'>,<1>,<'spark.sql.sources.schema.part.0'>)
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@11f08f3f"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@11f08f3f"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@407cfafc" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.439 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'parquet'>,<1>,<'spark.sql.sources.provider'>)
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@407cfafc"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@407cfafc"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6ca5e1dd" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'2362'>,<1>,<'totalSize'>)
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6ca5e1dd"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6ca5e1dd"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6fff6b74" for connection "com.jolbox.bonecp.ConnectionHandle@27e61c0"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'false'>,<1>,<'COLUMN_STATS_ACCURATE'>)
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6fff6b74"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6fff6b74"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.HashMap" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" field "parameters" is having its SCO wrapper initialised with a container with 13 values
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MTable.database"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MTable.sd"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MTable.partitionKeys"
07:05:27.440 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MTable.partitionKeys" is being persisted for "cascade-persist".
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" field "partitionKeys" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" field "partitionKeys" is having its SCO wrapper initialised with a container with 0 values
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:830)
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MTable"
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") taken from Level 1 cache (loadedFlags="[YYYYYYYYYYYY]") [cache size = 5]
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on single-valued relation field "org.apache.hadoop.hive.metastore.model.MTable.sd"
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:27.441 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols"
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.types.SCOListIterator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/store/types/SCOListIterator.class
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues"
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps"
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on single-valued relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo"
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on single-valued relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd"
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols"
07:05:27.442 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.identity.IdentityReference - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/identity/IdentityReference.class
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@528a0ef8" (id="org.datanucleus.identity.IdentityReference@1c41a46c") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@883aeec" (id="org.datanucleus.identity.IdentityReference@32e856d6") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1b0f395c" (id="org.datanucleus.identity.IdentityReference@19f00954") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MTable.partitionKeys"
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on single-valued relation field "org.apache.hadoop.hive.metastore.model.MTable.database"
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") taken from Level 1 cache (loadedFlags="[YYYYYY]") [cache size = 5]
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.443 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1770084b]]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1770084b is committing for transaction Xid=    with onePhase=true
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1770084b committed connection for transaction Xid=    with onePhase=true
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@27e61c0" closed
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@469552d3 [conn=com.jolbox.bonecp.ConnectionHandle@27e61c0, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" (depth=0)
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") being evicted from transactional cache
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb from StateManager[pc=org.apache.hadoop.hive.metastore.model.MSerDeInfo@18938beb, lifecycle=DETACHED_CLEAN]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" being removed from Level 1 cache [current cache size = 5]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" (depth=0)
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@528a0ef8" (depth=0)
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@528a0ef8" (id="org.datanucleus.identity.IdentityReference@1c41a46c") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@528a0ef8" (id="org.datanucleus.identity.IdentityReference@1c41a46c") being evicted from transactional cache
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@528a0ef8" (id="org.datanucleus.identity.IdentityReference@1c41a46c") is not transactional
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@528a0ef8 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@528a0ef8, lifecycle=DETACHED_CLEAN]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@1c41a46c" being removed from Level 1 cache [current cache size = 4]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@1c41a46c" couldnt be removed from Level 1 cache - wasnt present.
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@883aeec" (depth=0)
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@883aeec" (id="org.datanucleus.identity.IdentityReference@32e856d6") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@883aeec" (id="org.datanucleus.identity.IdentityReference@32e856d6") being evicted from transactional cache
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@883aeec" (id="org.datanucleus.identity.IdentityReference@32e856d6") is not transactional
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@883aeec from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@883aeec, lifecycle=DETACHED_CLEAN]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@32e856d6" being removed from Level 1 cache [current cache size = 4]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@32e856d6" couldnt be removed from Level 1 cache - wasnt present.
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@1b0f395c" (depth=0)
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1b0f395c" (id="org.datanucleus.identity.IdentityReference@19f00954") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1b0f395c" (id="org.datanucleus.identity.IdentityReference@19f00954") being evicted from transactional cache
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1b0f395c" (id="org.datanucleus.identity.IdentityReference@19f00954") is not transactional
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@1b0f395c from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@1b0f395c, lifecycle=DETACHED_CLEAN]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@19f00954" being removed from Level 1 cache [current cache size = 4]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@19f00954" couldnt be removed from Level 1 cache - wasnt present.
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") is having the SCO wrapper in field "cols" replaced by the unwrapped value
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") being evicted from transactional cache
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MColumnDescriptor@79d6e4f4, lifecycle=DETACHED_CLEAN]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" being removed from Level 1 cache [current cache size = 4]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (depth=0)
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (depth=1)
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2253f719" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") being evicted from transactional cache
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MDatabase@2253f719 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MDatabase@2253f719, lifecycle=DETACHED_CLEAN]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" being removed from Level 1 cache [current cache size = 3]
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.444 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "partitionKeys" replaced by the unwrapped value
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (depth=1)
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "bucketCols" replaced by the unwrapped value
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "skewedColNames" replaced by the unwrapped value
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "sortCols" replaced by the unwrapped value
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") being evicted from transactional cache
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4b52dd23, lifecycle=DETACHED_CLEAN]
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" being removed from Level 1 cache [current cache size = 2]
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@2310cda8" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") being evicted from transactional cache
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MTable@2310cda8 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MTable@2310cda8, lifecycle=DETACHED_CLEAN]
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MTable" being removed from Level 1 cache [current cache size = 1]
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 4 ms
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.CreateTableEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/CreateTableEvent.class
07:05:27.445 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.hadoop.hive.metastore.events.ListenerEvent - jar:file:/Users/lian/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar!/org/apache/hadoop/hive/metastore/events/ListenerEvent.class
07:05:27.451 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#41, j#42, k#43) AS #49]   Project [createexternalrow(if (isnull(i#41)) null else i#41, if (isnull(j#42)) null else j#42, if (isnull(k#43)) null else k#43.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #49]
 +- LocalRelation [i#41,j#42,k#43]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#41,j#42,k#43]

07:05:27.453 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveSqlParser: Parsing command: t2
07:05:27.453 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t2
07:05:27.454 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t2
07:05:27.454 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t2
07:05:27.454 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.454 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.454 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.454 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1903f9bc" opened with isolation level "read-committed" and auto-commit=false
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@63519b2b, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@63519b2b is starting for transaction Xid=    with flags 0
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@328c30c2 [conn=com.jolbox.bonecp.ConnectionHandle@1903f9bc, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@34c4f136"
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t2'> AND B0."NAME" = <'default'>
07:05:27.455 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 1 ms
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@63519b2b]]
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@63519b2b is committing for transaction Xid=    with onePhase=true
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@63519b2b committed connection for transaction Xid=    with onePhase=true
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@1903f9bc" closed
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@328c30c2 [conn=com.jolbox.bonecp.ConnectionHandle@1903f9bc, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.456 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 0 ms
07:05:27.457 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.458 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t2
07:05:27.458 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t2
07:05:27.458 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t2
07:05:27.458 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.458 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.458 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@72234d77" opened with isolation level "read-committed" and auto-commit=false
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1b3f2476, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1b3f2476 is starting for transaction Xid=    with flags 0
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@78c668d9 [conn=com.jolbox.bonecp.ConnectionHandle@72234d77, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1066ca43"
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t2'> AND B0."NAME" = <'default'>
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.459 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1b3f2476]]
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1b3f2476 is committing for transaction Xid=    with onePhase=true
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1b3f2476 committed connection for transaction Xid=    with onePhase=true
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@72234d77" closed
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@78c668d9 [conn=com.jolbox.bonecp.ConnectionHandle@72234d77, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.460 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 0 ms
07:05:27.463 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SparkOptimizer:
=== Result of Batch Finish Analysis ===
 CreateTableUsingAsSelect `t2`, parquet, false, [Ljava.lang.String;@4810d244, Some(BucketSpec(8,ArrayBuffer(j),List())), Overwrite, Map()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    CreateTableUsingAsSelect `t2`, parquet, false, [Ljava.lang.String;@4810d244, Some(BucketSpec(8,ArrayBuffer(j),List())), Overwrite, Map()
!+- SubqueryAlias df2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        +- Project [_1#37 AS i#41,_2#38 AS j#42,_3#39 AS k#43]
!   +- Project [_1#37 AS i#41,_2#38 AS j#42,_3#39 AS k#43]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      +- LocalRelation [_1#37,_2#38,_3#39], [[0,0,0,2000000001,30],[0,1,1,2000000001,31],[0,2,2,2000000001,32],[0,3,3,2000000001,33],[0,4,4,2000000001,34],[0,5,5,2000000001,35],[0,6,6,2000000001,36],[0,0,7,2000000001,37],[0,1,8,2000000001,38],[0,2,9,2000000001,39],[0,3,a,2000000002,3031],[0,4,0,2000000002,3131],[0,5,1,2000000002,3231],[0,6,2,2000000002,3331],[0,0,3,2000000002,3431],[0,1,4,2000000002,3531],[0,2,5,2000000002,3631],[0,3,6,2000000002,3731],[0,4,7,2000000002,3831],[0,5,8,2000000002,3931],[0,6,9,2000000002,3032],[0,0,a,2000000002,3132],[0,1,0,2000000002,3232],[0,2,1,2000000002,3332],[0,3,2,2000000002,3432],[0,4,3,2000000002,3532],[0,5,4,2000000002,3632],[0,6,5,2000000002,3732],[0,0,6,2000000002,3832],[0,1,7,2000000002,3932],[0,2,8,2000000002,3033],[0,3,9,2000000002,3133],[0,4,a,2000000002,3233],[0,5,0,2000000002,3333],[0,6,1,2000000002,3433],[0,0,2,2000000002,3533],[0,1,3,2000000002,3633],[0,2,4,2000000002,3733],[0,3,5,2000000002,3833],[0,4,6,2000000002,3933],[0,5,7,2000000002,3034],[0,6,8,2000000002,3134],[0,0,9,2000000002,3234],[0,1,a,2000000002,3334],[0,2,0,2000000002,3434],[0,3,1,2000000002,3534],[0,4,2,2000000002,3634],[0,5,3,2000000002,3734],[0,6,4,2000000002,3834],[0,0,5,2000000002,3934]]
!      +- LocalRelation [_1#37,_2#38,_3#39], [[0,0,0,2000000001,30],[0,1,1,2000000001,31],[0,2,2,2000000001,32],[0,3,3,2000000001,33],[0,4,4,2000000001,34],[0,5,5,2000000001,35],[0,6,6,2000000001,36],[0,0,7,2000000001,37],[0,1,8,2000000001,38],[0,2,9,2000000001,39],[0,3,a,2000000002,3031],[0,4,0,2000000002,3131],[0,5,1,2000000002,3231],[0,6,2,2000000002,3331],[0,0,3,2000000002,3431],[0,1,4,2000000002,3531],[0,2,5,2000000002,3631],[0,3,6,2000000002,3731],[0,4,7,2000000002,3831],[0,5,8,2000000002,3931],[0,6,9,2000000002,3032],[0,0,a,2000000002,3132],[0,1,0,2000000002,3232],[0,2,1,2000000002,3332],[0,3,2,2000000002,3432],[0,4,3,2000000002,3532],[0,5,4,2000000002,3632],[0,6,5,2000000002,3732],[0,0,6,2000000002,3832],[0,1,7,2000000002,3932],[0,2,8,2000000002,3033],[0,3,9,2000000002,3133],[0,4,a,2000000002,3233],[0,5,0,2000000002,3333],[0,6,1,2000000002,3433],[0,0,2,2000000002,3533],[0,1,3,2000000002,3633],[0,2,4,2000000002,3733],[0,3,5,2000000002,3833],[0,4,6,2000000002,3933],[0,5,7,2000000002,3034],[0,6,8,2000000002,3134],[0,0,9,2000000002,3234],[0,1,a,2000000002,3334],[0,2,0,2000000002,3434],[0,3,1,2000000002,3534],[0,4,2,2000000002,3634],[0,5,3,2000000002,3734],[0,6,4,2000000002,3834],[0,0,5,2000000002,3934]]

07:05:27.466 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SparkOptimizer:
=== Result of Batch LocalRelation ===
 CreateTableUsingAsSelect `t2`, parquet, false, [Ljava.lang.String;@4810d244, Some(BucketSpec(8,ArrayBuffer(j),List())), Overwrite, Map()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    CreateTableUsingAsSelect `t2`, parquet, false, [Ljava.lang.String;@4810d244, Some(BucketSpec(8,ArrayBuffer(j),List())), Overwrite, Map()
!+- SubqueryAlias df2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        +- LocalRelation [i#41,j#42,k#43], [[0,0,0],[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5],[6,6,6],[0,7,7],[1,8,8],[2,9,9],[3,10,10],[4,0,11],[5,1,12],[6,2,13],[0,3,14],[1,4,15],[2,5,16],[3,6,17],[4,7,18],[5,8,19],[6,9,20],[0,10,21],[1,0,22],[2,1,23],[3,2,24],[4,3,25],[5,4,26],[6,5,27],[0,6,28],[1,7,29],[2,8,30],[3,9,31],[4,10,32],[5,0,33],[6,1,34],[0,2,35],[1,3,36],[2,4,37],[3,5,38],[4,6,39],[5,7,40],[6,8,41],[0,9,42],[1,10,43],[2,0,44],[3,1,45],[4,2,46],[5,3,47],[6,4,48],[0,5,49]]
!   +- Project [_1#37 AS i#41,_2#38 AS j#42,_3#39 AS k#43]
!      +- LocalRelation [_1#37,_2#38,_3#39], [[0,0,0,2000000001,30],[0,1,1,2000000001,31],[0,2,2,2000000001,32],[0,3,3,2000000001,33],[0,4,4,2000000001,34],[0,5,5,2000000001,35],[0,6,6,2000000001,36],[0,0,7,2000000001,37],[0,1,8,2000000001,38],[0,2,9,2000000001,39],[0,3,a,2000000002,3031],[0,4,0,2000000002,3131],[0,5,1,2000000002,3231],[0,6,2,2000000002,3331],[0,0,3,2000000002,3431],[0,1,4,2000000002,3531],[0,2,5,2000000002,3631],[0,3,6,2000000002,3731],[0,4,7,2000000002,3831],[0,5,8,2000000002,3931],[0,6,9,2000000002,3032],[0,0,a,2000000002,3132],[0,1,0,2000000002,3232],[0,2,1,2000000002,3332],[0,3,2,2000000002,3432],[0,4,3,2000000002,3532],[0,5,4,2000000002,3632],[0,6,5,2000000002,3732],[0,0,6,2000000002,3832],[0,1,7,2000000002,3932],[0,2,8,2000000002,3033],[0,3,9,2000000002,3133],[0,4,a,2000000002,3233],[0,5,0,2000000002,3333],[0,6,1,2000000002,3433],[0,0,2,2000000002,3533],[0,1,3,2000000002,3633],[0,2,4,2000000002,3733],[0,3,5,2000000002,3833],[0,4,6,2000000002,3933],[0,5,7,2000000002,3034],[0,6,8,2000000002,3134],[0,0,9,2000000002,3234],[0,1,a,2000000002,3334],[0,2,0,2000000002,3434],[0,3,1,2000000002,3534],[0,4,2,2000000002,3634],[0,5,3,2000000002,3734],[0,6,4,2000000002,3834],[0,0,5,2000000002,3934]]

07:05:27.466 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_database: default
07:05:27.466 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_database: default
07:05:27.466 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.466 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SQL Query : "select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = ?"
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5eaf8592" opened with isolation level "read-committed" and auto-commit=false
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2ba1553, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2ba1553 is starting for transaction Xid=    with flags 0
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@33e5957e [conn=com.jolbox.bonecp.ConnectionHandle@5eaf8592, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@11c41585"
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = <'default'>
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SQL Query : "select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = ?  AND "PARAM_KEY" IS NOT NULL"
07:05:27.467 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@33e5957e [conn=com.jolbox.bonecp.ConnectionHandle@5eaf8592, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@237310cb"
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = <1>  AND "PARAM_KEY" IS NOT NULL
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaStoreDirectSql: getDatabase: directsql returning db default locn[file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5] desc [Default Hive database] owner [public] ownertype [ROLE]
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2ba1553]]
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2ba1553 is committing for transaction Xid=    with onePhase=true
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@2ba1553 committed connection for transaction Xid=    with onePhase=true
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@5eaf8592" closed
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@33e5957e [conn=com.jolbox.bonecp.ConnectionHandle@5eaf8592, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 0 ms
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: db details for db default retrieved using SQL in 1.94949ms
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t2
07:05:27.468 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t2
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t2
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@732b60df" opened with isolation level "read-committed" and auto-commit=false
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3349751e, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3349751e is starting for transaction Xid=    with flags 0
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@74e483de [conn=com.jolbox.bonecp.ConnectionHandle@732b60df, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5ce12805"
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t2'> AND B0."NAME" = <'default'>
07:05:27.469 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 1 ms
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3349751e]]
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3349751e is committing for transaction Xid=    with onePhase=true
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@3349751e committed connection for transaction Xid=    with onePhase=true
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@732b60df" closed
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@74e483de [conn=com.jolbox.bonecp.ConnectionHandle@732b60df, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 0 ms
07:05:27.470 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.471 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.475 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#41, j#42, k#43) AS #50]   Project [createexternalrow(if (isnull(i#41)) null else i#41, if (isnull(j#42)) null else j#42, if (isnull(k#43)) null else k#43.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #50]
 +- LocalRelation [i#41,j#42,k#43]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#41,j#42,k#43]

07:05:27.477 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.480 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.480 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.482 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)), i#41, j#42, k#43) AS #51]   Project [createexternalrow(if (isnull(i#41)) null else i#41, if (isnull(j#42)) null else j#42, if (isnull(k#43)) null else k#43.toString, StructField(i,IntegerType,false), StructField(j,IntegerType,false), StructField(k,StringType,true)) AS #51]
 +- LocalRelation [i#41,j#42,k#43]                                                                                                                                                                                                                                                                                                                              +- LocalRelation [i#41,j#42,k#43]

07:05:27.486 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     int value1 = i.getInt(1);
/* 037 */     rowWriter.write(1, value1);
/* 038 */
/* 039 */     /* input[2, string] */
/* 040 */     boolean isNull2 = i.isNullAt(2);
/* 041 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 042 */     if (isNull2) {
/* 043 */       rowWriter.setNullAt(2);
/* 044 */     } else {
/* 045 */       rowWriter.write(2, value2);
/* 046 */     }
/* 047 */     result.setTotalSize(holder.totalSize());
/* 048 */     return result;
/* 049 */   }
/* 050 */ }
/* 051 */

07:05:27.488 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DefaultSource: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
07:05:27.489 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DynamicPartitionWriterContainer: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
07:05:27.491 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1) +++
07:05:27.491 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:27.491 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1.serialVersionUID
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.sql.execution.metric.LongSQLMetric org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1.numOutputRows$1
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1.apply(java.lang.Object)
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final org.apache.spark.sql.catalyst.InternalRow org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1.apply(org.apache.spark.sql.catalyst.InternalRow)
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:27.492 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.sql.execution.LocalTableScan$$anonfun$doExecute$1) is now cleaned +++
07:05:27.493 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1) +++
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1.serialVersionUID
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.sql.execution.datasources.BaseWriterContainer org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1.writerContainer$1
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(java.lang.Object,java.lang.Object)
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final void org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:27.494 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$1) is now cleaned +++
07:05:27.508 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO SparkContext: Starting job: apply at Transformer.scala:22
07:05:27.509 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (apply at Transformer.scala:22) with 1 output partitions
07:05:27.509 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (apply at Transformer.scala:22)
07:05:27.509 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
07:05:27.509 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
07:05:27.509 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 1)
07:05:27.510 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List()
07:05:27.510 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at apply at Transformer.scala:22), which has no missing parents
07:05:27.510 dag-scheduler-event-loop DEBUG DAGScheduler: submitMissingTasks(ResultStage 1)
07:05:27.510 dag-scheduler-event-loop DEBUG DAGScheduler: partitionsToCompute Vector(0)
07:05:27.511 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ParallelCollectionPartition@735 of MapPartitionsRDD[5] at apply at Transformer.scala:22:
07:05:27.512 dag-scheduler-event-loop DEBUG ParallelCollectionRDD: Preferred locations for org.apache.spark.rdd.ParallelCollectionPartition@735 of ParallelCollectionRDD[4] at apply at Transformer.scala:22:
07:05:27.512 dag-scheduler-event-loop DEBUG DAGScheduler: taskIdToLocations Map(0 -> List())
07:05:27.514 Spark Context Cleaner DEBUG ContextCleaner: Got cleaning task CleanAccum(1)
07:05:27.517 Spark Context Cleaner DEBUG ContextCleaner: Cleaning accumulator 1
07:05:27.518 Spark Context Cleaner INFO ContextCleaner: Cleaned accumulator 1
07:05:27.518 Spark Context Cleaner DEBUG ContextCleaner: Got cleaning task CleanBroadcast(0)
07:05:27.518 Spark Context Cleaner DEBUG ContextCleaner: Cleaning broadcast 0
07:05:27.519 Spark Context Cleaner DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 0
07:05:27.520 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 52.1 KB, free 2.2 GB)
07:05:27.521 dag-scheduler-event-loop DEBUG BlockManager: Put block broadcast_1 locally took  1 ms
07:05:27.521 dag-scheduler-event-loop DEBUG BlockManager: Putting block broadcast_1 without replication took  1 ms
07:05:27.522 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 2.2 GB)
07:05:27.527 dispatcher-event-loop-1 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.102:52176 (size: 19.3 KB, free: 2.2 GB)
07:05:27.527 dag-scheduler-event-loop DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
07:05:27.527 dag-scheduler-event-loop DEBUG BlockManager: Told master about block broadcast_1_piece0
07:05:27.527 dag-scheduler-event-loop DEBUG BlockManager: Put block broadcast_1_piece0 locally took  5 ms
07:05:27.527 dag-scheduler-event-loop DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took  5 ms
07:05:27.528 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
07:05:27.528 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at apply at Transformer.scala:22)
07:05:27.528 dag-scheduler-event-loop DEBUG DAGScheduler: New pending partitions: Set(0)
07:05:27.528 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
07:05:27.528 dag-scheduler-event-loop DEBUG TaskSetManager: Epoch for TaskSet 1.0: 0
07:05:27.528 dag-scheduler-event-loop DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NO_PREF, ANY
07:05:27.528 dispatcher-event-loop-4 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1, runningTasks: 0
07:05:27.528 dispatcher-event-loop-4 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:27.529 block-manager-slave-async-thread-pool-0 DEBUG BlockManagerSlaveEndpoint: removing broadcast 0
07:05:27.529 block-manager-slave-async-thread-pool-0 DEBUG BlockManager: Removing broadcast 0
07:05:27.530 dispatcher-event-loop-4 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 8409 bytes)
07:05:27.530 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
07:05:27.531 block-manager-slave-async-thread-pool-0 DEBUG BlockManager: Removing block broadcast_0_piece0
07:05:27.532 Executor task launch worker-0 DEBUG Executor: Task 1's epoch is 0
07:05:27.532 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_1
07:05:27.532 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:27.532 block-manager-slave-async-thread-pool-0 DEBUG MemoryStore: Block broadcast_0_piece0 of size 19776 dropped from memory (free 2348945915)
07:05:27.534 dispatcher-event-loop-6 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.102:52176 in memory (size: 19.3 KB, free: 2.2 GB)
07:05:27.534 block-manager-slave-async-thread-pool-0 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
07:05:27.534 block-manager-slave-async-thread-pool-0 DEBUG BlockManager: Told master about block broadcast_0_piece0
07:05:27.535 block-manager-slave-async-thread-pool-0 DEBUG BlockManager: Removing block broadcast_0
07:05:27.535 block-manager-slave-async-thread-pool-0 DEBUG MemoryStore: Block broadcast_0 of size 53344 dropped from memory (free 2348999259)
07:05:27.536 block-manager-slave-async-thread-pool-2 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 0, response is 0
07:05:27.537 block-manager-slave-async-thread-pool-2 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.1.102:52175
07:05:27.537 Spark Context Cleaner DEBUG ContextCleaner: Cleaned broadcast 0
07:05:27.541 Executor task launch worker-0 INFO DynamicPartitionWriterContainer: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
07:05:27.548 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[1, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[1, int], 42), 8) */
/* 028 */     /* hash(input[1, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[1, int] */
/* 031 */     int value2 = i.getInt(1);
/* 032 */     value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 033 */
/* 034 */     int value = -1;
/* 035 */
/* 036 */     int r = value1 % 8;
/* 037 */     if (r < 0) {
/* 038 */       value = (r + 8) % 8;
/* 039 */     } else {
/* 040 */       value = r;
/* 041 */     }
/* 042 */     rowWriter.write(0, value);
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:27.549 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[1, int], 42), 8) */
/* 028 */     /* hash(input[1, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[1, int] */
/* 031 */     int value2 = i.getInt(1);
/* 032 */     value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 033 */
/* 034 */     int value = -1;
/* 035 */
/* 036 */     int r = value1 % 8;
/* 037 */     if (r < 0) {
/* 038 */       value = (r + 8) % 8;
/* 039 */     } else {
/* 040 */       value = r;
/* 041 */     }
/* 042 */     rowWriter.write(0, value);
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:27.557 Executor task launch worker-0 INFO CodeGenerator: Code generated in 8.836454 ms
07:05:27.559 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     int value1 = i.getInt(1);
/* 037 */     rowWriter.write(1, value1);
/* 038 */
/* 039 */     /* input[2, string] */
/* 040 */     boolean isNull2 = i.isNullAt(2);
/* 041 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 042 */     if (isNull2) {
/* 043 */       rowWriter.setNullAt(2);
/* 044 */     } else {
/* 045 */       rowWriter.write(2, value2);
/* 046 */     }
/* 047 */     result.setTotalSize(holder.totalSize());
/* 048 */     return result;
/* 049 */   }
/* 050 */ }
/* 051 */

07:05:27.560 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for concat():
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* concat() */
/* 032 */     boolean isNull = false;
/* 033 */     UTF8String value = UTF8String.concat();
/* 034 */     if (value == null) {
/* 035 */       isNull = true;
/* 036 */     }
/* 037 */     if (isNull) {
/* 038 */       rowWriter.setNullAt(0);
/* 039 */     } else {
/* 040 */       rowWriter.write(0, value);
/* 041 */     }
/* 042 */     result.setTotalSize(holder.totalSize());
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:27.561 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:27.561 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:27.561 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 1 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2df1e0fe
07:05:27.561 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 1 acquire 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2df1e0fe
07:05:27.572 Executor task launch worker-0 INFO DynamicPartitionWriterContainer: Sorting complete. Writing out partition files one at a time.
07:05:27.572 Executor task launch worker-0 DEBUG DynamicPartitionWriterContainer: Writing partition: [0,1]
07:05:27.575 Executor task launch worker-0 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "j",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "k",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 i;
  required int32 j;
  optional binary k (UTF8);
}


07:05:27.581 Executor task launch worker-0 INFO CodecPool: Got brand-new compressor [.snappy]
07:05:27.583 Executor task launch worker-0 DEBUG DynamicPartitionWriterContainer: Writing partition: [0,2]
07:05:27.584 Executor task launch worker-0 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "j",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "k",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 i;
  required int32 j;
  optional binary k (UTF8);
}


07:05:27.589 Executor task launch worker-0 INFO CodecPool: Got brand-new compressor [.snappy]
07:05:27.591 Executor task launch worker-0 DEBUG DynamicPartitionWriterContainer: Writing partition: [0,3]
07:05:27.593 Executor task launch worker-0 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "j",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "k",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 i;
  required int32 j;
  optional binary k (UTF8);
}


07:05:27.598 Executor task launch worker-0 INFO CodecPool: Got brand-new compressor [.snappy]
07:05:27.600 Executor task launch worker-0 DEBUG DynamicPartitionWriterContainer: Writing partition: [0,6]
07:05:27.601 Executor task launch worker-0 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "j",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "k",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 i;
  required int32 j;
  optional binary k (UTF8);
}


07:05:27.605 Executor task launch worker-0 INFO CodecPool: Got brand-new compressor [.snappy]
07:05:27.607 Executor task launch worker-0 DEBUG DynamicPartitionWriterContainer: Writing partition: [0,7]
07:05:27.609 Executor task launch worker-0 INFO CatalystWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "j",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "k",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 i;
  required int32 j;
  optional binary k (UTF8);
}


07:05:27.613 Executor task launch worker-0 INFO CodecPool: Got brand-new compressor [.snappy]
07:05:27.614 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 1 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2df1e0fe
07:05:27.614 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 1 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2df1e0fe
07:05:27.615 dispatcher-event-loop-7 DEBUG OutputCommitCoordinator: Authorizing attemptNumber=0 to commit for stage=1, partition=0
07:05:27.616 Executor task launch worker-0 INFO FileOutputCommitter: Saved output of task 'attempt_201604210705_0001_m_000000_0' to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/_temporary/0/task_201604210705_0001_m_000000
07:05:27.616 Executor task launch worker-0 INFO SparkHadoopMapRedUtil: attempt_201604210705_0001_m_000000_0: Committed
07:05:27.618 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2746 bytes result sent to driver
07:05:27.618 dispatcher-event-loop-0 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1, runningTasks: 0
07:05:27.618 dispatcher-event-loop-0 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:27.618 dispatcher-event-loop-0 INFO TaskSetManager: No tasks dequeued
07:05:27.618 dispatcher-event-loop-0 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
07:05:27.618 dispatcher-event-loop-0 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = ANY
 - allowedLocality = ANY

07:05:27.618 dispatcher-event-loop-0 INFO TaskSetManager: No tasks dequeued
07:05:27.619 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 91 ms on localhost (1/1)
07:05:27.619 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
07:05:27.619 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (apply at Transformer.scala:22) finished in 0.091 s
07:05:27.619 dag-scheduler-event-loop DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
07:05:27.619 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DAGScheduler: Job 1 finished: apply at Transformer.scala:22, took 0.111027 s
07:05:27.620 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/_temporary/0/task_201604210705_0001_m_000000; isDirectory=true; modification_time=1461247527000; access_time=0; owner=; group=; permission=rwxrwxrwx; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2
07:05:27.620 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/_temporary/0/task_201604210705_0001_m_000000/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet; isDirectory=false; length=731; replication=1; blocksize=33554432; modification_time=1461247527000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet
07:05:27.621 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/_temporary/0/task_201604210705_0001_m_000000/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet; isDirectory=false; length=728; replication=1; blocksize=33554432; modification_time=1461247527000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet
07:05:27.621 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/_temporary/0/task_201604210705_0001_m_000000/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet; isDirectory=false; length=773; replication=1; blocksize=33554432; modification_time=1461247527000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet
07:05:27.621 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/_temporary/0/task_201604210705_0001_m_000000/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet; isDirectory=false; length=753; replication=1; blocksize=33554432; modification_time=1461247527000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet
07:05:27.621 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG FileOutputCommitter: Merging data from RawLocalFileStatus{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/_temporary/0/task_201604210705_0001_m_000000/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet; isDirectory=false; length=740; replication=1; blocksize=33554432; modification_time=1461247527000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet
07:05:27.638 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DynamicPartitionWriterContainer: Job job_201604210705_0000 committed.
07:05:27.640 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HDFSFileCatalog: Listing file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2 on driver
07:05:27.681 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HDFSFileCatalog: Listing file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2 on driver
07:05:27.714 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetastoreCatalog: Persisting data source relation `t2` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2.
07:05:27.716 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: create_table: Table(tableName:t2, dbName:default, owner:lian, createTime:1461247527, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:i, type:int, comment:null), FieldSchema(name:j, type:int, comment:null), FieldSchema(name:k, type:string, comment:null)], location:file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.bucketCol.0=j, spark.sql.sources.schema.numBuckets=8, EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"i","type":"integer","nullable":true,"metadata":{}},{"name":"j","type":"integer","nullable":true,"metadata":{}},{"name":"k","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.schema.numBucketCols=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
07:05:27.716 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=create_table: Table(tableName:t2, dbName:default, owner:lian, createTime:1461247527, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:i, type:int, comment:null), FieldSchema(name:j, type:int, comment:null), FieldSchema(name:k, type:string, comment:null)], location:file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.bucketCol.0=j, spark.sql.sources.schema.numBuckets=8, EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"i","type":"integer","nullable":true,"metadata":{}},{"name":"j","type":"integer","nullable":true,"metadata":{}},{"name":"k","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.schema.numBucketCols=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
07:05:27.716 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.716 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.716 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
07:05:27.716 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.start(ObjectStore.java:2408)
07:05:27.716 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SQL Query : "select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = ?"
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4ac37300" opened with isolation level "read-committed" and auto-commit=false
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7bb2e84b, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7bb2e84b is starting for transaction Xid=    with flags 0
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@7a8a2c61"
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: select "DB_ID", "NAME", "DB_LOCATION_URI", "DESC", "OWNER_NAME", "OWNER_TYPE" FROM "DBS" where "NAME" = <'default'>
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: SQL Query : "select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = ?  AND "PARAM_KEY" IS NOT NULL"
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@442c1ffd"
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: select "PARAM_KEY", "PARAM_VALUE"  FROM "DATABASE_PARAMS"  WHERE "DB_ID" = <1>  AND "PARAM_KEY" IS NOT NULL
07:05:27.717 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MetaStoreDirectSql: getDatabase: directsql returning db default locn[file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5] desc [Default Hive database] owner [public] ownertype [ROLE]
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2445)
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: db details for db default retrieved using SQL in 1.499724ms
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 3, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@418346e7"
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t2'> AND B0."NAME" = <'default'>
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.718 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:27.719 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 2, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.719 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.719 pool-1-thread-1-ScalaTest-running-BucketedReadSuite WARN HiveMetaStore: Location: file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2 specified for non-external table:t2
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:812)
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 3, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:535)
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == dbname PARAMETERS java.lang.String dbnameFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == dbname PARAMETERS java.lang.String dbnameFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MDatabase WHERE name == dbname PARAMETERS java.lang.String dbname" ...
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2d61e222"
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MDatabase' AS NUCLEUS_TYPE,A0."DESC",A0.DB_LOCATION_URI,A0."NAME",A0.OWNER_NAME,A0.OWNER_TYPE,A0.DB_ID FROM DBS A0 WHERE A0."NAME" = <'default'>
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" not found in Level 1 cache [cache size = 0]
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") added to Level 1 cache (loadedFlags="[NNNNNN]")
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.720 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5b0807f3"
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM DATABASE_PARAMS A0 WHERE A0.DB_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1eadc07e"
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") enlisted in transactional cache
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 2, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:542)
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MTable@5016bc7"
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MTable (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=2
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") added to Level 1 cache (loadedFlags="[YYYYYYYYYYYY]")
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") enlisted in transactional cache
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" being inserted into table "TBLS"
07:05:27.721 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@989defa" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MTable.sd" is being persisted for "cascade-persist".
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=2
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") added to Level 1 cache (loadedFlags="[YYYYYYYYYYYYYY]")
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") enlisted in transactional cache
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" being inserted into table "SDS"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7786423b" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo" is being persisted for "cascade-persist".
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MSerDeInfo (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=2
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") added to Level 1 cache (loadedFlags="[YYY]")
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") enlisted in transactional cache
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" being inserted into table "SERDES"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e58df2d" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: The requested statement "INSERT INTO SERDES (SERDE_ID,SLIB,"NAME") VALUES (?,?,?)" has been made batchable
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Batch has been added to statement "INSERT INTO SERDES (SERDE_ID,SLIB,"NAME") VALUES (?,?,?)" for processing (batch size = 1)
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MSerDeInfo.parameters" is being persisted for "cascade-persist".
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: BATCH [INSERT INTO SERDES (SERDE_ID,SLIB,"NAME") VALUES (<2>,<'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'>,<null>)]
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e58df2d"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4c80016c"
07:05:27.722 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM SERDE_PARAMS A0 WHERE A0.SERDE_ID = <2> AND A0.PARAM_KEY = <'path'>
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4a37324d"
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@61ce8968"
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM SERDE_PARAMS A0 WHERE A0.SERDE_ID = <2> AND A0.PARAM_KEY = <'serialization.format'>
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@51df6eb5"
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@22898b99" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO SERDE_PARAMS (PARAM_VALUE,SERDE_ID,PARAM_KEY) VALUES (<'file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2'>,<2>,<'path'>)
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@22898b99"
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@22898b99"
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7b8ccf0b" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO SERDE_PARAMS (PARAM_VALUE,SERDE_ID,PARAM_KEY) VALUES (<'1'>,<2>,<'serialization.format'>)
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7b8ccf0b"
07:05:27.723 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7b8ccf0b"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.HashMap" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" field "parameters" is having its SCO wrapper initialised with a container with 2 values
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd" is being persisted for "cascade-persist".
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Making object persistent : "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ValueGeneration: Generated value for field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor (datastore id)" using strategy="custom" (Generator="org.datanucleus.store.rdbms.valuegenerator.TableGenerator") : value=2
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") added to Level 1 cache (loadedFlags="[Y]")
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") enlisted in transactional cache
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" being inserted into table "CDS"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@45d71fde" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO CDS (CD_ID) VALUES (<2>)
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@45d71fde"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@45d71fde"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols" is being persisted for "cascade-persist".
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4b7cc744" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: The requested statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " has been made batchable
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Batch has been added to statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " for processing (batch size = 1)
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: BATCH [INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (<2>,<null>,<'i'>,<'int'>,<0>) ]
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4b7cc744"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4b7cc744"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@725f4fa4" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: The requested statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " has been made batchable
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Batch has been added to statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " for processing (batch size = 1)
07:05:27.724 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: BATCH [INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (<2>,<null>,<'j'>,<'int'>,<1>) ]
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Execution Time = 1 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@725f4fa4"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@725f4fa4"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@16bb3b08" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: The requested statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " has been made batchable
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Batch has been added to statement "INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) " for processing (batch size = 1)
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: BATCH [INSERT INTO COLUMNS_V2 (CD_ID,COMMENT,"COLUMN_NAME",TYPE_NAME,INTEGER_IDX) VALUES (<2>,<null>,<'k'>,<'string'>,<2>) ]
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Execution Time = 0 ms (number of rows = [1]) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@16bb3b08"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@16bb3b08"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" field "cols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" field "cols" is having its SCO wrapper initialised with a container with 3 values
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO SDS (SD_ID,CD_ID,IS_STOREDASSUBDIRECTORIES,IS_COMPRESSED,SERDE_ID,LOCATION,NUM_BUCKETS,INPUT_FORMAT,OUTPUT_FORMAT) VALUES (<2>,<2>,<'N'>,<'N'>,<2>,<'file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2'>,<-1>,<'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'>,<'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'>)
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7786423b"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7786423b"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols" is being persisted for "cascade-persist".
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "sortCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "sortCols" is having its SCO wrapper initialised with a container with 0 values
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps" is being persisted for "cascade-persist".
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "skewedColValueLocationMaps" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.HashMap" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "skewedColValueLocationMaps" is having its SCO wrapper initialised with a container with 0 values
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.bucketCols" is being persisted for "cascade-persist".
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "bucketCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "bucketCols" is having its SCO wrapper initialised with a container with 0 values
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters"
07:05:27.725 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.parameters" is being persisted for "cascade-persist".
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.HashMap" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "parameters" is having its SCO wrapper initialised with a container with 0 values
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues" is being persisted for "cascade-persist".
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "skewedColValues" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "skewedColValues" is having its SCO wrapper initialised with a container with 0 values
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColNames" is being persisted for "cascade-persist".
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "skewedColNames" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" field "skewedColNames" is having its SCO wrapper initialised with a container with 0 values
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TBLS (TBL_ID,LAST_ACCESS_TIME,DB_ID,SD_ID,TBL_TYPE,OWNER,TBL_NAME,VIEW_ORIGINAL_TEXT,VIEW_EXPANDED_TEXT,CREATE_TIME,RETENTION) VALUES (<2>,<0>,<1>,<2>,<'MANAGED_TABLE'>,<'lian'>,<'t2'>,<null>,<null>,<1461247527>,<0>)
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@989defa"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MTable.parameters"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MTable.database"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MTable.sd"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" is calling insertPostProcessing for field "org.apache.hadoop.hive.metastore.model.MTable.partitionKeys"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@989defa"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MTable.parameters"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MTable.parameters" is being persisted for "cascade-persist".
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@f098f62"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'spark.sql.sources.schema.bucketCol.0'>
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5bca5205"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@67af6993"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'numRows'>
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5be3ad8b"
07:05:27.726 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4e4f8eac"
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'rawDataSize'>
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@580ce408"
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@e5ebd57"
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'transient_lastDdlTime'>
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@e4c2a87"
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@315fb3e2"
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'spark.sql.sources.schema.numBuckets'>
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@450f959f"
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2af897ab"
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'totalSize'>
07:05:27.727 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2c0a052c"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1590de5f"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'EXTERNAL'>
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3a836902"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@465c381b"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'spark.sql.sources.schema.part.0'>
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7483ff8b"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@55e94360"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'COLUMN_STATS_ACCURATE'>
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4c649fe9"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@27704e49"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'numFiles'>
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5b0502b4"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@33f645ea"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'spark.sql.sources.schema.numParts'>
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@41c74f4e"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@32a4e560"
07:05:27.728 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'spark.sql.sources.schema.numBucketCols'>
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2f8fc9f0"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@54c94e70"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY = <'spark.sql.sources.provider'>
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@130353fa"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4292c823" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'-1'>,<2>,<'numRows'>)
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4292c823"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4292c823"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@d5bde9b" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'-1'>,<2>,<'rawDataSize'>)
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@d5bde9b"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@d5bde9b"
07:05:27.729 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@11ee60d2" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'5'>,<2>,<'numFiles'>)
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@11ee60d2"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@11ee60d2"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@66c39568" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'1461247527'>,<2>,<'transient_lastDdlTime'>)
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@66c39568"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@66c39568"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2d611aac" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'j'>,<2>,<'spark.sql.sources.schema.bucketCol.0'>)
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2d611aac"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2d611aac"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@530c7839" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'8'>,<2>,<'spark.sql.sources.schema.numBuckets'>)
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@530c7839"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@530c7839"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@50c5fd00" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.730 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'3725'>,<2>,<'totalSize'>)
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@50c5fd00"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@50c5fd00"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4b2ea6db" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'1'>,<2>,<'spark.sql.sources.schema.numParts'>)
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4b2ea6db"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4b2ea6db"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e5e428e" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'1'>,<2>,<'spark.sql.sources.schema.numBucketCols'>)
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e5e428e"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e5e428e"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6393b91e" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'FALSE'>,<2>,<'EXTERNAL'>)
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6393b91e"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6393b91e"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@77153476" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'{"type":"struct","fields":[{"name":"i","type":"integer","nullable":true,"metadata":{}},{"name":"j","type":"integer","nullable":true,"metadata":{}},{"name":"k","type":"string","nullable":true,"metadata":{}}]}'>,<2>,<'spark.sql.sources.schema.part.0'>)
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@77153476"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@77153476"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7a6c4e94" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.731 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'parquet'>,<2>,<'spark.sql.sources.provider'>)
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 1 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7a6c4e94"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7a6c4e94"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Using PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7493daf0" for connection "com.jolbox.bonecp.ConnectionHandle@4ac37300"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (<'false'>,<2>,<'COLUMN_STATS_ACCURATE'>)
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persist: Execution Time = 0 ms (number of rows = 1) on PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7493daf0"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7493daf0"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.HashMap" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" field "parameters" is having its SCO wrapper initialised with a container with 13 values
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MTable.database"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MTable.sd"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" is calling postInsert for field "org.apache.hadoop.hive.metastore.model.MTable.partitionKeys"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Field "org.apache.hadoop.hive.metastore.model.MTable.partitionKeys" is being persisted for "cascade-persist".
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" field "partitionKeys" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.ArrayList" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" field "partitionKeys" is having its SCO wrapper initialised with a container with 0 values
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:830)
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 1 enlisted objects
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability algorithm on object with id "2[OID]org.apache.hadoop.hive.metastore.model.MTable"
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") taken from Level 1 cache (loadedFlags="[YYYYYYYYYYYY]") [cache size = 5]
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:27.732 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on single-valued relation field "org.apache.hadoop.hive.metastore.model.MTable.sd"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.sortCols"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValues"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.skewedColValueLocationMaps"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on single-valued relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.serDeInfo"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on single-valued relation field "org.apache.hadoop.hive.metastore.model.MStorageDescriptor.cd"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") lifecycle state "P_NEW" added to the list of reachables on commit.
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MColumnDescriptor.cols"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@7c2a0928" (id="org.datanucleus.identity.IdentityReference@16840552") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2ebeea3f" (id="org.datanucleus.identity.IdentityReference@95ce757") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@34e9ed5a" (id="org.datanucleus.identity.IdentityReference@18a6d42f") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on multi-valued (collection/map) relation field "org.apache.hadoop.hive.metastore.model.MTable.partitionKeys"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability on single-valued relation field "org.apache.hadoop.hive.metastore.model.MTable.database"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") lifecycle state "P_CLEAN" added to the list of reachables on commit.
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing reachability algorithm on object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") taken from Level 1 cache (loadedFlags="[YYYYYY]") [cache size = 5]
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7bb2e84b]]
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7bb2e84b is committing for transaction Xid=    with onePhase=true
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@7bb2e84b committed connection for transaction Xid=    with onePhase=true
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@4ac37300" closed
07:05:27.733 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@2b000172 [conn=com.jolbox.bonecp.ConnectionHandle@4ac37300, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" (depth=0)
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") being evicted from transactional cache
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MSerDeInfo@79e1a0e5, lifecycle=DETACHED_CLEAN]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" being removed from Level 1 cache [current cache size = 5]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" (depth=0)
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@7c2a0928" (depth=0)
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@7c2a0928" (id="org.datanucleus.identity.IdentityReference@16840552") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@7c2a0928" (id="org.datanucleus.identity.IdentityReference@16840552") being evicted from transactional cache
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@7c2a0928" (id="org.datanucleus.identity.IdentityReference@16840552") is not transactional
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@7c2a0928 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@7c2a0928, lifecycle=DETACHED_CLEAN]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@16840552" being removed from Level 1 cache [current cache size = 4]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@16840552" couldnt be removed from Level 1 cache - wasnt present.
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@2ebeea3f" (depth=0)
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2ebeea3f" (id="org.datanucleus.identity.IdentityReference@95ce757") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2ebeea3f" (id="org.datanucleus.identity.IdentityReference@95ce757") being evicted from transactional cache
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2ebeea3f" (id="org.datanucleus.identity.IdentityReference@95ce757") is not transactional
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@2ebeea3f from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@2ebeea3f, lifecycle=DETACHED_CLEAN]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@95ce757" being removed from Level 1 cache [current cache size = 4]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@95ce757" couldnt be removed from Level 1 cache - wasnt present.
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@34e9ed5a" (depth=0)
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@34e9ed5a" (id="org.datanucleus.identity.IdentityReference@18a6d42f") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@34e9ed5a" (id="org.datanucleus.identity.IdentityReference@18a6d42f") being evicted from transactional cache
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@34e9ed5a" (id="org.datanucleus.identity.IdentityReference@18a6d42f") is not transactional
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@34e9ed5a from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@34e9ed5a, lifecycle=DETACHED_CLEAN]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@18a6d42f" being removed from Level 1 cache [current cache size = 4]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@18a6d42f" couldnt be removed from Level 1 cache - wasnt present.
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") is having the SCO wrapper in field "cols" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") being evicted from transactional cache
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MColumnDescriptor@58be6755, lifecycle=DETACHED_CLEAN]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" being removed from Level 1 cache [current cache size = 4]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (depth=0)
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (depth=1)
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") being evicted from transactional cache
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MDatabase@30ea0399, lifecycle=DETACHED_CLEAN]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" being removed from Level 1 cache [current cache size = 3]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "partitionKeys" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (depth=1)
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "bucketCols" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "skewedColNames" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "sortCols" replaced by the unwrapped value
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") being evicted from transactional cache
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MStorageDescriptor@79575185, lifecycle=DETACHED_CLEAN]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" being removed from Level 1 cache [current cache size = 2]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "P_NEW"->"DETACHED_CLEAN"
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@5016bc7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") being evicted from transactional cache
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MTable@5016bc7 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MTable@5016bc7, lifecycle=DETACHED_CLEAN]
07:05:27.734 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MTable" being removed from Level 1 cache [current cache size = 1]
07:05:27.735 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 3 ms
07:05:27.739 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveSqlParser: Parsing command: t1
07:05:27.739 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t1
07:05:27.739 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t1
07:05:27.739 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t1
07:05:27.739 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.739 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@7abdea17" opened with isolation level "read-committed" and auto-commit=false
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@14f8be88, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@14f8be88 is starting for transaction Xid=    with flags 0
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@44cd5c06"
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t1'> AND B0."NAME" = <'default'>
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.740 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:27.741 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MTable"
07:05:27.741 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MTable" not found in Level 1 cache [cache size = 0]
07:05:27.741 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") added to Level 1 cache (loadedFlags="[NNNNNNNNNNNN]")
07:05:27.742 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" having fields "database,parameters,partitionKeys,sd,viewExpandedText,viewOriginalText" fetched from table "TBLS"
07:05:27.742 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.state.LockManagerImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/state/LockManagerImpl.class
07:05:27.742 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.747 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1ebcde97"
07:05:27.747 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0."DESC",B0.DB_LOCATION_URI,B0."NAME",B0.OWNER_NAME,B0.OWNER_TYPE,B0.DB_ID,C0.INPUT_FORMAT,C0.IS_COMPRESSED,C0.IS_STOREDASSUBDIRECTORIES,C0.LOCATION,C0.NUM_BUCKETS,C0.OUTPUT_FORMAT,C0.SD_ID,A0.VIEW_EXPANDED_TEXT,A0.VIEW_ORIGINAL_TEXT FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID LEFT OUTER JOIN SDS C0 ON A0.SD_ID = C0.SD_ID WHERE A0.TBL_ID = <1>
07:05:27.748 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.NestedLoopLeftOuterJoinResultSet - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/NestedLoopLeftOuterJoinResultSet.class
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 3 ms
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" not found in Level 1 cache [cache size = 1]
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2bc88067" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") added to Level 1 cache (loadedFlags="[NNNNNN]")
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor"
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" not found in Level 1 cache [cache size = 2]
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") added to Level 1 cache (loadedFlags="[NNNNNNNNNNNNNN]")
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@52dd6cfa"
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" field "partitionKeys" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.750 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.753 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@50e6e230"
07:05:27.753 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:27.753 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.753 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.MapEntrySetStore$EntryImpl - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/MapEntrySetStore$EntryImpl.class
07:05:27.754 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@560f044e"
07:05:27.754 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" field "partitionKeys" loading contents to SCO wrapper from the datastore
07:05:27.756 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.IteratorStatement - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/IteratorStatement.class
07:05:27.756 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.760 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@7ae6efbb"
07:05:27.760 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PKEY_COMMENT,A0.PKEY_NAME,A0.PKEY_TYPE,A0.INTEGER_IDX AS NUCORDER0 FROM PARTITION_KEYS A0 WHERE A0.TBL_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.760 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.760 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.ListStoreIterator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/ListStoreIterator.class
07:05:27.761 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@66bbeb5d"
07:05:27.761 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.761 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") enlisted in transactional cache
07:05:27.761 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.761 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2bc88067" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.761 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2bc88067" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") enlisted in transactional cache
07:05:27.761 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.761 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") enlisted in transactional cache
07:05:27.762 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" having fields "cd" fetched from table "SDS"
07:05:27.762 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.765 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2ef80e16"
07:05:27.765 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0.CD_ID FROM SDS A0 LEFT OUTER JOIN CDS B0 ON A0.CD_ID = B0.CD_ID WHERE A0.SD_ID = <1>
07:05:27.765 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.765 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor"
07:05:27.765 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" not found in Level 1 cache [cache size = 3]
07:05:27.765 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") added to Level 1 cache (loadedFlags="[N]")
07:05:27.765 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@650ee25b"
07:05:27.766 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.766 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") enlisted in transactional cache
07:05:27.766 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" field "cols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.766 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.767 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.apache.derby.impl.sql.execute.CountAggregator - jar:file:/Users/lian/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar!/org/apache/derby/impl/sql/execute/CountAggregator.class
07:05:27.770 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@3aa1493e"
07:05:27.771 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM COLUMNS_V2 THIS WHERE THIS.CD_ID=<1> AND THIS.INTEGER_IDX>=0
07:05:27.771 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.771 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3a4b6251"
07:05:27.771 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" field "cols" loading contents to SCO wrapper from the datastore
07:05:27.772 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.774 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@15d0f27f"
07:05:27.774 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.COMMENT,A0."COLUMN_NAME",A0.TYPE_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM COLUMNS_V2 A0 WHERE A0.CD_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.775 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.775 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2200504b" (id="org.datanucleus.identity.IdentityReference@30511f34") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.775 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.ExecutionContext$EmbeddedOwnerRelation - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/ExecutionContext$EmbeddedOwnerRelation.class
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1d5bc60c" (id="org.datanucleus.identity.IdentityReference@5d48caef") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@250a026a" (id="org.datanucleus.identity.IdentityReference@ab746ea") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3e0ebb96"
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2200504b" (id="org.datanucleus.identity.IdentityReference@30511f34") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2200504b" (id="org.datanucleus.identity.IdentityReference@30511f34") enlisted in transactional cache
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1d5bc60c" (id="org.datanucleus.identity.IdentityReference@5d48caef") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1d5bc60c" (id="org.datanucleus.identity.IdentityReference@5d48caef") enlisted in transactional cache
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@250a026a" (id="org.datanucleus.identity.IdentityReference@ab746ea") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.776 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@250a026a" (id="org.datanucleus.identity.IdentityReference@ab746ea") enlisted in transactional cache
07:05:27.777 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" having fields "serDeInfo" fetched from table "SDS"
07:05:27.777 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@49437df5"
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0."NAME",B0.SLIB,B0.SERDE_ID FROM SDS A0 LEFT OUTER JOIN SERDES B0 ON A0.SERDE_ID = B0.SERDE_ID WHERE A0.SD_ID = <1>
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo"
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" not found in Level 1 cache [cache size = 7]
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@247bb88e"
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.780 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") enlisted in transactional cache
07:05:27.781 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.781 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.781 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.783 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5455b942"
07:05:27.783 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM SERDE_PARAMS A0 WHERE A0.SERDE_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:27.783 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.784 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2779050e"
07:05:27.785 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "bucketCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.785 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: com.google.common.collect.Lists
07:05:27.785 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "bucketCols" loading contents to SCO wrapper from the datastore
07:05:27.785 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.788 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@10014d1e"
07:05:27.788 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.BUCKET_COL_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM BUCKETING_COLS A0 WHERE A0.SD_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.788 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.788 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@54fbd8e1"
07:05:27.788 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "sortCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.789 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.791 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1e20d158"
07:05:27.791 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SORT_COLS THIS WHERE THIS.SD_ID=<1> AND THIS.INTEGER_IDX>=0
07:05:27.792 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.792 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@29e9990c"
07:05:27.792 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "sortCols" loading contents to SCO wrapper from the datastore
07:05:27.792 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.794 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2e3d0615"
07:05:27.794 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0."COLUMN_NAME",A0."ORDER",A0.INTEGER_IDX AS NUCORDER0 FROM SORT_COLS A0 WHERE A0.SD_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.794 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.794 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6d3b36c8"
07:05:27.795 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.795 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.795 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.798 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@12863812"
07:05:27.798 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM SD_PARAMS A0 WHERE A0.SD_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:27.798 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.798 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5c015379"
07:05:27.798 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "skewedColNames" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.798 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "skewedColNames" loading contents to SCO wrapper from the datastore
07:05:27.798 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.801 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2601c23e"
07:05:27.801 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.SKEWED_COL_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM SKEWED_COL_NAMES A0 WHERE A0.SD_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.802 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.802 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@15d849ab"
07:05:27.802 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "skewedColValues" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.802 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.805 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@186fbfae"
07:05:27.805 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=<1> AND THIS.INTEGER_IDX>=0
07:05:27.805 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.806 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1e399fbb"
07:05:27.806 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "skewedColValues" loading contents to SCO wrapper from the datastore
07:05:27.806 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.810 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@606971d3"
07:05:27.810 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MStringList' AS NUCLEUS_TYPE,A1.STRING_LIST_ID,A0.INTEGER_IDX AS NUCORDER0 FROM SKEWED_VALUES A0 INNER JOIN SKEWED_STRING_LIST A1 ON A0.STRING_LIST_ID_EID = A1.STRING_LIST_ID WHERE A0.SD_ID_OID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.810 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.810 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3c9ae85f"
07:05:27.810 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "skewedColValueLocationMaps" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.810 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.814 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@21b6d914"
07:05:27.814 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SKEWED_COL_VALUE_LOC_MAP WHERE SD_ID=<1> AND STRING_LIST_ID_KID IS NOT NULL
07:05:27.814 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.814 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1737ab72"
07:05:27.815 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" field "skewedColValueLocationMaps" loading contents to SCO wrapper from the datastore
07:05:27.815 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.MapKeySetStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/MapKeySetStore.class
07:05:27.815 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.AbstractSetStore - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/AbstractSetStore.class
07:05:27.817 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.mapping.java.InterfaceMapping - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/mapping/java/InterfaceMapping.class
07:05:27.818 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.metadata.MapMetaData$MapType - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/metadata/MapMetaData$MapType.class
07:05:27.819 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.822 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@5e67eb1f"
07:05:27.823 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MStringList' AS NUCLEUS_TYPE,A0.STRING_LIST_ID FROM SKEWED_STRING_LIST A0 INNER JOIN SKEWED_COL_VALUE_LOC_MAP B0 ON A0.STRING_LIST_ID = B0.STRING_LIST_ID_KID WHERE B0.SD_ID = <1>
07:05:27.823 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.823 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.store.rdbms.scostore.SetStoreIterator - jar:file:/Users/lian/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar!/org/datanucleus/store/rdbms/scostore/SetStoreIterator.class
07:05:27.824 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@27813197"
07:05:27.824 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@569460c6"
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.STRING_LIST_ID_KID,A0.LOCATION FROM SKEWED_COL_VALUE_LOC_MAP A0 WHERE A0.SD_ID = <1> AND NOT (A0.STRING_LIST_ID_KID IS NULL)
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@682e97ba"
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.827 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@14f8be88]]
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@14f8be88 is committing for transaction Xid=    with onePhase=true
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@14f8be88 committed connection for transaction Xid=    with onePhase=true
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@7abdea17" closed
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@51b5e975 [conn=com.jolbox.bonecp.ConnectionHandle@7abdea17, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" (depth=0)
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") being evicted from transactional cache
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e from StateManager[pc=org.apache.hadoop.hive.metastore.model.MSerDeInfo@7605cf6e, lifecycle=DETACHED_CLEAN]
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" being removed from Level 1 cache [current cache size = 8]
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@2200504b" (depth=0)
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2200504b" (id="org.datanucleus.identity.IdentityReference@30511f34") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.828 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2200504b" (id="org.datanucleus.identity.IdentityReference@30511f34") being evicted from transactional cache
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@2200504b from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@2200504b, lifecycle=DETACHED_CLEAN]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@30511f34" being removed from Level 1 cache [current cache size = 7]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" (depth=0)
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@1d5bc60c" (depth=0)
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1d5bc60c" (id="org.datanucleus.identity.IdentityReference@5d48caef") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@1d5bc60c" (id="org.datanucleus.identity.IdentityReference@5d48caef") being evicted from transactional cache
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@1d5bc60c from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@1d5bc60c, lifecycle=DETACHED_CLEAN]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@5d48caef" being removed from Level 1 cache [current cache size = 6]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@250a026a" (depth=0)
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@250a026a" (id="org.datanucleus.identity.IdentityReference@ab746ea") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@250a026a" (id="org.datanucleus.identity.IdentityReference@ab746ea") being evicted from transactional cache
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@250a026a from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@250a026a, lifecycle=DETACHED_CLEAN]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@ab746ea" being removed from Level 1 cache [current cache size = 5]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") is having the SCO wrapper in field "cols" replaced by the unwrapped value
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") being evicted from transactional cache
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e from StateManager[pc=org.apache.hadoop.hive.metastore.model.MColumnDescriptor@b68c59e, lifecycle=DETACHED_CLEAN]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" being removed from Level 1 cache [current cache size = 4]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" (depth=0)
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MDatabase@2bc88067" (depth=1)
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2bc88067" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@2bc88067" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") being evicted from transactional cache
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MDatabase@2bc88067 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MDatabase@2bc88067, lifecycle=DETACHED_CLEAN]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" being removed from Level 1 cache [current cache size = 3]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "partitionKeys" replaced by the unwrapped value
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (depth=1)
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "bucketCols" replaced by the unwrapped value
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "skewedColNames" replaced by the unwrapped value
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "sortCols" replaced by the unwrapped value
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") being evicted from transactional cache
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MStorageDescriptor@775e3360, lifecycle=DETACHED_CLEAN]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" being removed from Level 1 cache [current cache size = 2]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@9578c1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") being evicted from transactional cache
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MTable@9578c1c from StateManager[pc=org.apache.hadoop.hive.metastore.model.MTable@9578c1c, lifecycle=DETACHED_CLEAN]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MTable" being removed from Level 1 cache [current cache size = 1]
07:05:27.829 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 2 ms
07:05:27.830 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$5$$anonfun$3 - 1656188365
07:05:27.830 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.collection.mutable.Buffer$
07:05:27.830 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.collection.mutable.Buffer
07:05:27.830 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$5$$anonfun$4 - -287183410
07:05:27.830 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.collection.mutable.BufferLike
07:05:27.831 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$5$$anonfun$5 - -1888156024
07:05:27.831 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: scala.collection.immutable.Nil$
07:05:27.831 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.Shim_v0_12$$anonfun$getDataLocation$1 - 1345668360
07:05:27.832 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$5$$anonfun$6 - -1871689952
07:05:27.832 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: custom defining: org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$5$$anonfun$7 - 1932004138
07:05:27.832 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: shared class: org.apache.spark.sql.catalyst.catalog.CatalogTable$
07:05:27.833 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveMetastoreCatalog: Creating new cached data source for QualifiedTableName(default,t1)
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG IsolatedClientLoader: hive class: sun.reflect.MethodAccessorImpl - jar:file:/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/rt.jar!/sun/reflect/MethodAccessorImpl.class
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t1
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t1
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t1
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@70f5ae1f" opened with isolation level "read-committed" and auto-commit=false
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4a025ca5, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4a025ca5 is starting for transaction Xid=    with flags 0
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@36dd9b9f"
07:05:27.834 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t1'> AND B0."NAME" = <'default'>
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 1 ms
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MTable"
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MTable" not found in Level 1 cache [cache size = 0]
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") added to Level 1 cache (loadedFlags="[NNNNNNNNNNNN]")
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" having fields "database,parameters,partitionKeys,sd,viewExpandedText,viewOriginalText" fetched from table "TBLS"
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@14b2bd53"
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0."DESC",B0.DB_LOCATION_URI,B0."NAME",B0.OWNER_NAME,B0.OWNER_TYPE,B0.DB_ID,C0.INPUT_FORMAT,C0.IS_COMPRESSED,C0.IS_STOREDASSUBDIRECTORIES,C0.LOCATION,C0.NUM_BUCKETS,C0.OUTPUT_FORMAT,C0.SD_ID,A0.VIEW_EXPANDED_TEXT,A0.VIEW_ORIGINAL_TEXT FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID LEFT OUTER JOIN SDS C0 ON A0.SD_ID = C0.SD_ID WHERE A0.TBL_ID = <1>
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" not found in Level 1 cache [cache size = 1]
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@247797cb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") added to Level 1 cache (loadedFlags="[NNNNNN]")
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor"
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" not found in Level 1 cache [cache size = 2]
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") added to Level 1 cache (loadedFlags="[NNNNNNNNNNNNNN]")
07:05:27.835 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@784f8321"
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" field "partitionKeys" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@3212ffd2"
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1d56ac54"
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" field "partitionKeys" loading contents to SCO wrapper from the datastore
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@389f2c43"
07:05:27.836 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PKEY_COMMENT,A0.PKEY_NAME,A0.PKEY_TYPE,A0.INTEGER_IDX AS NUCORDER0 FROM PARTITION_KEYS A0 WHERE A0.TBL_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@20209b3"
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") enlisted in transactional cache
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@247797cb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@247797cb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") enlisted in transactional cache
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") enlisted in transactional cache
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" having fields "cd" fetched from table "SDS"
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@433ed084"
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0.CD_ID FROM SDS A0 LEFT OUTER JOIN CDS B0 ON A0.CD_ID = B0.CD_ID WHERE A0.SD_ID = <1>
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor"
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" not found in Level 1 cache [cache size = 3]
07:05:27.837 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") added to Level 1 cache (loadedFlags="[N]")
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3dec0fb1"
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") enlisted in transactional cache
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" field "cols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@606a3dca"
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM COLUMNS_V2 THIS WHERE THIS.CD_ID=<1> AND THIS.INTEGER_IDX>=0
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@18070ae2"
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" field "cols" loading contents to SCO wrapper from the datastore
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.838 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@34d66409"
07:05:27.839 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.COMMENT,A0."COLUMN_NAME",A0.TYPE_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM COLUMNS_V2 A0 WHERE A0.CD_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.845 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 6 ms
07:05:27.845 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@d0f1189" (id="org.datanucleus.identity.IdentityReference@afecaf3") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.845 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2f9e2807" (id="org.datanucleus.identity.IdentityReference@1ef8481c") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@263d16fc" (id="org.datanucleus.identity.IdentityReference@53cbd655") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7a59128f"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@d0f1189" (id="org.datanucleus.identity.IdentityReference@afecaf3") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@d0f1189" (id="org.datanucleus.identity.IdentityReference@afecaf3") enlisted in transactional cache
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2f9e2807" (id="org.datanucleus.identity.IdentityReference@1ef8481c") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2f9e2807" (id="org.datanucleus.identity.IdentityReference@1ef8481c") enlisted in transactional cache
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@263d16fc" (id="org.datanucleus.identity.IdentityReference@53cbd655") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@263d16fc" (id="org.datanucleus.identity.IdentityReference@53cbd655") enlisted in transactional cache
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" having fields "serDeInfo" fetched from table "SDS"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2459d3d3"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0."NAME",B0.SLIB,B0.SERDE_ID FROM SDS A0 LEFT OUTER JOIN SERDES B0 ON A0.SERDE_ID = B0.SERDE_ID WHERE A0.SD_ID = <1>
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" not found in Level 1 cache [cache size = 7]
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3fbb847e"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") enlisted in transactional cache
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@52db003d"
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM SERDE_PARAMS A0 WHERE A0.SERDE_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.846 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3fd546b2"
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "bucketCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "bucketCols" loading contents to SCO wrapper from the datastore
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@573d0301"
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.BUCKET_COL_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM BUCKETING_COLS A0 WHERE A0.SD_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1c60e6f3"
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "sortCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@2e77769"
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SORT_COLS THIS WHERE THIS.SD_ID=<1> AND THIS.INTEGER_IDX>=0
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1a42bfdc"
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "sortCols" loading contents to SCO wrapper from the datastore
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@10e9519"
07:05:27.847 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0."COLUMN_NAME",A0."ORDER",A0.INTEGER_IDX AS NUCORDER0 FROM SORT_COLS A0 WHERE A0.SD_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6f08e4fd"
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@60bb1a9f"
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM SD_PARAMS A0 WHERE A0.SD_ID = <1> AND A0.PARAM_KEY IS NOT NULL
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5a501c22"
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "skewedColNames" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "skewedColNames" loading contents to SCO wrapper from the datastore
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@62078a15"
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.SKEWED_COL_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM SKEWED_COL_NAMES A0 WHERE A0.SD_ID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7bc3a9fe"
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "skewedColValues" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.848 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@21280ba8"
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=<1> AND THIS.INTEGER_IDX>=0
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5336f3e7"
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "skewedColValues" loading contents to SCO wrapper from the datastore
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@355c3c26"
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MStringList' AS NUCLEUS_TYPE,A1.STRING_LIST_ID,A0.INTEGER_IDX AS NUCORDER0 FROM SKEWED_VALUES A0 INNER JOIN SKEWED_STRING_LIST A1 ON A0.STRING_LIST_ID_EID = A1.STRING_LIST_ID WHERE A0.SD_ID_OID = <1> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@204850f7"
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "skewedColValueLocationMaps" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@448526d2"
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SKEWED_COL_VALUE_LOC_MAP WHERE SD_ID=<1> AND STRING_LIST_ID_KID IS NOT NULL
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.849 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@493126f"
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" field "skewedColValueLocationMaps" loading contents to SCO wrapper from the datastore
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@66be25e2"
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MStringList' AS NUCLEUS_TYPE,A0.STRING_LIST_ID FROM SKEWED_STRING_LIST A0 INNER JOIN SKEWED_COL_VALUE_LOC_MAP B0 ON A0.STRING_LIST_ID = B0.STRING_LIST_ID_KID WHERE B0.SD_ID = <1>
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7f03658a"
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@631f106f"
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.STRING_LIST_ID_KID,A0.LOCATION FROM SKEWED_COL_VALUE_LOC_MAP A0 WHERE A0.SD_ID = <1> AND NOT (A0.STRING_LIST_ID_KID IS NULL)
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1ada85d0"
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.850 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4a025ca5]]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4a025ca5 is committing for transaction Xid=    with onePhase=true
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@4a025ca5 committed connection for transaction Xid=    with onePhase=true
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@70f5ae1f" closed
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@6b8555a6 [conn=com.jolbox.bonecp.ConnectionHandle@70f5ae1f, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" (depth=0)
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244" (id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") being evicted from transactional cache
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MSerDeInfo@55161244, lifecycle=DETACHED_CLEAN]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" being removed from Level 1 cache [current cache size = 8]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@2f9e2807" (depth=0)
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2f9e2807" (id="org.datanucleus.identity.IdentityReference@1ef8481c") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@2f9e2807" (id="org.datanucleus.identity.IdentityReference@1ef8481c") being evicted from transactional cache
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@2f9e2807 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@2f9e2807, lifecycle=DETACHED_CLEAN]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@1ef8481c" being removed from Level 1 cache [current cache size = 7]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" (depth=0)
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@d0f1189" (depth=0)
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@d0f1189" (id="org.datanucleus.identity.IdentityReference@afecaf3") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@d0f1189" (id="org.datanucleus.identity.IdentityReference@afecaf3") being evicted from transactional cache
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@d0f1189 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@d0f1189, lifecycle=DETACHED_CLEAN]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@afecaf3" being removed from Level 1 cache [current cache size = 6]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@263d16fc" (depth=0)
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@263d16fc" (id="org.datanucleus.identity.IdentityReference@53cbd655") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@263d16fc" (id="org.datanucleus.identity.IdentityReference@53cbd655") being evicted from transactional cache
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@263d16fc from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@263d16fc, lifecycle=DETACHED_CLEAN]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@53cbd655" being removed from Level 1 cache [current cache size = 5]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") is having the SCO wrapper in field "cols" replaced by the unwrapped value
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9" (id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") being evicted from transactional cache
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MColumnDescriptor@8e354d9, lifecycle=DETACHED_CLEAN]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" being removed from Level 1 cache [current cache size = 4]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" (depth=0)
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MDatabase@247797cb" (depth=1)
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@247797cb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@247797cb" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") being evicted from transactional cache
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MDatabase@247797cb from StateManager[pc=org.apache.hadoop.hive.metastore.model.MDatabase@247797cb, lifecycle=DETACHED_CLEAN]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" being removed from Level 1 cache [current cache size = 3]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "partitionKeys" replaced by the unwrapped value
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (depth=1)
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "bucketCols" replaced by the unwrapped value
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "skewedColNames" replaced by the unwrapped value
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "sortCols" replaced by the unwrapped value
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f" (id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") being evicted from transactional cache
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f from StateManager[pc=org.apache.hadoop.hive.metastore.model.MStorageDescriptor@1890d14f, lifecycle=DETACHED_CLEAN]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" being removed from Level 1 cache [current cache size = 2]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@6ada003b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MTable") being evicted from transactional cache
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MTable@6ada003b from StateManager[pc=org.apache.hadoop.hive.metastore.model.MTable@6ada003b, lifecycle=DETACHED_CLEAN]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MTable" being removed from Level 1 cache [current cache size = 1]
07:05:27.851 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 1 ms
07:05:27.857 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HDFSFileCatalog: Listing file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1 on driver
07:05:27.890 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.890 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.893 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)), i#52, j#53, k#54) AS #55]   Project [createexternalrow(if (isnull(i#52)) null else i#52, if (isnull(j#53)) null else j#53, if (isnull(k#54)) null else k#54.toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)) AS #55]
 +- LocalRelation [i#52,j#53,k#54]                                                                                                                                                                                                                                                                                                                            +- LocalRelation [i#52,j#53,k#54]

07:05:27.894 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveSqlParser: Parsing command: t2
07:05:27.894 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t2
07:05:27.894 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t2
07:05:27.894 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t2
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@24bf2a8e" opened with isolation level "read-committed" and auto-commit=false
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@459f19d8, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@459f19d8 is starting for transaction Xid=    with flags 0
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@673eb5a0"
07:05:27.895 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t2'> AND B0."NAME" = <'default'>
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 1 ms
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "2[OID]org.apache.hadoop.hive.metastore.model.MTable"
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "2[OID]org.apache.hadoop.hive.metastore.model.MTable" not found in Level 1 cache [cache size = 0]
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") added to Level 1 cache (loadedFlags="[NNNNNNNNNNNN]")
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" having fields "database,parameters,partitionKeys,sd,viewExpandedText,viewOriginalText" fetched from table "TBLS"
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@56a62c1f"
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0."DESC",B0.DB_LOCATION_URI,B0."NAME",B0.OWNER_NAME,B0.OWNER_TYPE,B0.DB_ID,C0.INPUT_FORMAT,C0.IS_COMPRESSED,C0.IS_STOREDASSUBDIRECTORIES,C0.LOCATION,C0.NUM_BUCKETS,C0.OUTPUT_FORMAT,C0.SD_ID,A0.VIEW_EXPANDED_TEXT,A0.VIEW_ORIGINAL_TEXT FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID LEFT OUTER JOIN SDS C0 ON A0.SD_ID = C0.SD_ID WHERE A0.TBL_ID = <2>
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" not found in Level 1 cache [cache size = 1]
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@20defb1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") added to Level 1 cache (loadedFlags="[NNNNNN]")
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor"
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" not found in Level 1 cache [cache size = 2]
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") added to Level 1 cache (loadedFlags="[NNNNNNNNNNNNNN]")
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@51f68aa"
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" field "partitionKeys" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.896 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6a80b53d"
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY IS NOT NULL
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@25ed6c6e"
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" field "partitionKeys" loading contents to SCO wrapper from the datastore
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4aab1cb3"
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PKEY_COMMENT,A0.PKEY_NAME,A0.PKEY_TYPE,A0.INTEGER_IDX AS NUCORDER0 FROM PARTITION_KEYS A0 WHERE A0.TBL_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.897 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@596d6183"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") enlisted in transactional cache
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@20defb1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@20defb1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") enlisted in transactional cache
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") enlisted in transactional cache
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" having fields "cd" fetched from table "SDS"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4b31a099"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0.CD_ID FROM SDS A0 LEFT OUTER JOIN CDS B0 ON A0.CD_ID = B0.CD_ID WHERE A0.SD_ID = <2>
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" not found in Level 1 cache [cache size = 3]
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") added to Level 1 cache (loadedFlags="[N]")
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2530332e"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") enlisted in transactional cache
07:05:27.898 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" field "cols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4a5bf240"
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM COLUMNS_V2 THIS WHERE THIS.CD_ID=<2> AND THIS.INTEGER_IDX>=0
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3476ec79"
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" field "cols" loading contents to SCO wrapper from the datastore
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@212ac145"
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.COMMENT,A0."COLUMN_NAME",A0.TYPE_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM COLUMNS_V2 A0 WHERE A0.CD_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@6f8b6a23" (id="org.datanucleus.identity.IdentityReference@18dca094") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.899 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@16cd406d" (id="org.datanucleus.identity.IdentityReference@59dd06f4") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5f062f0c" (id="org.datanucleus.identity.IdentityReference@6be89a20") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6e5d78be"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@6f8b6a23" (id="org.datanucleus.identity.IdentityReference@18dca094") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@6f8b6a23" (id="org.datanucleus.identity.IdentityReference@18dca094") enlisted in transactional cache
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@16cd406d" (id="org.datanucleus.identity.IdentityReference@59dd06f4") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@16cd406d" (id="org.datanucleus.identity.IdentityReference@59dd06f4") enlisted in transactional cache
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5f062f0c" (id="org.datanucleus.identity.IdentityReference@6be89a20") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5f062f0c" (id="org.datanucleus.identity.IdentityReference@6be89a20") enlisted in transactional cache
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" having fields "serDeInfo" fetched from table "SDS"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@12ef9330"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0."NAME",B0.SLIB,B0.SERDE_ID FROM SDS A0 LEFT OUTER JOIN SERDES B0 ON A0.SERDE_ID = B0.SERDE_ID WHERE A0.SD_ID = <2>
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" not found in Level 1 cache [cache size = 7]
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@38ec718d"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") enlisted in transactional cache
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.900 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@57249eb4"
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM SERDE_PARAMS A0 WHERE A0.SERDE_ID = <2> AND A0.PARAM_KEY IS NOT NULL
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@e73edeb"
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "bucketCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "bucketCols" loading contents to SCO wrapper from the datastore
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@70e45956"
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.BUCKET_COL_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM BUCKETING_COLS A0 WHERE A0.SD_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.901 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6c0a7f44"
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "sortCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@19334511"
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SORT_COLS THIS WHERE THIS.SD_ID=<2> AND THIS.INTEGER_IDX>=0
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@8c6cbc2"
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "sortCols" loading contents to SCO wrapper from the datastore
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.902 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@3491b5fc"
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0."COLUMN_NAME",A0."ORDER",A0.INTEGER_IDX AS NUCORDER0 FROM SORT_COLS A0 WHERE A0.SD_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3d43e242"
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4118af46"
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM SD_PARAMS A0 WHERE A0.SD_ID = <2> AND A0.PARAM_KEY IS NOT NULL
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@74ae1f1a"
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "skewedColNames" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "skewedColNames" loading contents to SCO wrapper from the datastore
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@7048ec56"
07:05:27.903 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.SKEWED_COL_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM SKEWED_COL_NAMES A0 WHERE A0.SD_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4d6279a6"
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "skewedColValues" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4930f9df"
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=<2> AND THIS.INTEGER_IDX>=0
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@31ecc8c6"
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "skewedColValues" loading contents to SCO wrapper from the datastore
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6f9d3814"
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MStringList' AS NUCLEUS_TYPE,A1.STRING_LIST_ID,A0.INTEGER_IDX AS NUCORDER0 FROM SKEWED_VALUES A0 INNER JOIN SKEWED_STRING_LIST A1 ON A0.STRING_LIST_ID_EID = A1.STRING_LIST_ID WHERE A0.SD_ID_OID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3325dae1"
07:05:27.904 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "skewedColValueLocationMaps" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@64588ebb"
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SKEWED_COL_VALUE_LOC_MAP WHERE SD_ID=<2> AND STRING_LIST_ID_KID IS NOT NULL
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2db025d7"
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" field "skewedColValueLocationMaps" loading contents to SCO wrapper from the datastore
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@305f0052"
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MStringList' AS NUCLEUS_TYPE,A0.STRING_LIST_ID FROM SKEWED_STRING_LIST A0 INNER JOIN SKEWED_COL_VALUE_LOC_MAP B0 ON A0.STRING_LIST_ID = B0.STRING_LIST_ID_KID WHERE B0.SD_ID = <2>
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6814f6b5"
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@4426f8fd"
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.STRING_LIST_ID_KID,A0.LOCATION FROM SKEWED_COL_VALUE_LOC_MAP A0 WHERE A0.SD_ID = <2> AND NOT (A0.STRING_LIST_ID_KID IS NULL)
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.905 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@40e1a05e"
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@459f19d8]]
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@459f19d8 is committing for transaction Xid=    with onePhase=true
07:05:27.906 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@459f19d8 committed connection for transaction Xid=    with onePhase=true
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@24bf2a8e" closed
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@3a080335 [conn=com.jolbox.bonecp.ConnectionHandle@24bf2a8e, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" (depth=0)
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") being evicted from transactional cache
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MSerDeInfo@56f40e70, lifecycle=DETACHED_CLEAN]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" being removed from Level 1 cache [current cache size = 8]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" (depth=0)
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@6f8b6a23" (depth=0)
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@6f8b6a23" (id="org.datanucleus.identity.IdentityReference@18dca094") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@6f8b6a23" (id="org.datanucleus.identity.IdentityReference@18dca094") being evicted from transactional cache
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@6f8b6a23 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@6f8b6a23, lifecycle=DETACHED_CLEAN]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@18dca094" being removed from Level 1 cache [current cache size = 7]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@16cd406d" (depth=0)
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@16cd406d" (id="org.datanucleus.identity.IdentityReference@59dd06f4") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@16cd406d" (id="org.datanucleus.identity.IdentityReference@59dd06f4") being evicted from transactional cache
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@16cd406d from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@16cd406d, lifecycle=DETACHED_CLEAN]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@59dd06f4" being removed from Level 1 cache [current cache size = 6]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@5f062f0c" (depth=0)
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5f062f0c" (id="org.datanucleus.identity.IdentityReference@6be89a20") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5f062f0c" (id="org.datanucleus.identity.IdentityReference@6be89a20") being evicted from transactional cache
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@5f062f0c from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@5f062f0c, lifecycle=DETACHED_CLEAN]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@6be89a20" being removed from Level 1 cache [current cache size = 5]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") is having the SCO wrapper in field "cols" replaced by the unwrapped value
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") being evicted from transactional cache
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MColumnDescriptor@63c372a7, lifecycle=DETACHED_CLEAN]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" being removed from Level 1 cache [current cache size = 4]
07:05:27.907 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" (depth=0)
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MDatabase@20defb1c" (depth=1)
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@20defb1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@20defb1c" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") being evicted from transactional cache
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MDatabase@20defb1c from StateManager[pc=org.apache.hadoop.hive.metastore.model.MDatabase@20defb1c, lifecycle=DETACHED_CLEAN]
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" being removed from Level 1 cache [current cache size = 3]
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "partitionKeys" replaced by the unwrapped value
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (depth=1)
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "bucketCols" replaced by the unwrapped value
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "skewedColNames" replaced by the unwrapped value
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "sortCols" replaced by the unwrapped value
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") being evicted from transactional cache
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MStorageDescriptor@2f25de04, lifecycle=DETACHED_CLEAN]
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" being removed from Level 1 cache [current cache size = 2]
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@1db576fe" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") being evicted from transactional cache
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MTable@1db576fe from StateManager[pc=org.apache.hadoop.hive.metastore.model.MTable@1db576fe, lifecycle=DETACHED_CLEAN]
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MTable" being removed from Level 1 cache [current cache size = 1]
07:05:27.908 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 2 ms
07:05:27.909 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveMetastoreCatalog: Creating new cached data source for QualifiedTableName(default,t2)
07:05:27.909 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG HiveClientImpl: Looking up default.t2
07:05:27.909 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HiveMetaStore: 0: get_table : db=default tbl=t2
07:05:27.909 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO audit: ugi=lian	ip=unknown-ip-addr	cmd=get_table : db=default tbl=t2
07:05:27.909 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction created [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.909 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction begun for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 (optimistic=false)
07:05:27.909 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 1, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)
07:05:27.909 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Open transaction: count = 2, isActive = true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" has been run before so reusing existing generic compilation
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: Query "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String dbFetchPlan [default]" of language "JDOQL" for datastore "rdbms-derby" has been run before so reusing existing datastore compilation
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@53b0f362" opened with isolation level "read-committed" and auto-commit=false
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Running enlist operation on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1e1e5a1a, error code TMNOFLAGS and transaction: [DataNucleus Transaction, ID=Xid=   , enlisted resources=[]]
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1e1e5a1a is starting for transaction Xid=    with flags 0
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection added to the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Executing "SELECT UNIQUE FROM org.apache.hadoop.hive.metastore.model.MTable WHERE tableName == table && database.name == db PARAMETERS java.lang.String table, java.lang.String db" ...
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@60f827a"
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT DISTINCT 'org.apache.hadoop.hive.metastore.model.MTable' AS NUCLEUS_TYPE,A0.CREATE_TIME,A0.LAST_ACCESS_TIME,A0.OWNER,A0.RETENTION,A0.TBL_NAME,A0.TBL_TYPE,A0.TBL_ID FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID WHERE A0.TBL_NAME = <'t2'> AND B0."NAME" = <'default'>
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Query: JDOQL Query : Execution Time = 0 ms
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "2[OID]org.apache.hadoop.hive.metastore.model.MTable"
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "2[OID]org.apache.hadoop.hive.metastore.model.MTable" not found in Level 1 cache [cache size = 0]
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") added to Level 1 cache (loadedFlags="[NNNNNNNNNNNN]")
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" having fields "database,parameters,partitionKeys,sd,viewExpandedText,viewOriginalText" fetched from table "TBLS"
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.910 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@39b052b6"
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0."DESC",B0.DB_LOCATION_URI,B0."NAME",B0.OWNER_NAME,B0.OWNER_TYPE,B0.DB_ID,C0.INPUT_FORMAT,C0.IS_COMPRESSED,C0.IS_STOREDASSUBDIRECTORIES,C0.LOCATION,C0.NUM_BUCKETS,C0.OUTPUT_FORMAT,C0.SD_ID,A0.VIEW_EXPANDED_TEXT,A0.VIEW_ORIGINAL_TEXT FROM TBLS A0 LEFT OUTER JOIN DBS B0 ON A0.DB_ID = B0.DB_ID LEFT OUTER JOIN SDS C0 ON A0.SD_ID = C0.SD_ID WHERE A0.TBL_ID = <2>
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase"
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" not found in Level 1 cache [cache size = 1]
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MDatabase@1e0268b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") added to Level 1 cache (loadedFlags="[NNNNNN]")
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor"
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" not found in Level 1 cache [cache size = 2]
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") added to Level 1 cache (loadedFlags="[NNNNNNNNNNNNNN]")
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@2122bed1"
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" field "partitionKeys" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@eaa541d"
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM TABLE_PARAMS A0 WHERE A0.TBL_ID = <2> AND A0.PARAM_KEY IS NOT NULL
07:05:27.911 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@1f006f6b"
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" field "partitionKeys" loading contents to SCO wrapper from the datastore
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@43d6ca2a"
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PKEY_COMMENT,A0.PKEY_NAME,A0.PKEY_TYPE,A0.INTEGER_IDX AS NUCORDER0 FROM PARTITION_KEYS A0 WHERE A0.TBL_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@174e0188"
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.912 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") enlisted in transactional cache
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 1, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@1e0268b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@1e0268b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") enlisted in transactional cache
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") enlisted in transactional cache
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" having fields "cd" fetched from table "SDS"
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@623c0ed1"
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0.CD_ID FROM SDS A0 LEFT OUTER JOIN CDS B0 ON A0.CD_ID = B0.CD_ID WHERE A0.SD_ID = <2>
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor"
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" not found in Level 1 cache [cache size = 3]
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") added to Level 1 cache (loadedFlags="[N]")
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3d988b78"
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") enlisted in transactional cache
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" field "cols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1066988e"
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM COLUMNS_V2 THIS WHERE THIS.CD_ID=<2> AND THIS.INTEGER_IDX>=0
07:05:27.913 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3d37b352"
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" field "cols" loading contents to SCO wrapper from the datastore
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@19b1fe92"
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.COMMENT,A0."COLUMN_NAME",A0.TYPE_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM COLUMNS_V2 A0 WHERE A0.CD_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@22348888" (id="org.datanucleus.identity.IdentityReference@34e7b5e8") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@52d513f3" (id="org.datanucleus.identity.IdentityReference@37921a3c") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5de20796" (id="org.datanucleus.identity.IdentityReference@5b14b4b4") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@7a86a5ec"
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@22348888" (id="org.datanucleus.identity.IdentityReference@34e7b5e8") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@22348888" (id="org.datanucleus.identity.IdentityReference@34e7b5e8") enlisted in transactional cache
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@52d513f3" (id="org.datanucleus.identity.IdentityReference@37921a3c") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@52d513f3" (id="org.datanucleus.identity.IdentityReference@37921a3c") enlisted in transactional cache
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5de20796" (id="org.datanucleus.identity.IdentityReference@5b14b4b4") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5de20796" (id="org.datanucleus.identity.IdentityReference@5b14b4b4") enlisted in transactional cache
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" having fields "serDeInfo" fetched from table "SDS"
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@b67bc4"
07:05:27.914 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT B0."NAME",B0.SLIB,B0.SERDE_ID FROM SDS A0 LEFT OUTER JOIN SERDES B0 ON A0.SERDE_ID = B0.SERDE_ID WHERE A0.SD_ID = <2>
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 1 ms
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Retrieved object with OID "2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo"
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id "2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" not found in Level 1 cache [cache size = 7]
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") added to Level 1 cache (loadedFlags="[NNN]")
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4fd7eed6"
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "HOLLOW"->"P_CLEAN"
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") enlisted in transactional cache
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@19a54ab9"
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM SERDE_PARAMS A0 WHERE A0.SERDE_ID = <2> AND A0.PARAM_KEY IS NOT NULL
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6517e6f6"
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "bucketCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "bucketCols" loading contents to SCO wrapper from the datastore
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@7f52448b"
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.BUCKET_COL_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM BUCKETING_COLS A0 WHERE A0.SD_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.915 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@73ff5f3c"
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "sortCols" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@11ab6cc"
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SORT_COLS THIS WHERE THIS.SD_ID=<2> AND THIS.INTEGER_IDX>=0
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@6ce16d98"
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "sortCols" loading contents to SCO wrapper from the datastore
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@65eec987"
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0."COLUMN_NAME",A0."ORDER",A0.INTEGER_IDX AS NUCORDER0 FROM SORT_COLS A0 WHERE A0.SD_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5316d776"
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "parameters" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "parameters" loading contents to SCO wrapper from the datastore
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@347ced21"
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.PARAM_KEY,A0.PARAM_VALUE FROM SD_PARAMS A0 WHERE A0.SD_ID = <2> AND A0.PARAM_KEY IS NOT NULL
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@46ee22cb"
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "skewedColNames" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.916 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "skewedColNames" loading contents to SCO wrapper from the datastore
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@1528dad2"
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.SKEWED_COL_NAME,A0.INTEGER_IDX AS NUCORDER0 FROM SKEWED_COL_NAMES A0 WHERE A0.SD_ID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@4b3e6f98"
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "skewedColValues" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.List" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@7cf1e44e"
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=<2> AND THIS.INTEGER_IDX>=0
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@397d7ee7"
07:05:27.917 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "skewedColValues" loading contents to SCO wrapper from the datastore
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@18784ff"
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MStringList' AS NUCLEUS_TYPE,A1.STRING_LIST_ID,A0.INTEGER_IDX AS NUCORDER0 FROM SKEWED_VALUES A0 INNER JOIN SKEWED_STRING_LIST A1 ON A0.STRING_LIST_ID_EID = A1.STRING_LIST_ID WHERE A0.SD_ID_OID = <2> AND A0.INTEGER_IDX >= 0 ORDER BY NUCORDER0
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@3fcce416"
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "skewedColValueLocationMaps" is replaced by a SCO wrapper of type "org.datanucleus.store.types.backed.Map" [cache-values=true, lazy-loading=true, queued-operations=false, allow-nulls=true]
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6d143f4e"
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT COUNT(*) FROM SKEWED_COL_VALUE_LOC_MAP WHERE SD_ID=<2> AND STRING_LIST_ID_KID IS NOT NULL
07:05:27.918 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@44ce8bdd"
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" field "skewedColValueLocationMaps" loading contents to SCO wrapper from the datastore
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@86c3245"
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT 'org.apache.hadoop.hive.metastore.model.MStringList' AS NUCLEUS_TYPE,A0.STRING_LIST_ID FROM SKEWED_STRING_LIST A0 INNER JOIN SKEWED_COL_VALUE_LOC_MAP B0 ON A0.STRING_LIST_ID = B0.STRING_LIST_ID_KID WHERE B0.SD_ID = <2>
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@5a3e9760"
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection found in the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "com.jolbox.bonecp.PreparedStatementHandle@6a1e7f99"
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Native: SELECT A0.STRING_LIST_ID_KID,A0.LOCATION FROM SKEWED_COL_VALUE_LOC_MAP A0 WHERE A0.SD_ID = <2> AND NOT (A0.STRING_LIST_ID_KID IS NULL)
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Retrieve: Execution Time = 0 ms
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Datastore: Closing PreparedStatement "org.datanucleus.store.rdbms.ParamLoggingPreparedStatement@748f2c21"
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ObjectStore: Commit transaction: count = 0, isactive true at:
	org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committing for ExecutionContext org.datanucleus.ExecutionContextThreadedImpl@4acf81d3
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process started using ordered flush - 0 enlisted objects
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: ExecutionContext.internalFlush() process finished
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Performing check of objects for "persistence-by-reachability" (commit) ...
07:05:27.919 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Completed check of objects for "persistence-by-reachability" (commit).
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Committing [DataNucleus Transaction, ID=Xid=   , enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1e1e5a1a]]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1e1e5a1a is committing for transaction Xid=    with onePhase=true
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Managed connection org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1e1e5a1a committed connection for transaction Xid=    with onePhase=true
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection "com.jolbox.bonecp.ConnectionHandle@53b0f362" closed
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Connection: Connection removed from the pool : org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl@527040b5 [conn=com.jolbox.bonecp.ConnectionHandle@53b0f362, commitOnRelease=false, closeOnRelease=false, closeOnTxnEnd=true] for key=org.datanucleus.ExecutionContextThreadedImpl@4acf81d3 in factory=ConnectionFactory:tx[org.datanucleus.store.rdbms.ConnectionFactoryImpl@78011e64]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@5de20796" (depth=0)
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5de20796" (id="org.datanucleus.identity.IdentityReference@5b14b4b4") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@5de20796" (id="org.datanucleus.identity.IdentityReference@5b14b4b4") being evicted from transactional cache
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@5de20796 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@5de20796, lifecycle=DETACHED_CLEAN]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@5b14b4b4" being removed from Level 1 cache [current cache size = 8]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" (depth=0)
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8" (id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo") being evicted from transactional cache
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MSerDeInfo@7c8cd2d8, lifecycle=DETACHED_CLEAN]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MSerDeInfo" being removed from Level 1 cache [current cache size = 7]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" (depth=0)
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@22348888" (depth=0)
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@22348888" (id="org.datanucleus.identity.IdentityReference@34e7b5e8") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@22348888" (id="org.datanucleus.identity.IdentityReference@34e7b5e8") being evicted from transactional cache
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@22348888 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@22348888, lifecycle=DETACHED_CLEAN]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@34e7b5e8" being removed from Level 1 cache [current cache size = 6]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MFieldSchema@52d513f3" (depth=0)
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@52d513f3" (id="org.datanucleus.identity.IdentityReference@37921a3c") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MFieldSchema@52d513f3" (id="org.datanucleus.identity.IdentityReference@37921a3c") being evicted from transactional cache
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MFieldSchema@52d513f3 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MFieldSchema@52d513f3, lifecycle=DETACHED_CLEAN]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="org.datanucleus.identity.IdentityReference@37921a3c" being removed from Level 1 cache [current cache size = 5]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") is having the SCO wrapper in field "cols" replaced by the unwrapped value
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5" (id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor") being evicted from transactional cache
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MColumnDescriptor@3f5050c5, lifecycle=DETACHED_CLEAN]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MColumnDescriptor" being removed from Level 1 cache [current cache size = 4]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" (depth=0)
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MDatabase@1e0268b" (depth=1)
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MDatabase@1e0268b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MDatabase@1e0268b" (id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase") being evicted from transactional cache
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MDatabase@1e0268b from StateManager[pc=org.apache.hadoop.hive.metastore.model.MDatabase@1e0268b, lifecycle=DETACHED_CLEAN]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="1[OID]org.apache.hadoop.hive.metastore.model.MDatabase" being removed from Level 1 cache [current cache size = 3]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") is having the SCO wrapper in field "partitionKeys" replaced by the unwrapped value
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Detaching object from persistence : "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (depth=1)
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "bucketCols" replaced by the unwrapped value
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "parameters" replaced by the unwrapped value
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "skewedColNames" replaced by the unwrapped value
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") is having the SCO wrapper in field "sortCols" replaced by the unwrapped value
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0" (id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor") being evicted from transactional cache
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4434d1b0, lifecycle=DETACHED_CLEAN]
07:05:27.920 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MStorageDescriptor" being removed from Level 1 cache [current cache size = 2]
07:05:27.921 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Lifecycle: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") has a lifecycle change : "P_CLEAN"->"DETACHED_CLEAN"
07:05:27.921 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Object "org.apache.hadoop.hive.metastore.model.MTable@3d2857a6" (id="2[OID]org.apache.hadoop.hive.metastore.model.MTable") being evicted from transactional cache
07:05:27.921 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Persistence: Disconnecting org.apache.hadoop.hive.metastore.model.MTable@3d2857a6 from StateManager[pc=org.apache.hadoop.hive.metastore.model.MTable@3d2857a6, lifecycle=DETACHED_CLEAN]
07:05:27.921 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Cache: Object with id="2[OID]org.apache.hadoop.hive.metastore.model.MTable" being removed from Level 1 cache [current cache size = 1]
07:05:27.921 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG Transaction: Transaction committed in 2 ms
07:05:27.924 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO HDFSFileCatalog: Listing file:/Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2 on driver
07:05:27.961 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.962 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.964 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)), i#56, j#57, k#58) AS #59]   Project [createexternalrow(if (isnull(i#56)) null else i#56, if (isnull(j#57)) null else j#57, if (isnull(k#58)) null else k#58.toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)) AS #59]
 +- LocalRelation [i#56,j#57,k#58]                                                                                                                                                                                                                                                                                                                            +- LocalRelation [i#56,j#57,k#58]

07:05:27.974 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.980 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.989 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, if (isnull(input[3, int])) null else input[3, int], if (isnull(input[4, int])) null else input[4, int], if (isnull(input[5, string])) null else input[5, string].toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true), StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)), i#52, j#53, k#54, i#56, j#57, k#58) AS #60]   Project [createexternalrow(if (isnull(i#52)) null else i#52, if (isnull(j#53)) null else j#53, if (isnull(k#54)) null else k#54.toString, if (isnull(i#56)) null else i#56, if (isnull(j#57)) null else j#57, if (isnull(k#58)) null else k#58.toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true), StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)) AS #60]
 +- LocalRelation [i#52,j#53,k#54,i#56,j#57,k#58]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- LocalRelation [i#52,j#53,k#54,i#56,j#57,k#58]

07:05:27.991 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.991 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.993 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)), i#52, j#53, k#54) AS #61]   Project [createexternalrow(if (isnull(i#52)) null else i#52, if (isnull(j#53)) null else j#53, if (isnull(k#54)) null else k#54.toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)) AS #61]
 +- LocalRelation [i#52,j#53,k#54]                                                                                                                                                                                                                                                                                                                            +- LocalRelation [i#52,j#53,k#54]

07:05:27.994 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.995 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.998 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)), i#56, j#57, k#58) AS #62]   Project [createexternalrow(if (isnull(i#56)) null else i#56, if (isnull(j#57)) null else j#57, if (isnull(k#58)) null else k#58.toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)) AS #62]
 +- LocalRelation [i#56,j#57,k#58]                                                                                                                                                                                                                                                                                                                            +- LocalRelation [i#56,j#57,k#58]

07:05:27.999 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:27.999 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:28.006 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, if (isnull(input[3, int])) null else input[3, int], if (isnull(input[4, int])) null else input[4, int], if (isnull(input[5, string])) null else input[5, string].toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true), StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)), i#52, j#53, k#54, i#56, j#57, k#58) AS #63]   Project [createexternalrow(if (isnull(i#52)) null else i#52, if (isnull(j#53)) null else j#53, if (isnull(k#54)) null else k#54.toString, if (isnull(i#56)) null else i#56, if (isnull(j#57)) null else j#57, if (isnull(k#58)) null else k#58.toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true), StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)) AS #63]
 +- LocalRelation [i#52,j#53,k#54,i#56,j#57,k#58]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- LocalRelation [i#52,j#53,k#54,i#56,j#57,k#58]

07:05:28.010 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG TestHive: Query references test tables:
07:05:28.019 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, if (isnull(input[3, int])) null else input[3, int], if (isnull(input[4, int])) null else input[4, int], if (isnull(input[5, string])) null else input[5, string].toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true), StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)), i#52, j#53, k#54, i#56, j#57, k#58) AS #64]   Project [createexternalrow(if (isnull(i#52)) null else i#52, if (isnull(j#53)) null else j#53, if (isnull(k#54)) null else k#54.toString, if (isnull(i#56)) null else i#56, if (isnull(j#57)) null else j#57, if (isnull(k#58)) null else k#58.toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true), StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)) AS #64]
 +- LocalRelation [i#52,j#53,k#54,i#56,j#57,k#58]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- LocalRelation [i#52,j#53,k#54,i#56,j#57,k#58]

07:05:28.027 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SimpleAnalyzer:
=== Result of Batch Resolution ===
!'Project [unresolveddeserializer(createexternalrow(if (isnull(input[0, int])) null else input[0, int], if (isnull(input[1, int])) null else input[1, int], if (isnull(input[2, string])) null else input[2, string].toString, if (isnull(input[3, int])) null else input[3, int], if (isnull(input[4, int])) null else input[4, int], if (isnull(input[5, string])) null else input[5, string].toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true), StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)), i#52, j#53, k#54, i#56, j#57, k#58) AS #65]   Project [createexternalrow(if (isnull(i#52)) null else i#52, if (isnull(j#53)) null else j#53, if (isnull(k#54)) null else k#54.toString, if (isnull(i#56)) null else i#56, if (isnull(j#57)) null else j#57, if (isnull(k#58)) null else k#58.toString, StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true), StructField(i,IntegerType,true), StructField(j,IntegerType,true), StructField(k,StringType,true)) AS #65]
 +- LocalRelation [i#52,j#53,k#54,i#56,j#57,k#58]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            +- LocalRelation [i#52,j#53,k#54,i#56,j#57,k#58]

07:05:28.030 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SparkOptimizer:
=== Result of Batch Finish Analysis ===
 Sort [k#54 ASC,k#58 ASC], true                  Sort [k#54 ASC,k#58 ASC], true
 +- Join Inner, Some((i#52 = i#56))              +- Join Inner, Some((i#52 = i#56))
!   :- SubqueryAlias t1                             :- Relation[i#52,j#53,k#54] HadoopFiles
!   :  +- Relation[i#52,j#53,k#54] HadoopFiles      +- Relation[i#56,j#57,k#58] HadoopFiles
!   +- SubqueryAlias t2
!      +- Relation[i#56,j#57,k#58] HadoopFiles

07:05:28.049 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG SparkOptimizer:
=== Result of Batch Operator Optimizations ===
 Sort [k#54 ASC,k#58 ASC], true                  Sort [k#54 ASC,k#58 ASC], true
 +- Join Inner, Some((i#52 = i#56))              +- Join Inner, Some((i#52 = i#56))
!   :- SubqueryAlias t1                             :- Filter isnotnull(i#52)
    :  +- Relation[i#52,j#53,k#54] HadoopFiles      :  +- Relation[i#52,j#53,k#54] HadoopFiles
!   +- SubqueryAlias t2                             +- Filter isnotnull(i#56)
       +- Relation[i#56,j#57,k#58] HadoopFiles         +- Relation[i#56,j#57,k#58] HadoopFiles

07:05:28.050 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: Considering join on: Some((i#52 = i#56))
07:05:28.052 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: leftKeys:List(i#52) | rightKeys:List(i#56)
07:05:28.052 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: Considering join on: Some((i#52 = i#56))
07:05:28.052 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: leftKeys:List(i#52) | rightKeys:List(i#56)
07:05:28.052 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: Considering join on: Some((i#52 = i#56))
07:05:28.052 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: leftKeys:List(i#52) | rightKeys:List(i#56)
07:05:28.054 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: Considering join on: Some((i#52 = i#56))
07:05:28.054 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: leftKeys:List(i#52) | rightKeys:List(i#56)
07:05:28.054 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: Considering join on: Some((i#52 = i#56))
07:05:28.054 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: leftKeys:List(i#52) | rightKeys:List(i#56)
07:05:28.054 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: Considering join on: Some((i#52 = i#56))
07:05:28.054 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ExtractEquiJoinKeys: leftKeys:List(i#52) | rightKeys:List(i#56)
07:05:28.058 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Pruning directories with:
07:05:28.059 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Post-Scan Filters: isnotnull(i#52)
07:05:28.061 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Pruned Data Schema: struct<i: int, j: int, k: string ... 1 more fields>
07:05:28.062 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Pushed Filters: IsNotNull(i)
07:05:28.086 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 134.8 KB, free 2.2 GB)
07:05:28.087 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Put block broadcast_2 locally took  13 ms
07:05:28.087 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Putting block broadcast_2 without replication took  13 ms
07:05:28.092 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.0 KB, free 2.2 GB)
07:05:28.092 dispatcher-event-loop-2 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.102:52176 (size: 15.0 KB, free: 2.2 GB)
07:05:28.092 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0
07:05:28.092 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Told master about block broadcast_2_piece0
07:05:28.092 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Put block broadcast_2_piece0 locally took  0 ms
07:05:28.092 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Putting block broadcast_2_piece0 without replication took  0 ms
07:05:28.093 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO SparkContext: Created broadcast 2 from apply at Transformer.scala:22
07:05:28.096 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Planning with 8 buckets
07:05:28.107 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Pruning directories with:
07:05:28.107 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Post-Scan Filters: isnotnull(i#56)
07:05:28.107 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Pruned Data Schema: struct<i: int, j: int, k: string ... 1 more fields>
07:05:28.107 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Pushed Filters: IsNotNull(i)
07:05:28.112 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 134.8 KB, free 2.2 GB)
07:05:28.113 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Put block broadcast_3 locally took  4 ms
07:05:28.113 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Putting block broadcast_3 without replication took  4 ms
07:05:28.119 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 15.0 KB, free 2.2 GB)
07:05:28.119 dispatcher-event-loop-1 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.102:52176 (size: 15.0 KB, free: 2.2 GB)
07:05:28.119 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0
07:05:28.119 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Told master about block broadcast_3_piece0
07:05:28.119 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Put block broadcast_3_piece0 locally took  1 ms
07:05:28.119 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG BlockManager: Putting block broadcast_3_piece0 without replication took  1 ms
07:05:28.120 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO SparkContext: Created broadcast 3 from apply at Transformer.scala:22
07:05:28.120 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO FileSourceStrategy: Planning with 8 buckets
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2) +++
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2.serialVersionUID
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.sql.execution.metric.LongSQLMetric org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2.numOutputRows$2
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2.apply(java.lang.Object)
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final org.apache.spark.sql.catalyst.InternalRow org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2.apply(org.apache.spark.sql.catalyst.InternalRow)
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:28.147 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:28.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:28.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:28.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:28.148 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2) is now cleaned +++
07:05:28.153 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2) +++
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2.serialVersionUID
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.sql.execution.metric.LongSQLMetric org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2.numOutputRows$2
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2.apply(java.lang.Object)
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final org.apache.spark.sql.catalyst.InternalRow org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2.apply(org.apache.spark.sql.catalyst.InternalRow)
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:28.154 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.sql.execution.RowDataSourceScan$$anonfun$doExecute$2) is now cleaned +++
07:05:28.170 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1) +++
07:05:28.173 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 3
07:05:28.173 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1.serialVersionUID
07:05:28.173 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.sql.execution.joins.SortMergeJoin org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1.$outer
07:05:28.173 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final org.apache.spark.sql.execution.metric.LongSQLMetric org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1.numOutputRows$1
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 3
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final scala.collection.Iterator org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1.apply(scala.collection.Iterator,scala.collection.Iterator)
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1.apply(java.lang.Object,java.lang.Object)
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public org.apache.spark.sql.execution.joins.SortMergeJoin org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1.org$apache$spark$sql$execution$joins$SortMergeJoin$$anonfun$$$outer()
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 6
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1$$anon$1
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1$$anonfun$3
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1$$anon$1$$anonfun$4
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1$$anonfun$2
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1$$anonfun$2$$anonfun$apply$1
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1$$anonfun$1
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 1
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.sql.execution.joins.SortMergeJoin
07:05:28.174 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 1
07:05:28.175 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      SortMergeJoin [i#52], [i#56], Inner, None
:- Sort [i#52 ASC], false, 0
:  +- Project [i#52,j#53,k#54]
:     +- Filter isnotnull(i#52)
:        +- Scan HadoopFiles[i#52,j#53,k#54] Format: ParquetFormat, PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>
+- Sort [i#56 ASC], false, 0
   +- Exchange hashpartitioning(i#56, 8), None
      +- Project [i#56,j#57,k#58]
         +- Filter isnotnull(i#56)
            +- Scan HadoopFiles[i#56,j#57,k#58] Format: ParquetFormat, PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>

07:05:28.176 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:28.185 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 1
07:05:28.186 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      (class org.apache.spark.sql.execution.joins.SortMergeJoin,Set(right, joinType, leftKeys, condition, rightKeys, left))
07:05:28.187 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.sql.execution.joins.SortMergeJoin,SortMergeJoin [i#52], [i#56], Inner, None
:- Sort [i#52 ASC], false, 0
:  +- Project [i#52,j#53,k#54]
:     +- Filter isnotnull(i#52)
:        +- Scan HadoopFiles[i#52,j#53,k#54] Format: ParquetFormat, PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>
+- Sort [i#56 ASC], false, 0
   +- Exchange hashpartitioning(i#56, 8), None
      +- Project [i#56,j#57,k#58]
         +- Filter isnotnull(i#56)
            +- Scan HadoopFiles[i#56,j#57,k#58] Format: ParquetFormat, PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>
)
07:05:28.187 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1) is now cleaned +++
07:05:28.220 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     UTF8String primitiveA;
/* 023 */     {
/* 024 */       /* input[2, string] */
/* 025 */       boolean isNull = i.isNullAt(2);
/* 026 */       UTF8String value = isNull ? null : (i.getUTF8String(2));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     UTF8String primitiveB;
/* 033 */     {
/* 034 */       /* input[2, string] */
/* 035 */       boolean isNull = i.isNullAt(2);
/* 036 */       UTF8String value = isNull ? null : (i.getUTF8String(2));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = primitiveA.compare(primitiveB);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */
/* 054 */     i = a;
/* 055 */     boolean isNullA1;
/* 056 */     UTF8String primitiveA1;
/* 057 */     {
/* 058 */       /* input[5, string] */
/* 059 */       boolean isNull1 = i.isNullAt(5);
/* 060 */       UTF8String value1 = isNull1 ? null : (i.getUTF8String(5));
/* 061 */       isNullA1 = isNull1;
/* 062 */       primitiveA1 = value1;
/* 063 */     }
/* 064 */     i = b;
/* 065 */     boolean isNullB1;
/* 066 */     UTF8String primitiveB1;
/* 067 */     {
/* 068 */       /* input[5, string] */
/* 069 */       boolean isNull1 = i.isNullAt(5);
/* 070 */       UTF8String value1 = isNull1 ? null : (i.getUTF8String(5));
/* 071 */       isNullB1 = isNull1;
/* 072 */       primitiveB1 = value1;
/* 073 */     }
/* 074 */     if (isNullA1 && isNullB1) {
/* 075 */       // Nothing
/* 076 */     } else if (isNullA1) {
/* 077 */       return -1;
/* 078 */     } else if (isNullB1) {
/* 079 */       return 1;
/* 080 */     } else {
/* 081 */       int comp = primitiveA1.compare(primitiveB1);
/* 082 */       if (comp != 0) {
/* 083 */         return comp;
/* 084 */       }
/* 085 */     }
/* 086 */
/* 087 */     return 0;
/* 088 */   }
/* 089 */ }

07:05:28.220 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG CodeGenerator:
/* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     UTF8String primitiveA;
/* 023 */     {
/* 024 */       /* input[2, string] */
/* 025 */       boolean isNull = i.isNullAt(2);
/* 026 */       UTF8String value = isNull ? null : (i.getUTF8String(2));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     UTF8String primitiveB;
/* 033 */     {
/* 034 */       /* input[2, string] */
/* 035 */       boolean isNull = i.isNullAt(2);
/* 036 */       UTF8String value = isNull ? null : (i.getUTF8String(2));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = primitiveA.compare(primitiveB);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */
/* 054 */     i = a;
/* 055 */     boolean isNullA1;
/* 056 */     UTF8String primitiveA1;
/* 057 */     {
/* 058 */       /* input[5, string] */
/* 059 */       boolean isNull1 = i.isNullAt(5);
/* 060 */       UTF8String value1 = isNull1 ? null : (i.getUTF8String(5));
/* 061 */       isNullA1 = isNull1;
/* 062 */       primitiveA1 = value1;
/* 063 */     }
/* 064 */     i = b;
/* 065 */     boolean isNullB1;
/* 066 */     UTF8String primitiveB1;
/* 067 */     {
/* 068 */       /* input[5, string] */
/* 069 */       boolean isNull1 = i.isNullAt(5);
/* 070 */       UTF8String value1 = isNull1 ? null : (i.getUTF8String(5));
/* 071 */       isNullB1 = isNull1;
/* 072 */       primitiveB1 = value1;
/* 073 */     }
/* 074 */     if (isNullA1 && isNullB1) {
/* 075 */       // Nothing
/* 076 */     } else if (isNullA1) {
/* 077 */       return -1;
/* 078 */     } else if (isNullB1) {
/* 079 */       return 1;
/* 080 */     } else {
/* 081 */       int comp = primitiveA1.compare(primitiveB1);
/* 082 */       if (comp != 0) {
/* 083 */         return comp;
/* 084 */       }
/* 085 */     }
/* 086 */
/* 087 */     return 0;
/* 088 */   }
/* 089 */ }

07:05:28.226 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO CodeGenerator: Code generated in 6.347117 ms
07:05:28.230 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.231 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@0 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@1 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@2 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet, range: 0-815, partition values: [empty row])) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@3 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@4 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@5 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet, range: 0-820, partition values: [empty row])) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@6 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet, range: 0-727, partition values: [empty row])) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.233 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@7 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.RangePartitioner$$anonfun$5) +++
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 1
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.RangePartitioner$$anonfun$5.serialVersionUID
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$5.apply(java.lang.Object)
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$5.apply(scala.Product2)
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:28.235 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:28.236 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:28.236 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:28.236 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.RangePartitioner$$anonfun$5) is now cleaned +++
07:05:28.237 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.RangePartitioner$$anonfun$9) +++
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 4
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.RangePartitioner$$anonfun$9.serialVersionUID
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final int org.apache.spark.RangePartitioner$$anonfun$9.sampleSizePerPartition$2
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final scala.reflect.ClassTag org.apache.spark.RangePartitioner$$anonfun$9.evidence$3$1
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final int org.apache.spark.RangePartitioner$$anonfun$9.shift$1
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.RangePartitioner$$anonfun$9.apply(java.lang.Object,java.lang.Object)
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final scala.collection.Iterator org.apache.spark.RangePartitioner$$anonfun$9.apply(int,scala.collection.Iterator)
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:28.238 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.RangePartitioner$$anonfun$9) is now cleaned +++
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) +++
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.serialVersionUID
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.$outer
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(java.lang.Object)
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(scala.collection.Iterator)
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 2
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD$$anonfun$collect$1
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 2
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      <function0>
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      MapPartitionsRDD[25] at apply at Transformer.scala:22
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:28.241 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
07:05:28.242 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
07:05:28.242 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
07:05:28.242 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[25] at apply at Transformer.scala:22)
07:05:28.242 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
07:05:28.243 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 1
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 1
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 1
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      MapPartitionsRDD[25] at apply at Transformer.scala:22
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,MapPartitionsRDD[25] at apply at Transformer.scala:22)
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
07:05:28.244 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15) is now cleaned +++
07:05:28.270 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared fields: 2
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + declared methods: 2
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + inner classes: 0
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer classes: 0
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + outer objects: 0
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  + there are no enclosing objects!
07:05:28.271 pool-1-thread-1-ScalaTest-running-BucketedReadSuite DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
07:05:28.276 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO SparkContext: Starting job: apply at Transformer.scala:22
07:05:28.279 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 19 (apply at Transformer.scala:22)
07:05:28.279 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (apply at Transformer.scala:22) with 8 output partitions
07:05:28.279 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (apply at Transformer.scala:22)
07:05:28.279 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
07:05:28.280 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
07:05:28.280 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.280 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.280 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.280 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List()
07:05:28.280 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[19] at apply at Transformer.scala:22), which has no missing parents
07:05:28.280 dag-scheduler-event-loop DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 2)
07:05:28.281 dag-scheduler-event-loop DEBUG DAGScheduler: partitionsToCompute Vector(0, 1, 2, 3, 4, 5, 6, 7)
07:05:28.281 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[19] at apply at Transformer.scala:22:
07:05:28.281 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[18] at apply at Transformer.scala:22:
07:05:28.281 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[17] at apply at Transformer.scala:22:
07:05:28.281 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[16] at apply at Transformer.scala:22:
07:05:28.281 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[15] at apply at Transformer.scala:22:
07:05:28.282 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(0,List()) of FileScanRDD[9] at apply at Transformer.scala:22:
07:05:28.282 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet, range: 0-731, partition values: [empty row])) of MapPartitionsRDD[19] at apply at Transformer.scala:22:
07:05:28.282 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet, range: 0-731, partition values: [empty row])) of MapPartitionsRDD[18] at apply at Transformer.scala:22:
07:05:28.282 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet, range: 0-731, partition values: [empty row])) of MapPartitionsRDD[17] at apply at Transformer.scala:22:
07:05:28.282 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet, range: 0-731, partition values: [empty row])) of MapPartitionsRDD[16] at apply at Transformer.scala:22:
07:05:28.282 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet, range: 0-731, partition values: [empty row])) of MapPartitionsRDD[15] at apply at Transformer.scala:22:
07:05:28.283 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(1,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet, range: 0-731, partition values: [empty row])) of FileScanRDD[9] at apply at Transformer.scala:22: localhost
07:05:28.284 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet, range: 0-728, partition values: [empty row])) of MapPartitionsRDD[19] at apply at Transformer.scala:22:
07:05:28.284 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet, range: 0-728, partition values: [empty row])) of MapPartitionsRDD[18] at apply at Transformer.scala:22:
07:05:28.284 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet, range: 0-728, partition values: [empty row])) of MapPartitionsRDD[17] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet, range: 0-728, partition values: [empty row])) of MapPartitionsRDD[16] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet, range: 0-728, partition values: [empty row])) of MapPartitionsRDD[15] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(2,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet, range: 0-728, partition values: [empty row])) of FileScanRDD[9] at apply at Transformer.scala:22: localhost
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet, range: 0-773, partition values: [empty row])) of MapPartitionsRDD[19] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet, range: 0-773, partition values: [empty row])) of MapPartitionsRDD[18] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet, range: 0-773, partition values: [empty row])) of MapPartitionsRDD[17] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet, range: 0-773, partition values: [empty row])) of MapPartitionsRDD[16] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet, range: 0-773, partition values: [empty row])) of MapPartitionsRDD[15] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet, range: 0-773, partition values: [empty row])) of FileScanRDD[9] at apply at Transformer.scala:22: localhost
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[19] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[18] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[17] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[16] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[15] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(4,List()) of FileScanRDD[9] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[19] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[18] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[17] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[16] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[15] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(5,List()) of FileScanRDD[9] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet, range: 0-753, partition values: [empty row])) of MapPartitionsRDD[19] at apply at Transformer.scala:22:
07:05:28.285 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet, range: 0-753, partition values: [empty row])) of MapPartitionsRDD[18] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet, range: 0-753, partition values: [empty row])) of MapPartitionsRDD[17] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet, range: 0-753, partition values: [empty row])) of MapPartitionsRDD[16] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet, range: 0-753, partition values: [empty row])) of MapPartitionsRDD[15] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet, range: 0-753, partition values: [empty row])) of FileScanRDD[9] at apply at Transformer.scala:22: localhost
07:05:28.286 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet, range: 0-740, partition values: [empty row])) of MapPartitionsRDD[19] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet, range: 0-740, partition values: [empty row])) of MapPartitionsRDD[18] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet, range: 0-740, partition values: [empty row])) of MapPartitionsRDD[17] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet, range: 0-740, partition values: [empty row])) of MapPartitionsRDD[16] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet, range: 0-740, partition values: [empty row])) of MapPartitionsRDD[15] at apply at Transformer.scala:22:
07:05:28.286 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet, range: 0-740, partition values: [empty row])) of FileScanRDD[9] at apply at Transformer.scala:22: localhost
07:05:28.286 dag-scheduler-event-loop DEBUG DAGScheduler: taskIdToLocations Map(0 -> List(), 5 -> List(), 1 -> List(localhost), 6 -> List(localhost), 2 -> List(localhost), 7 -> List(localhost), 3 -> List(localhost), 4 -> List())
07:05:28.288 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.9 KB, free 2.2 GB)
07:05:28.289 dag-scheduler-event-loop DEBUG BlockManager: Put block broadcast_4 locally took  1 ms
07:05:28.289 dag-scheduler-event-loop DEBUG BlockManager: Putting block broadcast_4 without replication took  1 ms
07:05:28.297 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.2 KB, free 2.2 GB)
07:05:28.297 dispatcher-event-loop-3 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.102:52176 (size: 5.2 KB, free: 2.2 GB)
07:05:28.297 dag-scheduler-event-loop DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0
07:05:28.297 dag-scheduler-event-loop DEBUG BlockManager: Told master about block broadcast_4_piece0
07:05:28.297 dag-scheduler-event-loop DEBUG BlockManager: Put block broadcast_4_piece0 locally took  0 ms
07:05:28.298 dag-scheduler-event-loop DEBUG BlockManager: Putting block broadcast_4_piece0 without replication took  1 ms
07:05:28.298 dag-scheduler-event-loop INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1012
07:05:28.299 dag-scheduler-event-loop INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at apply at Transformer.scala:22)
07:05:28.299 dag-scheduler-event-loop DEBUG DAGScheduler: New pending partitions: Set(0, 1, 5, 2, 6, 3, 7, 4)
07:05:28.299 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 8 tasks
07:05:28.300 dag-scheduler-event-loop DEBUG TaskSetManager: Epoch for TaskSet 2.0: 0
07:05:28.301 Spark Context Cleaner DEBUG ContextCleaner: Got cleaning task CleanAccum(117)
07:05:28.301 Spark Context Cleaner DEBUG ContextCleaner: Cleaning accumulator 117
07:05:28.301 Spark Context Cleaner INFO ContextCleaner: Cleaned accumulator 117
07:05:28.301 Spark Context Cleaner DEBUG ContextCleaner: Got cleaning task CleanBroadcast(1)
07:05:28.301 Spark Context Cleaner DEBUG ContextCleaner: Cleaning broadcast 1
07:05:28.301 Spark Context Cleaner DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 1
07:05:28.302 block-manager-slave-async-thread-pool-2 DEBUG BlockManagerSlaveEndpoint: removing broadcast 1
07:05:28.302 block-manager-slave-async-thread-pool-2 DEBUG BlockManager: Removing broadcast 1
07:05:28.302 block-manager-slave-async-thread-pool-2 DEBUG BlockManager: Removing block broadcast_1
07:05:28.302 block-manager-slave-async-thread-pool-2 DEBUG MemoryStore: Block broadcast_1 of size 53344 dropped from memory (free 2348730328)
07:05:28.302 block-manager-slave-async-thread-pool-2 DEBUG BlockManager: Removing block broadcast_1_piece0
07:05:28.302 block-manager-slave-async-thread-pool-2 DEBUG MemoryStore: Block broadcast_1_piece0 of size 19781 dropped from memory (free 2348750109)
07:05:28.302 dag-scheduler-event-loop DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: NODE_LOCAL, NO_PREF, ANY
07:05:28.303 dispatcher-event-loop-6 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.102:52176 in memory (size: 19.3 KB, free: 2.2 GB)
07:05:28.303 block-manager-slave-async-thread-pool-2 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
07:05:28.303 block-manager-slave-async-thread-pool-2 DEBUG BlockManager: Told master about block broadcast_1_piece0
07:05:28.303 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.303 dispatcher-event-loop-7 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.303 block-manager-slave-async-thread-pool-1 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 1, response is 0
07:05:28.303 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.303 block-manager-slave-async-thread-pool-1 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.1.102:52175
07:05:28.303 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.303 Spark Context Cleaner DEBUG ContextCleaner: Cleaned broadcast 1
07:05:28.304 dispatcher-event-loop-7 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.306 dispatcher-event-loop-7 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 2, localhost, partition 1,NODE_LOCAL, 5835 bytes)
07:05:28.306 Executor task launch worker-0 INFO Executor: Running task 1.0 in stage 2.0 (TID 2)
07:05:28.306 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.306 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.306 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.307 Executor task launch worker-0 DEBUG Executor: Task 2's epoch is 0
07:05:28.307 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_4
07:05:28.307 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.312 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.313 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.313 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.319 Executor task launch worker-0 INFO CodeGenerator: Code generated in 6.070098 ms
07:05:28.322 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.322 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.324 Executor task launch worker-0 INFO CodeGenerator: Code generated in 2.432994 ms
07:05:28.328 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.328 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.332 Executor task launch worker-0 INFO CodeGenerator: Code generated in 4.629539 ms
07:05:28.334 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.334 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.342 Executor task launch worker-0 INFO CodeGenerator: Code generated in 8.645027 ms
07:05:28.343 Executor task launch worker-0 INFO FileScanRDD: Reading File path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00001.snappy.parquet, range: 0-731, partition values: [empty row]
07:05:28.345 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_3
07:05:28.345 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.355 Executor task launch worker-0 DEBUG DefaultSource: Appending StructType() [empty row]
07:05:28.364 Executor task launch worker-0 INFO CodecPool: Got brand-new decompressor [.snappy]
07:05:28.394 Executor task launch worker-0 INFO Executor: Finished task 1.0 in stage 2.0 (TID 2). 2910 bytes result sent to driver
07:05:28.395 dispatcher-event-loop-2 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.395 dispatcher-event-loop-2 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.396 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 3, localhost, partition 2,NODE_LOCAL, 5835 bytes)
07:05:28.396 Executor task launch worker-0 INFO Executor: Running task 2.0 in stage 2.0 (TID 3)
07:05:28.396 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.396 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.396 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.397 Executor task launch worker-0 DEBUG Executor: Task 3's epoch is 0
07:05:28.397 task-result-getter-2 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 2) in 94 ms on localhost (1/8)
07:05:28.397 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_4
07:05:28.397 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.398 dag-scheduler-event-loop DEBUG DAGScheduler: ShuffleMapTask finished on driver
07:05:28.398 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.398 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.398 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.399 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.400 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.400 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.400 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.401 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.401 Executor task launch worker-0 INFO FileScanRDD: Reading File path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00002.snappy.parquet, range: 0-728, partition values: [empty row]
07:05:28.401 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_3
07:05:28.401 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.403 Executor task launch worker-0 DEBUG DefaultSource: Appending StructType() [empty row]
07:05:28.403 Executor task launch worker-0 DEBUG CodecPool: Got recycled decompressor
07:05:28.408 Executor task launch worker-0 INFO Executor: Finished task 2.0 in stage 2.0 (TID 3). 2901 bytes result sent to driver
07:05:28.408 dispatcher-event-loop-3 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.409 dispatcher-event-loop-3 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.409 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 4, localhost, partition 3,NODE_LOCAL, 5835 bytes)
07:05:28.409 Executor task launch worker-0 INFO Executor: Running task 3.0 in stage 2.0 (TID 4)
07:05:28.409 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.409 task-result-getter-3 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 3) in 14 ms on localhost (2/8)
07:05:28.409 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.409 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.410 dag-scheduler-event-loop DEBUG DAGScheduler: ShuffleMapTask finished on driver
07:05:28.410 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.410 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.410 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.410 Executor task launch worker-0 DEBUG Executor: Task 4's epoch is 0
07:05:28.410 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_4
07:05:28.410 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.411 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.412 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.412 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.413 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.413 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.413 Executor task launch worker-0 INFO FileScanRDD: Reading File path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00003.snappy.parquet, range: 0-773, partition values: [empty row]
07:05:28.413 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_3
07:05:28.413 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.415 Executor task launch worker-0 DEBUG DefaultSource: Appending StructType() [empty row]
07:05:28.415 Executor task launch worker-0 DEBUG CodecPool: Got recycled decompressor
07:05:28.420 Executor task launch worker-0 INFO Executor: Finished task 3.0 in stage 2.0 (TID 4). 2901 bytes result sent to driver
07:05:28.420 dispatcher-event-loop-5 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.420 dispatcher-event-loop-5 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.421 dispatcher-event-loop-5 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 5, localhost, partition 6,NODE_LOCAL, 5835 bytes)
07:05:28.421 Executor task launch worker-0 INFO Executor: Running task 6.0 in stage 2.0 (TID 5)
07:05:28.421 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.421 task-result-getter-0 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 4) in 12 ms on localhost (3/8)
07:05:28.421 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.421 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.421 dag-scheduler-event-loop DEBUG DAGScheduler: ShuffleMapTask finished on driver
07:05:28.421 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.421 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.421 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.422 Executor task launch worker-0 DEBUG Executor: Task 5's epoch is 0
07:05:28.422 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_4
07:05:28.422 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.423 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.425 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.425 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.425 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.426 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.426 Executor task launch worker-0 INFO FileScanRDD: Reading File path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00006.snappy.parquet, range: 0-753, partition values: [empty row]
07:05:28.426 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_3
07:05:28.426 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.428 Executor task launch worker-0 DEBUG DefaultSource: Appending StructType() [empty row]
07:05:28.428 Executor task launch worker-0 DEBUG CodecPool: Got recycled decompressor
07:05:28.437 Executor task launch worker-0 INFO Executor: Finished task 6.0 in stage 2.0 (TID 5). 2901 bytes result sent to driver
07:05:28.437 dispatcher-event-loop-7 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.437 dispatcher-event-loop-7 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.438 dispatcher-event-loop-7 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 6, localhost, partition 7,NODE_LOCAL, 5835 bytes)
07:05:28.438 Executor task launch worker-0 INFO Executor: Running task 7.0 in stage 2.0 (TID 6)
07:05:28.438 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.438 task-result-getter-1 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 5) in 18 ms on localhost (4/8)
07:05:28.438 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.438 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.438 dag-scheduler-event-loop DEBUG DAGScheduler: ShuffleMapTask finished on driver
07:05:28.438 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.438 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.438 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.438 Executor task launch worker-0 DEBUG Executor: Task 6's epoch is 0
07:05:28.438 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_4
07:05:28.438 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.439 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.440 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.440 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.441 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.441 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.441 Executor task launch worker-0 INFO FileScanRDD: Reading File path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t2/part-r-00000-57d515a4-4026-4db7-86cc-c48bbc84671c_00007.snappy.parquet, range: 0-740, partition values: [empty row]
07:05:28.441 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_3
07:05:28.441 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.443 Executor task launch worker-0 DEBUG DefaultSource: Appending StructType() [empty row]
07:05:28.443 Executor task launch worker-0 DEBUG CodecPool: Got recycled decompressor
07:05:28.447 Executor task launch worker-0 INFO Executor: Finished task 7.0 in stage 2.0 (TID 6). 2901 bytes result sent to driver
07:05:28.447 dispatcher-event-loop-2 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.447 dispatcher-event-loop-2 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level NO_PREF
07:05:28.448 dispatcher-event-loop-2 DEBUG TaskSetManager: Moving to ANY after waiting for 0ms
07:05:28.448 dispatcher-event-loop-2 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.448 dispatcher-event-loop-2 INFO TaskSetManager: No tasks dequeued
07:05:28.448 dispatcher-event-loop-2 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:28.448 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 7, localhost, partition 0,PROCESS_LOCAL, 5169 bytes)
07:05:28.449 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.449 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 2.0 (TID 7)
07:05:28.449 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.449 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.449 task-result-getter-2 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 6) in 12 ms on localhost (5/8)
07:05:28.449 dag-scheduler-event-loop DEBUG DAGScheduler: ShuffleMapTask finished on driver
07:05:28.449 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.449 Executor task launch worker-0 DEBUG Executor: Task 7's epoch is 0
07:05:28.449 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.449 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.449 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_4
07:05:28.449 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.450 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.451 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.451 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.452 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.452 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.453 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 2.0 (TID 7). 2865 bytes result sent to driver
07:05:28.453 dispatcher-event-loop-3 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.454 dispatcher-event-loop-3 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.454 dispatcher-event-loop-3 INFO TaskSetManager: No tasks dequeued
07:05:28.454 dispatcher-event-loop-3 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:28.455 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 8, localhost, partition 4,PROCESS_LOCAL, 5169 bytes)
07:05:28.455 Executor task launch worker-0 INFO Executor: Running task 4.0 in stage 2.0 (TID 8)
07:05:28.455 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.455 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 7) in 7 ms on localhost (6/8)
07:05:28.455 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.455 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.455 dag-scheduler-event-loop DEBUG DAGScheduler: ShuffleMapTask finished on driver
07:05:28.455 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.455 Executor task launch worker-0 DEBUG Executor: Task 8's epoch is 0
07:05:28.455 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.455 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.455 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_4
07:05:28.455 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.456 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.457 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.458 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.458 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.459 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.460 Executor task launch worker-0 INFO Executor: Finished task 4.0 in stage 2.0 (TID 8). 2865 bytes result sent to driver
07:05:28.460 dispatcher-event-loop-5 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.460 dispatcher-event-loop-5 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.460 dispatcher-event-loop-5 INFO TaskSetManager: No tasks dequeued
07:05:28.460 dispatcher-event-loop-5 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:28.461 dispatcher-event-loop-5 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 9, localhost, partition 5,PROCESS_LOCAL, 5169 bytes)
07:05:28.461 Executor task launch worker-0 INFO Executor: Running task 5.0 in stage 2.0 (TID 9)
07:05:28.461 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.461 task-result-getter-0 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 8) in 7 ms on localhost (7/8)
07:05:28.461 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.461 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.462 Executor task launch worker-0 DEBUG Executor: Task 9's epoch is 0
07:05:28.462 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_4
07:05:28.462 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.463 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.463 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.463 dag-scheduler-event-loop DEBUG DAGScheduler: ShuffleMapTask finished on driver
07:05:28.463 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.463 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List(ShuffleMapStage 2)
07:05:28.463 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ShuffleMapStage 2)
07:05:28.464 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.464 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.465 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int], 42), 8):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     /* pmod(hash(input[0, int], 42), 8) */
/* 028 */     /* hash(input[0, int], 42) */
/* 029 */     int value1 = 42;
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull2 = i.isNullAt(0);
/* 032 */     int value2 = isNull2 ? -1 : (i.getInt(0));
/* 033 */     if (!isNull2) {
/* 034 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 035 */     }
/* 036 */
/* 037 */     int value = -1;
/* 038 */
/* 039 */     int r = value1 % 8;
/* 040 */     if (r < 0) {
/* 041 */       value = (r + 8) % 8;
/* 042 */     } else {
/* 043 */       value = r;
/* 044 */     }
/* 045 */     rowWriter.write(0, value);
/* 046 */     return result;
/* 047 */   }
/* 048 */ }
/* 049 */

07:05:28.466 Executor task launch worker-0 INFO Executor: Finished task 5.0 in stage 2.0 (TID 9). 2865 bytes result sent to driver
07:05:28.467 dispatcher-event-loop-7 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2, runningTasks: 0
07:05:28.467 dispatcher-event-loop-7 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.467 dispatcher-event-loop-7 INFO TaskSetManager: No tasks dequeued
07:05:28.467 dispatcher-event-loop-7 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:28.468 dispatcher-event-loop-7 INFO TaskSetManager: No tasks dequeued
07:05:28.468 dispatcher-event-loop-7 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = ANY
 - allowedLocality = ANY

07:05:28.468 dispatcher-event-loop-7 INFO TaskSetManager: No tasks dequeued
07:05:28.468 task-result-getter-1 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 9) in 8 ms on localhost (8/8)
07:05:28.468 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
07:05:28.468 dag-scheduler-event-loop DEBUG DAGScheduler: ShuffleMapTask finished on driver
07:05:28.468 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 2 (apply at Transformer.scala:22) finished in 0.165 s
07:05:28.469 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
07:05:28.469 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
07:05:28.470 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 3)
07:05:28.470 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
07:05:28.471 dag-scheduler-event-loop DEBUG MapOutputTrackerMaster: Increasing epoch to 1
07:05:28.471 dag-scheduler-event-loop DEBUG DAGScheduler: submitStage(ResultStage 3)
07:05:28.471 dag-scheduler-event-loop DEBUG DAGScheduler: missing: List()
07:05:28.472 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[25] at apply at Transformer.scala:22), which has no missing parents
07:05:28.472 dag-scheduler-event-loop DEBUG DAGScheduler: submitMissingTasks(ResultStage 3)
07:05:28.472 dag-scheduler-event-loop DEBUG DAGScheduler: partitionsToCompute Vector(0, 1, 2, 3, 4, 5, 6, 7)
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@0 of MapPartitionsRDD[25] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@0 of MapPartitionsRDD[24] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@0 of MapPartitionsRDD[23] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG ZippedPartitionsRDD2: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@0 of ZippedPartitionsRDD2[22] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[13] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[12] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[11] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(0,List()) of MapPartitionsRDD[10] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(0,List()) of FileScanRDD[8] at apply at Transformer.scala:22:
07:05:28.472 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@0 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.477 dag-scheduler-event-loop DEBUG ShuffledRowRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@0 of ShuffledRowRDD[20] at apply at Transformer.scala:22:
07:05:28.477 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@1 of MapPartitionsRDD[25] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@1 of MapPartitionsRDD[24] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@1 of MapPartitionsRDD[23] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG ZippedPartitionsRDD2: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@1 of ZippedPartitionsRDD2[22] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List()) of MapPartitionsRDD[13] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List()) of MapPartitionsRDD[12] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List()) of MapPartitionsRDD[11] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(1,List()) of MapPartitionsRDD[10] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(1,List()) of FileScanRDD[8] at apply at Transformer.scala:22:
07:05:28.478 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@1 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG ShuffledRowRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@1 of ShuffledRowRDD[20] at apply at Transformer.scala:22: 192.168.1.102
07:05:28.479 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@2 of MapPartitionsRDD[25] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@2 of MapPartitionsRDD[24] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@2 of MapPartitionsRDD[23] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG ZippedPartitionsRDD2: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@2 of ZippedPartitionsRDD2[22] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List()) of MapPartitionsRDD[13] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List()) of MapPartitionsRDD[12] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List()) of MapPartitionsRDD[11] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(2,List()) of MapPartitionsRDD[10] at apply at Transformer.scala:22:
07:05:28.479 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(2,List()) of FileScanRDD[8] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@2 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG ShuffledRowRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@2 of ShuffledRowRDD[20] at apply at Transformer.scala:22: 192.168.1.102
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@3 of MapPartitionsRDD[25] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@3 of MapPartitionsRDD[24] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@3 of MapPartitionsRDD[23] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG ZippedPartitionsRDD2: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@3 of ZippedPartitionsRDD2[22] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet, range: 0-815, partition values: [empty row])) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet, range: 0-815, partition values: [empty row])) of MapPartitionsRDD[13] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet, range: 0-815, partition values: [empty row])) of MapPartitionsRDD[12] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet, range: 0-815, partition values: [empty row])) of MapPartitionsRDD[11] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet, range: 0-815, partition values: [empty row])) of MapPartitionsRDD[10] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(3,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet, range: 0-815, partition values: [empty row])) of FileScanRDD[8] at apply at Transformer.scala:22: localhost
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@4 of MapPartitionsRDD[25] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@4 of MapPartitionsRDD[24] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@4 of MapPartitionsRDD[23] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG ZippedPartitionsRDD2: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@4 of ZippedPartitionsRDD2[22] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[13] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[12] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[11] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(4,List()) of MapPartitionsRDD[10] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(4,List()) of FileScanRDD[8] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@4 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG ShuffledRowRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@4 of ShuffledRowRDD[20] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@5 of MapPartitionsRDD[25] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@5 of MapPartitionsRDD[24] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@5 of MapPartitionsRDD[23] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG ZippedPartitionsRDD2: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@5 of ZippedPartitionsRDD2[22] at apply at Transformer.scala:22:
07:05:28.480 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[13] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[12] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[11] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(5,List()) of MapPartitionsRDD[10] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(5,List()) of FileScanRDD[8] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@5 of MapPartitionsRDD[21] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG ShuffledRowRDD: Preferred locations for org.apache.spark.sql.execution.ShuffledRowRDDPartition@5 of ShuffledRowRDD[20] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@6 of MapPartitionsRDD[25] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@6 of MapPartitionsRDD[24] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@6 of MapPartitionsRDD[23] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG ZippedPartitionsRDD2: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@6 of ZippedPartitionsRDD2[22] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet, range: 0-820, partition values: [empty row])) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet, range: 0-820, partition values: [empty row])) of MapPartitionsRDD[13] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet, range: 0-820, partition values: [empty row])) of MapPartitionsRDD[12] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet, range: 0-820, partition values: [empty row])) of MapPartitionsRDD[11] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet, range: 0-820, partition values: [empty row])) of MapPartitionsRDD[10] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(6,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet, range: 0-820, partition values: [empty row])) of FileScanRDD[8] at apply at Transformer.scala:22: localhost
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@7 of MapPartitionsRDD[25] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@7 of MapPartitionsRDD[24] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@7 of MapPartitionsRDD[23] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG ZippedPartitionsRDD2: Preferred locations for org.apache.spark.rdd.ZippedPartitionsPartition@7 of ZippedPartitionsRDD2[22] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet, range: 0-727, partition values: [empty row])) of MapPartitionsRDD[14] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet, range: 0-727, partition values: [empty row])) of MapPartitionsRDD[13] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet, range: 0-727, partition values: [empty row])) of MapPartitionsRDD[12] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet, range: 0-727, partition values: [empty row])) of MapPartitionsRDD[11] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG MapPartitionsRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet, range: 0-727, partition values: [empty row])) of MapPartitionsRDD[10] at apply at Transformer.scala:22:
07:05:28.481 dag-scheduler-event-loop DEBUG FileScanRDD: Preferred locations for FilePartition(7,List(path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet, range: 0-727, partition values: [empty row])) of FileScanRDD[8] at apply at Transformer.scala:22: localhost
07:05:28.481 dag-scheduler-event-loop DEBUG DAGScheduler: taskIdToLocations Map(0 -> List(), 5 -> List(), 1 -> List(192.168.1.102), 6 -> List(localhost), 2 -> List(192.168.1.102), 7 -> List(localhost), 3 -> List(localhost), 4 -> List())
07:05:28.489 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.6 KB, free 2.2 GB)
07:05:28.489 dag-scheduler-event-loop DEBUG BlockManager: Put block broadcast_5 locally took  2 ms
07:05:28.489 dag-scheduler-event-loop DEBUG BlockManager: Putting block broadcast_5 without replication took  2 ms
07:05:28.490 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.2 KB, free 2.2 GB)
07:05:28.490 dispatcher-event-loop-0 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.102:52176 (size: 7.2 KB, free: 2.2 GB)
07:05:28.491 dag-scheduler-event-loop DEBUG BlockManagerMaster: Updated info of block broadcast_5_piece0
07:05:28.491 dag-scheduler-event-loop DEBUG BlockManager: Told master about block broadcast_5_piece0
07:05:28.491 dag-scheduler-event-loop DEBUG BlockManager: Put block broadcast_5_piece0 locally took  1 ms
07:05:28.491 dag-scheduler-event-loop DEBUG BlockManager: Putting block broadcast_5_piece0 without replication took  1 ms
07:05:28.491 dag-scheduler-event-loop INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1012
07:05:28.491 dag-scheduler-event-loop INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 3 (MapPartitionsRDD[25] at apply at Transformer.scala:22)
07:05:28.491 dag-scheduler-event-loop DEBUG DAGScheduler: New pending partitions: Set(0, 1, 5, 2, 6, 3, 7, 4)
07:05:28.491 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks
07:05:28.491 dag-scheduler-event-loop DEBUG TaskSetManager: Epoch for TaskSet 3.0: 1
07:05:28.491 dag-scheduler-event-loop DEBUG TaskSetManager: Valid locality levels for TaskSet 3.0: NODE_LOCAL, NO_PREF, ANY
07:05:28.491 dispatcher-event-loop-2 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3, runningTasks: 0
07:05:28.491 dispatcher-event-loop-2 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.494 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 10, localhost, partition 3,NODE_LOCAL, 6200 bytes)
07:05:28.494 Executor task launch worker-0 INFO Executor: Running task 3.0 in stage 3.0 (TID 10)
07:05:28.494 Executor task launch worker-0 DEBUG Executor: Task 10's epoch is 1
07:05:28.494 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_5
07:05:28.494 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_5 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.496 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.497 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.498 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.498 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.499 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.499 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.500 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 10 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@785ca4b1
07:05:28.500 Executor task launch worker-0 INFO FileScanRDD: Reading File path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00003.snappy.parquet, range: 0-815, partition values: [empty row]
07:05:28.500 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_2
07:05:28.500 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.501 Executor task launch worker-0 DEBUG DefaultSource: Appending StructType() [empty row]
07:05:28.502 Executor task launch worker-0 DEBUG CodecPool: Got recycled decompressor
07:05:28.502 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 10 acquire 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@785ca4b1
07:05:28.516 Executor task launch worker-0 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 3-4
07:05:28.519 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
07:05:28.522 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks out of 8 blocks
07:05:28.523 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
07:05:28.528 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  9 ms
07:05:28.530 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.531 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.531 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 10 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2a9edd8e
07:05:28.533 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 10 acquire 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2a9edd8e
07:05:28.541 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.542 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.542 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.550 Executor task launch worker-0 INFO CodeGenerator: Code generated in 7.32435 ms
07:05:28.551 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.551 Executor task launch worker-0 DEBUG CodeGenerator:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.555 Executor task launch worker-0 INFO CodeGenerator: Code generated in 4.231776 ms
07:05:28.556 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.557 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.561 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.565 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 10 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2a9edd8e
07:05:28.565 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 10 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2a9edd8e
07:05:28.565 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 10 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@785ca4b1
07:05:28.565 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 10 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@785ca4b1
07:05:28.567 Executor task launch worker-0 INFO Executor: Finished task 3.0 in stage 3.0 (TID 10). 7258 bytes result sent to driver
07:05:28.567 dispatcher-event-loop-3 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3, runningTasks: 0
07:05:28.567 dispatcher-event-loop-3 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.568 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 11, localhost, partition 6,NODE_LOCAL, 6200 bytes)
07:05:28.568 Executor task launch worker-0 INFO Executor: Running task 6.0 in stage 3.0 (TID 11)
07:05:28.568 task-result-getter-2 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 10) in 77 ms on localhost (1/8)
07:05:28.568 Executor task launch worker-0 DEBUG Executor: Task 11's epoch is 1
07:05:28.569 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_5
07:05:28.569 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_5 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.570 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.571 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.571 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.571 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.572 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.572 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.572 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 11 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2238b0ce
07:05:28.572 Executor task launch worker-0 INFO FileScanRDD: Reading File path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00006.snappy.parquet, range: 0-820, partition values: [empty row]
07:05:28.572 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_2
07:05:28.572 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.574 Executor task launch worker-0 DEBUG DefaultSource: Appending StructType() [empty row]
07:05:28.574 Executor task launch worker-0 DEBUG CodecPool: Got recycled decompressor
07:05:28.575 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 11 acquire 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2238b0ce
07:05:28.575 Executor task launch worker-0 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 6-7
07:05:28.575 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
07:05:28.575 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks out of 8 blocks
07:05:28.575 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
07:05:28.576 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  1 ms
07:05:28.576 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.577 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.577 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 11 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@39535119
07:05:28.577 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 11 acquire 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@39535119
07:05:28.578 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.579 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.579 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.580 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.580 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.581 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.581 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 11 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@39535119
07:05:28.581 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 11 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@39535119
07:05:28.581 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 11 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2238b0ce
07:05:28.581 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 11 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2238b0ce
07:05:28.582 Executor task launch worker-0 INFO Executor: Finished task 6.0 in stage 3.0 (TID 11). 7258 bytes result sent to driver
07:05:28.583 dispatcher-event-loop-5 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3, runningTasks: 0
07:05:28.583 dispatcher-event-loop-5 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.584 dispatcher-event-loop-5 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 12, localhost, partition 7,NODE_LOCAL, 6200 bytes)
07:05:28.584 Executor task launch worker-0 INFO Executor: Running task 7.0 in stage 3.0 (TID 12)
07:05:28.584 task-result-getter-3 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 11) in 17 ms on localhost (2/8)
07:05:28.584 Executor task launch worker-0 DEBUG Executor: Task 12's epoch is 1
07:05:28.584 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_5
07:05:28.584 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_5 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.585 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.586 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.586 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.587 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.587 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.588 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.588 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 12 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@61f6241a
07:05:28.588 Executor task launch worker-0 INFO FileScanRDD: Reading File path: file:///Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5/t1/part-r-00000-2af93934-6b89-49d5-a223-475e9b17fd4f_00007.snappy.parquet, range: 0-727, partition values: [empty row]
07:05:28.588 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_2
07:05:28.588 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.590 Executor task launch worker-0 DEBUG DefaultSource: Appending StructType() [empty row]
07:05:28.590 Executor task launch worker-0 DEBUG CodecPool: Got recycled decompressor
07:05:28.591 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 12 acquire 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@61f6241a
07:05:28.591 Executor task launch worker-0 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 7-8
07:05:28.591 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
07:05:28.591 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks out of 8 blocks
07:05:28.591 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
07:05:28.591 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  0 ms
07:05:28.592 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.592 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.592 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 12 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@19256476
07:05:28.593 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 12 acquire 64.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@19256476
07:05:28.593 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.594 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.594 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.595 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.595 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.596 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.596 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 12 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@19256476
07:05:28.596 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 12 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@19256476
07:05:28.596 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 12 release 64.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@61f6241a
07:05:28.597 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 12 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@61f6241a
07:05:28.598 Executor task launch worker-0 INFO Executor: Finished task 7.0 in stage 3.0 (TID 12). 7258 bytes result sent to driver
07:05:28.598 dispatcher-event-loop-7 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3, runningTasks: 0
07:05:28.599 dispatcher-event-loop-7 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.599 dispatcher-event-loop-7 INFO TaskSetManager: No tasks dequeued
07:05:28.599 dispatcher-event-loop-7 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:28.599 dispatcher-event-loop-7 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 13, localhost, partition 0,PROCESS_LOCAL, 5534 bytes)
07:05:28.600 Executor task launch worker-0 INFO Executor: Running task 0.0 in stage 3.0 (TID 13)
07:05:28.600 task-result-getter-0 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 12) in 17 ms on localhost (3/8)
07:05:28.600 Executor task launch worker-0 DEBUG Executor: Task 13's epoch is 1
07:05:28.600 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_5
07:05:28.600 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_5 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.602 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.603 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.603 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.604 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.604 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.605 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.605 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 13 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6d6240c4
07:05:28.605 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 13 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6d6240c4
07:05:28.605 Executor task launch worker-0 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 0-1
07:05:28.605 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
07:05:28.605 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 8 blocks
07:05:28.605 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
07:05:28.605 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  0 ms
07:05:28.606 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.606 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.606 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 13 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@549d4fc
07:05:28.606 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 13 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@549d4fc
07:05:28.607 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.608 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.608 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.609 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.609 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.610 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.611 Executor task launch worker-0 INFO Executor: Finished task 0.0 in stage 3.0 (TID 13). 3635 bytes result sent to driver
07:05:28.611 dispatcher-event-loop-2 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3, runningTasks: 0
07:05:28.611 dispatcher-event-loop-2 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.611 dispatcher-event-loop-2 INFO TaskSetManager: No tasks dequeued
07:05:28.612 dispatcher-event-loop-2 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:28.612 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 14, localhost, partition 4,PROCESS_LOCAL, 5534 bytes)
07:05:28.612 Executor task launch worker-0 INFO Executor: Running task 4.0 in stage 3.0 (TID 14)
07:05:28.612 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 13) in 13 ms on localhost (4/8)
07:05:28.613 Executor task launch worker-0 DEBUG Executor: Task 14's epoch is 1
07:05:28.613 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_5
07:05:28.613 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_5 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.614 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.614 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.615 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.615 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.616 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.616 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.616 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 14 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7c01ef30
07:05:28.616 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 14 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7c01ef30
07:05:28.616 Executor task launch worker-0 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 4-5
07:05:28.617 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
07:05:28.617 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 8 blocks
07:05:28.617 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
07:05:28.617 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  1 ms
07:05:28.617 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.617 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.618 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 14 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3b59b9c7
07:05:28.618 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 14 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3b59b9c7
07:05:28.618 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.619 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.619 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.619 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.620 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.620 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.621 Executor task launch worker-0 INFO Executor: Finished task 4.0 in stage 3.0 (TID 14). 3635 bytes result sent to driver
07:05:28.621 dispatcher-event-loop-3 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3, runningTasks: 0
07:05:28.622 dispatcher-event-loop-3 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.622 dispatcher-event-loop-3 INFO TaskSetManager: No tasks dequeued
07:05:28.622 dispatcher-event-loop-3 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:28.622 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 15, localhost, partition 5,PROCESS_LOCAL, 5534 bytes)
07:05:28.622 Executor task launch worker-0 INFO Executor: Running task 5.0 in stage 3.0 (TID 15)
07:05:28.622 task-result-getter-2 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 14) in 11 ms on localhost (5/8)
07:05:28.623 Executor task launch worker-0 DEBUG Executor: Task 15's epoch is 1
07:05:28.623 Executor task launch worker-0 DEBUG BlockManager: Getting local block broadcast_5
07:05:28.623 Executor task launch worker-0 DEBUG BlockManager: Level for block broadcast_5 is StorageLevel(disk=true, memory=true, offheap=false, deserialized=true, replication=1)
07:05:28.625 Executor task launch worker-0 DEBUG SparkHadoopUtil: Couldn't find method for retrieving thread-level FileSystem input data
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()
	at java.lang.Class.getDeclaredMethod(Class.java:2122)
	at org.apache.spark.util.Utils$.invoke(Utils.scala:2088)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:184)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:183)
	at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:149)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.<init>(FileScanRDD.scala:67)
	at org.apache.spark.sql.execution.datasources.FileScanRDD.compute(FileScanRDD.scala:60)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:322)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
07:05:28.626 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */     result.setTotalSize(holder.totalSize());
/* 058 */     return result;
/* 059 */   }
/* 060 */ }
/* 061 */

07:05:28.626 Executor task launch worker-0 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int])':
/* 001 */
/* 002 */ public SpecificPredicate generate(Object[] references) {
/* 003 */   return new SpecificPredicate(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.codegen.Predicate {
/* 007 */   private final Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificPredicate(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public boolean eval(InternalRow i) {
/* 017 */     /* isnotnull(input[0, int]) */
/* 018 */     /* input[0, int] */
/* 019 */     boolean isNull1 = i.isNullAt(0);
/* 020 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 021 */     return !false && (!(isNull1));
/* 022 */   }
/* 023 */ }

07:05:28.627 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(3);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     int value = i.getInt(0);
/* 033 */     rowWriter.write(0, value);
/* 034 */
/* 035 */     /* input[1, int] */
/* 036 */     boolean isNull1 = i.isNullAt(1);
/* 037 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 038 */     if (isNull1) {
/* 039 */       rowWriter.setNullAt(1);
/* 040 */     } else {
/* 041 */       rowWriter.write(1, value1);
/* 042 */     }
/* 043 */
/* 044 */     /* input[2, string] */
/* 045 */     boolean isNull2 = i.isNullAt(2);
/* 046 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 047 */     if (isNull2) {
/* 048 */       rowWriter.setNullAt(2);
/* 049 */     } else {
/* 050 */       rowWriter.write(2, value2);
/* 051 */     }
/* 052 */     result.setTotalSize(holder.totalSize());
/* 053 */     return result;
/* 054 */   }
/* 055 */ }
/* 056 */

07:05:28.627 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.628 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.628 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 15 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@38aeae56
07:05:28.628 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 15 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@38aeae56
07:05:28.628 Executor task launch worker-0 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 5-6
07:05:28.628 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
07:05:28.628 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 8 blocks
07:05:28.628 Executor task launch worker-0 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
07:05:28.628 Executor task launch worker-0 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  0 ms
07:05:28.629 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.629 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for sortprefix(input[0, int] ASC):
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* sortprefix(input[0, int] ASC) */
/* 030 */     /* input[0, int] */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 033 */     long value = -9223372036854775808L;
/* 034 */     boolean isNull = false;
/* 035 */     if (!isNull1) {
/* 036 */       value = (long) value1;
/* 037 */     }
/* 038 */     if (isNull) {
/* 039 */       rowWriter.setNullAt(0);
/* 040 */     } else {
/* 041 */       rowWriter.write(0, value);
/* 042 */     }
/* 043 */     return result;
/* 044 */   }
/* 045 */ }
/* 046 */

07:05:28.629 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 15 acquire 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@444db476
07:05:28.629 Executor task launch worker-0 DEBUG TaskMemoryManager: Task 15 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@444db476
07:05:28.630 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.631 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.638 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.638 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(1);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     rowWriter.zeroOutNullBytes();
/* 028 */
/* 029 */     /* input[0, int] */
/* 030 */     boolean isNull = i.isNullAt(0);
/* 031 */     int value = isNull ? -1 : (i.getInt(0));
/* 032 */     if (isNull) {
/* 033 */       rowWriter.setNullAt(0);
/* 034 */     } else {
/* 035 */       rowWriter.write(0, value);
/* 036 */     }
/* 037 */     return result;
/* 038 */   }
/* 039 */ }
/* 040 */

07:05:28.638 Spark Context Cleaner DEBUG ContextCleaner: Got cleaning task CleanBroadcast(4)
07:05:28.638 Spark Context Cleaner DEBUG ContextCleaner: Cleaning broadcast 4
07:05:28.638 Spark Context Cleaner DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 4
07:05:28.639 block-manager-slave-async-thread-pool-1 DEBUG BlockManagerSlaveEndpoint: removing broadcast 4
07:05:28.639 block-manager-slave-async-thread-pool-1 DEBUG BlockManager: Removing broadcast 4
07:05:28.639 block-manager-slave-async-thread-pool-1 DEBUG BlockManager: Removing block broadcast_4
07:05:28.639 block-manager-slave-async-thread-pool-1 DEBUG MemoryStore: Block broadcast_4 of size 10136 dropped from memory (free 2348737871)
07:05:28.639 Executor task launch worker-0 DEBUG GenerateOrdering: Generated Ordering: /* 001 */
/* 002 */ public SpecificOrdering generate(Object[] references) {
/* 003 */   return new SpecificOrdering(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificOrdering(Object[] references) {
/* 013 */     this.references = references;
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public int compare(InternalRow a, InternalRow b) {
/* 018 */     InternalRow i = null;  // Holds current row being evaluated.
/* 019 */
/* 020 */     i = a;
/* 021 */     boolean isNullA;
/* 022 */     int primitiveA;
/* 023 */     {
/* 024 */       /* input[0, int] */
/* 025 */       boolean isNull = i.isNullAt(0);
/* 026 */       int value = isNull ? -1 : (i.getInt(0));
/* 027 */       isNullA = isNull;
/* 028 */       primitiveA = value;
/* 029 */     }
/* 030 */     i = b;
/* 031 */     boolean isNullB;
/* 032 */     int primitiveB;
/* 033 */     {
/* 034 */       /* input[0, int] */
/* 035 */       boolean isNull = i.isNullAt(0);
/* 036 */       int value = isNull ? -1 : (i.getInt(0));
/* 037 */       isNullB = isNull;
/* 038 */       primitiveB = value;
/* 039 */     }
/* 040 */     if (isNullA && isNullB) {
/* 041 */       // Nothing
/* 042 */     } else if (isNullA) {
/* 043 */       return -1;
/* 044 */     } else if (isNullB) {
/* 045 */       return 1;
/* 046 */     } else {
/* 047 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 048 */       if (comp != 0) {
/* 049 */         return comp;
/* 050 */       }
/* 051 */     }
/* 052 */
/* 053 */     return 0;
/* 054 */   }
/* 055 */ }

07:05:28.639 block-manager-slave-async-thread-pool-1 DEBUG BlockManager: Removing block broadcast_4_piece0
07:05:28.639 block-manager-slave-async-thread-pool-1 DEBUG MemoryStore: Block broadcast_4_piece0 of size 5349 dropped from memory (free 2348743220)
07:05:28.640 dispatcher-event-loop-7 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.102:52176 in memory (size: 5.2 KB, free: 2.2 GB)
07:05:28.640 block-manager-slave-async-thread-pool-1 DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0
07:05:28.640 block-manager-slave-async-thread-pool-1 DEBUG BlockManager: Told master about block broadcast_4_piece0
07:05:28.640 block-manager-slave-async-thread-pool-2 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 4, response is 0
07:05:28.640 block-manager-slave-async-thread-pool-2 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.1.102:52175
07:05:28.640 Spark Context Cleaner DEBUG ContextCleaner: Cleaned broadcast 4
07:05:28.640 Executor task launch worker-0 DEBUG GenerateUnsafeProjection: code for input[0, int],input[1, int],input[2, string],input[3, int],input[4, int],input[5, string]:
/* 001 */
/* 002 */ public java.lang.Object generate(Object[] references) {
/* 003 */   return new SpecificUnsafeProjection(references);
/* 004 */ }
/* 005 */
/* 006 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 007 */
/* 008 */   private Object[] references;
/* 009 */   private UnsafeRow result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 012 */
/* 013 */
/* 014 */   public SpecificUnsafeProjection(Object[] references) {
/* 015 */     this.references = references;
/* 016 */     result = new UnsafeRow(6);
/* 017 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 64);
/* 018 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 6);
/* 019 */   }
/* 020 */
/* 021 */   // Scala.Function1 need this
/* 022 */   public java.lang.Object apply(java.lang.Object row) {
/* 023 */     return apply((InternalRow) row);
/* 024 */   }
/* 025 */
/* 026 */   public UnsafeRow apply(InternalRow i) {
/* 027 */     holder.reset();
/* 028 */
/* 029 */     rowWriter.zeroOutNullBytes();
/* 030 */
/* 031 */     /* input[0, int] */
/* 032 */     boolean isNull = i.isNullAt(0);
/* 033 */     int value = isNull ? -1 : (i.getInt(0));
/* 034 */     if (isNull) {
/* 035 */       rowWriter.setNullAt(0);
/* 036 */     } else {
/* 037 */       rowWriter.write(0, value);
/* 038 */     }
/* 039 */
/* 040 */     /* input[1, int] */
/* 041 */     boolean isNull1 = i.isNullAt(1);
/* 042 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 043 */     if (isNull1) {
/* 044 */       rowWriter.setNullAt(1);
/* 045 */     } else {
/* 046 */       rowWriter.write(1, value1);
/* 047 */     }
/* 048 */
/* 049 */     /* input[2, string] */
/* 050 */     boolean isNull2 = i.isNullAt(2);
/* 051 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 052 */     if (isNull2) {
/* 053 */       rowWriter.setNullAt(2);
/* 054 */     } else {
/* 055 */       rowWriter.write(2, value2);
/* 056 */     }
/* 057 */
/* 058 */     /* input[3, int] */
/* 059 */     boolean isNull3 = i.isNullAt(3);
/* 060 */     int value3 = isNull3 ? -1 : (i.getInt(3));
/* 061 */     if (isNull3) {
/* 062 */       rowWriter.setNullAt(3);
/* 063 */     } else {
/* 064 */       rowWriter.write(3, value3);
/* 065 */     }
/* 066 */
/* 067 */     /* input[4, int] */
/* 068 */     boolean isNull4 = i.isNullAt(4);
/* 069 */     int value4 = isNull4 ? -1 : (i.getInt(4));
/* 070 */     if (isNull4) {
/* 071 */       rowWriter.setNullAt(4);
/* 072 */     } else {
/* 073 */       rowWriter.write(4, value4);
/* 074 */     }
/* 075 */
/* 076 */     /* input[5, string] */
/* 077 */     boolean isNull5 = i.isNullAt(5);
/* 078 */     UTF8String value5 = isNull5 ? null : (i.getUTF8String(5));
/* 079 */     if (isNull5) {
/* 080 */       rowWriter.setNullAt(5);
/* 081 */     } else {
/* 082 */       rowWriter.write(5, value5);
/* 083 */     }
/* 084 */     result.setTotalSize(holder.totalSize());
/* 085 */     return result;
/* 086 */   }
/* 087 */ }
/* 088 */

07:05:28.641 Executor task launch worker-0 INFO Executor: Finished task 5.0 in stage 3.0 (TID 15). 3653 bytes result sent to driver
07:05:28.641 dispatcher-event-loop-0 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3, runningTasks: 0
07:05:28.641 dispatcher-event-loop-0 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NODE_LOCAL
 - allowedLocality = NODE_LOCAL

07:05:28.641 dispatcher-event-loop-0 INFO TaskSetManager: No tasks dequeued
07:05:28.641 dispatcher-event-loop-0 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = NO_PREF
 - allowedLocality = NO_PREF

07:05:28.641 dispatcher-event-loop-0 INFO TaskSetManager: No tasks dequeued
07:05:28.641 dispatcher-event-loop-0 INFO TaskSetManager: Dequeuing tasks:

 - execId          = driver
 - host            = localhost
 - maxLocality     = ANY
 - allowedLocality = NODE_LOCAL

07:05:28.641 dispatcher-event-loop-0 INFO TaskSetManager: No tasks dequeued
07:05:28.642 task-result-getter-3 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 15) in 20 ms on localhost (6/8)
07:06:28.122 Thread-1 INFO SparkContext: Invoking stop() from shutdown hook
07:06:28.123 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.Server@7ec8f6ee
07:06:28.124 Thread-1 DEBUG AbstractLifeCycle: stopping SelectChannelConnector@0.0.0.0:4040
07:06:28.125 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.nio.SelectChannelConnector$ConnectorSelectorManager@78fd5c5f
07:06:28.126 SparkUI-41 Selector0 DEBUG nio: Stopped Thread[SparkUI-41 Selector0,5,main] on org.eclipse.jetty.io.nio.SelectorManager$1@7fd0ddb4
07:06:28.139 SparkUI-42 Selector1 DEBUG nio: Stopped Thread[SparkUI-42 Selector1,5,main] on org.eclipse.jetty.io.nio.SelectorManager$1@4a3d1e79
07:06:28.151 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.nio.SelectChannelConnector$ConnectorSelectorManager@78fd5c5f
07:06:28.151 Thread-1 DEBUG AbstractLifeCycle: stopping PooledBuffers [0/1024@6144,0/1024@16384,0/1024@-]/PooledBuffers [0/1024@6144,0/1024@32768,0/1024@-]
07:06:28.151 Thread-1 DEBUG AbstractLifeCycle: STOPPED null/null
07:06:28.151 Thread-1 DEBUG AbstractLifeCycle: STOPPED SelectChannelConnector@0.0.0.0:4040
07:06:28.151 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873
07:06:28.151 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873
07:06:28.152 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/static/sql,null}
07:06:28.152 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@28eb3784
07:06:28.152 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@28eb3784
07:06:28.152 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.DefaultServlet-be2e3d
07:06:28.152 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.DefaultServlet-be2e3d
07:06:28.152 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@28eb3784
07:06:28.152 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/static/sql,null}
07:06:28.152 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/static/sql,null}
07:06:28.152 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/static/sql,null} - org.eclipse.jetty.servlet.ServletHandler@28eb3784 as handler
07:06:28.152 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/static/sql,null}
07:06:28.152 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/SQL/execution/json,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@7d05cc82
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@7d05cc82
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-42241ea3
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-42241ea3
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@7d05cc82
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/SQL/execution/json,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/SQL/execution/json,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/SQL/execution/json,null} - org.eclipse.jetty.servlet.ServletHandler@7d05cc82 as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/SQL/execution/json,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/SQL/execution,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@28ba0350
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@28ba0350
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-70d04fff
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-70d04fff
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@28ba0350
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/SQL/execution,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/SQL/execution,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/SQL/execution,null} - org.eclipse.jetty.servlet.ServletHandler@28ba0350 as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/SQL/execution,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/SQL/json,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@2af04b26
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@2af04b26
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-4edb1a66
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@2af04b26
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/SQL/json,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/SQL/json,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/SQL/json,null} - org.eclipse.jetty.servlet.ServletHandler@2af04b26 as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/SQL/json,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/SQL,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@591a2d21
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@591a2d21
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-8dc8643
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-8dc8643
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@591a2d21
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/SQL,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/SQL,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/SQL,null} - org.eclipse.jetty.servlet.ServletHandler@591a2d21 as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/SQL,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/metrics/json,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@1ac7e89c
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@1ac7e89c
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-1a1f9631
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@1ac7e89c
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/metrics/json,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/metrics/json,null} - org.eclipse.jetty.servlet.ServletHandler@1ac7e89c as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/metrics/json,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@678b4ff2
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@a53afc7
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@a53afc7
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$3-3350a470
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$3-3350a470
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@a53afc7
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/stage/kill,null} - org.eclipse.jetty.servlet.ServletHandler@a53afc7 as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@678b4ff2
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@678b4ff2
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@4fb1bb33
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/api,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@3025cd3c
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@3025cd3c
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED com.sun.jersey.spi.container.servlet.ServletContainer-2c469ccf
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@3025cd3c
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/api,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/api,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/api,null} - org.eclipse.jetty.servlet.ServletHandler@3025cd3c as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/api,null}
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@4fb1bb33
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@4fb1bb33
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@3ee43dbb
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@7e6a3001
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@7e6a3001
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$3-4dc9f6df
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@7e6a3001
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/,null} - org.eclipse.jetty.servlet.ServletHandler@7e6a3001 as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/,null}
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@3ee43dbb
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@3ee43dbb
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@4cb30b63
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/static,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@307035c3
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@307035c3
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.DefaultServlet-41922a8c
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.DefaultServlet-41922a8c
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@307035c3
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/static,null}
07:06:28.153 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
07:06:28.153 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/static,null} - org.eclipse.jetty.servlet.ServletHandler@307035c3 as handler
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/static,null}
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@4cb30b63
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@4cb30b63
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@496c2fd8
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@6518423e
07:06:28.153 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@6518423e
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa
07:06:28.153 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-37adf0fa
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@6518423e
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/executors/threadDump/json,null} - org.eclipse.jetty.servlet.ServletHandler@6518423e as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@496c2fd8
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@496c2fd8
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@284b2df1
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/executors/threadDump,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@20b4cf64
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@20b4cf64
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-4d765fca
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-4d765fca
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@20b4cf64
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/executors/threadDump,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/executors/threadDump,null} - org.eclipse.jetty.servlet.ServletHandler@20b4cf64 as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/executors/threadDump,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@284b2df1
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@284b2df1
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@4ff7ba54
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/executors/json,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@6b932df6
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@6b932df6
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-6817f2ac
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@6b932df6
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/executors/json,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/executors/json,null} - org.eclipse.jetty.servlet.ServletHandler@6b932df6 as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/executors/json,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@4ff7ba54
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@4ff7ba54
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@a21ff61
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/executors,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@390d8618
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@390d8618
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-30594f19
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-30594f19
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@390d8618
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/executors,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/executors,null} - org.eclipse.jetty.servlet.ServletHandler@390d8618 as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/executors,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@a21ff61
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@a21ff61
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@b0d4961
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/environment/json,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@5e8002c
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@5e8002c
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-662fcb3d
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@5e8002c
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/environment/json,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/environment/json,null} - org.eclipse.jetty.servlet.ServletHandler@5e8002c as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/environment/json,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@b0d4961
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@b0d4961
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@5a77a58e
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/environment,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@77b79b63
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@77b79b63
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-71f96237
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-71f96237
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@77b79b63
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/environment,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/environment,null} - org.eclipse.jetty.servlet.ServletHandler@77b79b63 as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/environment,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@5a77a58e
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@5a77a58e
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@162a6726
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@1a0918af
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@1a0918af
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-5c028ddd
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@1a0918af
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/storage/rdd/json,null} - org.eclipse.jetty.servlet.ServletHandler@1a0918af as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@162a6726
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@162a6726
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@533a42fb
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/storage/rdd,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@3a7efd6
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@3a7efd6
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-104bf26c
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-104bf26c
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@3a7efd6
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/storage/rdd,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/storage/rdd,null} - org.eclipse.jetty.servlet.ServletHandler@3a7efd6 as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/storage/rdd,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@533a42fb
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@533a42fb
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@1408a46d
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/storage/json,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@3fa9a091
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@3fa9a091
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-1cc90593
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-1cc90593
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@3fa9a091
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/storage/json,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/storage/json,null} - org.eclipse.jetty.servlet.ServletHandler@3fa9a091 as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/storage/json,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@1408a46d
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@1408a46d
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@d98212c
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/storage,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@2449235e
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@2449235e
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-77c76850
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-77c76850
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@2449235e
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/storage,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/storage,null} - org.eclipse.jetty.servlet.ServletHandler@2449235e as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/storage,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@d98212c
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@d98212c
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@5a03ead0
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/stages/pool/json,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@30f5284d
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@30f5284d
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-3c0893b9
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@30f5284d
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/stages/pool/json,null}
07:06:28.154 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
07:06:28.154 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/pool/json,null} - org.eclipse.jetty.servlet.ServletHandler@30f5284d as handler
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/stages/pool/json,null}
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@5a03ead0
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@5a03ead0
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@2efd2106
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/stages/pool,null}
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@3de02a58
07:06:28.154 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@3de02a58
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-1f7b01ef
07:06:28.154 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@3de02a58
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/stages/pool,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/pool,null} - org.eclipse.jetty.servlet.ServletHandler@3de02a58 as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/stages/pool,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@2efd2106
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@2efd2106
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@2e26f5ff
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/stages/stage/json,null}
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-4479d261
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-4479d261
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/stages/stage/json,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/stage/json,null} - org.eclipse.jetty.servlet.ServletHandler@4a1a3c2a as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/stages/stage/json,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@2e26f5ff
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@2e26f5ff
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@1ba98cb0
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/stages/stage,null}
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@3f0575b8
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@3f0575b8
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-5fdf81f8
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@3f0575b8
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/stages/stage,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/stage,null} - org.eclipse.jetty.servlet.ServletHandler@3f0575b8 as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/stages/stage,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@1ba98cb0
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@1ba98cb0
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@35e82c4d
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/stages/json,null}
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@24752da4
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@24752da4
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-5e67ff86
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@24752da4
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/stages/json,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages/json,null} - org.eclipse.jetty.servlet.ServletHandler@24752da4 as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/stages/json,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@35e82c4d
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@35e82c4d
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@554ab237
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/stages,null}
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@37581ab3
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@37581ab3
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-31d55d07
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-31d55d07
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@37581ab3
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/stages,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/stages,null} - org.eclipse.jetty.servlet.ServletHandler@37581ab3 as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/stages,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@554ab237
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@554ab237
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@5d3d10fa
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/jobs/job/json,null}
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@154c14d6
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@154c14d6
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-78a8c75c
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@154c14d6
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/jobs/job/json,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/jobs/job/json,null} - org.eclipse.jetty.servlet.ServletHandler@154c14d6 as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/jobs/job/json,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@5d3d10fa
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@5d3d10fa
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@32638189
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/jobs/job,null}
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@b920f35
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@b920f35
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-749369c2
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-749369c2
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@b920f35
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/jobs/job,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/jobs/job,null} - org.eclipse.jetty.servlet.ServletHandler@b920f35 as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/jobs/job,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@32638189
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@32638189
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@53367e85
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/jobs/json,null}
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@1b0107e3
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@1b0107e3
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-ac83420
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-ac83420
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@1b0107e3
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/jobs/json,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/jobs/json,null} - org.eclipse.jetty.servlet.ServletHandler@1b0107e3 as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/jobs/json,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@53367e85
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@53367e85
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.GzipHandler@79ce262c
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping o.e.j.s.ServletContextHandler{/jobs,null}
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.servlet.ServletHandler@4a2a22e9
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.servlet.ServletHandler@4a2a22e9
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.apache.spark.ui.JettyUtils$$anon$2-1c2693ee
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.servlet.ServletHandler@4a2a22e9
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping o.e.j.s.ServletContextHandler{/jobs,null}
07:06:28.155 Thread-1 INFO ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
07:06:28.155 Thread-1 DEBUG Container: Container o.e.j.s.ServletContextHandler{/jobs,null} - org.eclipse.jetty.servlet.ServletHandler@4a2a22e9 as handler
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED o.e.j.s.ServletContextHandler{/jobs,null}
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.GzipHandler@79ce262c
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.GzipHandler@79ce262c
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.ContextHandlerCollection@33b7b873
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.Server@7ec8f6ee
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping org.eclipse.jetty.server.handler.ErrorHandler@231967b4
07:06:28.155 Thread-1 DEBUG AbstractHandler: stopping org.eclipse.jetty.server.handler.ErrorHandler@231967b4
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.handler.ErrorHandler@231967b4
07:06:28.155 Thread-1 DEBUG AbstractLifeCycle: stopping SparkUI{8<=8<=8/254,0}
07:06:28.207 Thread-1 DEBUG AbstractLifeCycle: STOPPED SparkUI{8<=0<=0/254,3}
07:06:28.207 Thread-1 DEBUG AbstractLifeCycle: STOPPED org.eclipse.jetty.server.Server@7ec8f6ee
07:06:28.208 Thread-1 INFO SparkUI: Stopped Spark web UI at http://192.168.1.102:4040
07:06:28.214 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO DAGScheduler: Job 2 failed: apply at Transformer.scala:22, took 59.935841 s
07:06:28.214 Thread-1 INFO DAGScheduler: ResultStage 3 (apply at Transformer.scala:22) failed in 59.722 s
07:06:28.214 Thread-1 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@6e0d4131)
07:06:28.215 Thread-1 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(2,1461247588214,JobFailed(org.apache.spark.SparkException: Job 2 cancelled because SparkContext was shut down))
07:06:28.216 pool-1-thread-1-ScalaTest-running-BucketedReadSuite ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerSQLExecutionEnd(2,1461247588216)
07:06:28.222 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
07:06:28.222 pool-1-thread-1-ScalaTest-running-BucketedReadSuite INFO BucketedReadSuite:

===== FINISHED o.a.s.sql.sources.BucketedReadSuite: 'a' =====

07:06:28.230 Thread-1 INFO MemoryStore: MemoryStore cleared
07:06:28.231 Thread-1 INFO BlockManager: BlockManager stopped
07:06:28.232 Thread-1 INFO BlockManagerMaster: BlockManagerMaster stopped
07:06:28.234 dispatcher-event-loop-6 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
07:06:28.236 Thread-1 INFO SparkContext: Successfully stopped SparkContext
07:06:28.236 Thread-1 INFO ShutdownHookManager: Shutdown hook called
07:06:28.237 Thread-1 INFO ShutdownHookManager: Deleting directory /Users/lian/local/src/spark/workspace-d/target/tmp/warehouse-b6bf920e-d71f-4306-b3f8-d015c2842fa5
07:06:28.241 Thread-1 INFO ShutdownHookManager: Deleting directory /Users/lian/local/src/spark/workspace-d/target/tmp/spark-6f232b61-e09c-446f-b2d3-90dc77a1ea2a
07:06:28.241 Thread-1 INFO ShutdownHookManager: Deleting directory /Users/lian/local/src/spark/workspace-d/target/tmp/spark-2879a26e-4e9f-405a-ab20-ea9478ea33e6
07:06:28.241 Thread-1 INFO ShutdownHookManager: Deleting directory /Users/lian/local/src/spark/workspace-d/target/tmp/catalystHiveFiles8543690501011710020
07:06:28.241 Thread-1 INFO ShutdownHookManager: Deleting directory /Users/lian/local/src/spark/workspace-d/target/tmp/spark-754e499a-1a3f-4bc7-9b64-5d6b18bb6a10
07:06:28.241 Thread-1 INFO ShutdownHookManager: Deleting directory /Users/lian/local/src/spark/workspace-d/target/tmp/spark-48befe79-2ad5-4bcd-991a-acfbb26a27a8
07:06:28.253 Thread-1 INFO ShutdownHookManager: Deleting directory /Users/lian/local/src/spark/workspace-d/target/tmp/scratch-c61d1db8-b342-46a1-9dbe-af53be6bb0ba
