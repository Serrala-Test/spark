/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.connect.service

import com.fasterxml.jackson.annotation.JsonIgnore
import com.google.protobuf.Message

import org.apache.spark.SparkContext
import org.apache.spark.connect.proto
import org.apache.spark.scheduler.SparkListenerEvent
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.catalyst.plans.logical.{Command, LogicalPlan}
import org.apache.spark.sql.connect.common.ProtoUtils
import org.apache.spark.util.{Clock, Utils}

object Events {
  // TODO: Make this configurable
  val MAX_STATEMENT_TEXT_SIZE = 65535
}

/**
 * Post Connect events to @link org.apache.spark.scheduler.LiveListenerBus.
 *
 * @param sessionHolder:
 *   Session for which the events are generated.
 * @param clock:
 *   Source of time for unit tests.
 */
case class Events(sessionHolder: SessionHolder, clock: Clock) {

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationStarted to @link
   * org.apache.spark.scheduler.LiveListenerBus.
   *
   * @param planHolder:
   *   The Connect request plan to execute.
   */
  def postStarted(planHolder: ExecutePlanHolder): Unit = {
    val sc = sessionHolder.session.sparkContext
    val request = planHolder.request
    val plan: Message =
      request.getPlan.getOpTypeCase match {
        case proto.Plan.OpTypeCase.COMMAND => request.getPlan.getCommand
        case proto.Plan.OpTypeCase.ROOT => request.getPlan.getRoot
        case _ =>
          throw new UnsupportedOperationException(
            s"${request.getPlan.getOpTypeCase} not supported.")
      }

    sc.listenerBus.post(
      SparkListenerConnectOperationStarted(
        planHolder.jobGroupId,
        planHolder.operationId,
        clock.getTimeMillis(),
        request.getSessionId,
        request.getUserContext.getUserId,
        request.getUserContext.getUserName,
        Utils.redact(
          sessionHolder.session.sessionState.conf.stringRedactionPattern,
          ProtoUtils.abbreviate(plan, Events.MAX_STATEMENT_TEXT_SIZE).toString),
        request.getClientType))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationParsed to @link
   * org.apache.spark.scheduler.LiveListenerBus.
   *
   * @param dataFrameOpt:
   *   The Dataframe generated by the Connect request plan. None when the request does not
   *   generate a plan.
   */
  def postParsed(dataFrameOpt: Option[DataFrame] = None): Unit = {
    assertExecutedPlanPrepared(dataFrameOpt)
    val jobGroupId = assertJobGroupId()
    val operationId = assertOperationId(jobGroupId)
    val event =
      SparkListenerConnectOperationParsed(jobGroupId, operationId, clock.getTimeMillis())
    event.analyzedPlan = dataFrameOpt.map(_.queryExecution.analyzed)
    sessionHolder.session.sparkContext.listenerBus.post(event)
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationCanceled to
   * \@link org.apache.spark.scheduler.LiveListenerBus.
   *
   * @param jobGroupIdOpt:
   *   jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect for
   *   request that was canceled. None when the jobGroupId is not set as a locale
   */
  def postCanceled(jobGroupIdOpt: Option[String] = None): Unit = {
    val jobGroupId = jobGroupIdOpt.getOrElse(assertJobGroupId())
    val operationId = assertOperationId(jobGroupId)
    sessionHolder.session.sparkContext.listenerBus
      .post(SparkListenerConnectOperationCanceled(jobGroupId, operationId, clock.getTimeMillis()))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationFailed to @link
   * org.apache.spark.scheduler.LiveListenerBus.
   *
   * @param errorMessage:
   *   The message of the error thrown during the request.
   */
  def postFailed(errorMessage: String): Unit = {
    val jobGroupId = assertJobGroupId()
    val operationId = assertOperationId(jobGroupId)
    sessionHolder.session.sparkContext.listenerBus.post(
      SparkListenerConnectOperationFailed(
        jobGroupId,
        operationId,
        clock.getTimeMillis(),
        errorMessage))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationParsed &
   * @link
   *   org.apache.spark.sql.connect.service.SparkListenerConnectOperationFinished to @link
   *   org.apache.spark.scheduler.LiveListenerBus.
   *
   * @param dataFrameOpt:
   *   The Dataframe generated by the Connect request plan. None when the request does not
   *   generate a plan.
   */
  def postParsedAndFinished(dataFrameOpt: Option[DataFrame] = None): Unit = {
    postParsed(dataFrameOpt)
    postFinished()
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationFinished to
   * \@link org.apache.spark.scheduler.LiveListenerBus.
   */
  def postFinished(): Unit = {
    val jobGroupId = assertJobGroupId()
    val operationId = assertOperationId(jobGroupId)
    sessionHolder.session.sparkContext.listenerBus
      .post(SparkListenerConnectOperationFinished(jobGroupId, operationId, clock.getTimeMillis()))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationClosed to @link
   * org.apache.spark.scheduler.LiveListenerBus.
   */
  def postClosed(): Unit = {
    val jobGroupId = assertJobGroupId()
    val operationId = assertOperationId(jobGroupId)
    sessionHolder.session.sparkContext.listenerBus
      .post(SparkListenerConnectOperationClosed(jobGroupId, operationId, clock.getTimeMillis()))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectSessionClosed to @link
   * org.apache.spark.scheduler.LiveListenerBus.
   */
  def postSessionClosed(): Unit = {
    sessionHolder.session.sparkContext.listenerBus
      .post(SparkListenerConnectSessionClosed(sessionHolder.sessionId, clock.getTimeMillis()))
  }

  private def assertExecutedPlanPrepared(dataFrameOpt: Option[DataFrame]): Unit = {
    dataFrameOpt.foreach { dataFrame =>
      val isEagerlyExecuted = dataFrame.queryExecution.analyzed.find {
        case _: Command => true
        case _ => false
      }.isDefined
      val isStreaming = dataFrame.queryExecution.analyzed.isStreaming

      if (!isEagerlyExecuted && !isStreaming) {
        dataFrame.queryExecution.executedPlan
      }
    }
  }

  private def assertOperationId(jobGroupId: String): String = {
    ExecutePlanHolder.getQueryOperationId(jobGroupId) match {
      case Some(operationId) =>
        operationId
      case None =>
        throw new RuntimeException(
          s"Connect operationId cannot be resolved during sessionId: ${sessionHolder.sessionId}")
    }
  }

  private def assertJobGroupId(): String = {
    getJobGroupId() match {
      case Some(jobGroupId) =>
        jobGroupId
      case None =>
        throw new RuntimeException(
          s"Connect jobGroupId is not set during sessionId: ${sessionHolder.sessionId}")
    }
  }

  private def getJobGroupId(): Option[String] = {
    Option(
      sessionHolder.session.sparkContext
        .getLocalProperty(SparkContext.SPARK_JOB_GROUP_ID))
  }
}

/**
 * Event sent after reception of a Connect request that is ready for execution (i.e. not queued),
 * but prior any analysis or execution.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   32 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param sessionId:
 *   32 characters UUID assigned by Connect the operation was executed on.
 * @param userId:
 *   Opaque userId set in the Connect request.
 * @param userName:
 *   Opaque userName set in the Connect request.
 * @param statementText:
 *   The connect request plan converted to text.
 * @param clientType:
 *   The clientType set in the Connect request.
 * @param extraTags:
 *   Additional metadata (i.e. spark context locale properties) during the request.
 */
case class SparkListenerConnectOperationStarted(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    sessionId: String,
    userId: String,
    userName: String,
    statementText: String,
    clientType: String,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * Event sent after a Connect request has been analyzed, but prior execution.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   32 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata (i.e. spark context locale properties) during the request.
 */
case class SparkListenerConnectOperationParsed(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent {

  /**
   * Analyzed Spark plan generated by the Connect request. None when the Connect request does not
   * generate a Spark plan.
   */
  @JsonIgnore var analyzedPlan: Option[LogicalPlan] = None
}

/**
 * Event sent after a Connect request has been canceled.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   32 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata (i.e. spark context locale properties) during the request.
 */
case class SparkListenerConnectOperationCanceled(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * Event sent after a Connect request has failed.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   32 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param errorMessage:
 *   The message of the error thrown during the request.
 * @param extraTags:
 *   Additional metadata (i.e. spark context locale properties) during the request.
 */
case class SparkListenerConnectOperationFailed(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    errorMessage: String,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * Event sent after a Connect request has finished executing, but prior results have been sent to
 * client.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   32 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata (i.e. spark context locale properties) during the request.
 */
case class SparkListenerConnectOperationFinished(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * Event sent after a Connect request has finished executing and results have been sent to client.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   32 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata (i.e. spark context locale properties) during the request.
 */
case class SparkListenerConnectOperationClosed(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * Event sent after a Connect session has been closed.
 *
 * @param sessionId:
 *   32 characters UUID assigned by Connect
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata (i.e. spark context locale properties).
 */
case class SparkListenerConnectSessionClosed(
    sessionId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent
