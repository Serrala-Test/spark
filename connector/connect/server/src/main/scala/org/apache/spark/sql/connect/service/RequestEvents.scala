/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.connect.service

import com.fasterxml.jackson.annotation.JsonIgnore
import com.google.protobuf.Message

import org.apache.spark.connect.proto
import org.apache.spark.scheduler.{LiveListenerBus, SparkListenerEvent}
import org.apache.spark.sql.catalyst.QueryPlanningTracker
import org.apache.spark.sql.catalyst.plans.logical.{Command, LogicalPlan}
import org.apache.spark.sql.connect.common.ProtoUtils
import org.apache.spark.sql.execution.QueryExecution
import org.apache.spark.util.{Clock, Utils}

object RequestEvents {
  // TODO: Make this configurable
  val MAX_STATEMENT_TEXT_SIZE = 65535
}

/**
 * Post request Connect events to @link org.apache.spark.scheduler.LiveListenerBus.
 *
 * @param planHolder:
 *   Request for which the events are generated.
 * @param clock:
 *   Source of time for unit tests.
 */
case class RequestEvents(planHolder: ExecutePlanHolder, clock: Clock) {

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationStarted.
   */
  def postStarted(): Unit = {
    val request = planHolder.request
    val plan: Message =
      request.getPlan.getOpTypeCase match {
        case proto.Plan.OpTypeCase.COMMAND => request.getPlan.getCommand
        case proto.Plan.OpTypeCase.ROOT => request.getPlan.getRoot
        case _ =>
          throw new UnsupportedOperationException(
            s"${request.getPlan.getOpTypeCase} not supported.")
      }

    listenerBus.post(
      SparkListenerConnectOperationStarted(
        jobGroupId,
        operationId,
        clock.getTimeMillis(),
        request.getSessionId,
        request.getUserContext.getUserId,
        request.getUserContext.getUserName,
        Utils.redact(
          sessionHolder.session.sessionState.conf.stringRedactionPattern,
          ProtoUtils.abbreviate(plan, RequestEvents.MAX_STATEMENT_TEXT_SIZE).toString),
        request.getClientType))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationParsed.
   *
   * @param qe
   *   The execution generated by the Connect request plan. None when the request does not
   *   generate a execution.
   */
  def postParsed(qe: Option[QueryExecution] = None): Unit = {
    qe.foreach(assertExecutedPlanPrepared)
    val event =
      SparkListenerConnectOperationParsed(jobGroupId, operationId, clock.getTimeMillis())
    event.analyzedPlan = qe.map(_.analyzed)
    listenerBus.post(event)
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationCanceled.
   */
  def postCanceled(): Unit = {
    listenerBus
      .post(SparkListenerConnectOperationCanceled(jobGroupId, operationId, clock.getTimeMillis()))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationFailed.
   *
   * @param errorMessage
   *   The message of the error thrown during the request.
   */
  def postFailed(errorMessage: String): Unit = {
    listenerBus.post(
      SparkListenerConnectOperationFailed(
        jobGroupId,
        operationId,
        clock.getTimeMillis(),
        errorMessage))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationParsed &
   * @link
   *   org.apache.spark.sql.connect.service.SparkListenerConnectOperationFinished.
   *
   * @param qe
   *   The execution generated by the Connect request plan. None when the request does not
   *   generate a execution.
   */
  def postParsedAndFinished(qe: Option[QueryExecution] = None): Unit = {
    postParsed(qe)
    postFinished()
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationFinished.
   */
  def postFinished(): Unit = {
    listenerBus
      .post(SparkListenerConnectOperationFinished(jobGroupId, operationId, clock.getTimeMillis()))
  }

  /**
   * Post @link org.apache.spark.sql.connect.service.SparkListenerConnectOperationClosed.
   */
  def postClosed(): Unit = {
    listenerBus
      .post(SparkListenerConnectOperationClosed(jobGroupId, operationId, clock.getTimeMillis()))
  }

  /**
   * For commands & streaming, parsed event is sent prior optimization. For other queries, parsed
   * event is sent after planning. Assert during execution as there's no way to enforce this via
   * end to end test.
   * @param qe
   */
  private def assertExecutedPlanPrepared(qe: QueryExecution): Unit = {
    val isEagerlyExecuted = qe.analyzed.find {
      case _: Command => true
      case _ => false
    }.isDefined
    val isStreaming = qe.analyzed.isStreaming
    val phases = qe.tracker.phases.keySet
    assert(phases.contains(QueryPlanningTracker.PLANNING) != isEagerlyExecuted || isStreaming)
  }

  private def operationId(): String = {
    planHolder.operationId
  }

  private def jobGroupId(): String = {
    planHolder.jobTag
  }

  private def listenerBus(): LiveListenerBus = {
    sessionHolder.session.sparkContext.listenerBus
  }

  private def sessionHolder(): SessionHolder = {
    planHolder.sessionHolder
  }
}

/**
 * Event sent after reception of a Connect request that is ready for execution (i.e. not queued),
 * but prior any analysis or execution.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   36 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param sessionId:
 *   ID assigned by the client or Connect the operation was executed on.
 * @param userId:
 *   Opaque userId set in the Connect request.
 * @param userName:
 *   Opaque userName set in the Connect request.
 * @param statementText:
 *   The connect request plan converted to text.
 * @param clientType:
 *   The clientType set in the Connect request.
 * @param extraTags:
 *   Additional metadata during the request.
 */
case class SparkListenerConnectOperationStarted(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    sessionId: String,
    userId: String,
    userName: String,
    statementText: String,
    clientType: String,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * The event is sent after a Connect request has been analyzed, but prior execution. For commands
 * & streaming, this is before @link
 * org.apache.spark.sql.catalyst.QueryPlanningTracker.OPTIMIZATION. For other requests it is after
 * \@link org.apache.spark.sql.catalyst.QueryPlanningTracker.PLANNING
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   36 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata during the request.
 */
case class SparkListenerConnectOperationParsed(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent {

  /**
   * Analyzed Spark plan generated by the Connect request. None when the Connect request does not
   * generate a Spark plan.
   */
  @JsonIgnore var analyzedPlan: Option[LogicalPlan] = None
}

/**
 * Event sent after a Connect request has been canceled.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   36 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata during the request.
 */
case class SparkListenerConnectOperationCanceled(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * Event sent after a Connect request has failed.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   36 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param errorMessage:
 *   The message of the error thrown during the request.
 * @param extraTags:
 *   Additional metadata during the request.
 */
case class SparkListenerConnectOperationFailed(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    errorMessage: String,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * Event sent after a Connect request has finished executing, but prior results have been sent to
 * client.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   36 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata during the request.
 */
case class SparkListenerConnectOperationFinished(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent

/**
 * Event sent after a Connect request has finished executing and results have been sent to client.
 *
 * @param jobGroupId:
 *   Opaque Spark jobGroupId (@link org.apache.spark.SparkContext.setJobGroup) assigned by Connect
 *   during a request. Designed to be unique across sessions and requests.
 * @param operationId:
 *   36 characters UUID assigned by Connect during a request.
 * @param eventTime:
 *   The time in ms when the event was generated.
 * @param extraTags:
 *   Additional metadata during the request.
 */
case class SparkListenerConnectOperationClosed(
    jobGroupId: String,
    operationId: String,
    eventTime: Long,
    extraTags: Map[String, String] = Map.empty)
    extends SparkListenerEvent
