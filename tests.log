Using /Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home as default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
[0m[[0minfo[0m] [0mLoading project definition from /Users/royl/git/spark/project/project[0m
[0m[[0minfo[0m] [0mLoading project definition from /Users/royl/.sbt/0.13/staging/ad8e8574a5bcb2d22d23/sbt-pom-reader/project[0m
[0m[[33mwarn[0m] [0mMultiple resolvers having different access mechanism configured with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).[0m
[0m[[0minfo[0m] [0mLoading project definition from /Users/royl/git/spark/project[0m
[0m[[0minfo[0m] [0mSet current project to spark-parent (in build file:/Users/royl/git/spark/)[0m
[0m[[0minfo[0m] [0mCompiling 1 Scala source and 5 Java sources to /Users/royl/git/spark/unsafe/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 5 Java sources to /Users/royl/git/spark/launcher/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 1 Scala source to /Users/royl/git/spark/external/flume-sink/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 12 Java sources to /Users/royl/git/spark/network/common/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mANTLR: Grammar file 'org/apache/spark/sql/catalyst/parser/SparkSqlLexer.g' detected.[0m
[0m[[0minfo[0m] [0mANTLR: Grammar file 'org/apache/spark/sql/catalyst/parser/SparkSqlParser.g' detected.[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 1 second, 824 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for test-tags/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 2 seconds, 701 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for spark/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 33 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for launcher/test:testOnly[0m
[0m[[0minfo[0m] [0mCompiling 10 Java sources to /Users/royl/git/spark/network/shuffle/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 34 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for network-common/test:testOnly[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/core/src/main/scala/org/apache/spark/SparkEnv.scala:99: value actorSystem in class SparkEnv is deprecated: Actor system is no longer supported as of 1.4.0[0m
[0m[[33mwarn[0m] [0m      actorSystem.shutdown()[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[0minfo[0m] [0mCompiling 194 Scala sources and 18 Java sources to /Users/royl/git/spark/core/target/scala-2.10/test-classes...[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/graphx/src/main/scala/org/apache/spark/graphx/Pregel.scala:124: method mapReduceTriplets in class Graph is deprecated: use aggregateMessages[0m
[0m[[33mwarn[0m] [0m    var messages = g.mapReduceTriplets(sendMsg, mergeMsg)[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/graphx/src/main/scala/org/apache/spark/graphx/Pregel.scala:138: method mapReduceTriplets in class Graph is deprecated: use aggregateMessages[0m
[0m[[33mwarn[0m] [0m      messages = g.mapReduceTriplets([0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/streaming/src/main/scala/org/apache/spark/streaming/receiver/ActorReceiver.scala:191: value actorSystem in class SparkEnv is deprecated: Actor system is no longer supported as of 1.4.0[0m
[0m[[33mwarn[0m] [0m  protected lazy val actorSupervisor = SparkEnv.get.actorSystem.actorOf(Props(new Supervisor),[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 94 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for tools/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 47 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-mqtt-assembly/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 42 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-flume-assembly/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 19 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-kafka-assembly/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 33 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for network-shuffle/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 65 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-flume-sink/test:testOnly[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/mllib/src/main/scala/org/apache/spark/mllib/clustering/PowerIterationClustering.scala:388: method setRuns in class KMeans is deprecated: Support for runs is deprecated. This param will have no effect in 2.0.0.[0m
[0m[[33mwarn[0m] [0m      .setRuns(5)[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala:516: method setRuns in class KMeans is deprecated: Support for runs is deprecated. This param will have no effect in 2.0.0.[0m
[0m[[33mwarn[0m] [0m      .setRuns(runs)[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala:541: method setRuns in class KMeans is deprecated: Support for runs is deprecated. This param will have no effect in 2.0.0.[0m
[0m[[33mwarn[0m] [0m      .setRuns(runs)[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/mllib/src/main/scala/org/apache/spark/mllib/api/python/PythonMLLibAPI.scala:343: method setRuns in class KMeans is deprecated: Support for runs is deprecated. This param will have no effect in 2.0.0.[0m
[0m[[33mwarn[0m] [0m      .setRuns(runs)[0m
[0m[[33mwarn[0m] [0m[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 26 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for assembly/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 26 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for examples/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 17 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for unsafe/test:testOnly[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/core/src/test/scala/org/apache/spark/scheduler/SparkListenerSuite.scala:294: value actorSystem in class SparkEnv is deprecated: Actor system is no longer supported as of 1.4.0[0m
[0m[[33mwarn[0m] [0m      sc.env.actorSystem.settings.config.getBytes("akka.remote.netty.tcp.maximum-frame-size").toInt[0m
[0m[[33mwarn[0m] [0m             ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/core/src/test/scala/org/apache/spark/scheduler/TaskResultGetterSuite.scala:93: value actorSystem in class SparkEnv is deprecated: Actor system is no longer supported as of 1.4.0[0m
[0m[[33mwarn[0m] [0m      sc.env.actorSystem.settings.config.getBytes("akka.remote.netty.tcp.maximum-frame-size").toInt[0m
[0m[[33mwarn[0m] [0m             ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/core/src/test/scala/org/apache/spark/scheduler/TaskResultGetterSuite.scala:118: value actorSystem in class SparkEnv is deprecated: Actor system is no longer supported as of 1.4.0[0m
[0m[[33mwarn[0m] [0m      sc.env.actorSystem.settings.config.getBytes("akka.remote.netty.tcp.maximum-frame-size").toInt[0m
[0m[[33mwarn[0m] [0m             ^[0m
[0m[[33mwarn[0m] [0mthree warnings found[0m
[0m[[0minfo[0m] [0mCompiling 19 Scala sources to /Users/royl/git/spark/graphx/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 39 Scala sources and 8 Java sources to /Users/royl/git/spark/streaming/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 1 Scala source and 2 Java sources to /Users/royl/git/spark/external/zeromq/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 3 Scala sources and 3 Java sources to /Users/royl/git/spark/external/flume/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 2 Scala sources and 2 Java sources to /Users/royl/git/spark/external/mqtt/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 2 Scala sources to /Users/royl/git/spark/repl/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 5 Scala sources and 3 Java sources to /Users/royl/git/spark/external/kafka/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 1 Scala source and 2 Java sources to /Users/royl/git/spark/external/twitter/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 80 Scala sources to /Users/royl/git/spark/sql/catalyst/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 161 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for core/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 73 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-zeromq/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 160 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-twitter/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 85 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for repl/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 20 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-mqtt/test:testOnly[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 23 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-flume/test:testOnly[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/graphx/src/test/scala/org/apache/spark/graphx/GraphSuite.scala:224: method mapReduceTriplets in class Graph is deprecated: use aggregateMessages[0m
[0m[[33mwarn[0m] [0m      val result = graph.mapReduceTriplets[Int](et => Iterator((et.dstId, et.srcAttr)), _ + _)[0m
[0m[[33mwarn[0m] [0m                         ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/graphx/src/test/scala/org/apache/spark/graphx/GraphSuite.scala:289: method mapReduceTriplets in class Graph is deprecated: use aggregateMessages[0m
[0m[[33mwarn[0m] [0m      val neighborDegreeSums = starDeg.mapReduceTriplets([0m
[0m[[33mwarn[0m] [0m                                       ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/graphx/src/test/scala/org/apache/spark/graphx/GraphSuite.scala:299: method mapReduceTriplets in class Graph is deprecated: use aggregateMessages[0m
[0m[[33mwarn[0m] [0m      val numEvenNeighbors = vids.mapReduceTriplets(et => {[0m
[0m[[33mwarn[0m] [0m                                  ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/graphx/src/test/scala/org/apache/spark/graphx/GraphSuite.scala:315: method mapReduceTriplets in class Graph is deprecated: use aggregateMessages[0m
[0m[[33mwarn[0m] [0m      val numOddNeighbors = changedGraph.mapReduceTriplets(et => {[0m
[0m[[33mwarn[0m] [0m                                         ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/graphx/src/test/scala/org/apache/spark/graphx/GraphSuite.scala:350: method mapReduceTriplets in class Graph is deprecated: use aggregateMessages[0m
[0m[[33mwarn[0m] [0m      val neighborDegreeSums = reverseStarDegrees.mapReduceTriplets([0m
[0m[[33mwarn[0m] [0m                                                  ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/graphx/src/test/scala/org/apache/spark/graphx/GraphSuite.scala:423: method mapReduceTriplets in class Graph is deprecated: use aggregateMessages[0m
[0m[[33mwarn[0m] [0m      val neighborAttrSums = graph.mapReduceTriplets[Int]([0m
[0m[[33mwarn[0m] [0m                                   ^[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 23 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming-kafka/test:testOnly[0m
[0m[[33mwarn[0m] [0m6 warnings found[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 28 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for graphx/test:testOnly[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/streaming/src/test/scala/org/apache/spark/streaming/DStreamClosureSuite.scala:112: method foreach in class DStream is deprecated: use foreachRDD[0m
[0m[[33mwarn[0m] [0m    expectCorrectException { ds.foreach(foreachF1) }[0m
[0m[[33mwarn[0m] [0m                                ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/streaming/src/test/scala/org/apache/spark/streaming/DStreamClosureSuite.scala:113: method foreach in class DStream is deprecated: use foreachRDD[0m
[0m[[33mwarn[0m] [0m    expectCorrectException { ds.foreach(foreachF2) }[0m
[0m[[33mwarn[0m] [0m                                ^[0m
[0m[[33mwarn[0m] [0mtwo warnings found[0m
[0m[[0minfo[0m] [0mCompiling 143 Scala sources and 60 Java sources to /Users/royl/git/spark/mllib/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 21 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for streaming/test:testOnly[0m
[0m[[0minfo[0m] [0mCompiling 117 Scala sources and 16 Java sources to /Users/royl/git/spark/sql/core/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 17 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for catalyst/test:testOnly[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/mllib/src/test/scala/org/apache/spark/mllib/fpm/FPGrowthSuite.scala:303: inferred existential type Array[(scala.collection.immutable.Set[_$2], Long)] forSome { type _$2 }, which cannot be expressed by wildcards,  should be enabled[0m
[0m[[33mwarn[0m] [0mby making the implicit value scala.language.existentials visible.[0m
[0m[[33mwarn[0m] [0mThis can be achieved by adding the import clause 'import scala.language.existentials'[0m
[0m[[33mwarn[0m] [0mor by setting the compiler option -language:existentials.[0m
[0m[[33mwarn[0m] [0mSee the Scala docs for value scala.language.existentials for a discussion[0m
[0m[[33mwarn[0m] [0mwhy the feature should be explicitly enabled.[0m
[0m[[33mwarn[0m] [0m      val newFreqItemsets = newModel.freqItemsets.collect().map { itemset =>[0m
[0m[[33mwarn[0m] [0m                                                                ^[0m
[0m[[33mwarn[0m] [0m/Users/royl/git/spark/mllib/src/test/scala/org/apache/spark/mllib/fpm/FPGrowthSuite.scala:337: inferred existential type Array[(scala.collection.immutable.Set[_$2], Long)] forSome { type _$2 }, which cannot be expressed by wildcards,  should be enabled[0m
[0m[[33mwarn[0m] [0mby making the implicit value scala.language.existentials visible.[0m
[0m[[33mwarn[0m] [0m      val newFreqItemsets = newModel.freqItemsets.collect().map { itemset =>[0m
[0m[[33mwarn[0m] [0m                                                                ^[0m
[0m[[33mwarn[0m] [0mtwo warnings found[0m
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128M; support was removed in 8.0
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1g; support was removed in 8.0
[0m[[0minfo[0m] [0m[32mStreamingLinearRegressionSuite:[0m[0m
[0m[[0minfo[0m] [0mCompiling 57 Scala sources and 14 Java sources to /Users/royl/git/spark/sql/hive/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mCompiling 4 Scala sources to /Users/royl/git/spark/docker-integration-tests/target/scala-2.10/test-classes...[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 22 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for sql/test:testOnly[0m
[0m[[0minfo[0m] [0m[32m- parameter accuracy (16 seconds, 207 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- parameter convergence (2 seconds, 358 milliseconds)[0m[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 22 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for docker-integration-tests/test:testOnly[0m
[0m[[0minfo[0m] [0m[32m- predictions (10 seconds, 444 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- training and prediction (1 second, 899 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- handling empty RDDs in a stream (481 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mFPGrowthSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- FP-Growth using String type (322 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- FP-Growth String type association rule generation (138 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- FP-Growth using Int type (219 milliseconds)[0m[0m
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[0m[[0minfo[0m] [0m[32m- model save/load with String type (3 seconds, 266 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load with Int type (581 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mBinaryClassificationMetricsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- binary evaluation metrics (206 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- binary evaluation metrics for RDD where all examples have positive label (151 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- binary evaluation metrics for RDD where all examples have negative label (122 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- binary evaluation metrics with downsampling (77 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLinearRegressionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- linear regression (517 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- linear regression without intercept (481 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse linear regression without intercept (1 second, 547 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (402 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLinearRegressionClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in both training and prediction (10 seconds, 275 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHashingTFSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- hashing tf on a single doc (6 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- hashing tf on an RDD (16 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mPeriodicRDDCheckpointerSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Persisting (13 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Checkpointing (320 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mKernelDensitySuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- kernel density single sample (58 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- kernel density multiple samples (7 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mPrefixSpanSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- PrefixSpan internal (integer seq, 0 delim) run, singleton itemsets (231 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- PrefixSpan internal (integer seq, -1 delim) run, variable-size itemsets (49 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- PrefixSpan projections with multiple partial starts (93 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- PrefixSpan Integer type, variable-size itemsets (74 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- PrefixSpan String type, variable-size itemsets (94 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mStreamingTestSuite:[0m[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 15 milliseconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 0[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 0, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[33mNo tests were executed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 0, Failed 0, Errors 0, Passed 0[0m
[0m[[0minfo[0m] [0mNo tests to run for hive/test:testOnly[0m
[0m[[0minfo[0m] [0m[32m- accuracy for null hypothesis using welch t-test (10 seconds, 707 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- accuracy for alternative hypothesis using welch t-test (288 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- accuracy for null hypothesis using student t-test (308 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- accuracy for alternative hypothesis using student t-test (297 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- batches within same test window are grouped (346 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- entries in peace period are dropped (10 seconds, 298 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- null hypothesis when only data from one group is present (294 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mBinaryClassificationPMMLModelExportSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- logistic regression PMML export (28 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- linear SVM PMML export (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mRidgeRegressionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- ridge regression can help avoid overfitting (2 seconds, 397 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (208 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mRidgeRegressionClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in both training and prediction (7 seconds, 128 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mGradientBoostedTreesSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Regression with continuous features: SquaredError (5 seconds, 878 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Regression with continuous features: Absolute Error (5 seconds, 481 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification with continuous features: Log Loss (5 seconds, 314 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-5496: BoostingStrategy.defaultParams should recognize Classification (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (857 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- runWithValidation stops early and performs better on a validation dataset (12 seconds, 298 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Checkpointing (652 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mMatrixFactorizationModelSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- constructor (71 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- save/load (481 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- batch predict API recommendProductsForUsers (87 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- batch predict API recommendUsersForProducts (39 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mWord2VecSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Word2Vec (151 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Word2Vec throws exception when vocabulary is empty (18 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Word2VecModel (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- model load / save (284 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- big model load / save (6 seconds, 307 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mTestingUtilsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Comparing doubles using relative error. (15 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Comparing doubles using absolute error. (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Comparing vectors using relative error. (13 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Comparing vectors using absolute error. (9 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mMultivariateOnlineSummarizerSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- basic error handing (40 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- dense vector input (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse vector input (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- mixing dense and sparse vector input (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- merging two summarizers (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- merging summarizer with empty summarizer (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- merging summarizer when one side has zero mean (SPARK-4355) (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- merging summarizer with weighted samples (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mChiSqSelectorSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- ChiSqSelector transform test (sparse & dense vector) (87 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model load / save (233 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mRowMatrixSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- size (22 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- empty rows (9 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toBreeze (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- gram (22 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- similar columns (160 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- svd of a full-rank matrix (1 second, 70 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- svd of a low-rank matrix (62 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- validate k in svd (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- pca (157 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multiply a local matrix (29 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- compute column summary statistics (18 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- QR Decomposition (169 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- compute covariance (66 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- covariance matrix is symmetric (SPARK-10875) (37 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mRowMatrixClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in svd (8 seconds, 633 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in summarize (295 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLassoSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Lasso local random SGD (668 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Lasso local random SGD with initial weights (598 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (206 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLassoClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in both training and prediction (7 seconds, 207 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mNaiveBayesSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- model types (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- get, set params (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Naive Bayes Multinomial (294 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Naive Bayes Bernoulli (601 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- detect negative values (68 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- detect non zero or one values in Bernoulli (39 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load: 2.0 to 2.0 (492 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load: 1.0 to 2.0 (263 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mNaiveBayesClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in both training and prediction (5 seconds, 964 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLabeledPointSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- parse labeled points (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- parse labeled points with whitespaces (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- parse labeled points with v0.9 format (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mMultilabelMetricsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Multilabel evaluation metrics (142 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mBreezeVectorConversionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- dense to breeze (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse to breeze (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- dense breeze to vector (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse breeze to vector (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse breeze with partially-used arrays to vector (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mDecisionTreeSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification with continuous features: split and bin calculation (62 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification with binary (ordered) categorical features: split and bin calculation (32 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification with 3-ary (ordered) categorical features, with no samples for one category (27 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- extract categories from a number for multiclass classification (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- find splits for a continuous feature (376 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass classification with unordered categorical features: split and bin calculations (32 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass classification with ordered categorical features: split and bin calculations (47 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Avoid aggregation on the last level (63 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Avoid aggregation if impurity is 0.0 (54 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Second level node building with vs. without groups (261 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification stump with ordered categorical features (83 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Regression stump with 3-ary (ordered) categorical features (89 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Regression stump with binary (ordered) categorical features (110 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification stump with fixed label 0 for Gini (150 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification stump with fixed label 1 for Gini (165 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification stump with fixed label 0 for Entropy (130 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification stump with fixed label 1 for Entropy (124 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass classification stump with 3-ary (unordered) categorical features (139 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification stump with 1 continuous feature, to check off-by-1 error (49 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification stump with 2 continuous features (50 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass classification stump with unordered categorical features, with just enough bins (139 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass classification stump with continuous features (213 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass classification stump with continuous + unordered categorical features (222 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass classification stump with 10-ary (ordered) categorical features (155 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass classification tree with 10-ary (ordered) categorical features, with just enough bins (146 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- split must satisfy min instances per node requirements (55 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- do not choose split that does not satisfy min instance per node requirements (56 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- split must satisfy min info gain requirements (54 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Node.subtreeIterator (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (668 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mALSSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-1 matrices (1 second, 92 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-1 matrices bulk (1 second, 18 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-2 matrices (969 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-2 matrices bulk (1 second, 199 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-1 matrices implicit (1 second, 392 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-1 matrices implicit bulk (1 second, 540 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-2 matrices implicit (1 second, 397 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-2 matrices implicit bulk (1 second, 570 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-2 matrices implicit negative (1 second, 321 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rank-2 matrices with different user and product blocks (976 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- pseudorandomness (463 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Storage Level for RDDs in model (296 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- negative ids (804 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- NNALS, rank 2 (950 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mBaggedPointSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- BaggedPoint RDD: without subsampling (24 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- BaggedPoint RDD: with subsampling with replacement (fraction = 1.0) (145 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- BaggedPoint RDD: with subsampling with replacement (fraction = 0.5) (116 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- BaggedPoint RDD: with subsampling without replacement (fraction = 1.0) (69 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- BaggedPoint RDD: with subsampling without replacement (fraction = 0.5) (75 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mPMMLModelExportFactorySuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- PMMLModelExportFactory create KMeansPMMLModelExport when passing a KMeansModel (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- PMMLModelExportFactory create GeneralizedLinearPMMLModelExport when passing a LinearRegressionModel, RidgeRegressionModel or LassoModel (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- PMMLModelExportFactory create BinaryClassificationPMMLModelExport when passing a LogisticRegressionModel or SVMModel (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- PMMLModelExportFactory throw IllegalArgumentException when passing a Multinomial Logistic Regression (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- PMMLModelExportFactory throw IllegalArgumentException when passing an unsupported model (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mRandomForestSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification with continuous features: comparing DecisionTree vs. RandomForest(numTrees = 1) (857 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification with continuous features and node Id cache : comparing DecisionTree vs. RandomForest(numTrees = 1) (893 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Regression with continuous features: comparing DecisionTree vs. RandomForest(numTrees = 1) (662 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Regression with continuous features and node Id cache : comparing DecisionTree vs. RandomForest(numTrees = 1) (626 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification with continuous features: subsampling features (493 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Binary classification with continuous features and node Id cache: subsampling features (451 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alternating categorical and continuous features with multiclass labels to test indexing (69 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- subsampling rate in RandomForest (173 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (532 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mKMeansPMMLModelExportSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- KMeansPMMLModelExport generate PMML format (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mRDDFunctionsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- sliding (1 second, 651 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sliding with empty partitions (22 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mKMeansSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- single cluster (1 second, 932 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- no distinct points (287 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- more clusters than points (304 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- deterministic initialization (1 second, 371 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- single cluster with big dataset (2 seconds, 318 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- single cluster with sparse data (3 seconds, 261 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- k-means|| initialization (844 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- two clusters (426 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (541 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Initialize using given cluster centers (8 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mKMeansClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in both training and prediction (14 seconds, 531 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mStandardScalerSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Standardization with dense input when means and stds are provided (117 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Standardization with dense input (88 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Standardization with sparse input when means and stds are provided (45 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Standardization with sparse input (49 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Standardization with constant input when means and stds are provided (20 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Standardization with constant input (18 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- StandardScalerModel argument nulls are properly handled (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mStreamingLogisticRegressionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- parameter accuracy (2 seconds, 927 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- parameter convergence (2 seconds, 681 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- predictions (285 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- training and prediction (1 second, 166 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- handling empty RDDs in a stream (367 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mMultivariateGaussianSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- univariate (40 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multivariate (5 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multivariate degenerate (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-11302 (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mAssociationRulesSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- association rules using String type (51 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mRandomDataGeneratorSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- UniformGenerator (42 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- StandardNormalGenerator (59 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- LogNormalGenerator (300 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- PoissonGenerator (2 seconds, 200 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ExponentialGenerator (368 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- GammaGenerator (551 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- WeibullGenerator (645 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mMLUtilsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- epsilon computation (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- fast squared distance (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- loadLibSVMFile (96 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- loadLibSVMFile throws IllegalArgumentException when indices is zero-based (26 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- loadLibSVMFile throws IllegalArgumentException when indices is not in ascending order (26 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- saveAsLibSVMFile (46 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- appendBias (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- kFold (4 seconds, 423 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- loadVectors (74 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- loadLabeledPoints (83 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- log1pExp (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mBlockMatrixSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- size (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- grid partitioner (10 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toCoordinateMatrix (18 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toIndexedRowMatrix (35 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toBreeze and toLocalMatrix (15 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- add (183 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multiply (287 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- simulate multiply (18 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- validate (134 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- transpose (33 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mNNLSSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- NNLS: exact solution cases (21 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- NNLS: nonnegativity constraint active (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- NNLS: objective value test (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mMatricesSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- dense matrix construction (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- dense matrix construction with wrong dimension (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse matrix construction (8 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse matrix construction with wrong number of elements (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- index in matrices incorrect input (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- equals (34 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- matrix copies are deep copies (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- matrix indexing and updating (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toSparse, toDense (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- map, update (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- transpose (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- foreachActive (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- horzcat, vertcat, eye, speye (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- zeros (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- ones (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- eye (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- rand (209 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- randn (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- diag (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- sprand (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sprandn (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- MatrixUDT (5 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toString (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- numNonzeros and numActives (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mMLPairRDDFunctionsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- topByKey (25 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mIDFSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- idf (25 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- idf minimum document frequency filtering (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mIndexedRowMatrixSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- size (16 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- empty rows (7 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toBreeze (12 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toRowMatrix (15 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toCoordinateMatrix (22 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toBlockMatrix (43 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multiply a local matrix (35 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- gram (19 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- svd (47 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- validate matrix sizes of svd (17 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- validate k in svd (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- similar columns (30 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mCorrelationSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- corr(x, y) pearson, 1 value in data (106 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- corr(x, y) default, pearson (160 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- corr(x, y) spearman (272 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- corr(X) default, pearson (33 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- corr(X) spearman (40 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- method identification (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mFPTreeSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- add transaction (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- merge tree (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- extract freq itemsets (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLogisticRegressionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- logistic regression with SGD (786 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- logistic regression with LBFGS (944 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- logistic regression with initial weights with SGD (920 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- logistic regression with initial weights and non-default regularization parameter (664 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- logistic regression with initial weights with LBFGS (924 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- numerical stability of scaling features using logistic regression with LBFGS (4 seconds, 978 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multinomial logistic regression with LBFGS (38 seconds, 450 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load: binary classification (394 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load: multiclass classification (191 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLogisticRegressionClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in both training and prediction using SGD optimizer (7 seconds, 760 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in both training and prediction using LBFGS optimizer (2 seconds, 915 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mBLASSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- copy (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- scal (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- unitedIndices (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- axpy (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- dot (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- spr (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- syr (6 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- gemm (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- gemv (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mGeneralizedLinearPMMLModelExportSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- linear regression PMML export (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ridge regression PMML export (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- lasso PMML export (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mAreaUnderCurveSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- auc computation (19 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auc of an empty curve (12 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auc of a curve with a single point (8 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mPeriodicGraphCheckpointerSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Persisting (124 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Checkpointing (559 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mPowerIterationClusteringSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- power iteration clustering (933 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- power iteration clustering on graph (787 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- normalize and powerIter (116 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (264 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mIsotonicRegressionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- increasing isotonic regression (56 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (242 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression with size 0 (16 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression with size 1 (19 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression strictly increasing sequence (33 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression strictly decreasing sequence (19 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression with last element violating monotonicity (19 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression with first element violating monotonicity (23 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression with negative labels (17 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression with unordered input (20 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- weighted isotonic regression (19 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- weighted isotonic regression with weights lower than 1 (21 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- weighted isotonic regression with negative weights (24 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- weighted isotonic regression with zero weights (27 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression prediction (17 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression prediction with duplicate features (19 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- antitonic regression prediction with duplicate features (21 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- isotonic regression RDD prediction (29 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- antitonic regression prediction (17 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model construction (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mNumericParserSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- parser (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- parser with whitespaces (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mMulticlassMetricsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Multiclass evaluation metrics (64 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mRandomRDDsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- RandomRDD sizes (50 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- randomRDD for different distributions (2 seconds, 52 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- randomVectorRDD for different distributions (2 seconds, 264 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mNormalizerSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Normalization using L1 distance (85 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Normalization using L2 distance (31 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Normalization using L^Inf distance. (16 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mBreezeMatrixConversionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- dense matrix to breeze (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- dense breeze matrix to matrix (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse matrix to breeze (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse breeze matrix to sparse matrix (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mCoordinateMatrixSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- size (9 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- empty entries (16 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toBreeze (5 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- transpose (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toIndexedRowMatrix (21 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toRowMatrix (17 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toBlockMatrix (29 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mGradientDescentSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Assert the loss is decreasing. (719 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Test the loss and gradient of first iteration with regularization. (275 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- iteration should end with convergence tolerance (240 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mGradientDescentClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small (6 seconds, 91 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mVectorsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- dense vector construction with varargs (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- dense vector construction from a double array (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse vector construction (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse vector construction with unordered elements (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse vector construction with mismatched indices/values array (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse vector construction with too many indices vs size (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- dense to array (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- dense argmax (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse to array (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- sparse argmax (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- vector equals (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- vectors equals with explicit 0 (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- indexing dense vectors (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- indexing sparse vectors (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- parse vectors (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- zeros (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- Vector.copy (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- VectorUDT (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- fromBreeze (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- sqdist (17 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- foreachActive (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- vector p-norm (8 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Vector numActive and numNonzeros (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- Vector toSparse and toDense (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Vector.compressed (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SparseVector.slice (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- toJson/fromJson (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mPCASuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Correct computing use a PCA wrapper (63 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mGaussianMixtureSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- single cluster (198 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- two clusters (32 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- two clusters with distributed decompositions (236 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- single cluster with sparse data (170 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- two clusters with sparse data (24 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save / load (359 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model prediction, parallel and local (129 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLBFGSSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- LBFGS loss should be decreasing and match the result of Gradient Descent. (4 seconds, 134 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- LBFGS and Gradient Descent with L2 regularization should get the same result. (4 seconds, 712 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- The convergence criteria should work as we expect. (1 second, 636 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Optimize via class LBFGS. (4 seconds, 467 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLBFGSClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small (9 seconds, 869 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mRegressionMetricsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- regression metrics for unbiased (includes intercept term) predictor (25 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- regression metrics for biased (no intercept term) predictor (9 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- regression metrics with complete fitting (9 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mStreamingKMeansSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- accuracy for single center and equivalence to grand average (500 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- accuracy for two centers (483 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- detecting dying clusters (506 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-7946 setDecayFactor (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mPythonMLLibAPISuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- pickle vector (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- pickle labeled point (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- pickle double (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- pickle matrix (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- pickle rating (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mSVMSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- SVM with threshold (1 second, 395 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SVM using local random SGD (1 second, 167 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SVM local random SGD with initial weights (1 second, 41 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SVM with invalid labels (8 seconds, 59 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (373 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mSVMClusterSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- task size should be small in both training and prediction (7 seconds, 268 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mBisectingKMeansSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- default values (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- setter/getter (4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1D data (123 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- points are the same (24 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- more desired clusters than points (155 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- min divisible cluster (162 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- larger clusters get selected first (60 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 2D data (152 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mLDASuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- LocalLDAModel (14 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- running and DistributedLDAModel with default Optimizer (EM) (452 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- vertex indexing (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- setter alias (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- initializing with alpha length != k or 1 fails (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- initializing with elements in alpha < 0 fails (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- OnlineLDAOptimizer initialization (15 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- OnlineLDAOptimizer one iteration (52 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- OnlineLDAOptimizer with toy data (1 second, 659 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- LocalLDAModel logLikelihood (27 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- LocalLDAModel logPerplexity (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- LocalLDAModel predict (43 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- OnlineLDAOptimizer with asymmetric prior (1 second, 542 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- OnlineLDAOptimizer alpha hyperparameter optimization (1 second, 563 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- model save/load (1 second, 36 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- EMLDAOptimizer with empty docs (151 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- OnlineLDAOptimizer with empty docs (73 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mImpuritySuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Gini impurity does not support negative labels (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Entropy does not support negative labels (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mElementwiseProductSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- elementwise (hadamard) product should properly apply vector to dense data set (11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- elementwise (hadamard) product should properly apply vector to sparse data set (14 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHypothesisTestSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- chi squared pearson goodness of fit (5 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- chi squared pearson matrix independence (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- chi squared pearson RDD[LabeledPoint] (2 seconds, 277 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1 sample Kolmogorov-Smirnov test: apache commons math3 implementation equivalence (4 seconds, 12 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1 sample Kolmogorov-Smirnov test: R implementation equivalence (51 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mRankingMetricsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Ranking metrics: map, ndcg (69 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaNaiveBayesSuite[0m.[36mtestPredictJavaRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaNaiveBayesSuite[0m.[36mrunUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaNaiveBayesSuite[0m.[36mrunUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaNaiveBayesSuite[0m.[36mtestModelTypeSetters[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 4 total, 0.269s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaStreamingLogisticRegressionSuite[0m.[36mjavaAPI[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.362s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.fpm.[33mJavaFPGrowthSuite[0m.[36mrunFPGrowthSaveLoad[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.fpm.[33mJavaFPGrowthSuite[0m.[36mrunFPGrowth[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 0.496s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.fpm.[33mJavaPrefixSpanSuite[0m.[36mrunPrefixSpan[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.151s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaStreamingKMeansSuite[0m.[36mjavaAPI[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.28s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaLinearRegressionSuite[0m.[36mtestPredictJavaRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaLinearRegressionSuite[0m.[36mrunLinearRegressionUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaLinearRegressionSuite[0m.[36mrunLinearRegressionUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 3 total, 1.498s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaLogisticRegressionSuite[0m.[36mrunLRUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaLogisticRegressionSuite[0m.[36mrunLRUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 7.677s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaGaussianMixtureSuite[0m.[36mrunGaussianMixture[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.061s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaStreamingLinearRegressionSuite[0m.[36mjavaAPI[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.275s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaKMeansSuite[0m.[36mtestPredictJavaRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaKMeansSuite[0m.[36mrunKMeansUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaKMeansSuite[0m.[36mrunKMeansUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 3 total, 0.726s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.feature.[33mJavaTfIdfSuite[0m.[36mtfIdfMinimumDocumentFrequency[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.feature.[33mJavaTfIdfSuite[0m.[36mtfIdf[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 0.893s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.stat.[33mJavaStatisticsSuite[0m.[36mtestCorr[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.stat.[33mJavaStatisticsSuite[0m.[36mchiSqTest[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.stat.[33mJavaStatisticsSuite[0m.[36mstreamingTest[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.stat.[33mJavaStatisticsSuite[0m.[36mkolmogorovSmirnovTest[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 4 total, 0.501s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaIsotonicRegressionSuite[0m.[36mtestIsotonicRegressionJavaRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaIsotonicRegressionSuite[0m.[36mtestIsotonicRegressionPredictionsJavaRDD[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 0.167s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.recommendation.[33mJavaALSSuite[0m.[36mrunALSUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.recommendation.[33mJavaALSSuite[0m.[36mrunImplicitALSUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.recommendation.[33mJavaALSSuite[0m.[36mrunRecommend[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.recommendation.[33mJavaALSSuite[0m.[36mrunImplicitALSWithNegativeWeight[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.recommendation.[33mJavaALSSuite[0m.[36mrunImplicitALSUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.recommendation.[33mJavaALSSuite[0m.[36mrunALSUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 6 total, 7.752s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestNormalVectorRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestArbitrary[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestLogNormalVectorRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestExponentialVectorRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestUniformRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestRandomVectorRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestGammaRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestUniformVectorRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestPoissonRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestNormalRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestPoissonVectorRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestGammaVectorRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestExponentialRDD[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.random.[33mJavaRandomRDDsSuite[0m.[36mtestLNormalRDD[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 14 total, 1.054s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.tree.[33mJavaDecisionTreeSuite[0m.[36mrunDTUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.tree.[33mJavaDecisionTreeSuite[0m.[36mrunDTUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 0.286s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.linalg.[33mJavaVectorsSuite[0m.[36mdenseArrayConstruction[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.linalg.[33mJavaVectorsSuite[0m.[36msparseArrayConstruction[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 0.003s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaLDASuite[0m.[36monlineOptimizerCompatibility[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaLDASuite[0m.[36mdistributedLDAModel[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaLDASuite[0m.[36mlocalLDAModel[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaLDASuite[0m.[36mlocalLdaMethods[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 4 total, 0.663s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaRidgeRegressionSuite[0m.[36mrunRidgeRegressionUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaRidgeRegressionSuite[0m.[36mrunRidgeRegressionUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 4.14s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaSVMSuite[0m.[36mrunSVMUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.classification.[33mJavaSVMSuite[0m.[36mrunSVMUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 2.486s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.feature.[33mJavaWord2VecSuite[0m.[36mword2Vec[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.096s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.evaluation.[33mJavaRankingMetricsSuite[0m.[36mrankingMetrics[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.047s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.fpm.[33mJavaAssociationRulesSuite[0m.[36mrunAssociationRules[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.042s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.linalg.[33mJavaMatricesSuite[0m.[36mzerosMatrixConstruction[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.linalg.[33mJavaMatricesSuite[0m.[36midentityMatrixConstruction[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.linalg.[33mJavaMatricesSuite[0m.[36mconcatenateMatrices[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.linalg.[33mJavaMatricesSuite[0m.[36msparseDenseConversion[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.linalg.[33mJavaMatricesSuite[0m.[36mrandMatrixConstruction[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.linalg.[33mJavaMatricesSuite[0m.[36mdiagonalMatrixConstruction[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 6 total, 0.005s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaLassoSuite[0m.[36mrunLassoUsingConstructor[0m started[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.regression.[33mJavaLassoSuite[0m.[36mrunLassoUsingStaticMethods[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 2 total, 5.887s[0m[0m
[0m[[0minfo[0m] [0m[34mTest run started[0m[0m
[0m[[0minfo[0m] [0mTest org.apache.spark.mllib.clustering.[33mJavaBisectingKMeansSuite[0m.[36mtwoDimensionalData[0m started[0m
[0m[[0minfo[0m] [0m[34mTest run finished: [0m[34m0 failed[0m[34m, [0m[34m0 ignored[0m[34m, 1 total, 0.161s[0m[0m
-- org.jblas INFO Deleting /Users/royl/git/spark/target/tmp/jblas2572007263227157870/libjblas.dylib
-- org.jblas INFO Deleting /Users/royl/git/spark/target/tmp/jblas2572007263227157870/libjblas_arch_flavor.dylib
-- org.jblas INFO Deleting /Users/royl/git/spark/target/tmp/jblas2572007263227157870
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 6 minutes, 39 seconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 452[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 83, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 452, failed 0, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[32mAll tests passed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 523, Failed 0, Errors 0, Passed 523[0m
[0m[[32msuccess[0m] [0mTotal time: 617 s, completed Jan 14, 2016 5:22:11 AM[0m
