spark {
  # The master URL for the cluster egspark://host:port, mesos://host:port, yarn, or local.
  master = "local[*]"

  # allow old style environment variable to override if it exists
  master = ${?MASTER}

  # Should spark submit run in verbose mode: default is false
  verbose = false

  # Comma-separated list of files to be placed in the working directory of each executor
  # files = ""

  # Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.
  # submit.pyfiles =

  # Comma-separated list of local jars to include on the driver and executor classpaths.
  # jars = ""

  app {
    # Path to a bundled jar including your application and all dependencies.
    # The URL must be globally visible inside of your cluster, for instance,
    # an hdfs:// path or a file:// path that is present on all nodes.
    # primaryResource = ""

    # A name of your application.
    # name = ""

    # Your application's main class (for Java / Scala apps).
    # class = ""

    # Additional arguments to pass to the driver program
    # arguments = ""
  }


  # Whether to launch the driver program locally ("client") or
  # on one of the worker machines inside the cluster ("cluster")
  # (Default: client).
  deployMode = "client"

  # allow old style enrironmnet to override if it exists
  deployMode = ${?DEPLOY_MODE}

  # Spark standalone and Mesos only: Total cores for all executors.
  # cores.max:

  # executor configuration
  executor {
    # Memory per executor (e.g. 1000M, 2G) (Default: 1G).
    memory = 1G

    # Yarn client only:  Number of cores per executor (Default: 1).
    cores = 1

    # Yarn client only: Number of executors to launch (Default: 2).
    instances = 2
  }

  # configuration items that affect the driver application
  driver {
    # Standalone with cluster deploy mode only
    # Memory for driver (e.g. 1000M, 2G) (Default: 512M).
    memory = 512M

    # Spark standalone with cluster deploy mode only:
    # Number of cores for driver (default: 1)
    cores = 1

    # Spark standalone with cluster deploy mode only:
    # restart driver application on failure (default: false)
    supervise = "false"

    # Extra Java options to pass to the driver
    # extraJavaOptions = ""

    # Extra library path entries to pass to the driver.
    # extraLibraryPath = ""

    # Extra class path entries to pass to the driver. Note that
    # jars added with --jars are automatically included in the classpath.
    # extraClassPath = ""
  }

  # Yarn specific configurations
  yarn {
    # The YARN queue to submit to (Default: "default").
    queue = "default"

    # Number of executors to launch (Default: 2).
    numExecutors = 2

    # Yarn dist configuration
    dist {
      # Comma separated list of archives to be placed into the
      # working directory of each executor
      # archives = ""
    }

  }

}



