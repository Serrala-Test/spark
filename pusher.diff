diff --git a/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala b/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala
index d0765934af..c614b33bc3 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala
@@ -20,12 +20,10 @@ package org.apache.spark.shuffle
 import java.io.{File, FileNotFoundException}
 import java.net.ConnectException
 import java.nio.ByteBuffer
-import java.util.concurrent.ExecutorService
+import java.util.concurrent.{ConcurrentHashMap, ExecutorService, RejectedExecutionException}
 
 import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet, Queue}
 
-import com.google.common.base.Throwables
-
 import org.apache.spark.{ShuffleDependency, SparkConf, SparkEnv}
 import org.apache.spark.annotation.Since
 import org.apache.spark.internal.Logging
@@ -53,20 +51,14 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
   private[this] val maxBlockSizeToPush = conf.get(SHUFFLE_MAX_BLOCK_SIZE_TO_PUSH)
   private[this] val maxBlockBatchSize = conf.get(SHUFFLE_MAX_BLOCK_BATCH_SIZE_FOR_PUSH)
   private[this] val maxBytesInFlight =
-    conf.getSizeAsMb("spark.reducer.maxSizeInFlight", "48m") * 1024 * 1024
-  private[this] val maxReqsInFlight = conf.getInt("spark.reducer.maxReqsInFlight", Int.MaxValue)
-  private[this] val maxBlocksInFlightPerAddress = conf.get(REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS)
-  private[this] var bytesInFlight = 0L
-  private[this] var reqsInFlight = 0
-  private[this] val numBlocksInFlightPerAddress = new HashMap[BlockManagerId, Int]()
-  private[this] val deferredPushRequests = new HashMap[BlockManagerId, Queue[PushRequest]]()
-  private[this] val pushRequests = new Queue[PushRequest]
+    conf.getSizeAsMb("spark.shuffle.push.maxSizeInFlightPerExecutor", "48m") * 1024 * 1024
+  private[this] val maxReqsInFlight = conf.getInt("spark.shuffle.push.maxReqsInFlightPerExecutor",
+    Int.MaxValue)
+  private[this] val maxBlocksInFlightPerAddress = conf.getInt(
+    "spark.shuffle.push.maxBlocksInFlightPerAddrPerExecutor", Int.MaxValue)
   private[this] val errorHandler = createErrorHandler()
   // VisibleForTesting
   private[shuffle] val unreachableBlockMgrs = new HashSet[BlockManagerId]()
-  private[this] var shuffleId = -1
-  private[this] var mapIndex = -1
-  private[this] var pushCompletionNotified = false
 
   // VisibleForTesting
   private[shuffle] def createErrorHandler(): BlockPushErrorHandler = {
@@ -82,12 +74,10 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
           return false
         }
         // If the block is too late, there is no need to retry it
-        !Throwables.getStackTraceAsString(t).contains(BlockPushErrorHandler.TOO_LATE_MESSAGE_SUFFIX)
+        !t.getMessage.contains(BlockPushErrorHandler.TOO_LATE_MESSAGE_SUFFIX)
       }
     }
   }
-  // VisibleForTesting
-  private[shuffle] def isPushCompletionNotified = pushCompletionNotified
 
   /**
    * Initiates the block push.
@@ -105,28 +95,32 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
       mapIndex: Int): Unit = {
     val numPartitions = dep.partitioner.numPartitions
     val transportConf = SparkTransportConf.fromSparkConf(conf, "shuffle")
-    shuffleId = dep.shuffleId
-    this.mapIndex = mapIndex
+    val shuffleId = dep.shuffleId
     val requests = prepareBlockPushRequests(numPartitions, mapIndex, dep.shuffleId, dataFile,
       partitionLengths, dep.getMergerLocs, transportConf)
+    val deferredPushRequests = new HashMap[BlockManagerId, Queue[PushRequest]]()
+    val pushRequests = new Queue[PushRequest]
     // Randomize the orders of the PushRequest, so different mappers pushing blocks at the same
     // time won't be pushing the same ranges of shuffle partitions.
     pushRequests ++= Utils.randomize(requests)
     if (pushRequests.isEmpty) {
-      notifyDriverAboutPushCompletion()
+      notifyDriverAboutPushCompletion(shuffleId, mapIndex)
+    } else {
+      REQUEST.put((shuffleId, mapIndex),
+        PushRequestsInfo(shuffleId, mapIndex, pushRequests, deferredPushRequests,
+          pushCompletionNotified = false))
+      submitTask(() => {
+        try {
+          pushUpToMax(Some(shuffleId, mapIndex))
+        } catch {
+          case e: FileNotFoundException =>
+            logWarning("The shuffle files got deleted when this task was reading from them " +
+              "which could happen when the job finishes and the driver instructs the executor to " +
+              "cleanup the shuffle. In this case, push of the blocks belonging to this shuffle" +
+              "will stop.", e)
+        }
+      })
     }
-
-    submitTask(() => {
-      try {
-        pushUpToMax()
-      } catch {
-        case e: FileNotFoundException =>
-          logWarning("The shuffle files got deleted when this task was reading from them " +
-            "which could happen when the job finishes and the driver instructs the executor to " +
-            "cleanup the shuffle. In this case, push of the blocks belonging to this shuffle" +
-            "will stop.", e)
-      }
-    })
   }
 
   /**
@@ -149,37 +143,48 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
    * This code is similar to ShuffleBlockFetcherIterator#fetchUpToMaxBytes in how it throttles
    * the data transfer between shuffle client/server.
    */
-  private def pushUpToMax(): Unit = synchronized {
-    // Process any outstanding deferred push requests if possible.
-    if (deferredPushRequests.nonEmpty) {
-      for ((remoteAddress, defReqQueue) <- deferredPushRequests) {
-        while (isRemoteBlockPushable(defReqQueue) &&
-          !isRemoteAddressMaxedOut(remoteAddress, defReqQueue.front)) {
-          val request = defReqQueue.dequeue()
-          logDebug(s"Processing deferred push request for $remoteAddress with "
-            + s"${request.blocks.length} blocks")
-          sendRequest(request)
-          if (defReqQueue.isEmpty) {
-            deferredPushRequests -= remoteAddress
+  private def pushUpToMax(
+      shuffleIdAndMapIndex: Option[(Int, Int)] = None): Unit = {
+    val pushRequestsInfo: Option[PushRequestsInfo] = if (shuffleIdAndMapIndex.isDefined) {
+      val (shuffleId, mapIndex) = shuffleIdAndMapIndex.get
+      Option(REQUEST.get((shuffleId, mapIndex)))
+    } else {
+      Option(REQUEST.values().stream().filter(entry => entry.nonEmpty).findAny().orElse(null))
+    }
+    pushRequestsInfo.foreach { pushRequestsInfo =>
+      pushRequestsInfo.synchronized {
+        // Process any outstanding deferred push requests if possible.
+        if (pushRequestsInfo.deferredRequests.nonEmpty) {
+          for ((remoteAddress, defReqQueue) <- pushRequestsInfo.deferredRequests) {
+            while (isRemoteBlockPushable(defReqQueue) &&
+              !isRemoteAddressMaxedOut(remoteAddress, defReqQueue.front)) {
+              val request = defReqQueue.dequeue()
+              logDebug(s"Processing deferred push request for $remoteAddress with "
+                + s"${request.blocks.length} blocks")
+              sendRequest(request, pushRequestsInfo)
+              if (defReqQueue.isEmpty) {
+                pushRequestsInfo.deferredRequests -= remoteAddress
+              }
+            }
           }
         }
-      }
-    }
 
-    // Process any regular push requests if possible.
-    while (isRemoteBlockPushable(pushRequests)) {
-      val request = pushRequests.dequeue()
-      val remoteAddress = request.address
-      if (isRemoteAddressMaxedOut(remoteAddress, request)) {
-        logDebug(s"Deferring push request for $remoteAddress with ${request.blocks.size} blocks")
-        deferredPushRequests.getOrElseUpdate(remoteAddress, new Queue[PushRequest]())
-          .enqueue(request)
-      } else {
-        sendRequest(request)
+        // Process any regular push requests if possible.
+        while (isRemoteBlockPushable(pushRequestsInfo.requests)) {
+          val request = pushRequestsInfo.requests.dequeue()
+          val remoteAddress = request.address
+          if (isRemoteAddressMaxedOut(remoteAddress, request)) {
+            logDebug(s"Deferring push request for $remoteAddress with ${request.blocks.size} blocks")
+            pushRequestsInfo.deferredRequests.getOrElseUpdate(
+              remoteAddress, new Queue[PushRequest]()).enqueue(request)
+          } else {
+            sendRequest(request, pushRequestsInfo)
+          }
+        }
       }
     }
 
-    def isRemoteBlockPushable(pushReqQueue: Queue[PushRequest]): Boolean = {
+    def isRemoteBlockPushable(pushReqQueue: Queue[PushRequest]): Boolean = lock.synchronized {
       pushReqQueue.nonEmpty &&
         (bytesInFlight == 0 ||
           (reqsInFlight + 1 <= maxReqsInFlight &&
@@ -188,7 +193,8 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
 
     // Checks if sending a new push request will exceed the max no. of blocks being pushed to a
     // given remote address.
-    def isRemoteAddressMaxedOut(remoteAddress: BlockManagerId, request: PushRequest): Boolean = {
+    def isRemoteAddressMaxedOut(
+        remoteAddress: BlockManagerId, request: PushRequest): Boolean = lock.synchronized {
       (numBlocksInFlightPerAddress.getOrElse(remoteAddress, 0)
         + request.blocks.size) > maxBlocksInFlightPerAddress
     }
@@ -201,13 +207,16 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
    * client thread instead of task execution thread which takes care of majority of the block
    * pushes.
    */
-  private def sendRequest(request: PushRequest): Unit = {
-    bytesInFlight +=  request.size
-    reqsInFlight += 1
-    numBlocksInFlightPerAddress(request.address) = numBlocksInFlightPerAddress.getOrElseUpdate(
-      request.address, 0) + request.blocks.length
+  private def sendRequest(
+      request: PushRequest,
+      requestsInfo: PushRequestsInfo): Unit = {
+    lock.synchronized {
+      bytesInFlight += request.size
+      reqsInFlight += 1
+      numBlocksInFlightPerAddress(request.address) = numBlocksInFlightPerAddress.getOrElseUpdate(
+        request.address, 0) + request.blocks.length
+    }
 
-    val sizeMap = request.blocks.map { case (blockId, size) => (blockId.toString, size) }.toMap
     val address = request.address
     val blockIds = request.blocks.map(_._1.toString)
     val remainingBlocks = new HashSet[String]() ++= blockIds
@@ -223,12 +232,21 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
       // Once the blockPushListener is notified of the block push success or failure, we
       // just delegate it to block-push-threads.
       def handleResult(result: PushResult): Unit = {
-        submitTask(() => {
-          if (updateStateAndCheckIfPushMore(
-            sizeMap(result.blockId), address, remainingBlocks, result)) {
-            pushUpToMax()
+        try {
+          remainingBlocks -= result.blockId
+          if (remainingBlocks.isEmpty) {
+            submitTask(() => {
+              if (updateStateAndCheckIfPushMore(
+                request.size, request.blocks.size, address, result, requestsInfo)) {
+                pushUpToMax()
+              }
+            })
           }
-        })
+        } catch {
+          case e: RejectedExecutionException =>
+            logWarning("Block push task submission failed. The thread pool might have been " +
+              "terminated. Stop pushing more blocks.")
+        }
       }
 
       override def onBlockFetchSuccess(blockId: String, data: ManagedBuffer): Unit = {
@@ -246,9 +264,11 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
         handleResult(PushResult(blockId, exception))
       }
     }
+    val blocksToPush = Utils.randomize(blockIds.zip(
+      sliceReqBufferIntoBlockBuffers(request.reqBuffer, request.blocks.map(_._2))))
     SparkEnv.get.blockManager.blockStoreClient.pushBlocks(
-      address.host, address.port, blockIds.toArray,
-      sliceReqBufferIntoBlockBuffers(request.reqBuffer, request.blocks.map(_._2)),
+      address.host, address.port, blocksToPush.map(_._1).toArray,
+      blocksToPush.map(_._2).toArray,
       blockPushListener)
   }
 
@@ -292,19 +312,18 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
    *
    * @param bytesPushed     number of bytes pushed.
    * @param address         address of the remote service
-   * @param remainingBlocks remaining blocks
    * @param pushResult      result of the last push
    * @return true if more blocks should be pushed; false otherwise.
    */
   private def updateStateAndCheckIfPushMore(
       bytesPushed: Long,
+      blocksPushed: Int,
       address: BlockManagerId,
-      remainingBlocks: HashSet[String],
-      pushResult: PushResult): Boolean = synchronized {
-    remainingBlocks -= pushResult.blockId
-    bytesInFlight -= bytesPushed
-    numBlocksInFlightPerAddress(address) = numBlocksInFlightPerAddress(address) - 1
-    if (remainingBlocks.isEmpty) {
+      pushResult: PushResult,
+      requestInfo: PushRequestsInfo): Boolean = requestInfo.synchronized {
+    lock.synchronized {
+      bytesInFlight -= bytesPushed
+      numBlocksInFlightPerAddress(address) = numBlocksInFlightPerAddress(address) - blocksPushed
       reqsInFlight -= 1
     }
     if (pushResult.failure != null && pushResult.failure.getCause.isInstanceOf[ConnectException]) {
@@ -314,8 +333,8 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
       if (!unreachableBlockMgrs.contains(address)) {
         var removed = 0
         unreachableBlockMgrs.add(address)
-        removed += pushRequests.dequeueAll(req => req.address == address).length
-        removed += deferredPushRequests.remove(address).map(_.length).getOrElse(0)
+        removed += requestInfo.requests.dequeueAll(req => req.address == address).length
+        removed += requestInfo.deferredRequests.remove(address).map(_.length).getOrElse(0)
         logWarning(s"Received a ConnectException from $address. " +
           s"Dropping $removed push-requests and " +
           s"not pushing any more blocks to this address.")
@@ -326,12 +345,13 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
         s"stop.")
       return false
     } else {
-      if (remainingBlocks.isEmpty && pushRequests.isEmpty && deferredPushRequests.isEmpty &&
-        !pushCompletionNotified) {
-        notifyDriverAboutPushCompletion()
-        pushCompletionNotified = true
+      if (requestInfo.requests.isEmpty &&
+        requestInfo.deferredRequests.isEmpty && !requestInfo.pushCompletionNotified) {
+        REQUEST.remove((requestInfo.shuffleId, requestInfo.mapIndex))
+        notifyDriverAboutPushCompletion(requestInfo.shuffleId, requestInfo.mapIndex)
+        requestInfo.pushCompletionNotified = true
       }
-      remainingBlocks.isEmpty && (pushRequests.nonEmpty || deferredPushRequests.nonEmpty)
+      !REQUEST.isEmpty
     }
   }
 
@@ -341,7 +361,7 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
    * as sufficient map tasks have completed push instead of always waiting for a fixed amount of
    * time.
    */
-  private def notifyDriverAboutPushCompletion(): Unit = {
+  private def notifyDriverAboutPushCompletion(shuffleId: Int, mapIndex: Int): Unit = {
     assert(shuffleId >= 0 && mapIndex >= 0)
     val msg = ShufflePushCompletion(shuffleId, mapIndex)
     SparkEnv.driverRpcEndpoint match {
@@ -453,6 +473,14 @@ private[spark] class ShuffleBlockPusher(conf: SparkConf) extends Logging {
 
 private[spark] object ShuffleBlockPusher {
 
+  private val lock = new Object()
+
+  private val REQUEST = new ConcurrentHashMap[(Int, Int), PushRequestsInfo]()
+
+  private var bytesInFlight = 0L
+  private var reqsInFlight = 0
+  private val numBlocksInFlightPerAddress = new HashMap[BlockManagerId, Int]()
+
   /**
    * A request to push blocks to a remote shuffle service
    * @param address remote shuffle service location to push blocks to
@@ -467,6 +495,16 @@ private[spark] object ShuffleBlockPusher {
     val size = blocks.map(_._2).sum
   }
 
+  private[spark] case class PushRequestsInfo(
+    shuffleId: Int,
+    mapIndex: Int,
+    requests: Queue[PushRequest],
+    deferredRequests: HashMap[BlockManagerId, Queue[PushRequest]],
+    var pushCompletionNotified: Boolean) {
+
+    def nonEmpty: Boolean = requests.nonEmpty || deferredRequests.nonEmpty
+  }
+
   /**
    * Result of the block push.
    * @param blockId blockId
