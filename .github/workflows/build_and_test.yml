#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: Build and test

on:
  workflow_call:
    inputs:
      java:
        required: false
        type: string
        default: 17
      branch:
        description: Branch to run the build against
        required: false
        type: string
        # Change 'master' to 'branch-4.0' in branch-4.0 branch after cutting it.
        default: master
      hadoop:
        description: Hadoop version to run with. HADOOP_PROFILE environment variable should accept it.
        required: false
        type: string
        default: hadoop3
      envs:
        description: Additional environment variables to set when running the tests. Should be in JSON format.
        required: false
        type: string
        default: '{}'
      jobs:
        description: >-
          Jobs to run, and should be in JSON format. The values should be matched with the job's key defined
          in this file, e.g., build. See precondition job below.
        required: false
        type: string
        default: ''
jobs:
  precondition:
    name: Check changes
    runs-on: ubuntu-22.04
    env:
      GITHUB_PREV_SHA: ${{ github.event.before }}
    outputs:
      required: ${{ steps.set-outputs.outputs.required }}
      image_url: ${{ steps.infra-image-outputs.outputs.image_url }}
    steps:
    - name: Checkout Spark repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        repository: apache/spark
        ref: ${{ inputs.branch }}
    - name: Sync the current branch with the latest in Apache Spark
      if: github.repository != 'apache/spark'
      run: |
        echo "APACHE_SPARK_REF=$(git rev-parse HEAD)" >> $GITHUB_ENV
        git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit" --allow-empty
    - name: Check all modules
      id: set-outputs
      run: |
        if [ -z "${{ inputs.jobs }}" ]; then
          pyspark=true; sparkr=true; tpcds=true; docker=true;
          pyspark_modules=`cd dev && python -c "import sparktestsupport.modules as m; print(','.join(m.name for m in m.all_modules if m.name.startswith('pyspark')))"`
          pyspark=`./dev/is-changed.py -m $pyspark_modules`
          sparkr=`./dev/is-changed.py -m sparkr`
          tpcds=`./dev/is-changed.py -m sql`
          docker=`./dev/is-changed.py -m docker-integration-tests`
          # 'build' and 'java-other-versions' are always true for now.
          # It does not save significant time and most of PRs trigger the build.
          precondition="
            {
              \"build\": \"true\",
              \"pyspark\": \"$pyspark\",
              \"sparkr\": \"$sparkr\",
              \"tpcds-1g\": \"$tpcds\",
              \"docker-integration-tests\": \"$docker\",
              \"java-other-versions\": \"true\",
              \"lint\" : \"true\",
              \"k8s-integration-tests\" : \"true\",
              \"buf\" : \"true\",
              \"ui\" : \"true\",
            }"
          echo $precondition # For debugging
          # Remove `\n` to avoid "Invalid format" error
          precondition="${precondition//$'\n'/}}"
          echo "required=$precondition" >> $GITHUB_OUTPUT
        else
          # This is usually set by scheduled jobs.
          precondition='${{ inputs.jobs }}'
          echo $precondition # For debugging
          precondition="${precondition//$'\n'/}"
          echo "required=$precondition" >> $GITHUB_OUTPUT
        fi
    - name: Generate infra image URL
      id: infra-image-outputs
      run: |
        # Convert to lowercase to meet Docker repo name requirement
        REPO_OWNER=$(echo "${{ github.repository_owner }}" | tr '[:upper:]' '[:lower:]')
        IMG_NAME="apache-spark-ci-image:${{ inputs.branch }}-${{ github.run_id }}"
        IMG_URL="ghcr.io/$REPO_OWNER/$IMG_NAME"
        echo "image_url=$IMG_URL" >> $GITHUB_OUTPUT

  # Build: build Spark and run the tests for specified modules.
  build:
    name: "Build modules: ${{ matrix.modules }} ${{ matrix.comment }}"
    needs: precondition
    if: fromJson(needs.precondition.outputs.required).build == 'true'
    runs-on: ubuntu-22.04
    timeout-minutes: 300
    strategy:
      fail-fast: false
      matrix:
        java:
          - ${{ inputs.java }}
        hadoop:
          - ${{ inputs.hadoop }}
        hive:
          - hive2.3
        # TODO(SPARK-32246): We don't test 'streaming-kinesis-asl' for now.
        # Kinesis tests depends on external Amazon kinesis service.
        # Note that the modules below are from sparktestsupport/modules.py.
        modules:
          - >-
            core, unsafe, kvstore, avro, utils,
            network-common, network-shuffle, repl, launcher,
            examples, sketch
          - >-
            api, catalyst, hive-thriftserver
          - >-
            mllib-local, mllib, graphx
          - >-
            streaming, sql-kafka-0-10, streaming-kafka-0-10, streaming-kinesis-asl,
            yarn, kubernetes, hadoop-cloud, spark-ganglia-lgpl,
            connect, protobuf
        # Here, we split Hive and SQL tests into some of slow ones and the rest of them.
        included-tags: [""]
        excluded-tags: [""]
        comment: [""]
        include:
          # Hive tests
          - modules: hive
            java: ${{ inputs.java }}
            hadoop: ${{ inputs.hadoop }}
            hive: hive2.3
            included-tags: org.apache.spark.tags.SlowHiveTest
            comment: "- slow tests"
          - modules: hive
            java: ${{ inputs.java }}
            hadoop: ${{ inputs.hadoop }}
            hive: hive2.3
            excluded-tags: org.apache.spark.tags.SlowHiveTest
            comment: "- other tests"
          # SQL tests
          - modules: sql
            java: ${{ inputs.java }}
            hadoop: ${{ inputs.hadoop }}
            hive: hive2.3
            included-tags: org.apache.spark.tags.ExtendedSQLTest
            comment: "- extended tests"
          - modules: sql
            java: ${{ inputs.java }}
            hadoop: ${{ inputs.hadoop }}
            hive: hive2.3
            included-tags: org.apache.spark.tags.SlowSQLTest
            comment: "- slow tests"
          - modules: sql
            java: ${{ inputs.java }}
            hadoop: ${{ inputs.hadoop }}
            hive: hive2.3
            excluded-tags: org.apache.spark.tags.ExtendedSQLTest,org.apache.spark.tags.SlowSQLTest
            comment: "- other tests"
    env:
      MODULES_TO_TEST: ${{ matrix.modules }}
      EXCLUDED_TAGS: ${{ matrix.excluded-tags }}
      INCLUDED_TAGS: ${{ matrix.included-tags }}
      HADOOP_PROFILE: ${{ matrix.hadoop }}
      HIVE_PROFILE: ${{ matrix.hive }}
      GITHUB_PREV_SHA: ${{ github.event.before }}
      SPARK_LOCAL_IP: localhost
      SKIP_UNIDOC: true
      SKIP_MIMA: true
      SKIP_PACKAGING: true
    steps:
    - name: Checkout Spark repository
      uses: actions/checkout@v4
      # In order to fetch changed files
      with:
        fetch-depth: 0
        repository: apache/spark
        ref: ${{ inputs.branch }}
    - name: Sync the current branch with the latest in Apache Spark
      if: github.repository != 'apache/spark'
      run: |
        echo "APACHE_SPARK_REF=$(git rev-parse HEAD)" >> $GITHUB_ENV
        git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit" --allow-empty
    # Cache local repositories. Note that GitHub Actions cache has a 2G limit.
    - name: Cache Scala, SBT and Maven
      uses: actions/cache@v3
      with:
        path: |
          build/apache-maven-*
          build/scala-*
          build/*.jar
          ~/.sbt
        key: build-${{ hashFiles('**/pom.xml', 'project/build.properties', 'build/mvn', 'build/sbt', 'build/sbt-launch-lib.bash', 'build/spark-build-info') }}
        restore-keys: |
          build-
    - name: Cache Coursier local repository
      uses: actions/cache@v3
      with:
        path: ~/.cache/coursier
        key: ${{ matrix.java }}-${{ matrix.hadoop }}-coursier-${{ hashFiles('**/pom.xml', '**/plugins.sbt') }}
        restore-keys: |
          ${{ matrix.java }}-${{ matrix.hadoop }}-coursier-
    - name: Free up disk space
      run: |
        if [ -f ./dev/free_disk_space ]; then
          ./dev/free_disk_space
        fi
    - name: Install Java ${{ matrix.java }}
      uses: actions/setup-java@v4
      with:
        distribution: zulu
        java-version: ${{ matrix.java }}
    - name: Install Python 3.9
      uses: actions/setup-python@v5
      # We should install one Python that is higher than 3+ for SQL and Yarn because:
      # - SQL component also has Python related tests, for example, IntegratedUDFTestUtils.
      # - Yarn has a Python specific test too, for example, YarnClusterSuite.
      if: contains(matrix.modules, 'yarn') || (contains(matrix.modules, 'sql') && !contains(matrix.modules, 'sql-')) || contains(matrix.modules, 'connect')
      with:
        python-version: '3.9'
        architecture: x64
    - name: Install Python packages (Python 3.9)
      if: (contains(matrix.modules, 'sql') && !contains(matrix.modules, 'sql-')) || contains(matrix.modules, 'connect')
      run: |
        python3.9 -m pip install 'numpy>=1.20.0' pyarrow pandas scipy unittest-xml-reporting 'lxml==4.9.4' 'grpcio==1.59.3' 'grpcio-status==1.59.3' 'protobuf==4.25.1'
        python3.9 -m pip list
    # Run the tests.
    - name: Run tests
      env: ${{ fromJSON(inputs.envs) }}
      shell: 'script -q -e -c "bash {0}"'
      run: |
        # Fix for TTY related issues when launching the Ammonite REPL in tests.
        export TERM=vt100
        # Hive "other tests" test needs larger metaspace size based on experiment.
        if [[ "$MODULES_TO_TEST" == "hive" ]] && [[ "$EXCLUDED_TAGS" == "org.apache.spark.tags.SlowHiveTest" ]]; then export METASPACE_SIZE=2g; fi
        # SPARK-46283: should delete the following env replacement after SPARK 3.x EOL
        if [[ "$MODULES_TO_TEST" == *"streaming-kinesis-asl"* ]] && [[ "${{ inputs.branch }}" =~ ^branch-3 ]]; then 
          MODULES_TO_TEST=${MODULES_TO_TEST//streaming-kinesis-asl, /}
        fi
        export SERIAL_SBT_TESTS=1
        ./dev/run-tests --parallelism 1 --modules "$MODULES_TO_TEST" --included-tags "$INCLUDED_TAGS" --excluded-tags "$EXCLUDED_TAGS"
    - name: Upload test results to report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.modules }}-${{ matrix.comment }}-${{ matrix.java }}-${{ matrix.hadoop }}-${{ matrix.hive }}
        path: "**/target/test-reports/*.xml"
    - name: Upload unit tests log files
      if: ${{ !success() }}
      uses: actions/upload-artifact@v3
      with:
        name: unit-tests-log-${{ matrix.modules }}-${{ matrix.comment }}-${{ matrix.java }}-${{ matrix.hadoop }}-${{ matrix.hive }}
        path: "**/target/unit-tests.log"

  infra-image:
    name: "Base image build"
    needs: precondition
    # Currently, enable docker build from cache for `master` and branch (since 3.4) jobs
    if: >-
      fromJson(needs.precondition.outputs.required).pyspark == 'true' ||
      fromJson(needs.precondition.outputs.required).lint == 'true' ||
      fromJson(needs.precondition.outputs.required).sparkr == 'true'
    runs-on: ubuntu-latest
    permissions:
      packages: write
    steps:
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout Spark repository
        uses: actions/checkout@v4
        # In order to fetch changed files
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: ${{ inputs.branch }}
      - name: Sync the current branch with the latest in Apache Spark
        if: github.repository != 'apache/spark'
        run: |
          echo "APACHE_SPARK_REF=$(git rev-parse HEAD)" >> $GITHUB_ENV
          git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit" --allow-empty
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build and push
        id: docker_build
        uses: docker/build-push-action@v5
        with:
          context: ./dev/infra/
          push: true
          tags: |
            ${{ needs.precondition.outputs.image_url }}
          # Use the infra image cache to speed up
          cache-from: type=registry,ref=ghcr.io/apache/spark/apache-spark-github-action-image-cache:${{ inputs.branch }}

  pyspark:
    needs: [precondition, infra-image]
    # always run if pyspark == 'true', even infra-image is skip (such as non-master job)
    if: (!cancelled()) && fromJson(needs.precondition.outputs.required).pyspark == 'true'
    name: "Build modules: ${{ matrix.modules }}"
    runs-on: ubuntu-22.04
    timeout-minutes: 300
    container:
      image: ${{ needs.precondition.outputs.image_url }}
    strategy:
      fail-fast: false
      matrix:
        java:
          - ${{ inputs.java }}
        modules:
          - >-
            pyspark-sql, pyspark-resource, pyspark-testing
          - >-
            pyspark-core, pyspark-errors, pyspark-streaming
          - >-
            pyspark-mllib, pyspark-ml, pyspark-ml-connect
          - >-
            pyspark-pandas
          - >-
            pyspark-pandas-slow
          - >-
            pyspark-connect
          - >-
            pyspark-pandas-connect-part0
          - >-
            pyspark-pandas-connect-part1
          - >-
            pyspark-pandas-connect-part2
          - >-
            pyspark-pandas-connect-part3
    env:
      MODULES_TO_TEST: ${{ matrix.modules }}
      PYTHON_TO_TEST: 'pypy3'
      HADOOP_PROFILE: ${{ inputs.hadoop }}
      HIVE_PROFILE: hive2.3
      GITHUB_PREV_SHA: ${{ github.event.before }}
      SPARK_LOCAL_IP: localhost
      SKIP_UNIDOC: true
      SKIP_MIMA: true
      SKIP_PACKAGING: true
      METASPACE_SIZE: 1g
      BRANCH: ${{ inputs.branch }}
    steps:
    - name: Checkout Spark repository
      uses: actions/checkout@v4
      # In order to fetch changed files
      with:
        fetch-depth: 0
        repository: apache/spark
        ref: ${{ inputs.branch }}
    - name: Add GITHUB_WORKSPACE to git trust safe.directory
      run: |
        git config --global --add safe.directory ${GITHUB_WORKSPACE}
    - name: Sync the current branch with the latest in Apache Spark
      if: github.repository != 'apache/spark'
      run: |
        echo "APACHE_SPARK_REF=$(git rev-parse HEAD)" >> $GITHUB_ENV
        git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit" --allow-empty
    # Cache local repositories. Note that GitHub Actions cache has a 2G limit.
    - name: Cache Scala, SBT and Maven
      uses: actions/cache@v3
      with:
        path: |
          build/apache-maven-*
          build/scala-*
          build/*.jar
          ~/.sbt
        key: build-${{ hashFiles('**/pom.xml', 'project/build.properties', 'build/mvn', 'build/sbt', 'build/sbt-launch-lib.bash', 'build/spark-build-info') }}
        restore-keys: |
          build-
    - name: Cache Coursier local repository
      uses: actions/cache@v3
      with:
        path: ~/.cache/coursier
        key: pyspark-coursier-${{ hashFiles('**/pom.xml', '**/plugins.sbt') }}
        restore-keys: |
          pyspark-coursier-
    - name: Free up disk space
      shell: 'script -q -e -c "bash {0}"'
      run: |
        if [[ "$MODULES_TO_TEST" != *"pyspark-ml"* ]] && [[ "$BRANCH" != "branch-3.5" ]]; then
          # uninstall libraries dedicated for ML testing
          python3.9 -m pip uninstall -y torch torchvision torcheval torchtnt tensorboard mlflow deepspeed
        fi
        if [ -f ./dev/free_disk_space_container ]; then
          ./dev/free_disk_space_container
        fi
    - name: Install Java ${{ matrix.java }}
      uses: actions/setup-java@v4
      with:
        distribution: zulu
        java-version: ${{ matrix.java }}
    - name: List Python packages (${{ env.PYTHON_TO_TEST }})
      env: ${{ fromJSON(inputs.envs) }}
      shell: 'script -q -e -c "bash {0}"'
      run: |
        for py in $(echo $PYTHON_TO_TEST | tr "," "\n")
        do
          echo $py
          $py -m pip list
        done
    - name: Install Conda for pip packaging test
      if: contains(matrix.modules, 'pyspark-errors')
      run: |
        curl -s https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > miniconda.sh
        bash miniconda.sh -b -p $HOME/miniconda
        rm miniconda.sh
    # Run the tests.
    - name: Run tests
      env: ${{ fromJSON(inputs.envs) }}
      shell: 'script -q -e -c "bash {0}"'
      run: |
        if [[ "$MODULES_TO_TEST" == *"pyspark-errors"* ]]; then
          export PATH=$PATH:$HOME/miniconda/bin
          export SKIP_PACKAGING=false
          echo "Python Packaging Tests Enabled!"
        fi
        if [ ! -z "$PYTHON_TO_TEST" ]; then
          ./dev/run-tests --parallelism 1 --modules "$MODULES_TO_TEST" --python-executables "$PYTHON_TO_TEST"
        else
          # For branch-3.5 and below, it uses the default Python versions.
          ./dev/run-tests --parallelism 1 --modules "$MODULES_TO_TEST"
        fi
    - name: Upload coverage to Codecov
      if: fromJSON(inputs.envs).PYSPARK_CODECOV == 'true'
      uses: codecov/codecov-action@v2
      with:
        files: ./python/coverage.xml
        flags: unittests
        name: PySpark
    - name: Upload test results to report
      env: ${{ fromJSON(inputs.envs) }}
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.modules }}--${{ matrix.java }}-${{ inputs.hadoop }}-hive2.3
        path: "**/target/test-reports/*.xml"
    - name: Upload unit tests log files
      env: ${{ fromJSON(inputs.envs) }}
      if: ${{ !success() }}
      uses: actions/upload-artifact@v3
      with:
        name: unit-tests-log-${{ matrix.modules }}--${{ matrix.java }}-${{ inputs.hadoop }}-hive2.3
        path: "**/target/unit-tests.log"
