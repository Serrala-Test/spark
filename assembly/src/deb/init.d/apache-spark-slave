#!/bin/sh
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
#
# /etc/init.d/apache-spark-slave -- startup script for a Spark slave
#
### BEGIN INIT INFO
# Provides:          apache-spark-slave
# Required-Start:    $network $named
# Required-Stop:     $network $named
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: Starts a Apache Spark slave
# Description:       Starts a Apache Spark slave
### END INIT INFO

DESC="Apache Spark slave"
NAME=apache-spark-slave

export SPARK_HOME=/usr/share/apache-spark
export SPARK_PREFIX=$SPARK_HOME
export SPARK_CONF_DIR=/etc/apache-spark
export SPARK_LOG_DIR=/var/log/apache-spark
export SPARK_PID_DIR=/var/run/
export SPARK_WORKER_DIR=/tmp/apache-spark-worker
export SPARK_USER=apache-spark
export SPARK_GROUP=apache-spark
export SPARK_IDENT_STRING=$SPARK_USER

# ensure we don't try compute incorrect path
export SPARK_CONFIG_LOADED=1

if [ `id -u` -ne 0 ]; then
	echo "You need root privileges to run this script"
	exit 1
fi

. /lib/lsb/init-functions

if [ -r /etc/default/rcS ]; then
	. /etc/default/rcS
fi

. "$SPARK_HOME/bin/load-spark-env.sh"

set -a
[ -f /etc/default/apache-spark ] && . /etc/default/apache-spark
set -a

if [ "$SPARK_MASTER_PORT" = "" ]; then
  SPARK_MASTER_PORT=7077
fi
if [ "$SPARK_MASTER_IP" = "" ]; then
  SPARK_MASTER_IP=`hostname`
fi

case "$1" in
  start)
    #make sure the worker dir exists and has the proper rights
    mkdir $SPARK_WORKER_DIR
    chown -R $SPARK_USER:$SPARK_GROUP $SPARK_WORKER_DIR

    if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
	  log_daemon_msg "Starting Spark slave"
      "$SPARK_HOME/sbin/spark-daemon.sh" start org.apache.spark.deploy.worker.Worker 1 spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT
    else
      i=0
      while [ $i -lt $SPARK_WORKER_INSTANCES ]; do
        i=`expr $i + 1`
      	log_daemon_msg "Starting Spark slave $i"
        "$SPARK_HOME/sbin/spark-daemon.sh" start org.apache.spark.deploy.worker.Worker $i spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT
      done
    fi
	log_end_msg $?
	;;
  stop)
    if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
	  log_daemon_msg "Stopping Spark slave"
      "$SPARK_HOME/sbin/spark-daemon.sh" stop org.apache.spark.deploy.worker.Worker 1
    else
      i=0
      while [ $i -lt $SPARK_WORKER_INSTANCES ]; do
        i=`expr $i + 1`
        log_daemon_msg "Stopping Spark slave $i"
        "$SPARK_HOME/sbin/spark-daemon.sh" stop org.apache.spark.deploy.worker.Worker $i
      done
    fi
	log_end_msg 0
	;;
  *)
	log_success_msg "Usage: $0 {start|stop}"
	exit 1
	;;
esac

exit 0
