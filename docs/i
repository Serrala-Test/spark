---
layout: global
title: Matrix Factorization - MLlib
displayTitle: <a href="mllib-guide.html">MLlib</a> - Matrix Factorization
---

* Table of contents
{:toc}

#LU Factorization


The LU decomposition is a well studied algorithm for decomposing a matrix into a lower diagonal matrix $L$ and an upper diagonal matrix $U$.  

`\begin{equation}
    PA = LU\\
        P \begin{pmatrix}
            a_{11}&\cdots  &a_{1n} \\ 
             \vdots& \ddots &\vdots \\ 
             a_{m1}&\cdots  & a_{mn}
        \end{pmatrix} = 
        \begin{pmatrix}
            \ell_{11}&\ 0    &0 \\ 
             \vdots& \ddots &0 \\ 
             \ell_{m1}&\cdots  & \ell_{mn}
            \end{pmatrix}
            \begin{pmatrix}
               0&\ \cdots    &u_{1n} \\ 
             \vdots& \ddots &\vdots\\ 
            0&\cdots  & u_{mn}
        \end{pmatrix},
        \label{eq:generalLUFactorization}
\end{equation}`

where the $P$ is a row permutation matrix, which is needed to circumvent issues that arise when rebuilding our solution iteratively.The LU decomposition is a well studied algorithm for decomposing a matrix into a lower diagonal matrix $L$ and an upper diagonal matrix $U$.  

JNDB checking equation reference syntax `$\eqref{eq:generalLUFactorization}$` 

This algorithm is a highly stable method for inverting a matrix and solving linear systems of equations that appear in Machine Learning and other models, and are usually solved with SGD, BGFS, or other gradient based methods.  Being able to solve these equations at scale to numerical precision rather than to convergence should open up possiblities for new algorithms within MLlib as in other applications.

Once the decomposition is computed, the inverse is straightforward to compute (this will be addressed in a separate JIRA).  

The LU decomposition of $A$ can be written in 4 block form as:
`\begin{align}
    PA & = LU\\
        \begin{pmatrix}
            P_1 A_{11}&P_1 A_{12} \\ 
            P_2 A_{21}&P_2 A_{22}
        \end{pmatrix} 
        & = \begin{pmatrix}
            L_{11}&0 \\ 
            L_{21}&L_{22}
            \end{pmatrix}
            \begin{pmatrix}
                U_{11}&U_{12} \\ 
                0&U_{22}
        \end{pmatrix} \\
        & =         \begin{pmatrix}
            L_{11}U_{11}&L_{11}U_{12} \\ 
            L_{21}U_{11}&L_{21}U_{12}+L_{22}U_{22}
            \end{pmatrix}
        \label{eq:basicLUBlockDecomposition}
\end{align}`

Once the blocks are defined, we can then solve each matrix quadrant indiviudually.  `$\text{LU}_{LOCAL}(M)$` is the local LU factorization routine that returns $P$,$L$, and $U$. 

`\begin{align}
P_1 A_{11} & = L_{11}U_{11}               & \Rightarrow  & (P_{1},L_{11},U_{11})  & = & \text{LU}_{LOCAL}(A_{11}) \label{eq:A11Solve} \\
P_1 A_{12} & = L_{11}U_{12}               & \Rightarrow  & U_{12}               & = & L_{11}^{-1}P_1 A_{12} \label{eq:U12Solve} \\
P_2 A_{21} & = L_{21}U_{11}               & \Rightarrow  & L_{21}               & = &P_2^{-1} A_{21}U_{11}^{-1} \label{eq:L21Solve}\\
P_2 A_{22} & = L_{21}U_{12}+L_{22}U_{22}  & \Rightarrow  & (P_{2},L_{22},U_{22})  & = & \text{LU}_{RECURSIVE}(S) \label{eq:A22Solve}\\
\end{align}`

where
 
`\begin{equation}
S = A_{22} - L_{21} U_{12} 
\end{equation}`

is known as the Schur Complement.  An equivalent form of the Schur complement can be used if one wishes to compute `$\eqref$LU_{LOCAL}(S)$ independendly of the $A_{11}$ calculation with no dependency onies, which increases parallelism slightly at the expense of recomputing the inverse of $A_{11}$ on a separate process.
Brief discussion on P_2...why it is not needed because of recursive solve, etc..

`\begin{equation}
S=A_{22}-A_{21}A_{11}^{-1}A_{12}
\end{equation}`

$A_{11}$ is chosen to be a single block, so that the Breeze library can be called to carry out the $LU_{LOCAL}(A_{11})$ calculation. JN don't for get to label equations!!   The Schur Complement is constructed with BlockMatrix multiply operations, and the resulting submatrix is used in the next iteration to compute the LU decomposition until S is a single block.

Instead of building the solution incrementally, and using smaller block matrix operations to compute (L and U eqs), we construct large BlockMatrix structures, and carry out the multiplication at the end of the calculation.  This should leverage the optimizations present in the BlockMatrix routine more effectively.  The matrices formed to carry out the operations are described int the in the figure below.

<p style="text-align: center;">
  <img src="img/lu-factorization.png"
       title="LU Algorithm Description"
       alt="LU"
       width="100%" />
  <!-- Images are downsized intentionally to improve quality on retina displays -->
</p>
