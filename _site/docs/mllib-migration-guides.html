<p>The migration guide for the current Spark version is kept on the <a href="mllib-guide.html#migration-guide">MLlib Programming Guide main page</a>.</p>

<h2 id="from-11-to-12">From 1.1 to 1.2</h2>

<p>The only API changes in MLlib v1.2 are in
<a href="api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree"><code>DecisionTree</code></a>,
which continues to be an experimental API in MLlib 1.2:</p>

<ol>
  <li>
    <p><em>(Breaking change)</em> The Scala API for classification takes a named argument specifying the number
of classes.  In MLlib v1.1, this argument was called <code>numClasses</code> in Python and
<code>numClassesForClassification</code> in Scala.  In MLlib v1.2, the names are both set to <code>numClasses</code>.
This <code>numClasses</code> parameter is specified either via
<a href="api/scala/index.html#org.apache.spark.mllib.tree.configuration.Strategy"><code>Strategy</code></a>
or via <a href="api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree"><code>DecisionTree</code></a>
static <code>trainClassifier</code> and <code>trainRegressor</code> methods.</p>
  </li>
  <li>
    <p><em>(Breaking change)</em> The API for
<a href="api/scala/index.html#org.apache.spark.mllib.tree.model.Node"><code>Node</code></a> has changed.
This should generally not affect user code, unless the user manually constructs decision trees
(instead of using the <code>trainClassifier</code> or <code>trainRegressor</code> methods).
The tree <code>Node</code> now includes more information, including the probability of the predicted label
(for classification).</p>
  </li>
  <li>
    <p>Printing methodsâ€™ output has changed.  The <code>toString</code> (Scala/Java) and <code>__repr__</code> (Python) methods used to print the full model; they now print a summary.  For the full model, use <code>toDebugString</code>.</p>
  </li>
</ol>

<p>Examples in the Spark distribution and examples in the
<a href="mllib-decision-tree.html#examples">Decision Trees Guide</a> have been updated accordingly.</p>

<h2 id="from-10-to-11">From 1.0 to 1.1</h2>

<p>The only API changes in MLlib v1.1 are in
<a href="api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree"><code>DecisionTree</code></a>,
which continues to be an experimental API in MLlib 1.1:</p>

<ol>
  <li>
    <p><em>(Breaking change)</em> The meaning of tree depth has been changed by 1 in order to match
the implementations of trees in
<a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree">scikit-learn</a>
and in <a href="http://cran.r-project.org/web/packages/rpart/index.html">rpart</a>.
In MLlib v1.0, a depth-1 tree had 1 leaf node, and a depth-2 tree had 1 root node and 2 leaf nodes.
In MLlib v1.1, a depth-0 tree has 1 leaf node, and a depth-1 tree has 1 root node and 2 leaf nodes.
This depth is specified by the <code>maxDepth</code> parameter in
<a href="api/scala/index.html#org.apache.spark.mllib.tree.configuration.Strategy"><code>Strategy</code></a>
or via <a href="api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree"><code>DecisionTree</code></a>
static <code>trainClassifier</code> and <code>trainRegressor</code> methods.</p>
  </li>
  <li>
    <p><em>(Non-breaking change)</em> We recommend using the newly added <code>trainClassifier</code> and <code>trainRegressor</code>
methods to build a <a href="api/scala/index.html#org.apache.spark.mllib.tree.DecisionTree"><code>DecisionTree</code></a>,
rather than using the old parameter class <code>Strategy</code>.  These new training methods explicitly
separate classification and regression, and they replace specialized parameter types with
simple <code>String</code> types.</p>
  </li>
</ol>

<p>Examples of the new, recommended <code>trainClassifier</code> and <code>trainRegressor</code> are given in the
<a href="mllib-decision-tree.html#examples">Decision Trees Guide</a>.</p>

<h2 id="from-09-to-10">From 0.9 to 1.0</h2>

<p>In MLlib v1.0, we support both dense and sparse input in a unified way, which introduces a few
breaking changes.  If your data is sparse, please store it in a sparse format instead of dense to
take advantage of sparsity in both storage and computation. Details are described below.</p>

