<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (version 1.7.0_71) on Thu Apr 02 14:27:55 PDT 2015 -->
<title>Utils (Spark 1.4.0 JavaDoc)</title>
<meta name="date" content="2015-04-02">
<link rel="stylesheet" type="text/css" href="../../../../stylesheet.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="Utils (Spark 1.4.0 JavaDoc)";
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/util/TimeStampedWeakValueHashMap.html" title="class in org.apache.spark.util"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/util/Vector.html" title="class in org.apache.spark.util"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/util/Utils.html" target="_top">Frames</a></li>
<li><a href="Utils.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.apache.spark.util</div>
<h2 title="Class Utils" class="title">Class Utils</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>Object</li>
<li>
<ul class="inheritance">
<li>org.apache.spark.util.Utils</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<dl>
<dt>All Implemented Interfaces:</dt>
<dd><a href="../../../../org/apache/spark/Logging.html" title="interface in org.apache.spark">Logging</a></dd>
</dl>
<hr>
<br>
<pre>public class <span class="strong">Utils</span>
extends Object
implements <a href="../../../../org/apache/spark/Logging.html" title="interface in org.apache.spark">Logging</a></pre>
<div class="block">Various utility methods used by Spark.</div>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#Utils()">Utils</a></strong>()</code>&nbsp;</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method_summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span>Methods</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#bytesToString(long)">bytesToString</a></strong>(long&nbsp;size)</code>
<div class="block">Convert a quantity in bytes to a human-readable string such as "4.0 MB".</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#checkHost(java.lang.String,%20java.lang.String)">checkHost</a></strong>(String&nbsp;host,
         String&nbsp;message)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#checkHostPort(java.lang.String,%20java.lang.String)">checkHostPort</a></strong>(String&nbsp;hostPort,
             String&nbsp;message)</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#chmod700(java.io.File)">chmod700</a></strong>(java.io.File&nbsp;file)</code>
<div class="block">JDK equivalent of <code>chmod 700 file</code>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static Class&lt;?&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#classForName(java.lang.String)">classForName</a></strong>(String&nbsp;className)</code>
<div class="block">Preferred alternative to Class.forName(className)</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#classIsLoadable(java.lang.String)">classIsLoadable</a></strong>(String&nbsp;clazz)</code>
<div class="block">Determines whether the provided class is loadable in the current thread.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#clearLocalRootDirs()">clearLocalRootDirs</a></strong>()</code>
<div class="block">Used by unit tests.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;T</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#clone(T,%20org.apache.spark.serializer.SerializerInstance,%20scala.reflect.ClassTag)">clone</a></strong>(T&nbsp;value,
     <a href="../../../../org/apache/spark/serializer/SerializerInstance.html" title="class in org.apache.spark.serializer">SerializerInstance</a>&nbsp;serializer,
     scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$2)</code>
<div class="block">Clone an object using a Spark serializer.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#configTestLog4j(java.lang.String)">configTestLog4j</a></strong>(String&nbsp;level)</code>
<div class="block">config a log4j properties used for testsuite</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static java.net.URI</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#constructURIForAuthentication(java.net.URI,%20org.apache.spark.SecurityManager)">constructURIForAuthentication</a></strong>(java.net.URI&nbsp;uri,
                             <a href="../../../../org/apache/spark/SecurityManager.html" title="class in org.apache.spark">SecurityManager</a>&nbsp;securityMgr)</code>
<div class="block">Construct a URI container information used for authentication.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static long</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#copyStream(java.io.InputStream,%20java.io.OutputStream,%20boolean,%20boolean)">copyStream</a></strong>(java.io.InputStream&nbsp;in,
          java.io.OutputStream&nbsp;out,
          boolean&nbsp;closeStreams,
          boolean&nbsp;transferToEnabled)</code>
<div class="block">Copy all data from an InputStream to an OutputStream.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static java.io.File</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#createDirectory(java.lang.String,%20java.lang.String)">createDirectory</a></strong>(String&nbsp;root,
               String&nbsp;namePrefix)</code>
<div class="block">Create a directory inside the given parent directory.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static java.io.File</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#createTempDir(java.lang.String,%20java.lang.String)">createTempDir</a></strong>(String&nbsp;root,
             String&nbsp;namePrefix)</code>
<div class="block">Create a temporary directory inside the given parent directory.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#deleteRecursively(java.io.File)">deleteRecursively</a></strong>(java.io.File&nbsp;file)</code>
<div class="block">Delete a file or directory and its contents recursively.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#deleteRecursively(tachyon.client.TachyonFile,%20tachyon.client.TachyonFS)">deleteRecursively</a></strong>(tachyon.client.TachyonFile&nbsp;dir,
                 tachyon.client.TachyonFS&nbsp;client)</code>
<div class="block">Delete a file or directory and its contents recursively.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;T</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#deserialize(byte[])">deserialize</a></strong>(byte[]&nbsp;bytes)</code>
<div class="block">Deserialize an object using Java serialization</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;T</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#deserialize(byte[],%20java.lang.ClassLoader)">deserialize</a></strong>(byte[]&nbsp;bytes,
           ClassLoader&nbsp;loader)</code>
<div class="block">Deserialize an object using Java serialization and the given ClassLoader</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static long</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#deserializeLongValue(byte[])">deserializeLongValue</a></strong>(byte[]&nbsp;bytes)</code>
<div class="block">Deserialize a Long value (used for <code>PythonPartitioner</code>)</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#deserializeViaNestedStream(java.io.InputStream,%20org.apache.spark.serializer.SerializerInstance,%20scala.Function1)">deserializeViaNestedStream</a></strong>(java.io.InputStream&nbsp;is,
                          <a href="../../../../org/apache/spark/serializer/SerializerInstance.html" title="class in org.apache.spark.serializer">SerializerInstance</a>&nbsp;ser,
                          scala.Function1&lt;<a href="../../../../org/apache/spark/serializer/DeserializationStream.html" title="class in org.apache.spark.serializer">DeserializationStream</a>,scala.runtime.BoxedUnit&gt;&nbsp;f)</code>
<div class="block">Deserialize via nested stream using specific serializer</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#doesDirectoryContainAnyNewFiles(java.io.File,%20long)">doesDirectoryContainAnyNewFiles</a></strong>(java.io.File&nbsp;dir,
                               long&nbsp;cutoff)</code>
<div class="block">Determines if a directory contains any files newer than cutoff seconds.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static org.json4s.JsonAST.JObject</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#emptyJson()">emptyJson</a></strong>()</code>
<div class="block">Return an empty JSON object</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#exceptionString(java.lang.Throwable)">exceptionString</a></strong>(Throwable&nbsp;e)</code>
<div class="block">Return a nice string representation of the exception.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#executeAndGetOutput(scala.collection.Seq,%20java.io.File,%20scala.collection.Map,%20boolean)">executeAndGetOutput</a></strong>(scala.collection.Seq&lt;String&gt;&nbsp;command,
                   java.io.File&nbsp;workingDir,
                   scala.collection.Map&lt;String,String&gt;&nbsp;extraEnvironment,
                   boolean&nbsp;redirectStderr)</code>
<div class="block">Execute a command and get its output, throwing an exception if it yields a code other than 0.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static Process</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#executeCommand(scala.collection.Seq,%20java.io.File,%20scala.collection.Map,%20boolean)">executeCommand</a></strong>(scala.collection.Seq&lt;String&gt;&nbsp;command,
              java.io.File&nbsp;workingDir,
              scala.collection.Map&lt;String,String&gt;&nbsp;extraEnvironment,
              boolean&nbsp;redirectStderr)</code>
<div class="block">Execute a command and return the process running the command.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static scala.Tuple2&lt;String,Object&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#extractHostPortFromSparkUrl(java.lang.String)">extractHostPortFromSparkUrl</a></strong>(String&nbsp;sparkUrl)</code>
<div class="block">Return a pair of host and port extracted from the <code>sparkUrl</code>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#fetchFile(java.lang.String,%20java.io.File,%20org.apache.spark.SparkConf,%20org.apache.spark.SecurityManager,%20org.apache.hadoop.conf.Configuration,%20long,%20boolean)">fetchFile</a></strong>(String&nbsp;url,
         java.io.File&nbsp;targetDir,
         <a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
         <a href="../../../../org/apache/spark/SecurityManager.html" title="class in org.apache.spark">SecurityManager</a>&nbsp;securityMgr,
         org.apache.hadoop.conf.Configuration&nbsp;hadoopConf,
         long&nbsp;timestamp,
         boolean&nbsp;useCache)</code>
<div class="block">Download a file or directory to target directory.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#fetchHcfsFile(org.apache.hadoop.fs.Path,%20java.io.File,%20org.apache.hadoop.fs.FileSystem,%20org.apache.spark.SparkConf,%20org.apache.hadoop.conf.Configuration,%20boolean,%20scala.Option)">fetchHcfsFile</a></strong>(org.apache.hadoop.fs.Path&nbsp;path,
             java.io.File&nbsp;targetDir,
             org.apache.hadoop.fs.FileSystem&nbsp;fs,
             <a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
             org.apache.hadoop.conf.Configuration&nbsp;hadoopConf,
             boolean&nbsp;fileOverwrite,
             scala.Option&lt;String&gt;&nbsp;filename)</code>
<div class="block">Fetch a file or directory from a Hadoop-compatible filesystem.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#formatWindowsPath(java.lang.String)">formatWindowsPath</a></strong>(String&nbsp;path)</code>
<div class="block">Format a Windows path such that it can be safely passed to a URI.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getAddressHostName(java.lang.String)">getAddressHostName</a></strong>(String&nbsp;address)</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static <a href="../../../../org/apache/spark/util/CallSite.html" title="class in org.apache.spark.util">CallSite</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getCallSite(scala.Function1)">getCallSite</a></strong>(scala.Function1&lt;String,Object&gt;&nbsp;skipClass)</code>
<div class="block">When called inside a class in the spark package, returns the name of the user code class
 (outside the spark package) that called into Spark, as well as which Spark method they called.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static ClassLoader</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getContextOrSparkClassLoader()">getContextOrSparkClassLoader</a></strong>()</code>
<div class="block">Get the Context ClassLoader on this thread or, if not present, the ClassLoader that
 loaded Spark.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getCurrentUserName()">getCurrentUserName</a></strong>()</code>
<div class="block">Returns the current user name.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getDefaultPropertiesFile(scala.collection.Map)">getDefaultPropertiesFile</a></strong>(scala.collection.Map&lt;String,String&gt;&nbsp;env)</code>
<div class="block">Return the path of the default Spark properties file.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static org.apache.hadoop.fs.Path</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getFilePath(java.io.File,%20java.lang.String)">getFilePath</a></strong>(java.io.File&nbsp;dir,
           String&nbsp;fileName)</code>
<div class="block">Return the absolute path of a file in the given directory.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getFormattedClassName(java.lang.Object)">getFormattedClassName</a></strong>(Object&nbsp;obj)</code>
<div class="block">Return the class name of the given object, removing all dollar signs</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static org.apache.hadoop.fs.FileSystem</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getHadoopFileSystem(java.lang.String,%20org.apache.hadoop.conf.Configuration)">getHadoopFileSystem</a></strong>(String&nbsp;path,
                   org.apache.hadoop.conf.Configuration&nbsp;conf)</code>
<div class="block">Return a Hadoop FileSystem with the scheme encoded in the given path.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static org.apache.hadoop.fs.FileSystem</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getHadoopFileSystem(java.net.URI,%20org.apache.hadoop.conf.Configuration)">getHadoopFileSystem</a></strong>(java.net.URI&nbsp;path,
                   org.apache.hadoop.conf.Configuration&nbsp;conf)</code>
<div class="block">Return a Hadoop FileSystem with the scheme encoded in the given path.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;long</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getIteratorSize(scala.collection.Iterator)">getIteratorSize</a></strong>(scala.collection.Iterator&lt;T&gt;&nbsp;iterator)</code>
<div class="block">Counts the number of elements of an iterator using a while loop rather than calling
 <code>TraversableOnce.size()</code> because it uses a for loop, which is slightly slower
 in the current version of Scala.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getLocalDir(org.apache.spark.SparkConf)">getLocalDir</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</code>
<div class="block">Get the path of a temporary directory.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static long</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getMaxResultSize(org.apache.spark.SparkConf)">getMaxResultSize</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getOrCreateLocalRootDirs(org.apache.spark.SparkConf)">getOrCreateLocalRootDirs</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</code>
<div class="block">Gets or creates the directories listed in spark.local.dir or SPARK_LOCAL_DIRS,
 and returns only the directories that exist / could be created.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static scala.collection.Map&lt;String,String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getPropertiesFromFile(java.lang.String)">getPropertiesFromFile</a></strong>(String&nbsp;filename)</code>
<div class="block">Load properties present in the given file.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static ClassLoader</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getSparkClassLoader()">getSparkClassLoader</a></strong>()</code>
<div class="block">Get the ClassLoader which loaded Spark.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getSparkOrYarnConfig(org.apache.spark.SparkConf,%20java.lang.String,%20java.lang.String)">getSparkOrYarnConfig</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                    String&nbsp;key,
                    String&nbsp;default_)</code>
<div class="block">Return the value of a config either through the SparkConf or the Hadoop configuration
 if this is Yarn mode.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static scala.Option&lt;String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getStderr(java.lang.Process,%20long)">getStderr</a></strong>(Process&nbsp;process,
         long&nbsp;timeoutMs)</code>
<div class="block">Return the stderr of a process after waiting for the process to terminate.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static scala.collection.Map&lt;String,String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getSystemProperties()">getSystemProperties</a></strong>()</code>
<div class="block">Returns the system properties map that is thread-safe to iterator over.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static <a href="../../../../org/apache/spark/util/ThreadStackTrace.html" title="class in org.apache.spark.util">ThreadStackTrace</a>[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getThreadDump()">getThreadDump</a></strong>()</code>
<div class="block">Return a thread dump of all threads' stacktraces.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#getUsedTimeMs(long)">getUsedTimeMs</a></strong>(long&nbsp;startTimeMs)</code>
<div class="block">Return the string to tell how long has passed in milliseconds.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#hasRootAsShutdownDeleteDir(java.io.File)">hasRootAsShutdownDeleteDir</a></strong>(java.io.File&nbsp;file)</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#hasRootAsShutdownDeleteDir(tachyon.client.TachyonFile)">hasRootAsShutdownDeleteDir</a></strong>(tachyon.client.TachyonFile&nbsp;file)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#hasShutdownDeleteDir(java.io.File)">hasShutdownDeleteDir</a></strong>(java.io.File&nbsp;file)</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#hasShutdownDeleteTachyonDir(tachyon.client.TachyonFile)">hasShutdownDeleteTachyonDir</a></strong>(tachyon.client.TachyonFile&nbsp;file)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#inShutdown()">inShutdown</a></strong>()</code>
<div class="block">Detect whether this thread might be executing a shutdown hook.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static Object</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#invoke(java.lang.Class,%20java.lang.Object,%20java.lang.String,%20scala.collection.Seq)">invoke</a></strong>(Class&lt;?&gt;&nbsp;clazz,
      Object&nbsp;obj,
      String&nbsp;methodName,
      scala.collection.Seq&lt;scala.Tuple2&lt;Class&lt;?&gt;,Object&gt;&gt;&nbsp;args)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#isBindCollision(java.lang.Throwable)">isBindCollision</a></strong>(Throwable&nbsp;exception)</code>
<div class="block">Return whether the exception is caused by an address-port collision when binding.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#isFatalError(java.lang.Throwable)">isFatalError</a></strong>(Throwable&nbsp;e)</code>
<div class="block">Returns true if the given exception was fatal.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#isMac()">isMac</a></strong>()</code>
<div class="block">Whether the underlying operating system is Mac OS X.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#isRunningInYarnContainer(org.apache.spark.SparkConf)">isRunningInYarnContainer</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#isSymlink(java.io.File)">isSymlink</a></strong>(java.io.File&nbsp;file)</code>
<div class="block">Check to see if file is a symbolic link.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#isTesting()">isTesting</a></strong>()</code>
<div class="block">Indicates whether Spark is currently running unit tests.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#isWindows()">isWindows</a></strong>()</code>
<div class="block">Whether the underlying operating system is Windows.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static scala.Option&lt;org.json4s.JsonAST.JValue&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#jsonOption(org.json4s.JsonAST.JValue)">jsonOption</a></strong>(org.json4s.JsonAST.JValue&nbsp;json)</code>
<div class="block">Return an option that translates JNothing to None</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#libraryPathEnvName()">libraryPathEnvName</a></strong>()</code>
<div class="block">Return the current system LD_LIBRARY_PATH name</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#libraryPathEnvPrefix(scala.collection.Seq)">libraryPathEnvPrefix</a></strong>(scala.collection.Seq&lt;String&gt;&nbsp;libraryPaths)</code>
<div class="block">Return the prefix of a command that appends the given library paths to the
 system-specific library path environment variable.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#loadDefaultSparkProperties(org.apache.spark.SparkConf,%20java.lang.String)">loadDefaultSparkProperties</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                          String&nbsp;filePath)</code>
<div class="block">Load default Spark properties from the given file.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#localHostName()">localHostName</a></strong>()</code>
<div class="block">Get the local machine's hostname.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#localIpAddress()">localIpAddress</a></strong>()</code>
<div class="block">Get the local host's IP address in dotted-quad format (e.g.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#localIpAddressHostname()">localIpAddressHostname</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;T</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#logUncaughtExceptions(scala.Function0)">logUncaughtExceptions</a></strong>(scala.Function0&lt;T&gt;&nbsp;f)</code>
<div class="block">Execute the given block, logging and re-throwing any uncaught exception.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#megabytesToString(long)">megabytesToString</a></strong>(long&nbsp;megabytes)</code>
<div class="block">Convert a quantity in megabytes to a human-readable string such as "4.0 MB".</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static int</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#memoryStringToMb(java.lang.String)">memoryStringToMb</a></strong>(String&nbsp;str)</code>
<div class="block">Convert a Java memory parameter passed to -Xmx (such as 300m or 1g) to a number of megabytes.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#msDurationToString(long)">msDurationToString</a></strong>(long&nbsp;ms)</code>
<div class="block">Returns a human-readable string representing a duration such as "35ms"</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static java.util.concurrent.ThreadFactory</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#namedThreadFactory(java.lang.String)">namedThreadFactory</a></strong>(String&nbsp;prefix)</code>
<div class="block">Create a thread factory that names threads with a prefix and also sets the threads to daemon.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static java.util.concurrent.ThreadPoolExecutor</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#newDaemonCachedThreadPool(java.lang.String)">newDaemonCachedThreadPool</a></strong>(String&nbsp;prefix)</code>
<div class="block">Wrapper over newCachedThreadPool.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static java.util.concurrent.ThreadPoolExecutor</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#newDaemonFixedThreadPool(int,%20java.lang.String)">newDaemonFixedThreadPool</a></strong>(int&nbsp;nThreads,
                        String&nbsp;prefix)</code>
<div class="block">Wrapper over newFixedThreadPool.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#nonLocalPaths(java.lang.String,%20boolean)">nonLocalPaths</a></strong>(String&nbsp;paths,
             boolean&nbsp;testWindows)</code>
<div class="block">Return all non-local paths from a comma-separated list of paths.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static int</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#nonNegativeHash(java.lang.Object)">nonNegativeHash</a></strong>(Object&nbsp;obj)</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static int</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#nonNegativeMod(int,%20int)">nonNegativeMod</a></strong>(int&nbsp;x,
              int&nbsp;mod)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#offsetBytes(scala.collection.Seq,%20long,%20long)">offsetBytes</a></strong>(scala.collection.Seq&lt;java.io.File&gt;&nbsp;files,
           long&nbsp;start,
           long&nbsp;end)</code>
<div class="block">Return a string containing data across a set of files.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#offsetBytes(java.lang.String,%20long,%20long)">offsetBytes</a></strong>(String&nbsp;path,
           long&nbsp;start,
           long&nbsp;end)</code>
<div class="block">Return a string containing part of a file from byte 'start' to 'end'.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static scala.Tuple2&lt;String,Object&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#parseHostPort(java.lang.String)">parseHostPort</a></strong>(String&nbsp;hostPort)</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static int</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#portMaxRetries(org.apache.spark.SparkConf)">portMaxRetries</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</code>
<div class="block">Maximum number of retries when binding to a port before giving up.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static Thread</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#processStreamByLine(java.lang.String,%20java.io.InputStream,%20scala.Function1)">processStreamByLine</a></strong>(String&nbsp;threadName,
                   java.io.InputStream&nbsp;inputStream,
                   scala.Function1&lt;String,scala.runtime.BoxedUnit&gt;&nbsp;processLine)</code>
<div class="block">Return and start a daemon thread that processes the content of the input stream line by line.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static java.util.Random</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#random()">random</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;scala.collection.Seq&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#randomize(scala.collection.TraversableOnce,%20scala.reflect.ClassTag)">randomize</a></strong>(scala.collection.TraversableOnce&lt;T&gt;&nbsp;seq,
         scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$1)</code>
<div class="block">Shuffle the elements of a collection into a random order, returning the
 result in a new collection.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;Object</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#randomizeInPlace(java.lang.Object,%20java.util.Random)">randomizeInPlace</a></strong>(Object&nbsp;arr,
                java.util.Random&nbsp;rand)</code>
<div class="block">Shuffle the elements of an array into a random order, modifying the
 original array.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#registerShutdownDeleteDir(java.io.File)">registerShutdownDeleteDir</a></strong>(java.io.File&nbsp;file)</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#registerShutdownDeleteDir(tachyon.client.TachyonFile)">registerShutdownDeleteDir</a></strong>(tachyon.client.TachyonFile&nbsp;tachyonfile)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static java.net.URI</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#resolveURI(java.lang.String,%20boolean)">resolveURI</a></strong>(String&nbsp;path,
          boolean&nbsp;testWindows)</code>
<div class="block">Return a well-formed URI for the file described by a user input string.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#resolveURIs(java.lang.String,%20boolean)">resolveURIs</a></strong>(String&nbsp;paths,
           boolean&nbsp;testWindows)</code>
<div class="block">Resolve a comma-separated list of paths.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;byte[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#serialize(T)">serialize</a></strong>(T&nbsp;o)</code>
<div class="block">Serialize an object using Java serialization</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#serializeViaNestedStream(java.io.OutputStream,%20org.apache.spark.serializer.SerializerInstance,%20scala.Function1)">serializeViaNestedStream</a></strong>(java.io.OutputStream&nbsp;os,
                        <a href="../../../../org/apache/spark/serializer/SerializerInstance.html" title="class in org.apache.spark.serializer">SerializerInstance</a>&nbsp;ser,
                        scala.Function1&lt;<a href="../../../../org/apache/spark/serializer/SerializationStream.html" title="class in org.apache.spark.serializer">SerializationStream</a>,scala.runtime.BoxedUnit&gt;&nbsp;f)</code>
<div class="block">Serialize via nested stream using specific serializer</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#setCustomHostname(java.lang.String)">setCustomHostname</a></strong>(String&nbsp;hostname)</code>
<div class="block">Allow setting a custom host name because when we run on Mesos we need to use the same
 hostname it reports to the master.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static java.net.URLConnection</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#setupSecureURLConnection(java.net.URLConnection,%20org.apache.spark.SecurityManager)">setupSecureURLConnection</a></strong>(java.net.URLConnection&nbsp;urlConnection,
                        <a href="../../../../org/apache/spark/SecurityManager.html" title="class in org.apache.spark">SecurityManager</a>&nbsp;sm)</code>
<div class="block">If the given URL connection is HttpsURLConnection, it sets the SSL socket factory and
 the host verifier from the given security manager.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static scala.collection.Seq&lt;String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#sparkJavaOpts(org.apache.spark.SparkConf,%20scala.Function1)">sparkJavaOpts</a></strong>(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
             scala.Function1&lt;String,Object&gt;&nbsp;filterKey)</code>
<div class="block">Convert all spark properties set in the given SparkConf to a sequence of java options.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static scala.collection.Seq&lt;String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#splitCommandString(java.lang.String)">splitCommandString</a></strong>(String&nbsp;s)</code>
<div class="block">Split a string of potentially quoted arguments from the command line the way that a shell
 would do it to determine arguments to a command.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;scala.Tuple2&lt;T,Object&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#startServiceOnPort(int,%20scala.Function1,%20org.apache.spark.SparkConf,%20java.lang.String)">startServiceOnPort</a></strong>(int&nbsp;startPort,
                  scala.Function1&lt;Object,scala.Tuple2&lt;T,Object&gt;&gt;&nbsp;startService,
                  <a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                  String&nbsp;serviceName)</code>
<div class="block">Attempt to start a service on the given port, or fail after a number of attempts.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#stripDirectory(java.lang.String)">stripDirectory</a></strong>(String&nbsp;path)</code>
<div class="block">Strip the directory from a path name</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#symlink(java.io.File,%20java.io.File)">symlink</a></strong>(java.io.File&nbsp;src,
       java.io.File&nbsp;dst)</code>
<div class="block">Creates a symlink.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static long</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#timeIt(int,%20scala.Function0,%20scala.Option)">timeIt</a></strong>(int&nbsp;numIters,
      scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;f,
      scala.Option&lt;scala.Function0&lt;scala.runtime.BoxedUnit&gt;&gt;&nbsp;prepare)</code>
<div class="block">Timing method based on iterations that permit JVM JIT optimization.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#times(int,%20scala.Function0)">times</a></strong>(int&nbsp;numIters,
     scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;f)</code>
<div class="block">Method executed for repeating a task for side effects.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;scala.util.Try&lt;T&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#tryLog(scala.Function0)">tryLog</a></strong>(scala.Function0&lt;T&gt;&nbsp;f)</code>
<div class="block">Executes the given block in a Try, logging any uncaught exceptions.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#tryOrExit(scala.Function0)">tryOrExit</a></strong>(scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;block)</code>
<div class="block">Execute a block of code that evaluates to Unit, forwarding any uncaught exceptions to the
 default UncaughtExceptionHandler</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#tryOrIOException(scala.Function0)">tryOrIOException</a></strong>(scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;block)</code>
<div class="block">Execute a block of code that evaluates to Unit, re-throwing any non-fatal uncaught
 exceptions as IOException.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static &lt;T&gt;&nbsp;T</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#tryOrIOException(scala.Function0)">tryOrIOException</a></strong>(scala.Function0&lt;T&gt;&nbsp;block)</code>
<div class="block">Execute a block of code that returns a value, re-throwing any non-fatal uncaught
 exceptions as IOException.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#tryOrStopSparkContext(org.apache.spark.SparkContext,%20scala.Function0)">tryOrStopSparkContext</a></strong>(<a href="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</a>&nbsp;sc,
                     scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;block)</code>
<div class="block">Execute a block of code that evaluates to Unit, stop SparkContext is there is any uncaught 
 exception</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#waitForProcess(java.lang.Process,%20long)">waitForProcess</a></strong>(Process&nbsp;process,
              long&nbsp;timeoutMs)</code>
<div class="block">Wait for a process to terminate for at most the specified duration.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static scala.util.matching.Regex</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#windowsDrive()">windowsDrive</a></strong>()</code>
<div class="block">Pattern for matching a Windows drive, which contains only a single alphabet character.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/util/Utils.html#writeByteBuffer(java.nio.ByteBuffer,%20java.io.ObjectOutput)">writeByteBuffer</a></strong>(java.nio.ByteBuffer&nbsp;bb,
               java.io.ObjectOutput&nbsp;out)</code>
<div class="block">Primitive often used when writing <code>ByteBuffer</code> to <code>DataOutput</code></div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</code></li>
</ul>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_org.apache.spark.Logging">
<!--   -->
</a>
<h3>Methods inherited from interface&nbsp;org.apache.spark.<a href="../../../../org/apache/spark/Logging.html" title="interface in org.apache.spark">Logging</a></h3>
<code><a href="../../../../org/apache/spark/Logging.html#initializeIfNecessary()">initializeIfNecessary</a>, <a href="../../../../org/apache/spark/Logging.html#initializeLogging()">initializeLogging</a>, <a href="../../../../org/apache/spark/Logging.html#isTraceEnabled()">isTraceEnabled</a>, <a href="../../../../org/apache/spark/Logging.html#log_()">log_</a>, <a href="../../../../org/apache/spark/Logging.html#log()">log</a>, <a href="../../../../org/apache/spark/Logging.html#logDebug(scala.Function0)">logDebug</a>, <a href="../../../../org/apache/spark/Logging.html#logDebug(scala.Function0,%20java.lang.Throwable)">logDebug</a>, <a href="../../../../org/apache/spark/Logging.html#logError(scala.Function0)">logError</a>, <a href="../../../../org/apache/spark/Logging.html#logError(scala.Function0,%20java.lang.Throwable)">logError</a>, <a href="../../../../org/apache/spark/Logging.html#logInfo(scala.Function0)">logInfo</a>, <a href="../../../../org/apache/spark/Logging.html#logInfo(scala.Function0,%20java.lang.Throwable)">logInfo</a>, <a href="../../../../org/apache/spark/Logging.html#logName()">logName</a>, <a href="../../../../org/apache/spark/Logging.html#logTrace(scala.Function0)">logTrace</a>, <a href="../../../../org/apache/spark/Logging.html#logTrace(scala.Function0,%20java.lang.Throwable)">logTrace</a>, <a href="../../../../org/apache/spark/Logging.html#logWarning(scala.Function0)">logWarning</a>, <a href="../../../../org/apache/spark/Logging.html#logWarning(scala.Function0,%20java.lang.Throwable)">logWarning</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="Utils()">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>Utils</h4>
<pre>public&nbsp;Utils()</pre>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method_detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="random()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>random</h4>
<pre>public static&nbsp;java.util.Random&nbsp;random()</pre>
</li>
</ul>
<a name="serialize(java.lang.Object)">
<!--   -->
</a><a name="serialize(T)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>serialize</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;byte[]&nbsp;serialize(T&nbsp;o)</pre>
<div class="block">Serialize an object using Java serialization</div>
</li>
</ul>
<a name="deserialize(byte[])">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>deserialize</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;T&nbsp;deserialize(byte[]&nbsp;bytes)</pre>
<div class="block">Deserialize an object using Java serialization</div>
</li>
</ul>
<a name="deserialize(byte[], java.lang.ClassLoader)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>deserialize</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;T&nbsp;deserialize(byte[]&nbsp;bytes,
                ClassLoader&nbsp;loader)</pre>
<div class="block">Deserialize an object using Java serialization and the given ClassLoader</div>
</li>
</ul>
<a name="deserializeLongValue(byte[])">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>deserializeLongValue</h4>
<pre>public static&nbsp;long&nbsp;deserializeLongValue(byte[]&nbsp;bytes)</pre>
<div class="block">Deserialize a Long value (used for <code>PythonPartitioner</code>)</div>
</li>
</ul>
<a name="serializeViaNestedStream(java.io.OutputStream, org.apache.spark.serializer.SerializerInstance, scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>serializeViaNestedStream</h4>
<pre>public static&nbsp;void&nbsp;serializeViaNestedStream(java.io.OutputStream&nbsp;os,
                            <a href="../../../../org/apache/spark/serializer/SerializerInstance.html" title="class in org.apache.spark.serializer">SerializerInstance</a>&nbsp;ser,
                            scala.Function1&lt;<a href="../../../../org/apache/spark/serializer/SerializationStream.html" title="class in org.apache.spark.serializer">SerializationStream</a>,scala.runtime.BoxedUnit&gt;&nbsp;f)</pre>
<div class="block">Serialize via nested stream using specific serializer</div>
</li>
</ul>
<a name="deserializeViaNestedStream(java.io.InputStream, org.apache.spark.serializer.SerializerInstance, scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>deserializeViaNestedStream</h4>
<pre>public static&nbsp;void&nbsp;deserializeViaNestedStream(java.io.InputStream&nbsp;is,
                              <a href="../../../../org/apache/spark/serializer/SerializerInstance.html" title="class in org.apache.spark.serializer">SerializerInstance</a>&nbsp;ser,
                              scala.Function1&lt;<a href="../../../../org/apache/spark/serializer/DeserializationStream.html" title="class in org.apache.spark.serializer">DeserializationStream</a>,scala.runtime.BoxedUnit&gt;&nbsp;f)</pre>
<div class="block">Deserialize via nested stream using specific serializer</div>
</li>
</ul>
<a name="getSparkClassLoader()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getSparkClassLoader</h4>
<pre>public static&nbsp;ClassLoader&nbsp;getSparkClassLoader()</pre>
<div class="block">Get the ClassLoader which loaded Spark.</div>
</li>
</ul>
<a name="getContextOrSparkClassLoader()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getContextOrSparkClassLoader</h4>
<pre>public static&nbsp;ClassLoader&nbsp;getContextOrSparkClassLoader()</pre>
<div class="block">Get the Context ClassLoader on this thread or, if not present, the ClassLoader that
 loaded Spark.
 <p>
 This should be used whenever passing a ClassLoader to Class.ForName or finding the currently
 active loader when setting up ClassLoader delegation chains.</div>
</li>
</ul>
<a name="classIsLoadable(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>classIsLoadable</h4>
<pre>public static&nbsp;boolean&nbsp;classIsLoadable(String&nbsp;clazz)</pre>
<div class="block">Determines whether the provided class is loadable in the current thread.</div>
</li>
</ul>
<a name="classForName(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>classForName</h4>
<pre>public static&nbsp;Class&lt;?&gt;&nbsp;classForName(String&nbsp;className)</pre>
<div class="block">Preferred alternative to Class.forName(className)</div>
</li>
</ul>
<a name="writeByteBuffer(java.nio.ByteBuffer, java.io.ObjectOutput)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>writeByteBuffer</h4>
<pre>public static&nbsp;void&nbsp;writeByteBuffer(java.nio.ByteBuffer&nbsp;bb,
                   java.io.ObjectOutput&nbsp;out)</pre>
<div class="block">Primitive often used when writing <code>ByteBuffer</code> to <code>DataOutput</code></div>
</li>
</ul>
<a name="registerShutdownDeleteDir(java.io.File)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>registerShutdownDeleteDir</h4>
<pre>public static&nbsp;void&nbsp;registerShutdownDeleteDir(java.io.File&nbsp;file)</pre>
</li>
</ul>
<a name="registerShutdownDeleteDir(tachyon.client.TachyonFile)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>registerShutdownDeleteDir</h4>
<pre>public static&nbsp;void&nbsp;registerShutdownDeleteDir(tachyon.client.TachyonFile&nbsp;tachyonfile)</pre>
</li>
</ul>
<a name="hasShutdownDeleteDir(java.io.File)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hasShutdownDeleteDir</h4>
<pre>public static&nbsp;boolean&nbsp;hasShutdownDeleteDir(java.io.File&nbsp;file)</pre>
</li>
</ul>
<a name="hasShutdownDeleteTachyonDir(tachyon.client.TachyonFile)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hasShutdownDeleteTachyonDir</h4>
<pre>public static&nbsp;boolean&nbsp;hasShutdownDeleteTachyonDir(tachyon.client.TachyonFile&nbsp;file)</pre>
</li>
</ul>
<a name="hasRootAsShutdownDeleteDir(java.io.File)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hasRootAsShutdownDeleteDir</h4>
<pre>public static&nbsp;boolean&nbsp;hasRootAsShutdownDeleteDir(java.io.File&nbsp;file)</pre>
</li>
</ul>
<a name="hasRootAsShutdownDeleteDir(tachyon.client.TachyonFile)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>hasRootAsShutdownDeleteDir</h4>
<pre>public static&nbsp;boolean&nbsp;hasRootAsShutdownDeleteDir(tachyon.client.TachyonFile&nbsp;file)</pre>
</li>
</ul>
<a name="chmod700(java.io.File)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>chmod700</h4>
<pre>public static&nbsp;boolean&nbsp;chmod700(java.io.File&nbsp;file)</pre>
<div class="block">JDK equivalent of <code>chmod 700 file</code>.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>file</code> - the file whose permissions will be modified</dd>
<dt><span class="strong">Returns:</span></dt><dd>true if the permissions were successfully changed, false otherwise.</dd></dl>
</li>
</ul>
<a name="createDirectory(java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>createDirectory</h4>
<pre>public static&nbsp;java.io.File&nbsp;createDirectory(String&nbsp;root,
                           String&nbsp;namePrefix)</pre>
<div class="block">Create a directory inside the given parent directory. The directory is guaranteed to be
 newly created, and is not marked for automatic deletion.</div>
</li>
</ul>
<a name="createTempDir(java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>createTempDir</h4>
<pre>public static&nbsp;java.io.File&nbsp;createTempDir(String&nbsp;root,
                         String&nbsp;namePrefix)</pre>
<div class="block">Create a temporary directory inside the given parent directory. The directory will be
 automatically deleted when the VM shuts down.</div>
</li>
</ul>
<a name="copyStream(java.io.InputStream, java.io.OutputStream, boolean, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>copyStream</h4>
<pre>public static&nbsp;long&nbsp;copyStream(java.io.InputStream&nbsp;in,
              java.io.OutputStream&nbsp;out,
              boolean&nbsp;closeStreams,
              boolean&nbsp;transferToEnabled)</pre>
<div class="block">Copy all data from an InputStream to an OutputStream. NIO way of file stream to file stream
 copying is disabled by default unless explicitly set transferToEnabled as true,
 the parameter transferToEnabled should be configured by spark.file.transferTo = [true|false].</div>
</li>
</ul>
<a name="constructURIForAuthentication(java.net.URI, org.apache.spark.SecurityManager)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>constructURIForAuthentication</h4>
<pre>public static&nbsp;java.net.URI&nbsp;constructURIForAuthentication(java.net.URI&nbsp;uri,
                                         <a href="../../../../org/apache/spark/SecurityManager.html" title="class in org.apache.spark">SecurityManager</a>&nbsp;securityMgr)</pre>
<div class="block">Construct a URI container information used for authentication.
 This also sets the default authenticator to properly negotiation the
 user/password based on the URI.
 <p>
 Note this relies on the Authenticator.setDefault being set properly to decode
 the user name and password. This is currently set in the SecurityManager.</div>
</li>
</ul>
<a name="fetchFile(java.lang.String, java.io.File, org.apache.spark.SparkConf, org.apache.spark.SecurityManager, org.apache.hadoop.conf.Configuration, long, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>fetchFile</h4>
<pre>public static&nbsp;void&nbsp;fetchFile(String&nbsp;url,
             java.io.File&nbsp;targetDir,
             <a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
             <a href="../../../../org/apache/spark/SecurityManager.html" title="class in org.apache.spark">SecurityManager</a>&nbsp;securityMgr,
             org.apache.hadoop.conf.Configuration&nbsp;hadoopConf,
             long&nbsp;timestamp,
             boolean&nbsp;useCache)</pre>
<div class="block">Download a file or directory to target directory. Supports fetching the file in a variety of
 ways, including HTTP, Hadoop-compatible filesystems, and files on a standard filesystem, based
 on the URL parameter. Fetching directories is only supported from Hadoop-compatible
 filesystems.
 <p>
 If <code>useCache</code> is true, first attempts to fetch the file to a local cache that's shared
 across executors running the same application. <code>useCache</code> is used mainly for
 the executors, and not in local mode.
 <p>
 Throws SparkException if the target file already exists and has different contents than
 the requested file.</div>
</li>
</ul>
<a name="fetchHcfsFile(org.apache.hadoop.fs.Path, java.io.File, org.apache.hadoop.fs.FileSystem, org.apache.spark.SparkConf, org.apache.hadoop.conf.Configuration, boolean, scala.Option)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>fetchHcfsFile</h4>
<pre>public static&nbsp;void&nbsp;fetchHcfsFile(org.apache.hadoop.fs.Path&nbsp;path,
                 java.io.File&nbsp;targetDir,
                 org.apache.hadoop.fs.FileSystem&nbsp;fs,
                 <a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                 org.apache.hadoop.conf.Configuration&nbsp;hadoopConf,
                 boolean&nbsp;fileOverwrite,
                 scala.Option&lt;String&gt;&nbsp;filename)</pre>
<div class="block">Fetch a file or directory from a Hadoop-compatible filesystem.
 <p>
 Visible for testing</div>
</li>
</ul>
<a name="getLocalDir(org.apache.spark.SparkConf)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getLocalDir</h4>
<pre>public static&nbsp;String&nbsp;getLocalDir(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</pre>
<div class="block">Get the path of a temporary directory.  Spark's local directories can be configured through
 multiple settings, which are used with the following precedence:
 <p>
   - If called from inside of a YARN container, this will return a directory chosen by YARN.
   - If the SPARK_LOCAL_DIRS environment variable is set, this will return a directory from it.
   - Otherwise, if the spark.local.dir is set, this will return a directory from it.
   - Otherwise, this will return java.io.tmpdir.
 <p>
 Some of these configuration options might be lists of multiple paths, but this method will
 always return a single directory.</div>
</li>
</ul>
<a name="isRunningInYarnContainer(org.apache.spark.SparkConf)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isRunningInYarnContainer</h4>
<pre>public static&nbsp;boolean&nbsp;isRunningInYarnContainer(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</pre>
</li>
</ul>
<a name="getOrCreateLocalRootDirs(org.apache.spark.SparkConf)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getOrCreateLocalRootDirs</h4>
<pre>public static&nbsp;String[]&nbsp;getOrCreateLocalRootDirs(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</pre>
<div class="block">Gets or creates the directories listed in spark.local.dir or SPARK_LOCAL_DIRS,
 and returns only the directories that exist / could be created.
 <p>
 If no directories could be created, this will return an empty list.
 <p>
 This method will cache the local directories for the application when it's first invoked.
 So calling it multiple times with a different configuration will always return the same
 set of directories.</div>
</li>
</ul>
<a name="clearLocalRootDirs()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>clearLocalRootDirs</h4>
<pre>public static&nbsp;void&nbsp;clearLocalRootDirs()</pre>
<div class="block">Used by unit tests. Do not call from other places.</div>
</li>
</ul>
<a name="randomize(scala.collection.TraversableOnce, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>randomize</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;scala.collection.Seq&lt;T&gt;&nbsp;randomize(scala.collection.TraversableOnce&lt;T&gt;&nbsp;seq,
                                    scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$1)</pre>
<div class="block">Shuffle the elements of a collection into a random order, returning the
 result in a new collection. Unlike scala.util.Random.shuffle, this method
 uses a local random number generator, avoiding inter-thread contention.</div>
</li>
</ul>
<a name="randomizeInPlace(java.lang.Object, java.util.Random)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>randomizeInPlace</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;Object&nbsp;randomizeInPlace(Object&nbsp;arr,
                          java.util.Random&nbsp;rand)</pre>
<div class="block">Shuffle the elements of an array into a random order, modifying the
 original array. Returns the original array.</div>
</li>
</ul>
<a name="localIpAddress()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>localIpAddress</h4>
<pre>public static&nbsp;String&nbsp;localIpAddress()</pre>
<div class="block">Get the local host's IP address in dotted-quad format (e.g. 1.2.3.4).
 Note, this is typically not used from within core spark.</div>
</li>
</ul>
<a name="localIpAddressHostname()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>localIpAddressHostname</h4>
<pre>public static&nbsp;String&nbsp;localIpAddressHostname()</pre>
</li>
</ul>
<a name="setCustomHostname(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setCustomHostname</h4>
<pre>public static&nbsp;void&nbsp;setCustomHostname(String&nbsp;hostname)</pre>
<div class="block">Allow setting a custom host name because when we run on Mesos we need to use the same
 hostname it reports to the master.</div>
</li>
</ul>
<a name="localHostName()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>localHostName</h4>
<pre>public static&nbsp;String&nbsp;localHostName()</pre>
<div class="block">Get the local machine's hostname.</div>
</li>
</ul>
<a name="getAddressHostName(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getAddressHostName</h4>
<pre>public static&nbsp;String&nbsp;getAddressHostName(String&nbsp;address)</pre>
</li>
</ul>
<a name="checkHost(java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>checkHost</h4>
<pre>public static&nbsp;void&nbsp;checkHost(String&nbsp;host,
             String&nbsp;message)</pre>
</li>
</ul>
<a name="checkHostPort(java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>checkHostPort</h4>
<pre>public static&nbsp;void&nbsp;checkHostPort(String&nbsp;hostPort,
                 String&nbsp;message)</pre>
</li>
</ul>
<a name="parseHostPort(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>parseHostPort</h4>
<pre>public static&nbsp;scala.Tuple2&lt;String,Object&gt;&nbsp;parseHostPort(String&nbsp;hostPort)</pre>
</li>
</ul>
<a name="namedThreadFactory(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>namedThreadFactory</h4>
<pre>public static&nbsp;java.util.concurrent.ThreadFactory&nbsp;namedThreadFactory(String&nbsp;prefix)</pre>
<div class="block">Create a thread factory that names threads with a prefix and also sets the threads to daemon.</div>
</li>
</ul>
<a name="newDaemonCachedThreadPool(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>newDaemonCachedThreadPool</h4>
<pre>public static&nbsp;java.util.concurrent.ThreadPoolExecutor&nbsp;newDaemonCachedThreadPool(String&nbsp;prefix)</pre>
<div class="block">Wrapper over newCachedThreadPool. Thread names are formatted as prefix-ID, where ID is a
 unique, sequentially assigned integer.</div>
</li>
</ul>
<a name="newDaemonFixedThreadPool(int, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>newDaemonFixedThreadPool</h4>
<pre>public static&nbsp;java.util.concurrent.ThreadPoolExecutor&nbsp;newDaemonFixedThreadPool(int&nbsp;nThreads,
                                                               String&nbsp;prefix)</pre>
<div class="block">Wrapper over newFixedThreadPool. Thread names are formatted as prefix-ID, where ID is a
 unique, sequentially assigned integer.</div>
</li>
</ul>
<a name="getUsedTimeMs(long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getUsedTimeMs</h4>
<pre>public static&nbsp;String&nbsp;getUsedTimeMs(long&nbsp;startTimeMs)</pre>
<div class="block">Return the string to tell how long has passed in milliseconds.</div>
</li>
</ul>
<a name="deleteRecursively(java.io.File)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>deleteRecursively</h4>
<pre>public static&nbsp;void&nbsp;deleteRecursively(java.io.File&nbsp;file)</pre>
<div class="block">Delete a file or directory and its contents recursively.
 Don't follow directories if they are symlinks.
 Throws an exception if deletion is unsuccessful.</div>
</li>
</ul>
<a name="deleteRecursively(tachyon.client.TachyonFile, tachyon.client.TachyonFS)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>deleteRecursively</h4>
<pre>public static&nbsp;void&nbsp;deleteRecursively(tachyon.client.TachyonFile&nbsp;dir,
                     tachyon.client.TachyonFS&nbsp;client)</pre>
<div class="block">Delete a file or directory and its contents recursively.</div>
</li>
</ul>
<a name="isSymlink(java.io.File)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isSymlink</h4>
<pre>public static&nbsp;boolean&nbsp;isSymlink(java.io.File&nbsp;file)</pre>
<div class="block">Check to see if file is a symbolic link.</div>
</li>
</ul>
<a name="doesDirectoryContainAnyNewFiles(java.io.File, long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>doesDirectoryContainAnyNewFiles</h4>
<pre>public static&nbsp;boolean&nbsp;doesDirectoryContainAnyNewFiles(java.io.File&nbsp;dir,
                                      long&nbsp;cutoff)</pre>
<div class="block">Determines if a directory contains any files newer than cutoff seconds.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>dir</code> - must be the path to a directory, or IllegalArgumentException is thrown</dd><dd><code>cutoff</code> - measured in seconds. Returns true if there are any files or directories in the
               given directory whose last modified time is later than this many seconds ago</dd></dl>
</li>
</ul>
<a name="memoryStringToMb(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>memoryStringToMb</h4>
<pre>public static&nbsp;int&nbsp;memoryStringToMb(String&nbsp;str)</pre>
<div class="block">Convert a Java memory parameter passed to -Xmx (such as 300m or 1g) to a number of megabytes.</div>
</li>
</ul>
<a name="bytesToString(long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>bytesToString</h4>
<pre>public static&nbsp;String&nbsp;bytesToString(long&nbsp;size)</pre>
<div class="block">Convert a quantity in bytes to a human-readable string such as "4.0 MB".</div>
</li>
</ul>
<a name="msDurationToString(long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>msDurationToString</h4>
<pre>public static&nbsp;String&nbsp;msDurationToString(long&nbsp;ms)</pre>
<div class="block">Returns a human-readable string representing a duration such as "35ms"</div>
</li>
</ul>
<a name="megabytesToString(long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>megabytesToString</h4>
<pre>public static&nbsp;String&nbsp;megabytesToString(long&nbsp;megabytes)</pre>
<div class="block">Convert a quantity in megabytes to a human-readable string such as "4.0 MB".</div>
</li>
</ul>
<a name="executeCommand(scala.collection.Seq, java.io.File, scala.collection.Map, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>executeCommand</h4>
<pre>public static&nbsp;Process&nbsp;executeCommand(scala.collection.Seq&lt;String&gt;&nbsp;command,
                     java.io.File&nbsp;workingDir,
                     scala.collection.Map&lt;String,String&gt;&nbsp;extraEnvironment,
                     boolean&nbsp;redirectStderr)</pre>
<div class="block">Execute a command and return the process running the command.</div>
</li>
</ul>
<a name="executeAndGetOutput(scala.collection.Seq, java.io.File, scala.collection.Map, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>executeAndGetOutput</h4>
<pre>public static&nbsp;String&nbsp;executeAndGetOutput(scala.collection.Seq&lt;String&gt;&nbsp;command,
                         java.io.File&nbsp;workingDir,
                         scala.collection.Map&lt;String,String&gt;&nbsp;extraEnvironment,
                         boolean&nbsp;redirectStderr)</pre>
<div class="block">Execute a command and get its output, throwing an exception if it yields a code other than 0.</div>
</li>
</ul>
<a name="processStreamByLine(java.lang.String, java.io.InputStream, scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>processStreamByLine</h4>
<pre>public static&nbsp;Thread&nbsp;processStreamByLine(String&nbsp;threadName,
                         java.io.InputStream&nbsp;inputStream,
                         scala.Function1&lt;String,scala.runtime.BoxedUnit&gt;&nbsp;processLine)</pre>
<div class="block">Return and start a daemon thread that processes the content of the input stream line by line.</div>
</li>
</ul>
<a name="tryOrExit(scala.Function0)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>tryOrExit</h4>
<pre>public static&nbsp;void&nbsp;tryOrExit(scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;block)</pre>
<div class="block">Execute a block of code that evaluates to Unit, forwarding any uncaught exceptions to the
 default UncaughtExceptionHandler
 <p>
 NOTE: This method is to be called by the spark-started JVM process.</div>
</li>
</ul>
<a name="tryOrStopSparkContext(org.apache.spark.SparkContext, scala.Function0)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>tryOrStopSparkContext</h4>
<pre>public static&nbsp;void&nbsp;tryOrStopSparkContext(<a href="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</a>&nbsp;sc,
                         scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;block)</pre>
<div class="block">Execute a block of code that evaluates to Unit, stop SparkContext is there is any uncaught 
 exception
 <p>
 NOTE: This method is to be called by the driver-side components to avoid stopping the 
 user-started JVM process completely; in contrast, tryOrExit is to be called in the 
 spark-started JVM process .</div>
</li>
</ul>
<a name="tryOrIOException(scala.Function0)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>tryOrIOException</h4>
<pre>public static&nbsp;void&nbsp;tryOrIOException(scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;block)</pre>
<div class="block">Execute a block of code that evaluates to Unit, re-throwing any non-fatal uncaught
 exceptions as IOException.  This is used when implementing Externalizable and Serializable's
 read and write methods, since Java's serializer will not report non-IOExceptions properly;
 see SPARK-4080 for more context.</div>
</li>
</ul>
<a name="tryOrIOException(scala.Function0)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>tryOrIOException</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;T&nbsp;tryOrIOException(scala.Function0&lt;T&gt;&nbsp;block)</pre>
<div class="block">Execute a block of code that returns a value, re-throwing any non-fatal uncaught
 exceptions as IOException. This is used when implementing Externalizable and Serializable's
 read and write methods, since Java's serializer will not report non-IOExceptions properly;
 see SPARK-4080 for more context.</div>
</li>
</ul>
<a name="getCallSite(scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getCallSite</h4>
<pre>public static&nbsp;<a href="../../../../org/apache/spark/util/CallSite.html" title="class in org.apache.spark.util">CallSite</a>&nbsp;getCallSite(scala.Function1&lt;String,Object&gt;&nbsp;skipClass)</pre>
<div class="block">When called inside a class in the spark package, returns the name of the user code class
 (outside the spark package) that called into Spark, as well as which Spark method they called.
 This is used, for example, to tell users where in their code each RDD got created.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>skipClass</code> - Function that is used to exclude non-user-code classes.</dd></dl>
</li>
</ul>
<a name="offsetBytes(java.lang.String, long, long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>offsetBytes</h4>
<pre>public static&nbsp;String&nbsp;offsetBytes(String&nbsp;path,
                 long&nbsp;start,
                 long&nbsp;end)</pre>
<div class="block">Return a string containing part of a file from byte 'start' to 'end'.</div>
</li>
</ul>
<a name="offsetBytes(scala.collection.Seq, long, long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>offsetBytes</h4>
<pre>public static&nbsp;String&nbsp;offsetBytes(scala.collection.Seq&lt;java.io.File&gt;&nbsp;files,
                 long&nbsp;start,
                 long&nbsp;end)</pre>
<div class="block">Return a string containing data across a set of files. The <code>startIndex</code>
 and <code>endIndex</code> is based on the cumulative size of all the files take in
 the given order. See figure below for more details.</div>
</li>
</ul>
<a name="clone(java.lang.Object,org.apache.spark.serializer.SerializerInstance,scala.reflect.ClassTag)">
<!--   -->
</a><a name="clone(T, org.apache.spark.serializer.SerializerInstance, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>clone</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;T&nbsp;clone(T&nbsp;value,
          <a href="../../../../org/apache/spark/serializer/SerializerInstance.html" title="class in org.apache.spark.serializer">SerializerInstance</a>&nbsp;serializer,
          scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$2)</pre>
<div class="block">Clone an object using a Spark serializer.</div>
</li>
</ul>
<a name="inShutdown()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>inShutdown</h4>
<pre>public static&nbsp;boolean&nbsp;inShutdown()</pre>
<div class="block">Detect whether this thread might be executing a shutdown hook. Will always return true if
 the current thread is a running a shutdown hook but may spuriously return true otherwise (e.g.
 if System.exit was just called by a concurrent thread).
 <p>
 Currently, this detects whether the JVM is shutting down by Runtime#addShutdownHook throwing
 an IllegalStateException.</div>
</li>
</ul>
<a name="splitCommandString(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>splitCommandString</h4>
<pre>public static&nbsp;scala.collection.Seq&lt;String&gt;&nbsp;splitCommandString(String&nbsp;s)</pre>
<div class="block">Split a string of potentially quoted arguments from the command line the way that a shell
 would do it to determine arguments to a command. For example, if the string is 'a "b c" d',
 then it would be parsed as three arguments: 'a', 'b c' and 'd'.</div>
</li>
</ul>
<a name="nonNegativeMod(int, int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nonNegativeMod</h4>
<pre>public static&nbsp;int&nbsp;nonNegativeMod(int&nbsp;x,
                 int&nbsp;mod)</pre>
</li>
</ul>
<a name="nonNegativeHash(java.lang.Object)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nonNegativeHash</h4>
<pre>public static&nbsp;int&nbsp;nonNegativeHash(Object&nbsp;obj)</pre>
</li>
</ul>
<a name="getSystemProperties()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getSystemProperties</h4>
<pre>public static&nbsp;scala.collection.Map&lt;String,String&gt;&nbsp;getSystemProperties()</pre>
<div class="block">Returns the system properties map that is thread-safe to iterator over. It gets the
 properties which have been set explicitly, as well as those for which only a default value
 has been defined.</div>
</li>
</ul>
<a name="times(int, scala.Function0)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>times</h4>
<pre>public static&nbsp;void&nbsp;times(int&nbsp;numIters,
         scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;f)</pre>
<div class="block">Method executed for repeating a task for side effects.
 Unlike a for comprehension, it permits JVM JIT optimization</div>
</li>
</ul>
<a name="timeIt(int, scala.Function0, scala.Option)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>timeIt</h4>
<pre>public static&nbsp;long&nbsp;timeIt(int&nbsp;numIters,
          scala.Function0&lt;scala.runtime.BoxedUnit&gt;&nbsp;f,
          scala.Option&lt;scala.Function0&lt;scala.runtime.BoxedUnit&gt;&gt;&nbsp;prepare)</pre>
<div class="block">Timing method based on iterations that permit JVM JIT optimization.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>numIters</code> - number of iterations</dd><dd><code>f</code> - function to be executed. If prepare is not None, the running time of each call to f
          must be an order of magnitude longer than one millisecond for accurate timing.</dd><dd><code>prepare</code> - function to be executed before each call to f. Its running time doesn't count.</dd>
<dt><span class="strong">Returns:</span></dt><dd>the total time across all iterations (not couting preparation time)</dd></dl>
</li>
</ul>
<a name="getIteratorSize(scala.collection.Iterator)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getIteratorSize</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;long&nbsp;getIteratorSize(scala.collection.Iterator&lt;T&gt;&nbsp;iterator)</pre>
<div class="block">Counts the number of elements of an iterator using a while loop rather than calling
 <code>TraversableOnce.size()</code> because it uses a for loop, which is slightly slower
 in the current version of Scala.</div>
</li>
</ul>
<a name="symlink(java.io.File, java.io.File)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>symlink</h4>
<pre>public static&nbsp;void&nbsp;symlink(java.io.File&nbsp;src,
           java.io.File&nbsp;dst)</pre>
<div class="block">Creates a symlink. Note jdk1.7 has Files.createSymbolicLink but not used here
 for jdk1.6 support.  Supports windows by doing copy, everything else uses "ln -sf".</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>src</code> - absolute path to the source</dd><dd><code>dst</code> - relative path for the destination</dd></dl>
</li>
</ul>
<a name="getFormattedClassName(java.lang.Object)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getFormattedClassName</h4>
<pre>public static&nbsp;String&nbsp;getFormattedClassName(Object&nbsp;obj)</pre>
<div class="block">Return the class name of the given object, removing all dollar signs</div>
</li>
</ul>
<a name="jsonOption(org.json4s.JsonAST.JValue)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>jsonOption</h4>
<pre>public static&nbsp;scala.Option&lt;org.json4s.JsonAST.JValue&gt;&nbsp;jsonOption(org.json4s.JsonAST.JValue&nbsp;json)</pre>
<div class="block">Return an option that translates JNothing to None</div>
</li>
</ul>
<a name="emptyJson()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>emptyJson</h4>
<pre>public static&nbsp;org.json4s.JsonAST.JObject&nbsp;emptyJson()</pre>
<div class="block">Return an empty JSON object</div>
</li>
</ul>
<a name="getHadoopFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getHadoopFileSystem</h4>
<pre>public static&nbsp;org.apache.hadoop.fs.FileSystem&nbsp;getHadoopFileSystem(java.net.URI&nbsp;path,
                                                  org.apache.hadoop.conf.Configuration&nbsp;conf)</pre>
<div class="block">Return a Hadoop FileSystem with the scheme encoded in the given path.</div>
</li>
</ul>
<a name="getHadoopFileSystem(java.lang.String, org.apache.hadoop.conf.Configuration)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getHadoopFileSystem</h4>
<pre>public static&nbsp;org.apache.hadoop.fs.FileSystem&nbsp;getHadoopFileSystem(String&nbsp;path,
                                                  org.apache.hadoop.conf.Configuration&nbsp;conf)</pre>
<div class="block">Return a Hadoop FileSystem with the scheme encoded in the given path.</div>
</li>
</ul>
<a name="getFilePath(java.io.File, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getFilePath</h4>
<pre>public static&nbsp;org.apache.hadoop.fs.Path&nbsp;getFilePath(java.io.File&nbsp;dir,
                                    String&nbsp;fileName)</pre>
<div class="block">Return the absolute path of a file in the given directory.</div>
</li>
</ul>
<a name="isWindows()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isWindows</h4>
<pre>public static&nbsp;boolean&nbsp;isWindows()</pre>
<div class="block">Whether the underlying operating system is Windows.</div>
</li>
</ul>
<a name="isMac()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isMac</h4>
<pre>public static&nbsp;boolean&nbsp;isMac()</pre>
<div class="block">Whether the underlying operating system is Mac OS X.</div>
</li>
</ul>
<a name="windowsDrive()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>windowsDrive</h4>
<pre>public static&nbsp;scala.util.matching.Regex&nbsp;windowsDrive()</pre>
<div class="block">Pattern for matching a Windows drive, which contains only a single alphabet character.</div>
</li>
</ul>
<a name="formatWindowsPath(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>formatWindowsPath</h4>
<pre>public static&nbsp;String&nbsp;formatWindowsPath(String&nbsp;path)</pre>
<div class="block">Format a Windows path such that it can be safely passed to a URI.</div>
</li>
</ul>
<a name="isTesting()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isTesting</h4>
<pre>public static&nbsp;boolean&nbsp;isTesting()</pre>
<div class="block">Indicates whether Spark is currently running unit tests.</div>
</li>
</ul>
<a name="stripDirectory(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>stripDirectory</h4>
<pre>public static&nbsp;String&nbsp;stripDirectory(String&nbsp;path)</pre>
<div class="block">Strip the directory from a path name</div>
</li>
</ul>
<a name="waitForProcess(java.lang.Process, long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>waitForProcess</h4>
<pre>public static&nbsp;boolean&nbsp;waitForProcess(Process&nbsp;process,
                     long&nbsp;timeoutMs)</pre>
<div class="block">Wait for a process to terminate for at most the specified duration.
 Return whether the process actually terminated after the given timeout.</div>
</li>
</ul>
<a name="getStderr(java.lang.Process, long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getStderr</h4>
<pre>public static&nbsp;scala.Option&lt;String&gt;&nbsp;getStderr(Process&nbsp;process,
                             long&nbsp;timeoutMs)</pre>
<div class="block">Return the stderr of a process after waiting for the process to terminate.
 If the process does not terminate within the specified timeout, return None.</div>
</li>
</ul>
<a name="logUncaughtExceptions(scala.Function0)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>logUncaughtExceptions</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;T&nbsp;logUncaughtExceptions(scala.Function0&lt;T&gt;&nbsp;f)</pre>
<div class="block">Execute the given block, logging and re-throwing any uncaught exception.
 This is particularly useful for wrapping code that runs in a thread, to ensure
 that exceptions are printed, and to avoid having to catch Throwable.</div>
</li>
</ul>
<a name="tryLog(scala.Function0)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>tryLog</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;scala.util.Try&lt;T&gt;&nbsp;tryLog(scala.Function0&lt;T&gt;&nbsp;f)</pre>
<div class="block">Executes the given block in a Try, logging any uncaught exceptions.</div>
</li>
</ul>
<a name="isFatalError(java.lang.Throwable)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isFatalError</h4>
<pre>public static&nbsp;boolean&nbsp;isFatalError(Throwable&nbsp;e)</pre>
<div class="block">Returns true if the given exception was fatal. See docs for scala.util.control.NonFatal.</div>
</li>
</ul>
<a name="resolveURI(java.lang.String, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>resolveURI</h4>
<pre>public static&nbsp;java.net.URI&nbsp;resolveURI(String&nbsp;path,
                      boolean&nbsp;testWindows)</pre>
<div class="block">Return a well-formed URI for the file described by a user input string.
 <p>
 If the supplied path does not contain a scheme, or is a relative path, it will be
 converted into an absolute path with a file:// scheme.</div>
</li>
</ul>
<a name="resolveURIs(java.lang.String, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>resolveURIs</h4>
<pre>public static&nbsp;String&nbsp;resolveURIs(String&nbsp;paths,
                 boolean&nbsp;testWindows)</pre>
<div class="block">Resolve a comma-separated list of paths.</div>
</li>
</ul>
<a name="nonLocalPaths(java.lang.String, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nonLocalPaths</h4>
<pre>public static&nbsp;String[]&nbsp;nonLocalPaths(String&nbsp;paths,
                     boolean&nbsp;testWindows)</pre>
<div class="block">Return all non-local paths from a comma-separated list of paths.</div>
</li>
</ul>
<a name="loadDefaultSparkProperties(org.apache.spark.SparkConf, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>loadDefaultSparkProperties</h4>
<pre>public static&nbsp;String&nbsp;loadDefaultSparkProperties(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                                String&nbsp;filePath)</pre>
<div class="block">Load default Spark properties from the given file. If no file is provided,
 use the common defaults file. This mutates state in the given SparkConf and
 in this JVM's system properties if the config specified in the file is not
 already set. Return the path of the properties file used.</div>
</li>
</ul>
<a name="getPropertiesFromFile(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getPropertiesFromFile</h4>
<pre>public static&nbsp;scala.collection.Map&lt;String,String&gt;&nbsp;getPropertiesFromFile(String&nbsp;filename)</pre>
<div class="block">Load properties present in the given file.</div>
</li>
</ul>
<a name="getDefaultPropertiesFile(scala.collection.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getDefaultPropertiesFile</h4>
<pre>public static&nbsp;String&nbsp;getDefaultPropertiesFile(scala.collection.Map&lt;String,String&gt;&nbsp;env)</pre>
<div class="block">Return the path of the default Spark properties file.</div>
</li>
</ul>
<a name="exceptionString(java.lang.Throwable)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>exceptionString</h4>
<pre>public static&nbsp;String&nbsp;exceptionString(Throwable&nbsp;e)</pre>
<div class="block">Return a nice string representation of the exception. It will call "printStackTrace" to
 recursively generate the stack trace including the exception and its causes.</div>
</li>
</ul>
<a name="getThreadDump()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getThreadDump</h4>
<pre>public static&nbsp;<a href="../../../../org/apache/spark/util/ThreadStackTrace.html" title="class in org.apache.spark.util">ThreadStackTrace</a>[]&nbsp;getThreadDump()</pre>
<div class="block">Return a thread dump of all threads' stacktraces.  Used to capture dumps for the web UI</div>
</li>
</ul>
<a name="sparkJavaOpts(org.apache.spark.SparkConf, scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sparkJavaOpts</h4>
<pre>public static&nbsp;scala.collection.Seq&lt;String&gt;&nbsp;sparkJavaOpts(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                                         scala.Function1&lt;String,Object&gt;&nbsp;filterKey)</pre>
<div class="block">Convert all spark properties set in the given SparkConf to a sequence of java options.</div>
</li>
</ul>
<a name="portMaxRetries(org.apache.spark.SparkConf)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>portMaxRetries</h4>
<pre>public static&nbsp;int&nbsp;portMaxRetries(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</pre>
<div class="block">Maximum number of retries when binding to a port before giving up.</div>
</li>
</ul>
<a name="startServiceOnPort(int, scala.Function1, org.apache.spark.SparkConf, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>startServiceOnPort</h4>
<pre>public static&nbsp;&lt;T&gt;&nbsp;scala.Tuple2&lt;T,Object&gt;&nbsp;startServiceOnPort(int&nbsp;startPort,
                                            scala.Function1&lt;Object,scala.Tuple2&lt;T,Object&gt;&gt;&nbsp;startService,
                                            <a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                                            String&nbsp;serviceName)</pre>
<div class="block">Attempt to start a service on the given port, or fail after a number of attempts.
 Each subsequent attempt uses 1 + the port used in the previous attempt (unless the port is 0).
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>startPort</code> - The initial port to start the service on.</dd><dd><code>startService</code> - Function to start service on a given port.
                     This is expected to throw java.net.BindException on port collision.</dd><dd><code>conf</code> - A SparkConf used to get the maximum number of retries when binding to a port.</dd><dd><code>serviceName</code> - Name of the service.</dd></dl>
</li>
</ul>
<a name="isBindCollision(java.lang.Throwable)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isBindCollision</h4>
<pre>public static&nbsp;boolean&nbsp;isBindCollision(Throwable&nbsp;exception)</pre>
<div class="block">Return whether the exception is caused by an address-port collision when binding.</div>
</li>
</ul>
<a name="configTestLog4j(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>configTestLog4j</h4>
<pre>public static&nbsp;void&nbsp;configTestLog4j(String&nbsp;level)</pre>
<div class="block">config a log4j properties used for testsuite</div>
</li>
</ul>
<a name="setupSecureURLConnection(java.net.URLConnection, org.apache.spark.SecurityManager)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setupSecureURLConnection</h4>
<pre>public static&nbsp;java.net.URLConnection&nbsp;setupSecureURLConnection(java.net.URLConnection&nbsp;urlConnection,
                                              <a href="../../../../org/apache/spark/SecurityManager.html" title="class in org.apache.spark">SecurityManager</a>&nbsp;sm)</pre>
<div class="block">If the given URL connection is HttpsURLConnection, it sets the SSL socket factory and
 the host verifier from the given security manager.</div>
</li>
</ul>
<a name="invoke(java.lang.Class, java.lang.Object, java.lang.String, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>invoke</h4>
<pre>public static&nbsp;Object&nbsp;invoke(Class&lt;?&gt;&nbsp;clazz,
            Object&nbsp;obj,
            String&nbsp;methodName,
            scala.collection.Seq&lt;scala.Tuple2&lt;Class&lt;?&gt;,Object&gt;&gt;&nbsp;args)</pre>
</li>
</ul>
<a name="getMaxResultSize(org.apache.spark.SparkConf)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getMaxResultSize</h4>
<pre>public static&nbsp;long&nbsp;getMaxResultSize(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf)</pre>
</li>
</ul>
<a name="libraryPathEnvName()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>libraryPathEnvName</h4>
<pre>public static&nbsp;String&nbsp;libraryPathEnvName()</pre>
<div class="block">Return the current system LD_LIBRARY_PATH name</div>
</li>
</ul>
<a name="libraryPathEnvPrefix(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>libraryPathEnvPrefix</h4>
<pre>public static&nbsp;String&nbsp;libraryPathEnvPrefix(scala.collection.Seq&lt;String&gt;&nbsp;libraryPaths)</pre>
<div class="block">Return the prefix of a command that appends the given library paths to the
 system-specific library path environment variable. On Unix, for instance,
 this returns the string LD_LIBRARY_PATH="path1:path2:$LD_LIBRARY_PATH".</div>
</li>
</ul>
<a name="getSparkOrYarnConfig(org.apache.spark.SparkConf, java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getSparkOrYarnConfig</h4>
<pre>public static&nbsp;String&nbsp;getSparkOrYarnConfig(<a href="../../../../org/apache/spark/SparkConf.html" title="class in org.apache.spark">SparkConf</a>&nbsp;conf,
                          String&nbsp;key,
                          String&nbsp;default_)</pre>
<div class="block">Return the value of a config either through the SparkConf or the Hadoop configuration
 if this is Yarn mode. In the latter case, this defaults to the value set through SparkConf
 if the key is not set in the Hadoop configuration.</div>
</li>
</ul>
<a name="extractHostPortFromSparkUrl(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>extractHostPortFromSparkUrl</h4>
<pre>public static&nbsp;scala.Tuple2&lt;String,Object&gt;&nbsp;extractHostPortFromSparkUrl(String&nbsp;sparkUrl)</pre>
<div class="block">Return a pair of host and port extracted from the <code>sparkUrl</code>.
 <p>
 A spark url (<code>spark://host:port</code>) is a special URI that its scheme is <code>spark</code> and only contains
 host and port.
 <p></div>
<dl><dt><span class="strong">Throws:</span></dt>
<dd><code>SparkException</code> - if <code>sparkUrl</code> is invalid.</dd></dl>
</li>
</ul>
<a name="getCurrentUserName()">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>getCurrentUserName</h4>
<pre>public static&nbsp;String&nbsp;getCurrentUserName()</pre>
<div class="block">Returns the current user name. This is the currently logged in user, unless that's been
 overridden by the <code>SPARK_USER</code> environment variable.</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/util/TimeStampedWeakValueHashMap.html" title="class in org.apache.spark.util"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/util/Vector.html" title="class in org.apache.spark.util"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/util/Utils.html" target="_top">Frames</a></li>
<li><a href="Utils.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
</body>
</html>
