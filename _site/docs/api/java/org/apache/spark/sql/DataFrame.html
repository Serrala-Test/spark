<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (version 1.7.0_71) on Thu Apr 02 14:27:46 PDT 2015 -->
<title>DataFrame (Spark 1.4.0 JavaDoc)</title>
<meta name="date" content="2015-04-02">
<link rel="stylesheet" type="text/css" href="../../../../stylesheet.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="DataFrame (Spark 1.4.0 JavaDoc)";
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/sql/ColumnName.html" title="class in org.apache.spark.sql"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/sql/DataFrameHolder.html" title="class in org.apache.spark.sql"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/sql/DataFrame.html" target="_top">Frames</a></li>
<li><a href="DataFrame.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.apache.spark.sql</div>
<h2 title="Class DataFrame" class="title">Class DataFrame</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>Object</li>
<li>
<ul class="inheritance">
<li>org.apache.spark.sql.DataFrame</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<dl>
<dt>All Implemented Interfaces:</dt>
<dd>java.io.Serializable, <a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</dd>
</dl>
<hr>
<br>
<pre>public class <span class="strong">DataFrame</span>
extends Object
implements <a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;, scala.Serializable</pre>
<div class="block">:: Experimental ::
 A distributed collection of data organized into named columns.
 <p>
 A <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> is equivalent to a relational table in Spark SQL. There are multiple ways
 to create a <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>:
 <pre><code>
   // Create a DataFrame from Parquet files
   val people = sqlContext.parquetFile("...")

   // Create a DataFrame from data sources
   val df = sqlContext.load("...", "json")
 </code></pre>
 <p>
 Once created, it can be manipulated using the various domain-specific-language (DSL) functions
 defined in: <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> (this class), <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><code>Column</code></a>, and <a href="../../../../org/apache/spark/sql/functions.html" title="class in org.apache.spark.sql"><code>functions</code></a>.
 <p>
 To select a column from the data frame, use <code>apply</code> method in Scala and <code>col</code> in Java.
 <pre><code>
   val ageCol = people("age")  // in Scala
   Column ageCol = people.col("age")  // in Java
 </code></pre>
 <p>
 Note that the <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><code>Column</code></a> type can also be manipulated through its various functions.
 <pre><code>
   // The following creates a new column that increases everybody's age by 10.
   people("age") + 10  // in Scala
   people.col("age").plus(10);  // in Java
 </code></pre>
 <p>
 A more concrete example in Scala:
 <pre><code>
   // To create DataFrame using SQLContext
   val people = sqlContext.parquetFile("...")
   val department = sqlContext.parquetFile("...")

   people.filter("age &gt; 30")
     .join(department, people("deptId") === department("id"))
     .groupBy(department("name"), "gender")
     .agg(avg(people("salary")), max(people("age")))
 </code></pre>
 <p>
 and in Java:
 <pre><code>
   // To create DataFrame using SQLContext
   DataFrame people = sqlContext.parquetFile("...");
   DataFrame department = sqlContext.parquetFile("...");

   people.filter("age".gt(30))
     .join(department, people.col("deptId").equalTo(department("id")))
     .groupBy(department.col("name"), "gender")
     .agg(avg(people.col("salary")), max(people.col("age")));
 </code></pre>
 <p></div>
<dl><dt><span class="strong">See Also:</span></dt><dd><a href="../../../../serialized-form.html#org.apache.spark.sql.DataFrame">Serialized Form</a></dd></dl>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#DataFrame(org.apache.spark.sql.SQLContext,%20org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)">DataFrame</a></strong>(<a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a>&nbsp;sqlContext,
         org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&nbsp;logicalPlan)</code>
<div class="block">A constructor that automatically analyzes the logical plan.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#DataFrame(org.apache.spark.sql.SQLContext,%20org.apache.spark.sql.SQLContext.QueryExecution)">DataFrame</a></strong>(<a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a>&nbsp;sqlContext,
         org.apache.spark.sql.SQLContext.QueryExecution&nbsp;queryExecution)</code>&nbsp;</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method_summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span>Methods</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#agg(org.apache.spark.sql.Column,%20org.apache.spark.sql.Column...)">agg</a></strong>(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;expr,
   <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;exprs)</code>
<div class="block">Aggregates on the entire <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> without groups.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#agg(org.apache.spark.sql.Column,%20scala.collection.Seq)">agg</a></strong>(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;expr,
   scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;exprs)</code>
<div class="block">Aggregates on the entire <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> without groups.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#agg(scala.collection.immutable.Map)">agg</a></strong>(scala.collection.immutable.Map&lt;String,String&gt;&nbsp;exprs)</code>
<div class="block">(Scala-specific) Aggregates on the entire <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> without groups.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#agg(java.util.Map)">agg</a></strong>(java.util.Map&lt;String,String&gt;&nbsp;exprs)</code>
<div class="block">(Java-specific) Aggregates on the entire <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> without groups.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#agg(scala.Tuple2,%20scala.collection.Seq)">agg</a></strong>(scala.Tuple2&lt;String,String&gt;&nbsp;aggExpr,
   scala.collection.Seq&lt;scala.Tuple2&lt;String,String&gt;&gt;&nbsp;aggExprs)</code>
<div class="block">(Scala-specific) Compute aggregates by specifying a map from column name to
 aggregate methods.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#apply(java.lang.String)">apply</a></strong>(String&nbsp;colName)</code>
<div class="block">Selects column based on the column name and return it as a <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><code>Column</code></a>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#as(java.lang.String)">as</a></strong>(String&nbsp;alias)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with an alias set.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#as(scala.Symbol)">as</a></strong>(scala.Symbol&nbsp;alias)</code>
<div class="block">(Scala-specific) Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with an alias set.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#cache()">cache</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#col(java.lang.String)">col</a></strong>(String&nbsp;colName)</code>
<div class="block">Selects column based on the column name and return it as a <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><code>Column</code></a>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>org.apache.spark.sql.Row[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#collect()">collect</a></strong>()</code>
<div class="block">Returns an array that contains all of <code>Row</code>s in this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>java.util.List&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#collectAsList()">collectAsList</a></strong>()</code>
<div class="block">Returns a Java list that contains all of <code>Row</code>s in this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>String[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#columns()">columns</a></strong>()</code>
<div class="block">Returns all column names as an array.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>long</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#count()">count</a></strong>()</code>
<div class="block">Returns the number of rows in the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#createJDBCTable(java.lang.String,%20java.lang.String,%20boolean)">createJDBCTable</a></strong>(String&nbsp;url,
               String&nbsp;table,
               boolean&nbsp;allowExisting)</code>
<div class="block">Save this RDD to a JDBC database at <code>url</code> under the table name <code>table</code>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#describe(scala.collection.Seq)">describe</a></strong>(scala.collection.Seq&lt;String&gt;&nbsp;cols)</code>
<div class="block">Computes statistics for numeric columns, including count, mean, stddev, min, and max.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#describe(java.lang.String...)">describe</a></strong>(String...&nbsp;cols)</code>
<div class="block">Computes statistics for numeric columns, including count, mean, stddev, min, and max.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#distinct()">distinct</a></strong>()</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> that contains only the unique rows from this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>scala.Tuple2&lt;String,String&gt;[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#dtypes()">dtypes</a></strong>()</code>
<div class="block">Returns all column names and their data types as an array.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#except(org.apache.spark.sql.DataFrame)">except</a></strong>(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;other)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> containing rows in this frame but not in another frame.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#explain()">explain</a></strong>()</code>
<div class="block">Only prints the physical plan to the console for debugging purpose.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#explain(boolean)">explain</a></strong>(boolean&nbsp;extended)</code>
<div class="block">Prints the plans (logical and physical) to the console for debugging purpose.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>&lt;A extends scala.Product&gt;&nbsp;<br><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#explode(scala.collection.Seq,%20scala.Function1,%20scala.reflect.api.TypeTags.TypeTag)">explode</a></strong>(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;input,
       scala.Function1&lt;org.apache.spark.sql.Row,scala.collection.TraversableOnce&lt;A&gt;&gt;&nbsp;f,
       scala.reflect.api.TypeTags.TypeTag&lt;A&gt;&nbsp;evidence$1)</code>
<div class="block">(Scala-specific) Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> where each row has been expanded to zero or more
 rows by the provided function.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;A,B&gt;&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#explode(java.lang.String,%20java.lang.String,%20scala.Function1,%20scala.reflect.api.TypeTags.TypeTag)">explode</a></strong>(String&nbsp;inputColumn,
       String&nbsp;outputColumn,
       scala.Function1&lt;A,scala.collection.TraversableOnce&lt;B&gt;&gt;&nbsp;f,
       scala.reflect.api.TypeTags.TypeTag&lt;B&gt;&nbsp;evidence$2)</code>
<div class="block">(Scala-specific) Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> where a single column has been expanded to zero
 or more rows by the provided function.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#filter(org.apache.spark.sql.Column)">filter</a></strong>(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;condition)</code>
<div class="block">Filters rows using the given condition.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#filter(java.lang.String)">filter</a></strong>(String&nbsp;conditionExpr)</code>
<div class="block">Filters rows using the given SQL expression.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>org.apache.spark.sql.Row</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#first()">first</a></strong>()</code>
<div class="block">Returns the first row.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;R&gt;&nbsp;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;R&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#flatMap(scala.Function1,%20scala.reflect.ClassTag)">flatMap</a></strong>(scala.Function1&lt;org.apache.spark.sql.Row,scala.collection.TraversableOnce&lt;R&gt;&gt;&nbsp;f,
       scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$4)</code>
<div class="block">Returns a new RDD by first applying a function to all rows of this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>,
 and then flattening the results.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#foreach(scala.Function1)">foreach</a></strong>(scala.Function1&lt;org.apache.spark.sql.Row,scala.runtime.BoxedUnit&gt;&nbsp;f)</code>
<div class="block">Applies a function <code>f</code> to all rows.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#foreachPartition(scala.Function1)">foreachPartition</a></strong>(scala.Function1&lt;scala.collection.Iterator&lt;org.apache.spark.sql.Row&gt;,scala.runtime.BoxedUnit&gt;&nbsp;f)</code>
<div class="block">Applies a function f to each partition of this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#groupBy(org.apache.spark.sql.Column...)">groupBy</a></strong>(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;cols)</code>
<div class="block">Groups the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> using the specified columns, so we can run aggregation on them.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#groupBy(scala.collection.Seq)">groupBy</a></strong>(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;cols)</code>
<div class="block">Groups the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> using the specified columns, so we can run aggregation on them.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#groupBy(java.lang.String,%20scala.collection.Seq)">groupBy</a></strong>(String&nbsp;col1,
       scala.collection.Seq&lt;String&gt;&nbsp;cols)</code>
<div class="block">Groups the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> using the specified columns, so we can run aggregation on them.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#groupBy(java.lang.String,%20java.lang.String...)">groupBy</a></strong>(String&nbsp;col1,
       String...&nbsp;cols)</code>
<div class="block">Groups the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> using the specified columns, so we can run aggregation on them.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>org.apache.spark.sql.Row</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#head()">head</a></strong>()</code>
<div class="block">Returns the first row.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>org.apache.spark.sql.Row[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#head(int)">head</a></strong>(int&nbsp;n)</code>
<div class="block">Returns the first <code>n</code> rows.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#insertInto(java.lang.String)">insertInto</a></strong>(String&nbsp;tableName)</code>
<div class="block">:: Experimental ::
 Adds the rows from this RDD to the specified table.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#insertInto(java.lang.String,%20boolean)">insertInto</a></strong>(String&nbsp;tableName,
          boolean&nbsp;overwrite)</code>
<div class="block">:: Experimental ::
 Adds the rows from this RDD to the specified table, optionally overwriting the existing data.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#insertIntoJDBC(java.lang.String,%20java.lang.String,%20boolean)">insertIntoJDBC</a></strong>(String&nbsp;url,
              String&nbsp;table,
              boolean&nbsp;overwrite)</code>
<div class="block">Save this RDD to a JDBC database at <code>url</code> under the table name <code>table</code>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#intersect(org.apache.spark.sql.DataFrame)">intersect</a></strong>(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;other)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> containing rows only in both this frame and another frame.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>boolean</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#isLocal()">isLocal</a></strong>()</code>
<div class="block">Returns true if the <code>collect</code> and <code>take</code> methods can be run locally
 (without any Spark executors).</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</a>&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#javaRDD()">javaRDD</a></strong>()</code>
<div class="block">Returns the content of the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as a <code>JavaRDD</code> of <code>Row</code>s.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#join(org.apache.spark.sql.DataFrame)">join</a></strong>(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;right)</code>
<div class="block">Cartesian join with another <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#join(org.apache.spark.sql.DataFrame,%20org.apache.spark.sql.Column)">join</a></strong>(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;right,
    <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;joinExprs)</code>
<div class="block">Inner join with another <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>, using the given join expression.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#join(org.apache.spark.sql.DataFrame,%20org.apache.spark.sql.Column,%20java.lang.String)">join</a></strong>(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;right,
    <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;joinExprs,
    String&nbsp;joinType)</code>
<div class="block">Join with another <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>, using the given join expression.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#limit(int)">limit</a></strong>(int&nbsp;n)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> by taking the first <code>n</code> rows.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>&lt;R&gt;&nbsp;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;R&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#map(scala.Function1,%20scala.reflect.ClassTag)">map</a></strong>(scala.Function1&lt;org.apache.spark.sql.Row,R&gt;&nbsp;f,
   scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$3)</code>
<div class="block">Returns a new RDD by applying a function to all rows of this DataFrame.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>&lt;R&gt;&nbsp;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;R&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#mapPartitions(scala.Function1,%20scala.reflect.ClassTag)">mapPartitions</a></strong>(scala.Function1&lt;scala.collection.Iterator&lt;org.apache.spark.sql.Row&gt;,scala.collection.Iterator&lt;R&gt;&gt;&nbsp;f,
             scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$5)</code>
<div class="block">Returns a new RDD by applying a function to each partition of this DataFrame.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#orderBy(org.apache.spark.sql.Column...)">orderBy</a></strong>(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;sortExprs)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#orderBy(scala.collection.Seq)">orderBy</a></strong>(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;sortExprs)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#orderBy(java.lang.String,%20scala.collection.Seq)">orderBy</a></strong>(String&nbsp;sortCol,
       scala.collection.Seq&lt;String&gt;&nbsp;sortCols)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#orderBy(java.lang.String,%20java.lang.String...)">orderBy</a></strong>(String&nbsp;sortCol,
       String...&nbsp;sortCols)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#persist()">persist</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#persist(org.apache.spark.storage.StorageLevel)">persist</a></strong>(<a href="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</a>&nbsp;newLevel)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#printSchema()">printSchema</a></strong>()</code>
<div class="block">Prints the schema to the console in a nice tree format.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>org.apache.spark.sql.SQLContext.QueryExecution</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#queryExecution()">queryExecution</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#rdd()">rdd</a></strong>()</code>
<div class="block">Returns the content of the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as an <code>RDD</code> of <code>Row</code>s.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#registerTempTable(java.lang.String)">registerTempTable</a></strong>(String&nbsp;tableName)</code>
<div class="block">Registers this RDD as a temporary table using the given name.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#repartition(int)">repartition</a></strong>(int&nbsp;numPartitions)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> that has exactly <code>numPartitions</code> partitions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#sample(boolean,%20double)">sample</a></strong>(boolean&nbsp;withReplacement,
      double&nbsp;fraction)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> by sampling a fraction of rows, using a random seed.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#sample(boolean,%20double,%20long)">sample</a></strong>(boolean&nbsp;withReplacement,
      double&nbsp;fraction,
      long&nbsp;seed)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> by sampling a fraction of rows.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String)">save</a></strong>(String&nbsp;path)</code>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame to the given path,
 using the default data source configured by spark.sql.sources.default and
 <code>SaveMode.ErrorIfExists</code> as the save mode.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String,%20org.apache.spark.sql.SaveMode)">save</a></strong>(String&nbsp;path,
    <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode)</code>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame to the given path and <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode,
 using the default data source configured by spark.sql.sources.default.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String,%20org.apache.spark.sql.SaveMode,%20java.util.Map)">save</a></strong>(String&nbsp;source,
    <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode,
    java.util.Map&lt;String,String&gt;&nbsp;options)</code>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame based on the given data source,
 <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String,%20org.apache.spark.sql.SaveMode,%20scala.collection.immutable.Map)">save</a></strong>(String&nbsp;source,
    <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode,
    scala.collection.immutable.Map&lt;String,String&gt;&nbsp;options)</code>
<div class="block">:: Experimental ::
 (Scala-specific)
 Saves the contents of this DataFrame based on the given data source,
 <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String,%20java.lang.String)">save</a></strong>(String&nbsp;path,
    String&nbsp;source)</code>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame to the given path based on the given data source,
 using <code>SaveMode.ErrorIfExists</code> as the save mode.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String,%20java.lang.String,%20org.apache.spark.sql.SaveMode)">save</a></strong>(String&nbsp;path,
    String&nbsp;source,
    <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode)</code>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame to the given path based on the given data source and
 <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#saveAsParquetFile(java.lang.String)">saveAsParquetFile</a></strong>(String&nbsp;path)</code>
<div class="block">Saves the contents of this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as a parquet file, preserving the schema.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String)">saveAsTable</a></strong>(String&nbsp;tableName)</code>
<div class="block">:: Experimental ::
 Creates a table from the the contents of this DataFrame.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String,%20org.apache.spark.sql.SaveMode)">saveAsTable</a></strong>(String&nbsp;tableName,
           <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode)</code>
<div class="block">:: Experimental ::
 Creates a table from the the contents of this DataFrame, using the default data source
 configured by spark.sql.sources.default and <code>SaveMode.ErrorIfExists</code> as the save mode.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String,%20java.lang.String)">saveAsTable</a></strong>(String&nbsp;tableName,
           String&nbsp;source)</code>
<div class="block">:: Experimental ::
 Creates a table at the given path from the the contents of this DataFrame
 based on a given data source and a set of options,
 using <code>SaveMode.ErrorIfExists</code> as the save mode.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String,%20java.lang.String,%20org.apache.spark.sql.SaveMode)">saveAsTable</a></strong>(String&nbsp;tableName,
           String&nbsp;source,
           <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode)</code>
<div class="block">:: Experimental ::
 Creates a table at the given path from the the contents of this DataFrame
 based on a given data source, <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String,%20java.lang.String,%20org.apache.spark.sql.SaveMode,%20java.util.Map)">saveAsTable</a></strong>(String&nbsp;tableName,
           String&nbsp;source,
           <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode,
           java.util.Map&lt;String,String&gt;&nbsp;options)</code>
<div class="block">:: Experimental ::
 Creates a table at the given path from the the contents of this DataFrame
 based on a given data source, <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String,%20java.lang.String,%20org.apache.spark.sql.SaveMode,%20scala.collection.immutable.Map)">saveAsTable</a></strong>(String&nbsp;tableName,
           String&nbsp;source,
           <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode,
           scala.collection.immutable.Map&lt;String,String&gt;&nbsp;options)</code>
<div class="block">:: Experimental ::
 (Scala-specific)
 Creates a table from the the contents of this DataFrame based on a given data source,
 <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>org.apache.spark.sql.types.StructType</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#schema()">schema</a></strong>()</code>
<div class="block">Returns the schema of this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#select(org.apache.spark.sql.Column...)">select</a></strong>(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;cols)</code>
<div class="block">Selects a set of expressions.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#select(scala.collection.Seq)">select</a></strong>(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;cols)</code>
<div class="block">Selects a set of expressions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#select(java.lang.String,%20scala.collection.Seq)">select</a></strong>(String&nbsp;col,
      scala.collection.Seq&lt;String&gt;&nbsp;cols)</code>
<div class="block">Selects a set of columns.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#select(java.lang.String,%20java.lang.String...)">select</a></strong>(String&nbsp;col,
      String...&nbsp;cols)</code>
<div class="block">Selects a set of columns.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#selectExpr(scala.collection.Seq)">selectExpr</a></strong>(scala.collection.Seq&lt;String&gt;&nbsp;exprs)</code>
<div class="block">Selects a set of SQL expressions.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#selectExpr(java.lang.String...)">selectExpr</a></strong>(String...&nbsp;exprs)</code>
<div class="block">Selects a set of SQL expressions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#show()">show</a></strong>()</code>
<div class="block">Displays the top 20 rows of <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> in a tabular form.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#show(int)">show</a></strong>(int&nbsp;numRows)</code>
<div class="block">Displays the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> in a tabular form.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#showString(int)">showString</a></strong>(int&nbsp;numRows)</code>
<div class="block">Internal API for Python</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#sort(org.apache.spark.sql.Column...)">sort</a></strong>(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;sortExprs)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#sort(scala.collection.Seq)">sort</a></strong>(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;sortExprs)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#sort(java.lang.String,%20scala.collection.Seq)">sort</a></strong>(String&nbsp;sortCol,
    scala.collection.Seq&lt;String&gt;&nbsp;sortCols)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the specified column, all in ascending order.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#sort(java.lang.String,%20java.lang.String...)">sort</a></strong>(String&nbsp;sortCol,
    String...&nbsp;sortCols)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the specified column, all in ascending order.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#sqlContext()">sqlContext</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>org.apache.spark.sql.Row[]</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#take(int)">take</a></strong>(int&nbsp;n)</code>
<div class="block">Returns the first <code>n</code> rows in the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#toDF()">toDF</a></strong>()</code>
<div class="block">Returns the object itself.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#toDF(scala.collection.Seq)">toDF</a></strong>(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with columns renamed.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#toDF(java.lang.String...)">toDF</a></strong>(String...&nbsp;colNames)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with columns renamed.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</a>&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#toJavaRDD()">toJavaRDD</a></strong>()</code>
<div class="block">Returns the content of the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as a <code>JavaRDD</code> of <code>Row</code>s.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;String&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#toJSON()">toJSON</a></strong>()</code>
<div class="block">Returns the content of the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as a RDD of JSON strings.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#toSchemaRDD()">toSchemaRDD</a></strong>()</code>
<div class="block">Left here for backward compatibility.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>String</code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#toString()">toString</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#unionAll(org.apache.spark.sql.DataFrame)">unionAll</a></strong>(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;other)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> containing union of rows in this frame and another frame.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#unpersist()">unpersist</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#unpersist(boolean)">unpersist</a></strong>(boolean&nbsp;blocking)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#where(org.apache.spark.sql.Column)">where</a></strong>(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;condition)</code>
<div class="block">Filters rows using the given condition.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#withColumn(java.lang.String,%20org.apache.spark.sql.Column)">withColumn</a></strong>(String&nbsp;colName,
          <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;col)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> by adding a column.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a></code></td>
<td class="colLast"><code><strong><a href="../../../../org/apache/spark/sql/DataFrame.html#withColumnRenamed(java.lang.String,%20java.lang.String)">withColumnRenamed</a></strong>(String&nbsp;existingName,
                 String&nbsp;newName)</code>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with a column renamed.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, wait, wait, wait</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="DataFrame(org.apache.spark.sql.SQLContext, org.apache.spark.sql.SQLContext.QueryExecution)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>DataFrame</h4>
<pre>public&nbsp;DataFrame(<a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a>&nbsp;sqlContext,
         org.apache.spark.sql.SQLContext.QueryExecution&nbsp;queryExecution)</pre>
</li>
</ul>
<a name="DataFrame(org.apache.spark.sql.SQLContext, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>DataFrame</h4>
<pre>public&nbsp;DataFrame(<a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a>&nbsp;sqlContext,
         org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&nbsp;logicalPlan)</pre>
<div class="block">A constructor that automatically analyzes the logical plan.
 <p>
 This reports error eagerly as the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> is constructed, unless
 <code>SQLConf.dataFrameEagerAnalysis</code> is turned off.</div>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method_detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="toDF(java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toDF</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;toDF(String...&nbsp;colNames)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with columns renamed. This can be quite convenient in conversion
 from a RDD of tuples into a <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with meaningful names. For example:
 <pre><code>
   val rdd: RDD[(Int, String)] = ...
   rdd.toDF()  // this implicit conversion creates a DataFrame with column name _1 and _2
   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
 </code></pre></div>
</li>
</ul>
<a name="sort(java.lang.String, java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sort</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;sort(String&nbsp;sortCol,
             String...&nbsp;sortCols)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the specified column, all in ascending order.
 <pre><code>
   // The following 3 are equivalent
   df.sort("sortcol")
   df.sort($"sortcol")
   df.sort($"sortcol".asc)
 </code></pre></div>
</li>
</ul>
<a name="sort(org.apache.spark.sql.Column...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sort</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;sort(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;sortExprs)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions. For example:
 <pre><code>
   df.sort($"col1", $"col2".desc)
 </code></pre></div>
</li>
</ul>
<a name="orderBy(java.lang.String, java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>orderBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;orderBy(String&nbsp;sortCol,
                String...&nbsp;sortCols)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.
 This is an alias of the <code>sort</code> function.</div>
</li>
</ul>
<a name="orderBy(org.apache.spark.sql.Column...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>orderBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;orderBy(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;sortExprs)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.
 This is an alias of the <code>sort</code> function.</div>
</li>
</ul>
<a name="select(org.apache.spark.sql.Column...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>select</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;select(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;cols)</pre>
<div class="block">Selects a set of expressions.
 <pre><code>
   df.select($"colA", $"colB" + 1)
 </code></pre></div>
</li>
</ul>
<a name="select(java.lang.String, java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>select</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;select(String&nbsp;col,
               String...&nbsp;cols)</pre>
<div class="block">Selects a set of columns. This is a variant of <code>select</code> that can only select
 existing columns using column names (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // The following two are equivalent:
   df.select("colA", "colB")
   df.select($"colA", $"colB")
 </code></pre></div>
</li>
</ul>
<a name="selectExpr(java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>selectExpr</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;selectExpr(String...&nbsp;exprs)</pre>
<div class="block">Selects a set of SQL expressions. This is a variant of <code>select</code> that accepts
 SQL expressions.
 <p>
 <pre><code>
   df.selectExpr("colA", "colB as newName", "abs(colC)")
 </code></pre></div>
</li>
</ul>
<a name="groupBy(org.apache.spark.sql.Column...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>groupBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</a>&nbsp;groupBy(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;cols)</pre>
<div class="block">Groups the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> using the specified columns, so we can run aggregation on them.
 See <a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><code>GroupedData</code></a> for all the available aggregate functions.
 <p>
 <pre><code>
   // Compute the average for all numeric columns grouped by department.
   df.groupBy($"department").avg()

   // Compute the max age and average salary, grouped by department and gender.
   df.groupBy($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre></div>
</li>
</ul>
<a name="groupBy(java.lang.String, java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>groupBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</a>&nbsp;groupBy(String&nbsp;col1,
                  String...&nbsp;cols)</pre>
<div class="block">Groups the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> using the specified columns, so we can run aggregation on them.
 See <a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><code>GroupedData</code></a> for all the available aggregate functions.
 <p>
 This is a variant of groupBy that can only group by existing columns using column names
 (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // Compute the average for all numeric columns grouped by department.
   df.groupBy("department").avg()

   // Compute the max age and average salary, grouped by department and gender.
   df.groupBy($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre></div>
</li>
</ul>
<a name="agg(org.apache.spark.sql.Column, org.apache.spark.sql.Column...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>agg</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;agg(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;expr,
            <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>...&nbsp;exprs)</pre>
<div class="block">Aggregates on the entire <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> without groups.
 {{
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg(max($"age"), avg($"salary"))
   df.groupBy().agg(max($"age"), avg($"salary"))
 }}</div>
</li>
</ul>
<a name="describe(java.lang.String...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>describe</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;describe(String...&nbsp;cols)</pre>
<div class="block">Computes statistics for numeric columns, including count, mean, stddev, min, and max.
 If no columns are given, this function computes statistics for all numerical columns.
 <p>
 This function is meant for exploratory data analysis, as we make no guarantee about the
 backward compatibility of the schema of the resulting <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>. If you want to
 programmatically compute summary statistics, use the <code>agg</code> function instead.
 <p>
 <pre><code>
   df.describe("age", "height").show()

   // output:
   // summary age   height
   // count   10.0  10.0
   // mean    53.3  178.05
   // stddev  11.6  15.7
   // min     18.0  163.0
   // max     92.0  192.0
 </code></pre>
 <p></div>
</li>
</ul>
<a name="sqlContext()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sqlContext</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</a>&nbsp;sqlContext()</pre>
</li>
</ul>
<a name="queryExecution()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>queryExecution</h4>
<pre>public&nbsp;org.apache.spark.sql.SQLContext.QueryExecution&nbsp;queryExecution()</pre>
</li>
</ul>
<a name="showString(int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>showString</h4>
<pre>public&nbsp;String&nbsp;showString(int&nbsp;numRows)</pre>
<div class="block">Internal API for Python</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>numRows</code> - Number of rows to show</dd></dl>
</li>
</ul>
<a name="toString()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toString</h4>
<pre>public&nbsp;String&nbsp;toString()</pre>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code>toString</code>&nbsp;in class&nbsp;<code>Object</code></dd>
</dl>
</li>
</ul>
<a name="toSchemaRDD()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toSchemaRDD</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;toSchemaRDD()</pre>
<div class="block">Left here for backward compatibility.</div>
</li>
</ul>
<a name="toDF()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toDF</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;toDF()</pre>
<div class="block">Returns the object itself.</div>
</li>
</ul>
<a name="toDF(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toDF</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;toDF(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with columns renamed. This can be quite convenient in conversion
 from a RDD of tuples into a <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with meaningful names. For example:
 <pre><code>
   val rdd: RDD[(Int, String)] = ...
   rdd.toDF()  // this implicit conversion creates a DataFrame with column name _1 and _2
   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
 </code></pre></div>
</li>
</ul>
<a name="schema()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>schema</h4>
<pre>public&nbsp;org.apache.spark.sql.types.StructType&nbsp;schema()</pre>
<div class="block">Returns the schema of this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</li>
</ul>
<a name="dtypes()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>dtypes</h4>
<pre>public&nbsp;scala.Tuple2&lt;String,String&gt;[]&nbsp;dtypes()</pre>
<div class="block">Returns all column names and their data types as an array.</div>
</li>
</ul>
<a name="columns()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>columns</h4>
<pre>public&nbsp;String[]&nbsp;columns()</pre>
<div class="block">Returns all column names as an array.</div>
</li>
</ul>
<a name="printSchema()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>printSchema</h4>
<pre>public&nbsp;void&nbsp;printSchema()</pre>
<div class="block">Prints the schema to the console in a nice tree format.</div>
</li>
</ul>
<a name="explain(boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>explain</h4>
<pre>public&nbsp;void&nbsp;explain(boolean&nbsp;extended)</pre>
<div class="block">Prints the plans (logical and physical) to the console for debugging purpose.</div>
</li>
</ul>
<a name="explain()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>explain</h4>
<pre>public&nbsp;void&nbsp;explain()</pre>
<div class="block">Only prints the physical plan to the console for debugging purpose.</div>
</li>
</ul>
<a name="isLocal()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isLocal</h4>
<pre>public&nbsp;boolean&nbsp;isLocal()</pre>
<div class="block">Returns true if the <code>collect</code> and <code>take</code> methods can be run locally
 (without any Spark executors).</div>
</li>
</ul>
<a name="show(int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>show</h4>
<pre>public&nbsp;void&nbsp;show(int&nbsp;numRows)</pre>
<div class="block">Displays the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> in a tabular form. For example:
 <pre><code>
   year  month AVG('Adj Close) MAX('Adj Close)
   1980  12    0.503218        0.595103
   1981  01    0.523289        0.570307
   1982  02    0.436504        0.475256
   1983  03    0.410516        0.442194
   1984  04    0.450090        0.483521
 </code></pre></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>numRows</code> - Number of rows to show
 <p></dd></dl>
</li>
</ul>
<a name="show()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>show</h4>
<pre>public&nbsp;void&nbsp;show()</pre>
<div class="block">Displays the top 20 rows of <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> in a tabular form.</div>
</li>
</ul>
<a name="join(org.apache.spark.sql.DataFrame)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>join</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;join(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;right)</pre>
<div class="block">Cartesian join with another <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.
 <p>
 Note that cartesian joins are very expensive without an extra filter that can be pushed down.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>right</code> - Right side of the join operation.</dd></dl>
</li>
</ul>
<a name="join(org.apache.spark.sql.DataFrame, org.apache.spark.sql.Column)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>join</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;join(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;right,
             <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;joinExprs)</pre>
<div class="block">Inner join with another <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>, using the given join expression.
 <p>
 <pre><code>
   // The following two are equivalent:
   df1.join(df2, $"df1Key" === $"df2Key")
   df1.join(df2).where($"df1Key" === $"df2Key")
 </code></pre></div>
</li>
</ul>
<a name="join(org.apache.spark.sql.DataFrame, org.apache.spark.sql.Column, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>join</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;join(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;right,
             <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;joinExprs,
             String&nbsp;joinType)</pre>
<div class="block">Join with another <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>, using the given join expression. The following performs
 a full outer join between <code>df1</code> and <code>df2</code>.
 <p>
 <pre><code>
   // Scala:
   import org.apache.spark.sql.functions._
   df1.join(df2, $"df1Key" === $"df2Key", "outer")

   // Java:
   import static org.apache.spark.sql.functions.*;
   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
 </code></pre>
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>right</code> - Right side of the join.</dd><dd><code>joinExprs</code> - Join expression.</dd><dd><code>joinType</code> - One of: <code>inner</code>, <code>outer</code>, <code>left_outer</code>, <code>right_outer</code>, <code>semijoin</code>.</dd></dl>
</li>
</ul>
<a name="sort(java.lang.String, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sort</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;sort(String&nbsp;sortCol,
             scala.collection.Seq&lt;String&gt;&nbsp;sortCols)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the specified column, all in ascending order.
 <pre><code>
   // The following 3 are equivalent
   df.sort("sortcol")
   df.sort($"sortcol")
   df.sort($"sortcol".asc)
 </code></pre></div>
</li>
</ul>
<a name="sort(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sort</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;sort(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;sortExprs)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions. For example:
 <pre><code>
   df.sort($"col1", $"col2".desc)
 </code></pre></div>
</li>
</ul>
<a name="orderBy(java.lang.String, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>orderBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;orderBy(String&nbsp;sortCol,
                scala.collection.Seq&lt;String&gt;&nbsp;sortCols)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.
 This is an alias of the <code>sort</code> function.</div>
</li>
</ul>
<a name="orderBy(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>orderBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;orderBy(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;sortExprs)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> sorted by the given expressions.
 This is an alias of the <code>sort</code> function.</div>
</li>
</ul>
<a name="apply(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>apply</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;apply(String&nbsp;colName)</pre>
<div class="block">Selects column based on the column name and return it as a <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><code>Column</code></a>.</div>
</li>
</ul>
<a name="col(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>col</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;col(String&nbsp;colName)</pre>
<div class="block">Selects column based on the column name and return it as a <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><code>Column</code></a>.</div>
</li>
</ul>
<a name="as(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>as</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;as(String&nbsp;alias)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with an alias set.</div>
</li>
</ul>
<a name="as(scala.Symbol)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>as</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;as(scala.Symbol&nbsp;alias)</pre>
<div class="block">(Scala-specific) Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with an alias set.</div>
</li>
</ul>
<a name="select(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>select</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;select(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;cols)</pre>
<div class="block">Selects a set of expressions.
 <pre><code>
   df.select($"colA", $"colB" + 1)
 </code></pre></div>
</li>
</ul>
<a name="select(java.lang.String, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>select</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;select(String&nbsp;col,
               scala.collection.Seq&lt;String&gt;&nbsp;cols)</pre>
<div class="block">Selects a set of columns. This is a variant of <code>select</code> that can only select
 existing columns using column names (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // The following two are equivalent:
   df.select("colA", "colB")
   df.select($"colA", $"colB")
 </code></pre></div>
</li>
</ul>
<a name="selectExpr(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>selectExpr</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;selectExpr(scala.collection.Seq&lt;String&gt;&nbsp;exprs)</pre>
<div class="block">Selects a set of SQL expressions. This is a variant of <code>select</code> that accepts
 SQL expressions.
 <p>
 <pre><code>
   df.selectExpr("colA", "colB as newName", "abs(colC)")
 </code></pre></div>
</li>
</ul>
<a name="filter(org.apache.spark.sql.Column)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>filter</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;filter(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;condition)</pre>
<div class="block">Filters rows using the given condition.
 <pre><code>
   // The following are equivalent:
   peopleDf.filter($"age" &gt; 15)
   peopleDf.where($"age" &gt; 15)
   peopleDf($"age" &gt; 15)
 </code></pre></div>
</li>
</ul>
<a name="filter(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>filter</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;filter(String&nbsp;conditionExpr)</pre>
<div class="block">Filters rows using the given SQL expression.
 <pre><code>
   peopleDf.filter("age &gt; 15")
 </code></pre></div>
</li>
</ul>
<a name="where(org.apache.spark.sql.Column)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>where</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;where(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;condition)</pre>
<div class="block">Filters rows using the given condition. This is an alias for <code>filter</code>.
 <pre><code>
   // The following are equivalent:
   peopleDf.filter($"age" &gt; 15)
   peopleDf.where($"age" &gt; 15)
   peopleDf($"age" &gt; 15)
 </code></pre></div>
</li>
</ul>
<a name="groupBy(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>groupBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</a>&nbsp;groupBy(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;cols)</pre>
<div class="block">Groups the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> using the specified columns, so we can run aggregation on them.
 See <a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><code>GroupedData</code></a> for all the available aggregate functions.
 <p>
 <pre><code>
   // Compute the average for all numeric columns grouped by department.
   df.groupBy($"department").avg()

   // Compute the max age and average salary, grouped by department and gender.
   df.groupBy($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre></div>
</li>
</ul>
<a name="groupBy(java.lang.String, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>groupBy</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</a>&nbsp;groupBy(String&nbsp;col1,
                  scala.collection.Seq&lt;String&gt;&nbsp;cols)</pre>
<div class="block">Groups the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> using the specified columns, so we can run aggregation on them.
 See <a href="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><code>GroupedData</code></a> for all the available aggregate functions.
 <p>
 This is a variant of groupBy that can only group by existing columns using column names
 (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // Compute the average for all numeric columns grouped by department.
   df.groupBy("department").avg()

   // Compute the max age and average salary, grouped by department and gender.
   df.groupBy($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre></div>
</li>
</ul>
<a name="agg(scala.Tuple2, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>agg</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;agg(scala.Tuple2&lt;String,String&gt;&nbsp;aggExpr,
            scala.collection.Seq&lt;scala.Tuple2&lt;String,String&gt;&gt;&nbsp;aggExprs)</pre>
<div class="block">(Scala-specific) Compute aggregates by specifying a map from column name to
 aggregate methods. The resulting <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> will also contain the grouping columns.
 <p>
 The available aggregate methods are <code>avg</code>, <code>max</code>, <code>min</code>, <code>sum</code>, <code>count</code>.
 <pre><code>
   // Selects the age of the oldest employee and the aggregate expense for each department
   df.groupBy("department").agg(
     "age" -&gt; "max",
     "expense" -&gt; "sum"
   )
 </code></pre></div>
</li>
</ul>
<a name="agg(scala.collection.immutable.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>agg</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;agg(scala.collection.immutable.Map&lt;String,String&gt;&nbsp;exprs)</pre>
<div class="block">(Scala-specific) Aggregates on the entire <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> without groups.
 {{
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg(Map("age" -> "max", "salary" -> "avg"))
   df.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
 }}</div>
</li>
</ul>
<a name="agg(java.util.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>agg</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;agg(java.util.Map&lt;String,String&gt;&nbsp;exprs)</pre>
<div class="block">(Java-specific) Aggregates on the entire <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> without groups.
 {{
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg(Map("age" -> "max", "salary" -> "avg"))
   df.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
 }}</div>
</li>
</ul>
<a name="agg(org.apache.spark.sql.Column, scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>agg</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;agg(<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;expr,
            scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;exprs)</pre>
<div class="block">Aggregates on the entire <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> without groups.
 {{
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg(max($"age"), avg($"salary"))
   df.groupBy().agg(max($"age"), avg($"salary"))
 }}</div>
</li>
</ul>
<a name="limit(int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>limit</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;limit(int&nbsp;n)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> by taking the first <code>n</code> rows. The difference between this function
 and <code>head</code> is that <code>head</code> returns an array while <code>limit</code> returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
</li>
</ul>
<a name="unionAll(org.apache.spark.sql.DataFrame)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>unionAll</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;unionAll(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;other)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> containing union of rows in this frame and another frame.
 This is equivalent to <code>UNION ALL</code> in SQL.</div>
</li>
</ul>
<a name="intersect(org.apache.spark.sql.DataFrame)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>intersect</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;intersect(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;other)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> containing rows only in both this frame and another frame.
 This is equivalent to <code>INTERSECT</code> in SQL.</div>
</li>
</ul>
<a name="except(org.apache.spark.sql.DataFrame)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>except</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;except(<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;other)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> containing rows in this frame but not in another frame.
 This is equivalent to <code>EXCEPT</code> in SQL.</div>
</li>
</ul>
<a name="sample(boolean, double, long)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sample</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;sample(boolean&nbsp;withReplacement,
               double&nbsp;fraction,
               long&nbsp;seed)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> by sampling a fraction of rows.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>withReplacement</code> - Sample with replacement or not.</dd><dd><code>fraction</code> - Fraction of rows to generate.</dd><dd><code>seed</code> - Seed for sampling.</dd></dl>
</li>
</ul>
<a name="sample(boolean, double)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>sample</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;sample(boolean&nbsp;withReplacement,
               double&nbsp;fraction)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> by sampling a fraction of rows, using a random seed.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>withReplacement</code> - Sample with replacement or not.</dd><dd><code>fraction</code> - Fraction of rows to generate.</dd></dl>
</li>
</ul>
<a name="explode(scala.collection.Seq, scala.Function1, scala.reflect.api.TypeTags.TypeTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>explode</h4>
<pre>public&nbsp;&lt;A extends scala.Product&gt;&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;explode(scala.collection.Seq&lt;<a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&gt;&nbsp;input,
                                          scala.Function1&lt;org.apache.spark.sql.Row,scala.collection.TraversableOnce&lt;A&gt;&gt;&nbsp;f,
                                          scala.reflect.api.TypeTags.TypeTag&lt;A&gt;&nbsp;evidence$1)</pre>
<div class="block">(Scala-specific) Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> where each row has been expanded to zero or more
 rows by the provided function.  This is similar to a <code>LATERAL VIEW</code> in HiveQL. The columns of
 the input row are implicitly joined with each row that is output by the function.
 <p>
 The following example uses this function to count the number of books which contain
 a given word:
 <p>
 <pre><code>
   case class Book(title: String, words: String)
   val df: RDD[Book]

   case class Word(word: String)
   val allWords = df.explode('words) {
     case Row(words: String) =&gt; words.split(" ").map(Word(_))
   }

   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
 </code></pre></div>
</li>
</ul>
<a name="explode(java.lang.String, java.lang.String, scala.Function1, scala.reflect.api.TypeTags.TypeTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>explode</h4>
<pre>public&nbsp;&lt;A,B&gt;&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;explode(String&nbsp;inputColumn,
                      String&nbsp;outputColumn,
                      scala.Function1&lt;A,scala.collection.TraversableOnce&lt;B&gt;&gt;&nbsp;f,
                      scala.reflect.api.TypeTags.TypeTag&lt;B&gt;&nbsp;evidence$2)</pre>
<div class="block">(Scala-specific) Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> where a single column has been expanded to zero
 or more rows by the provided function.  This is similar to a <code>LATERAL VIEW</code> in HiveQL. All
 columns of the input row are implicitly joined with each value that is output by the function.
 <p>
 <pre><code>
   df.explode("words", "word")(words: String =&gt; words.split(" "))
 </code></pre></div>
</li>
</ul>
<a name="withColumn(java.lang.String, org.apache.spark.sql.Column)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>withColumn</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;withColumn(String&nbsp;colName,
                   <a href="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</a>&nbsp;col)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> by adding a column.</div>
</li>
</ul>
<a name="withColumnRenamed(java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>withColumnRenamed</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;withColumnRenamed(String&nbsp;existingName,
                          String&nbsp;newName)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> with a column renamed.</div>
</li>
</ul>
<a name="describe(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>describe</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;describe(scala.collection.Seq&lt;String&gt;&nbsp;cols)</pre>
<div class="block">Computes statistics for numeric columns, including count, mean, stddev, min, and max.
 If no columns are given, this function computes statistics for all numerical columns.
 <p>
 This function is meant for exploratory data analysis, as we make no guarantee about the
 backward compatibility of the schema of the resulting <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>. If you want to
 programmatically compute summary statistics, use the <code>agg</code> function instead.
 <p>
 <pre><code>
   df.describe("age", "height").show()

   // output:
   // summary age   height
   // count   10.0  10.0
   // mean    53.3  178.05
   // stddev  11.6  15.7
   // min     18.0  163.0
   // max     92.0  192.0
 </code></pre>
 <p></div>
</li>
</ul>
<a name="head(int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>head</h4>
<pre>public&nbsp;org.apache.spark.sql.Row[]&nbsp;head(int&nbsp;n)</pre>
<div class="block">Returns the first <code>n</code> rows.</div>
</li>
</ul>
<a name="head()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>head</h4>
<pre>public&nbsp;org.apache.spark.sql.Row&nbsp;head()</pre>
<div class="block">Returns the first row.</div>
</li>
</ul>
<a name="first()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>first</h4>
<pre>public&nbsp;org.apache.spark.sql.Row&nbsp;first()</pre>
<div class="block">Returns the first row. Alias for head().</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#first()">first</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="map(scala.Function1, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>map</h4>
<pre>public&nbsp;&lt;R&gt;&nbsp;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;R&gt;&nbsp;map(scala.Function1&lt;org.apache.spark.sql.Row,R&gt;&nbsp;f,
             scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$3)</pre>
<div class="block">Returns a new RDD by applying a function to all rows of this DataFrame.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#map(scala.Function1,%20scala.reflect.ClassTag)">map</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="flatMap(scala.Function1, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>flatMap</h4>
<pre>public&nbsp;&lt;R&gt;&nbsp;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;R&gt;&nbsp;flatMap(scala.Function1&lt;org.apache.spark.sql.Row,scala.collection.TraversableOnce&lt;R&gt;&gt;&nbsp;f,
                 scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$4)</pre>
<div class="block">Returns a new RDD by first applying a function to all rows of this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>,
 and then flattening the results.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#flatMap(scala.Function1,%20scala.reflect.ClassTag)">flatMap</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="mapPartitions(scala.Function1, scala.reflect.ClassTag)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>mapPartitions</h4>
<pre>public&nbsp;&lt;R&gt;&nbsp;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;R&gt;&nbsp;mapPartitions(scala.Function1&lt;scala.collection.Iterator&lt;org.apache.spark.sql.Row&gt;,scala.collection.Iterator&lt;R&gt;&gt;&nbsp;f,
                       scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$5)</pre>
<div class="block">Returns a new RDD by applying a function to each partition of this DataFrame.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#mapPartitions(scala.Function1,%20scala.reflect.ClassTag)">mapPartitions</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="foreach(scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>foreach</h4>
<pre>public&nbsp;void&nbsp;foreach(scala.Function1&lt;org.apache.spark.sql.Row,scala.runtime.BoxedUnit&gt;&nbsp;f)</pre>
<div class="block">Applies a function <code>f</code> to all rows.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#foreach(scala.Function1)">foreach</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="foreachPartition(scala.Function1)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>foreachPartition</h4>
<pre>public&nbsp;void&nbsp;foreachPartition(scala.Function1&lt;scala.collection.Iterator&lt;org.apache.spark.sql.Row&gt;,scala.runtime.BoxedUnit&gt;&nbsp;f)</pre>
<div class="block">Applies a function f to each partition of this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#foreachPartition(scala.Function1)">foreachPartition</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="take(int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>take</h4>
<pre>public&nbsp;org.apache.spark.sql.Row[]&nbsp;take(int&nbsp;n)</pre>
<div class="block">Returns the first <code>n</code> rows in the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#take(int)">take</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="collect()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>collect</h4>
<pre>public&nbsp;org.apache.spark.sql.Row[]&nbsp;collect()</pre>
<div class="block">Returns an array that contains all of <code>Row</code>s in this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#collect()">collect</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="collectAsList()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>collectAsList</h4>
<pre>public&nbsp;java.util.List&lt;org.apache.spark.sql.Row&gt;&nbsp;collectAsList()</pre>
<div class="block">Returns a Java list that contains all of <code>Row</code>s in this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#collectAsList()">collectAsList</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="count()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>count</h4>
<pre>public&nbsp;long&nbsp;count()</pre>
<div class="block">Returns the number of rows in the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#count()">count</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="repartition(int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>repartition</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;repartition(int&nbsp;numPartitions)</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> that has exactly <code>numPartitions</code> partitions.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#repartition(int)">repartition</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="distinct()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>distinct</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;distinct()</pre>
<div class="block">Returns a new <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> that contains only the unique rows from this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#distinct()">distinct</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="persist()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>persist</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;persist()</pre>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#persist()">persist</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="cache()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>cache</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;cache()</pre>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#cache()">cache</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="persist(org.apache.spark.storage.StorageLevel)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>persist</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;persist(<a href="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</a>&nbsp;newLevel)</pre>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#persist(org.apache.spark.storage.StorageLevel)">persist</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="unpersist(boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>unpersist</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;unpersist(boolean&nbsp;blocking)</pre>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#unpersist(boolean)">unpersist</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="unpersist()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>unpersist</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</a>&nbsp;unpersist()</pre>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../org/apache/spark/sql/RDDApi.html#unpersist()">unpersist</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../org/apache/spark/sql/RDDApi.html" title="interface in org.apache.spark.sql">RDDApi</a>&lt;org.apache.spark.sql.Row&gt;</code></dd>
</dl>
</li>
</ul>
<a name="rdd()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>rdd</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;&nbsp;rdd()</pre>
<div class="block">Returns the content of the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as an <code>RDD</code> of <code>Row</code>s.</div>
</li>
</ul>
<a name="toJavaRDD()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toJavaRDD</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</a>&lt;org.apache.spark.sql.Row&gt;&nbsp;toJavaRDD()</pre>
<div class="block">Returns the content of the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as a <code>JavaRDD</code> of <code>Row</code>s.</div>
</li>
</ul>
<a name="javaRDD()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>javaRDD</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</a>&lt;org.apache.spark.sql.Row&gt;&nbsp;javaRDD()</pre>
<div class="block">Returns the content of the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as a <code>JavaRDD</code> of <code>Row</code>s.</div>
</li>
</ul>
<a name="registerTempTable(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>registerTempTable</h4>
<pre>public&nbsp;void&nbsp;registerTempTable(String&nbsp;tableName)</pre>
<div class="block">Registers this RDD as a temporary table using the given name.  The lifetime of this temporary
 table is tied to the <a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><code>SQLContext</code></a> that was used to create this DataFrame.
 <p></div>
</li>
</ul>
<a name="saveAsParquetFile(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsParquetFile</h4>
<pre>public&nbsp;void&nbsp;saveAsParquetFile(String&nbsp;path)</pre>
<div class="block">Saves the contents of this <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as a parquet file, preserving the schema.
 Files that are written out using this method can be read back in as a <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a>
 using the <code>parquetFile</code> function in <a href="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><code>SQLContext</code></a>.</div>
</li>
</ul>
<a name="saveAsTable(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName)</pre>
<div class="block">:: Experimental ::
 Creates a table from the the contents of this DataFrame.
 It will use the default data source configured by spark.sql.sources.default.
 This will fail if the table already exists.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.</div>
</li>
</ul>
<a name="saveAsTable(java.lang.String, org.apache.spark.sql.SaveMode)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName,
               <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode)</pre>
<div class="block">:: Experimental ::
 Creates a table from the the contents of this DataFrame, using the default data source
 configured by spark.sql.sources.default and <code>SaveMode.ErrorIfExists</code> as the save mode.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.</div>
</li>
</ul>
<a name="saveAsTable(java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName,
               String&nbsp;source)</pre>
<div class="block">:: Experimental ::
 Creates a table at the given path from the the contents of this DataFrame
 based on a given data source and a set of options,
 using <code>SaveMode.ErrorIfExists</code> as the save mode.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.</div>
</li>
</ul>
<a name="saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName,
               String&nbsp;source,
               <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode)</pre>
<div class="block">:: Experimental ::
 Creates a table at the given path from the the contents of this DataFrame
 based on a given data source, <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.</div>
</li>
</ul>
<a name="saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode, java.util.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName,
               String&nbsp;source,
               <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode,
               java.util.Map&lt;String,String&gt;&nbsp;options)</pre>
<div class="block">:: Experimental ::
 Creates a table at the given path from the the contents of this DataFrame
 based on a given data source, <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.</div>
</li>
</ul>
<a name="saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode, scala.collection.immutable.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>saveAsTable</h4>
<pre>public&nbsp;void&nbsp;saveAsTable(String&nbsp;tableName,
               String&nbsp;source,
               <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode,
               scala.collection.immutable.Map&lt;String,String&gt;&nbsp;options)</pre>
<div class="block">:: Experimental ::
 (Scala-specific)
 Creates a table from the the contents of this DataFrame based on a given data source,
 <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.</div>
</li>
</ul>
<a name="save(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save(String&nbsp;path)</pre>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame to the given path,
 using the default data source configured by spark.sql.sources.default and
 <code>SaveMode.ErrorIfExists</code> as the save mode.</div>
</li>
</ul>
<a name="save(java.lang.String, org.apache.spark.sql.SaveMode)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save(String&nbsp;path,
        <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode)</pre>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame to the given path and <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode,
 using the default data source configured by spark.sql.sources.default.</div>
</li>
</ul>
<a name="save(java.lang.String, java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save(String&nbsp;path,
        String&nbsp;source)</pre>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame to the given path based on the given data source,
 using <code>SaveMode.ErrorIfExists</code> as the save mode.</div>
</li>
</ul>
<a name="save(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save(String&nbsp;path,
        String&nbsp;source,
        <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode)</pre>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame to the given path based on the given data source and
 <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode.</div>
</li>
</ul>
<a name="save(java.lang.String, org.apache.spark.sql.SaveMode, java.util.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save(String&nbsp;source,
        <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode,
        java.util.Map&lt;String,String&gt;&nbsp;options)</pre>
<div class="block">:: Experimental ::
 Saves the contents of this DataFrame based on the given data source,
 <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options.</div>
</li>
</ul>
<a name="save(java.lang.String, org.apache.spark.sql.SaveMode, scala.collection.immutable.Map)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>save</h4>
<pre>public&nbsp;void&nbsp;save(String&nbsp;source,
        <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</a>&nbsp;mode,
        scala.collection.immutable.Map&lt;String,String&gt;&nbsp;options)</pre>
<div class="block">:: Experimental ::
 (Scala-specific)
 Saves the contents of this DataFrame based on the given data source,
 <a href="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><code>SaveMode</code></a> specified by mode, and a set of options</div>
</li>
</ul>
<a name="insertInto(java.lang.String, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>insertInto</h4>
<pre>public&nbsp;void&nbsp;insertInto(String&nbsp;tableName,
              boolean&nbsp;overwrite)</pre>
<div class="block">:: Experimental ::
 Adds the rows from this RDD to the specified table, optionally overwriting the existing data.</div>
</li>
</ul>
<a name="insertInto(java.lang.String)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>insertInto</h4>
<pre>public&nbsp;void&nbsp;insertInto(String&nbsp;tableName)</pre>
<div class="block">:: Experimental ::
 Adds the rows from this RDD to the specified table.
 Throws an exception if the table already exists.</div>
</li>
</ul>
<a name="toJSON()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>toJSON</h4>
<pre>public&nbsp;<a href="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;String&gt;&nbsp;toJSON()</pre>
<div class="block">Returns the content of the <a href="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><code>DataFrame</code></a> as a RDD of JSON strings.</div>
</li>
</ul>
<a name="createJDBCTable(java.lang.String, java.lang.String, boolean)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>createJDBCTable</h4>
<pre>public&nbsp;void&nbsp;createJDBCTable(String&nbsp;url,
                   String&nbsp;table,
                   boolean&nbsp;allowExisting)</pre>
<div class="block">Save this RDD to a JDBC database at <code>url</code> under the table name <code>table</code>.
 This will run a <code>CREATE TABLE</code> and a bunch of <code>INSERT INTO</code> statements.
 If you pass <code>true</code> for <code>allowExisting</code>, it will drop any table with the
 given name; if you pass <code>false</code>, it will throw if the table already
 exists.</div>
</li>
</ul>
<a name="insertIntoJDBC(java.lang.String, java.lang.String, boolean)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>insertIntoJDBC</h4>
<pre>public&nbsp;void&nbsp;insertIntoJDBC(String&nbsp;url,
                  String&nbsp;table,
                  boolean&nbsp;overwrite)</pre>
<div class="block">Save this RDD to a JDBC database at <code>url</code> under the table name <code>table</code>.
 Assumes the table already exists and has a compatible schema.  If you
 pass <code>true</code> for <code>overwrite</code>, it will <code>TRUNCATE</code> the table before
 performing the <code>INSERT</code>s.
 <p>
 The table must already exist on the database.  It must have a schema
 that is compatible with the schema of this RDD; inserting the rows of
 the RDD in order via the simple statement
 <code>INSERT INTO table VALUES (?, ?, ..., ?)</code> should not fail.</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/apache/spark/sql/ColumnName.html" title="class in org.apache.spark.sql"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../org/apache/spark/sql/DataFrameHolder.html" title="class in org.apache.spark.sql"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/apache/spark/sql/DataFrame.html" target="_top">Frames</a></li>
<li><a href="DataFrame.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
</body>
</html>
