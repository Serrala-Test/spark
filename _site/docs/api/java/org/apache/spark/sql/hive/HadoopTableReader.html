<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (version 1.7.0_71) on Thu Apr 02 14:27:48 PDT 2015 -->
<title>HadoopTableReader (Spark 1.4.0 JavaDoc)</title>
<meta name="date" content="2015-04-02">
<link rel="stylesheet" type="text/css" href="../../../../../stylesheet.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="HadoopTableReader (Spark 1.4.0 JavaDoc)";
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../../index-all.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../../org/apache/spark/sql/hive/ExtendedHiveQlParser.html" title="class in org.apache.spark.sql.hive"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../../org/apache/spark/sql/hive/HiveContext.html" title="class in org.apache.spark.sql.hive"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../../index.html?org/apache/spark/sql/hive/HadoopTableReader.html" target="_top">Frames</a></li>
<li><a href="HadoopTableReader.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.apache.spark.sql.hive</div>
<h2 title="Class HadoopTableReader" class="title">Class HadoopTableReader</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>Object</li>
<li>
<ul class="inheritance">
<li>org.apache.spark.sql.hive.HadoopTableReader</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<dl>
<dt>All Implemented Interfaces:</dt>
<dd><a href="../../../../../org/apache/spark/sql/hive/TableReader.html" title="interface in org.apache.spark.sql.hive">TableReader</a></dd>
</dl>
<hr>
<br>
<pre>public class <span class="strong">HadoopTableReader</span>
extends Object
implements <a href="../../../../../org/apache/spark/sql/hive/TableReader.html" title="interface in org.apache.spark.sql.hive">TableReader</a></pre>
<div class="block">Helper class for scanning tables stored in Hadoop - e.g., to read Hive tables that reside in the
 data warehouse directory.</div>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../../../../org/apache/spark/sql/hive/HadoopTableReader.html#HadoopTableReader(scala.collection.Seq,%20org.apache.spark.sql.hive.MetastoreRelation,%20org.apache.spark.sql.hive.HiveContext,%20org.apache.hadoop.hive.conf.HiveConf)">HadoopTableReader</a></strong>(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.Attribute&gt;&nbsp;attributes,
                 <a href="../../../../../org/apache/spark/sql/hive/MetastoreRelation.html" title="class in org.apache.spark.sql.hive">MetastoreRelation</a>&nbsp;relation,
                 <a href="../../../../../org/apache/spark/sql/hive/HiveContext.html" title="class in org.apache.spark.sql.hive">HiveContext</a>&nbsp;sc,
                 org.apache.hadoop.hive.conf.HiveConf&nbsp;hiveExtraConf)</code>&nbsp;</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method_summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span>Methods</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static scala.collection.Iterator&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/hive/HadoopTableReader.html#fillObject(scala.collection.Iterator,%20org.apache.hadoop.hive.serde2.Deserializer,%20scala.collection.Seq,%20org.apache.spark.sql.catalyst.expressions.MutableRow,%20org.apache.hadoop.hive.serde2.Deserializer)">fillObject</a></strong>(scala.collection.Iterator&lt;org.apache.hadoop.io.Writable&gt;&nbsp;iterator,
          org.apache.hadoop.hive.serde2.Deserializer&nbsp;rawDeser,
          scala.collection.Seq&lt;scala.Tuple2&lt;org.apache.spark.sql.catalyst.expressions.Attribute,Object&gt;&gt;&nbsp;nonPartitionKeyAttrs,
          org.apache.spark.sql.catalyst.expressions.MutableRow&nbsp;mutableRow,
          org.apache.hadoop.hive.serde2.Deserializer&nbsp;tableDeser)</code>
<div class="block">Transform all given raw <code>Writable</code>s into <code>Row</code>s.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static void</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/hive/HadoopTableReader.html#initializeLocalJobConfFunc(java.lang.String,%20org.apache.hadoop.hive.ql.plan.TableDesc,%20org.apache.hadoop.mapred.JobConf)">initializeLocalJobConfFunc</a></strong>(String&nbsp;path,
                          org.apache.hadoop.hive.ql.plan.TableDesc&nbsp;tableDesc,
                          org.apache.hadoop.mapred.JobConf&nbsp;jobConf)</code>
<div class="block">Curried.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/hive/HadoopTableReader.html#makeRDDForPartitionedTable(scala.collection.immutable.Map,%20scala.Option)">makeRDDForPartitionedTable</a></strong>(scala.collection.immutable.Map&lt;org.apache.hadoop.hive.ql.metadata.Partition,Class&lt;? extends org.apache.hadoop.hive.serde2.Deserializer&gt;&gt;&nbsp;partitionToDeserializer,
                          scala.Option&lt;org.apache.hadoop.fs.PathFilter&gt;&nbsp;filterOpt)</code>
<div class="block">Create a HadoopRDD for every partition key specified in the query.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/hive/HadoopTableReader.html#makeRDDForPartitionedTable(scala.collection.Seq)">makeRDDForPartitionedTable</a></strong>(scala.collection.Seq&lt;org.apache.hadoop.hive.ql.metadata.Partition&gt;&nbsp;partitions)</code>&nbsp;</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/hive/HadoopTableReader.html#makeRDDForTable(org.apache.hadoop.hive.ql.metadata.Table)">makeRDDForTable</a></strong>(org.apache.hadoop.hive.ql.metadata.Table&nbsp;hiveTable)</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code><a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;</code></td>
<td class="colLast"><code><strong><a href="../../../../../org/apache/spark/sql/hive/HadoopTableReader.html#makeRDDForTable(org.apache.hadoop.hive.ql.metadata.Table,%20java.lang.Class,%20scala.Option)">makeRDDForTable</a></strong>(org.apache.hadoop.hive.ql.metadata.Table&nbsp;hiveTable,
               Class&lt;? extends org.apache.hadoop.hive.serde2.Deserializer&gt;&nbsp;deserializerClass,
               scala.Option&lt;org.apache.hadoop.fs.PathFilter&gt;&nbsp;filterOpt)</code>
<div class="block">Creates a Hadoop RDD to read data from the target table's data directory.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="HadoopTableReader(scala.collection.Seq, org.apache.spark.sql.hive.MetastoreRelation, org.apache.spark.sql.hive.HiveContext, org.apache.hadoop.hive.conf.HiveConf)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>HadoopTableReader</h4>
<pre>public&nbsp;HadoopTableReader(scala.collection.Seq&lt;org.apache.spark.sql.catalyst.expressions.Attribute&gt;&nbsp;attributes,
                 <a href="../../../../../org/apache/spark/sql/hive/MetastoreRelation.html" title="class in org.apache.spark.sql.hive">MetastoreRelation</a>&nbsp;relation,
                 <a href="../../../../../org/apache/spark/sql/hive/HiveContext.html" title="class in org.apache.spark.sql.hive">HiveContext</a>&nbsp;sc,
                 org.apache.hadoop.hive.conf.HiveConf&nbsp;hiveExtraConf)</pre>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method_detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="initializeLocalJobConfFunc(java.lang.String, org.apache.hadoop.hive.ql.plan.TableDesc, org.apache.hadoop.mapred.JobConf)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>initializeLocalJobConfFunc</h4>
<pre>public static&nbsp;void&nbsp;initializeLocalJobConfFunc(String&nbsp;path,
                              org.apache.hadoop.hive.ql.plan.TableDesc&nbsp;tableDesc,
                              org.apache.hadoop.mapred.JobConf&nbsp;jobConf)</pre>
<div class="block">Curried. After given an argument for 'path', the resulting JobConf => Unit closure is used to
 instantiate a HadoopRDD.</div>
</li>
</ul>
<a name="fillObject(scala.collection.Iterator, org.apache.hadoop.hive.serde2.Deserializer, scala.collection.Seq, org.apache.spark.sql.catalyst.expressions.MutableRow, org.apache.hadoop.hive.serde2.Deserializer)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>fillObject</h4>
<pre>public static&nbsp;scala.collection.Iterator&lt;org.apache.spark.sql.Row&gt;&nbsp;fillObject(scala.collection.Iterator&lt;org.apache.hadoop.io.Writable&gt;&nbsp;iterator,
                                                             org.apache.hadoop.hive.serde2.Deserializer&nbsp;rawDeser,
                                                             scala.collection.Seq&lt;scala.Tuple2&lt;org.apache.spark.sql.catalyst.expressions.Attribute,Object&gt;&gt;&nbsp;nonPartitionKeyAttrs,
                                                             org.apache.spark.sql.catalyst.expressions.MutableRow&nbsp;mutableRow,
                                                             org.apache.hadoop.hive.serde2.Deserializer&nbsp;tableDeser)</pre>
<div class="block">Transform all given raw <code>Writable</code>s into <code>Row</code>s.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>iterator</code> - Iterator of all <code>Writable</code>s to be transformed</dd><dd><code>rawDeser</code> - The <code>Deserializer</code> associated with the input <code>Writable</code></dd><dd><code>nonPartitionKeyAttrs</code> - Attributes that should be filled together with their corresponding
                             positions in the output schema</dd><dd><code>mutableRow</code> - A reusable <code>MutableRow</code> that should be filled</dd><dd><code>tableDeser</code> - Table Deserializer</dd>
<dt><span class="strong">Returns:</span></dt><dd>An <code>Iterator[Row]</code> transformed from <code>iterator</code></dd></dl>
</li>
</ul>
<a name="makeRDDForTable(org.apache.hadoop.hive.ql.metadata.Table)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>makeRDDForTable</h4>
<pre>public&nbsp;<a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;&nbsp;makeRDDForTable(org.apache.hadoop.hive.ql.metadata.Table&nbsp;hiveTable)</pre>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../../org/apache/spark/sql/hive/TableReader.html#makeRDDForTable(org.apache.hadoop.hive.ql.metadata.Table)">makeRDDForTable</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../../org/apache/spark/sql/hive/TableReader.html" title="interface in org.apache.spark.sql.hive">TableReader</a></code></dd>
</dl>
</li>
</ul>
<a name="makeRDDForTable(org.apache.hadoop.hive.ql.metadata.Table, java.lang.Class, scala.Option)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>makeRDDForTable</h4>
<pre>public&nbsp;<a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;&nbsp;makeRDDForTable(org.apache.hadoop.hive.ql.metadata.Table&nbsp;hiveTable,
                                            Class&lt;? extends org.apache.hadoop.hive.serde2.Deserializer&gt;&nbsp;deserializerClass,
                                            scala.Option&lt;org.apache.hadoop.fs.PathFilter&gt;&nbsp;filterOpt)</pre>
<div class="block">Creates a Hadoop RDD to read data from the target table's data directory. Returns a transformed
 RDD that contains deserialized rows.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>hiveTable</code> - Hive metadata for the table being scanned.</dd><dd><code>deserializerClass</code> - Class of the SerDe used to deserialize Writables read from Hadoop.</dd><dd><code>filterOpt</code> - If defined, then the filter is used to reject files contained in the data
                  directory being read. If None, then all files are accepted.</dd></dl>
</li>
</ul>
<a name="makeRDDForPartitionedTable(scala.collection.Seq)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>makeRDDForPartitionedTable</h4>
<pre>public&nbsp;<a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;&nbsp;makeRDDForPartitionedTable(scala.collection.Seq&lt;org.apache.hadoop.hive.ql.metadata.Partition&gt;&nbsp;partitions)</pre>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../../../../org/apache/spark/sql/hive/TableReader.html#makeRDDForPartitionedTable(scala.collection.Seq)">makeRDDForPartitionedTable</a></code>&nbsp;in interface&nbsp;<code><a href="../../../../../org/apache/spark/sql/hive/TableReader.html" title="interface in org.apache.spark.sql.hive">TableReader</a></code></dd>
</dl>
</li>
</ul>
<a name="makeRDDForPartitionedTable(scala.collection.immutable.Map, scala.Option)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>makeRDDForPartitionedTable</h4>
<pre>public&nbsp;<a href="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</a>&lt;org.apache.spark.sql.Row&gt;&nbsp;makeRDDForPartitionedTable(scala.collection.immutable.Map&lt;org.apache.hadoop.hive.ql.metadata.Partition,Class&lt;? extends org.apache.hadoop.hive.serde2.Deserializer&gt;&gt;&nbsp;partitionToDeserializer,
                                                       scala.Option&lt;org.apache.hadoop.fs.PathFilter&gt;&nbsp;filterOpt)</pre>
<div class="block">Create a HadoopRDD for every partition key specified in the query. Note that for on-disk Hive
 tables, a data directory is created for each partition corresponding to keys specified using
 'PARTITION BY'.
 <p></div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>partitionToDeserializer</code> - Mapping from a Hive Partition metadata object to the SerDe
     class to use to deserialize input Writables from the corresponding partition.</dd><dd><code>filterOpt</code> - If defined, then the filter is used to reject files contained in the data
     subdirectory of each partition being read. If None, then all files are accepted.</dd></dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../../index-all.html">Index</a></li>
<li><a href="../../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../../org/apache/spark/sql/hive/ExtendedHiveQlParser.html" title="class in org.apache.spark.sql.hive"><span class="strong">Prev Class</span></a></li>
<li><a href="../../../../../org/apache/spark/sql/hive/HiveContext.html" title="class in org.apache.spark.sql.hive"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../../index.html?org/apache/spark/sql/hive/HadoopTableReader.html" target="_top">Frames</a></li>
<li><a href="HadoopTableReader.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
</body>
</html>
