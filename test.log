Using /home/wf/tools/jdk1.7.0_67 as default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
[0m[[0minfo[0m] [0mLoading project definition from /home/wf/code/spark1/project/project[0m
[0m[[0minfo[0m] [0mLoading project definition from /home/kf/.sbt/0.13/staging/ad8e8574a5bcb2d22d23/sbt-pom-reader/project[0m
[0m[[33mwarn[0m] [0mMultiple resolvers having different access mechanism configured with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).[0m
[0m[[33mwarn[0m] [0mThere may be incompatibilities among your library dependencies.[0m
[0m[[33mwarn[0m] [0mHere are some of the libraries that were evicted:[0m
[0m[[33mwarn[0m] [0m	* com.typesafe.sbt:sbt-git:0.6.1 -> 0.6.2[0m
[0m[[33mwarn[0m] [0m	* com.typesafe.sbt:sbt-site:0.7.0 -> 0.7.1[0m
[0m[[33mwarn[0m] [0mRun 'evicted' to see detailed eviction warnings[0m
[0m[[0minfo[0m] [0mLoading project definition from /home/wf/code/spark1/project[0m
[0m[[33mwarn[0m] [0mThere may be incompatibilities among your library dependencies.[0m
[0m[[33mwarn[0m] [0mHere are some of the libraries that were evicted:[0m
[0m[[33mwarn[0m] [0m	* org.apache.maven.wagon:wagon-provider-api:1.0-beta-6 -> 2.2[0m
[0m[[33mwarn[0m] [0mRun 'evicted' to see detailed eviction warnings[0m
Note: We ignore environment variables, when use of profile is detected in conjunction with environment variable.
[0m[[0minfo[0m] [0mSet current project to spark-parent (in build file:/home/wf/code/spark1/)[0m
[0m[[33mwarn[0m] [0mThere may be incompatibilities among your library dependencies.[0m
[0m[[33mwarn[0m] [0mHere are some of the libraries that were evicted:[0m
[0m[[33mwarn[0m] [0m	* commons-net:commons-net:2.2 -> 3.1[0m
[0m[[33mwarn[0m] [0m	* com.google.guava:guava:11.0.2 -> 14.0.1[0m
[0m[[33mwarn[0m] [0mRun 'evicted' to see detailed eviction warnings[0m
[0m[[0minfo[0m] [0mCompiling 1 Scala source to /home/wf/code/spark1/sql/hive/target/scala-2.10/test-classes...[0m
[0m[[33mwarn[0m] [0mthere were 1 deprecation warning(s); re-run with -deprecation for details[0m
[0m[[33mwarn[0m] [0mone warning found[0m
21:25:44.266 WARN org.apache.spark.util.Utils: Your hostname, kf resolves to a loopback address: 127.0.1.1; using 192.168.1.100 instead (on interface eth0)
21:25:44.267 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21:25:44.938 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21:26:21.442 WARN org.apache.hadoop.hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
[0m[[0minfo[0m] [0m[32mBigDataBenchmarkSuite:[0m[0m
[0m[[0minfo[0m] [0m[33m- No data files found for BigDataBenchmark tests. !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32mConcurrentHiveSuite:[0m[0m
[0m[[0minfo[0m] [0m[33m- multiple instances not supported !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32mSQLQuerySuite:[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ctas1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ctas2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ctas3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ctas4
[0m[[0minfo[0m] [0m[32m- CTAS with serde (7 seconds, 85 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ordering not in select (320 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ordering not in agg (503 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_ctas_1234
[0m[[0minfo[0m] [0m[32m- double nested data (885 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_ctas_123
[0m[[0minfo[0m] [0m[32m- test CTAS (533 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test
[0m[[0minfo[0m] [0m[32m- SPARK-4825 save join to table (1 second, 183 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3708 Backticks aren't handled correctly is aliases (194 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3834 Backticks not correctly handled in subquery aliases (181 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3814 Support Bitwise & operator (170 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3814 Support Bitwise | operator (184 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3814 Support Bitwise ^ operator (164 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3814 Support Bitwise ~ operator (162 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-4154 Query does not work if it has 'not between' in Spark SQL and HQL (301 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-2554 SumDistinct partial aggregation (463 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mParquetMetastoreSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- project the partitioning column partitioned_parquet (483 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- project partitioning and non-partitioning columns partitioned_parquet (419 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- simple count partitioned_parquet (295 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- pruned count partitioned_parquet (136 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-existant partition partitioned_parquet (96 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multi-partition pruned count partitioned_parquet (177 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-partition predicates partitioned_parquet (342 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sum partitioned_parquet (148 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- hive udfs partitioned_parquet (536 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- project the partitioning column partitioned_parquet_with_key (397 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- project partitioning and non-partitioning columns partitioned_parquet_with_key (336 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- simple count partitioned_parquet_with_key (331 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- pruned count partitioned_parquet_with_key (153 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-existant partition partitioned_parquet_with_key (87 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multi-partition pruned count partitioned_parquet_with_key (216 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-partition predicates partitioned_parquet_with_key (385 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sum partitioned_parquet_with_key (175 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- hive udfs partitioned_parquet_with_key (505 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-part select(*) (114 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- conversion is working (136 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mParquetSourceSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- project the partitioning column partitioned_parquet (153 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- project partitioning and non-partitioning columns partitioned_parquet (190 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- simple count partitioned_parquet (121 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- pruned count partitioned_parquet (82 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-existant partition partitioned_parquet (62 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multi-partition pruned count partitioned_parquet (91 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-partition predicates partitioned_parquet (125 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sum partitioned_parquet (82 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- hive udfs partitioned_parquet (163 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- project the partitioning column partitioned_parquet_with_key (140 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- project partitioning and non-partitioning columns partitioned_parquet_with_key (178 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- simple count partitioned_parquet_with_key (116 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- pruned count partitioned_parquet_with_key (72 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-existant partition partitioned_parquet_with_key (53 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multi-partition pruned count partitioned_parquet_with_key (85 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-partition predicates partitioned_parquet_with_key (125 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sum partitioned_parquet_with_key (74 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- hive udfs partitioned_parquet_with_key (167 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- non-part select(*) (59 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveTypeCoercionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- 1 + 1 (14 seconds, 36 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1 + 1.0 (2 seconds, 196 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1 + 1L (988 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1 + 1S (3 seconds, 427 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1 + 1Y (742 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1 + '1' (839 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1.0 + 1 (751 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1.0 + 1.0 (996 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1.0 + 1L (734 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1.0 + 1S (680 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1.0 + 1Y (681 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1.0 + '1' (747 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1L + 1 (727 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1L + 1.0 (1 second, 23 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1L + 1L (692 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1L + 1S (658 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1L + 1Y (713 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1L + '1' (1 second, 73 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1S + 1 (757 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1S + 1.0 (696 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1S + 1L (1 second, 721 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1S + 1S (702 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1S + 1Y (671 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1S + '1' (1 second, 610 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1Y + 1 (751 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1Y + 1.0 (696 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1Y + 1L (685 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1Y + 1S (701 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1Y + 1Y (684 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 1Y + '1' (618 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- '1' + 1 (710 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- '1' + 1.0 (1 second, 48 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- '1' + 1L (770 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- '1' + 1S (682 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- '1' + 1Y (676 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- '1' + '1' (638 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then 1 else null end  (660 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then null else 1 end  (654 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then 1.0 else null end  (757 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then null else 1.0 end  (597 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then 1L else null end  (692 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then null else 1L end  (701 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then 1S else null end  (671 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then null else 1S end  (706 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then 1Y else null end  (610 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case when then null else 1Y end  (958 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- [SPARK-2210] boolean cast on boolean value should be removed (44 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mQueryTest:[0m[0m
[0m[[0minfo[0m] [0m[32mHiveQuerySuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- constant object inspector for generic udf (822 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- NaN to Decimal (717 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- constant null testing (828 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- constant array (657 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 0 values (704 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 1 value strings (828 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 1 value (894 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 2 values (814 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 2 values including null (916 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 1 value + null (700 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 1 value long (834 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 2 values long (725 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count distinct 1 value + null long (844 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- null case (958 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- single case (744 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- double case (778 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case else null (868 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- having no references (2 seconds, 779 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- boolean = number (1 second, 353 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/foo
[0m[[0minfo[0m] [0m[32m- CREATE TABLE AS runs once (374 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- between (1 second, 501 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- div (1 second, 505 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- division (73 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- modulus (1 second, 826 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Query expressed in SQL (79 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Query expressed in HiveQL (78 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Query with constant folding the CAST (73 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Constant Folding Optimization for AVG_SUM_COUNT (2 seconds, 474 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Cast Timestamp to Timestamp in UDF (1 second, 605 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Simple Average (2 seconds, 322 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Simple Average + 1 (1 second, 360 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Simple Average + 1 with group (1 second, 406 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- string literal (878 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Escape sequences (987 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- IgnoreExplain (946 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- trivial join where clause (1 second, 138 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- trivial join ON clause (853 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- small.cartesian (1 second, 328 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- length.udf (793 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- partitioned table scan (894 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- hash (765 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/createdtable
[0m[[0minfo[0m] [0m[32m- create table as (1 second, 177 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/testdb.db/createdtable
[0m[[0minfo[0m] [0m[32m- create table as with db name (4 seconds, 52 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- insert table with db name (1 second, 778 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/createdtable
[0m[[0minfo[0m] [0m[32m- insert into and insert overwrite (3 seconds, 444 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- transform (2 seconds, 84 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- LIKE (863 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- DISTINCT (1 second, 79 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- empty aggregate input (720 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- lateral view1 (1 second, 143 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- lateral view2 (739 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- lateral view3 (1 second, 108 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv2
[0m[[0minfo[0m] [0m[32m- lateral view4 (1 second, 295 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- lateral view5 (914 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- lateral view6 (665 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sampling (30 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SchemaRDD toString (77 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case statements with key #1 (1 second, 273 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case statements with key #2 (907 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case statements with key #3 (869 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case statements with key #4 (1 second, 118 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case statements WITHOUT key #1 (795 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case statements WITHOUT key #2 (704 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case statements WITHOUT key #3 (812 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case statements WITHOUT key #4 (712 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp cast #1 (44 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp cast #2 (878 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp cast #3 (774 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp cast #4 (767 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp cast #5 (717 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp cast #6 (773 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp cast #7 (731 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp cast #8 (752 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- select null from table (816 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- predicates contains an empty AttributeSet() references (44 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- implement identity function using case statement (87 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- non-boolean conditions in a CaseWhen are illegal !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- case sensitivity when query Hive table (1 second, 303 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case sensitivity: registered table (45 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-1704: Explain commands as a SchemaRDD (981 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-2180: HAVING support in GROUP BY clauses (positive) (1 second, 297 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-2180: HAVING with non-boolean clause raises no exceptions (93 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-2225: turn HAVING without GROUP BY into a simple filter (46 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Query Hive native command execution result (884 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Exactly once semantics for DDL and command statements (99 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- DESCRIBE commands (482 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/m
[0m[[0minfo[0m] [0m[32m- SPARK-2263: Insert Map<K, V> values (325 milliseconds)[0m[0m
21:29:04.471 ERROR hive.ql.exec.DDLTask: java.lang.RuntimeException: MetaException(message:java.lang.ClassNotFoundException Class org.apache.hadoop.hive.serde2.TestSerDe not found)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:290)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:281)
	at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3644)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:312)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:61)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:94)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite$$anonfun$22$$anonfun$apply$mcV$sp$9.apply(HiveQuerySuite.scala:635)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite$$anonfun$22$$anonfun$apply$mcV$sp$9.apply(HiveQuerySuite.scala:635)
	at org.scalatest.Assertions$class.intercept(Assertions.scala:997)
	at org.scalatest.FunSuite.intercept(FunSuite.scala:1555)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite$$anonfun$22.apply$mcV$sp(HiveQuerySuite.scala:634)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite$$anonfun$22.apply(HiveQuerySuite.scala:631)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite$$anonfun$22.apply(HiveQuerySuite.scala:631)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveQuerySuite.scala:41)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite.runTest(HiveQuerySuite.scala:41)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite.org$scalatest$BeforeAndAfter$$super$run(HiveQuerySuite.scala:41)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveQuerySuite.run(HiveQuerySuite.scala:41)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: MetaException(message:java.lang.ClassNotFoundException Class org.apache.hadoop.hive.serde2.TestSerDe not found)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:346)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:288)
	... 75 more

21:29:04.471 ERROR org.apache.hadoop.hive.ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.ClassNotFoundException Class org.apache.hadoop.hive.serde2.TestSerDe not found)
21:29:04.483 ERROR org.apache.spark.sql.hive.test.TestHive: 
======================
HIVE FAILURE OUTPUT
======================
rc stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=11)
Partition default.srcpart{ds=2008-04-08, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=12)
Partition default.srcpart{ds=2008-04-08, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=11)
Partition default.srcpart{ds=2008-04-09, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=12)
Partition default.srcpart{ds=2008-04-09, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
RESET 
set hive.table.parameters.default=
set datanucleus.cache.collections=true
set datanucleus.cache.collections.lazy=true
set hive.metastore.partition.name.whitelist.pattern=.*
SET javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/tmp/sparkHiveMetastore7605840045424272893;create=true
SET hive.metastore.warehouse.dir=/tmp/sparkHiveWarehouse7773807525406879524
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.src
Table default.src stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=11)
Partition default.srcpart{ds=2008-04-08, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=12)
Partition default.srcpart{ds=2008-04-08, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=11)
Partition default.srcpart{ds=2008-04-09, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=12)
Partition default.srcpart{ds=2008-04-09, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
OK
RESET 
set hive.table.parameters.default=
set datanucleus.cache.collections=true
set datanucleus.cache.collections.lazy=true
set hive.metastore.partition.name.whitelist.pattern=.*
SET javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/tmp/sparkHiveMetastore7605840045424272893;create=true
SET hive.metastore.warehouse.dir=/tmp/sparkHiveWarehouse7773807525406879524
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.src
Table default.src stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=11)
Partition default.srcpart{ds=2008-04-08, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=12)
Partition default.srcpart{ds=2008-04-08, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=11)
Partition default.srcpart{ds=2008-04-09, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=12)
Partition default.srcpart{ds=2008-04-09, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
RESET 
set hive.table.parameters.default=
set datanucleus.cache.collections=true
set datanucleus.cache.collections.lazy=true
set hive.metastore.partition.name.whitelist.pattern=.*
SET javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/tmp/sparkHiveMetastore7605840045424272893;create=true
SET hive.metastore.warehouse.dir=/tmp/sparkHiveWarehouse7773807525406879524
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.src
Table default.src stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=11)
Partition default.srcpart{ds=2008-04-08, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=12)
Partition default.srcpart{ds=2008-04-08, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=11)
Partition default.srcpart{ds=2008-04-09, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=12)
Partition default.srcpart{ds=2008-04-09, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
OK
OK
OK
RESET 
set hive.table.parameters.default=
set datanucleus.cache.collections=true
set datanucleus.cache.collections.lazy=true
set hive.metastore.partition.name.whitelist.pattern=.*
SET javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/tmp/sparkHiveMetastore7605840045424272893;create=true
SET hive.metastore.warehouse.dir=/tmp/sparkHiveWarehouse7773807525406879524
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.src
Table default.src stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=11)
Partition default.srcpart{ds=2008-04-08, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-08, hr=12)
Partition default.srcpart{ds=2008-04-08, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=11)
Partition default.srcpart{ds=2008-04-09, hr=11} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
Copying data from file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Copying file: file:/home/wf/code/spark1/sql/hive/src/test/resources/data/files/kv1.txt
Loading data to table default.srcpart partition (ds=2008-04-09, hr=12)
Partition default.srcpart{ds=2008-04-09, hr=12} stats: [numFiles=1, numRows=0, totalSize=5812, rawDataSize=0]
OK
OK
OK
OK
OK
OK
OK
OK
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.ClassNotFoundException Class org.apache.hadoop.hive.serde2.TestSerDe not found)

======================
END HIVE FAILURE OUTPUT
======================
          
[0m[[0minfo[0m] [0m[32m- ADD JAR command (176 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ADD FILE command (86 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- dynamic_partition (2 seconds, 221 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Dynamic partition folder layout (2 seconds, 751 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition spec validation (153 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3414 regression: should store analyzed logical plan when registering a temp table (73 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3810: PreInsertionCasts static partitioning support (226 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3810: PreInsertionCasts dynamic partitioning support (24 seconds, 607 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- parse HQL set commands (13 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SET commands semantics for a HiveContext (20 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- select from thrift based table (6 seconds, 264 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveMetastoreCatalogSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- struct field should accept underscore in sub-column name (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- udt to metastore type conversion (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32mJavaHiveQLSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- SELECT * FROM src (98 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Query Hive native command execution result (3 seconds, 401 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Exactly once semantics for DDL and command statements (132 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mCachedTableSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- cache table (293 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- cache invalidation (989 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Drop cached table (207 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- DROP nonexistant table (27 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correct error on uncache of non-cached table (22 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- 'CACHE TABLE' and 'UNCACHE TABLE' HiveQL statement (121 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- CACHE TABLE tableName AS SELECT * FROM anotherTable (123 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- CACHE TABLE tableName AS SELECT ... (121 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- CACHE LAZY TABLE tableName (120 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveInspectorSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Test wrap SettableStructObjectInspector (3 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- oi => datatype => oi (4 milliseconds)[0m[0m
########1
true
0
0
0
0
0.0
0.0
0
3914-10-23
123.123
1969-12-31 16:02:03.123
[B@3719e702
List(1, 2, 3)
Map(1 -> 2, 2 -> 1)
[1,2.0,3.0]
########
true
0
0
0
0
0.0
0.0
0
3914-10-23
123.123
1969-12-31 16:02:03.123
[B@3719e702
List(1, 2, 3)
Map(1 -> 2, 2 -> 1)
########
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector@22c3aade
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector@5c06e6a6
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector@4910cb7d
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector@465b037d
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector@11df5c7e
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector@6901922b
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector@5f315d5c
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector@2efceed0
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector@6d9fa104
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector@b0445d6
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector@4584ea40
org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector@192f3368
org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector@69d7fdc2
org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector@635501d7
########2
3914-10-23 ::: 3914-10-23 
[0m[[0minfo[0m] [0m[32m- wrap / unwrap null, constant null and writables (6 milliseconds)[0m[0m
3914-10-23 ::: 3914-10-23 
[0m[[0minfo[0m] [0m[32m- wrap / unwrap primitive writable object inspector (1 millisecond)[0m[0m
3914-10-23 ::: 3914-10-23 
[0m[[0minfo[0m] [0m[32m- wrap / unwrap primitive java object inspector (1 millisecond)[0m[0m
3914-10-23 ::: 3914-10-23 
[0m[[0minfo[0m] [0m[32m- wrap / unwrap Struct Type (1 millisecond)[0m[0m
[0m[[0minfo[0m] [0m[32m- wrap / unwrap Array Type (0 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- wrap / unwrap Map Type (2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mStatisticsSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- parse analyze commands (141 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- analyze MetastoreRelations (1 second, 723 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- estimates the size of a test MetastoreRelation (62 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto converts to broadcast hash join, by size estimate of a relation (286 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mInsertIntoHiveTableSuite:[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/createandinserttest
[0m[[0minfo[0m] [0m[32m- insertInto() HiveTable (807 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Double create fails when allowExisting = false (68 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Double create does not fail when allowExisting = true (13 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hivetablewithmapvalue
[0m[[0minfo[0m] [0m[32m- SPARK-4052: scala.collection.Map as value type of MapType (376 milliseconds)[0m[0m
21:29:51.388 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/tmp/1419686991353-0 specified for non-external table:table_with_partition
[0m[[0minfo[0m] [0m[32m- SPARK-4203:random partition directory order (1 second, 16 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hivetablewitharrayvalue
[0m[[0minfo[0m] [0m[32m- Insert ArrayType.containsNull == false (391 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hivetablewithmapvalue
[0m[[0minfo[0m] [0m[32m- Insert MapType.valueContainsNull == false (408 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hivetablewithstructvalue
[0m[[0minfo[0m] [0m[32m- Insert StructType.fields.exists(_.nullable == false) (466 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveCompatibilitySuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- add_part_exist (2 seconds, 69 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- add_part_multiple (3 seconds, 864 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- add_partition_no_whitelist (1 second, 804 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- add_partition_with_whitelist (1 second, 24 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alias_casted_column (899 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alter2 (2 seconds, 749 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter3_src
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter3_db.db/alter3_src
[0m[[0minfo[0m] [0m[32m- alter3 (3 seconds, 396 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alter4 (1 second, 555 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter5_src
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter5/parta
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter5_db.db/alter5_src
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter5_db.db/alter5/parta
[0m[[0minfo[0m] [0m[32m- alter5 (2 seconds, 199 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- alter_char1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- alter_char2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- alter_db_owner !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- alter_index (1 second, 287 milliseconds)[0m[0m
21:30:17.528 WARN org.apache.hadoop.mapreduce.JobSubmitter: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
21:30:17.646 WARN org.apache.hadoop.conf.Configuration: file:/tmp/hadoop-kf/mapred/staging/kf1954455609/.staging/job_local1954455609_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
21:30:17.648 WARN org.apache.hadoop.conf.Configuration: file:/tmp/hadoop-kf/mapred/staging/kf1954455609/.staging/job_local1954455609_0001/job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
21:30:17.709 WARN org.apache.hadoop.conf.Configuration: file:/tmp/hadoop-kf/mapred/local/localRunner/kf/job_local1954455609_0001/job_local1954455609_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
21:30:17.710 WARN org.apache.hadoop.conf.Configuration: file:/tmp/hadoop-kf/mapred/local/localRunner/kf/job_local1954455609_0001/job_local1954455609_0001.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_rc_merge_test_part/ds=2012-01-03/ts=2012-01-03+14%3A46%3A31
[0m[[0minfo[0m] [0m[32m- alter_merge_2 (4 seconds, 211 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- alter_partition_coltype !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- alter_partition_format_loc (1 second, 724 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alter_partition_protect_mode (2 seconds, 674 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alter_partition_with_whitelist (1 second, 1 millisecond)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter_rename_partition_src
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter_rename_partition_db.db/alter_rename_partition_src
[0m[[0minfo[0m] [0m[32m- alter_rename_partition (2 seconds, 826 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alter_table_serde (1 second, 965 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter_varchar_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/alter_varchar_1
[0m[[0minfo[0m] [0m[32m- alter_varchar1 (1 second, 900 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alter_varchar2 (1 second, 451 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alter_view_as_select (1 second, 275 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- alter_view_rename !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- ambiguous_col (1 second, 265 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ansi_sql_arithmetic !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- archive !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- archive_excludeHadoop20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- archive_multi !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join0 (1 second, 369 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- auto_join1 (1 second, 287 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join10 (982 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join11 (837 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join12 (909 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join13 (1 second, 890 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join14 (1 second, 389 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join14_hadoop20 (1 second, 141 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join15 (1 second, 13 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- auto_join16 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join17 (1 second, 242 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join18 (1 second, 168 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join19 (1 second, 186 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j2
[0m[[0minfo[0m] [0m[32m- auto_join2 (1 second, 137 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join20 (1 second, 368 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join21 (901 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join22 (1 second, 449 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join23 (829 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tst1
[0m[[0minfo[0m] [0m[32m- auto_join24 (1 second, 162 milliseconds)[0m[0m
21:30:57.083 WARN org.apache.spark.sql.hive.execution.HiveCompatibilitySuite: Simplifications made on unsupported operations for test auto_join25
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- auto_join25 (2 seconds, 120 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- auto_join26 (1 second, 414 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join27 (1 second, 135 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join28 (888 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join3 (1 second, 38 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join30 (2 seconds, 201 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join31 (935 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join32 (2 seconds, 864 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join4 (1 second, 251 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join5 (1 second, 263 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join6 (1 second, 640 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join7 (1 second, 365 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join8 (1 second, 84 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- auto_join9 (1 second, 73 milliseconds)[0m[0m
21:31:17.371 WARN org.apache.spark.sql.hive.execution.HiveCompatibilitySuite: Simplifications made on unsupported operations for test auto_join_filters
[0m[[0minfo[0m] [0m[32m- auto_join_filters (8 seconds, 330 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_join_nulls (3 seconds, 807 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/testsrc
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/orderpayment_small
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/user_small
[0m[[0minfo[0m] [0m[32m- auto_join_reordering_values (1 second, 622 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- auto_join_without_localtask !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- auto_smb_mapjoin_14 (5 seconds, 48 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_1 (2 seconds, 794 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_10 (2 seconds, 263 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_11 (2 seconds, 702 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_12 (2 seconds, 946 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_13 (3 seconds, 239 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_14 (1 second, 740 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_15 (1 second, 254 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_16 (2 seconds, 189 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_2 (4 seconds, 958 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_3 (4 seconds, 848 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_4 (2 seconds, 684 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_5 (2 seconds, 109 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl4
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_6 (3 seconds, 927 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_7 (3 seconds, 97 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_8 (3 seconds, 81 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
[0m[[0minfo[0m] [0m[32m- auto_sortmerge_join_9 (6 seconds, 209 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ba_table1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ba_table2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ba_table3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ba_table_udfs !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ba_table_union !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- binary_constant (764 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- binary_output_format !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- binary_table_bincolserde !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- binary_table_colserde !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- binarysortable_1 (957 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- cast1 (1 second, 55 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- cast_to_int !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_cast !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_comparison !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_join1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_nested_types !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_serde !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_udf1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_union1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- char_varchar_udf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- cluster (2 seconds, 658 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- columnarserde_create_shortcut !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/combine1_1
[0m[[0minfo[0m] [0m[32m- combine1 (1 second, 260 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- combine2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- combine2_hadoop20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- combine2_win !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- combine3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- compile_processor !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- compute_stats_binary (1 second, 145 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- compute_stats_boolean (940 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- compute_stats_decimal !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- compute_stats_double (905 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- compute_stats_empty_table (1 second, 297 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- compute_stats_long (924 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- compute_stats_string (1 second, 720 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- constant_prop !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- convert_enum_to_string (727 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer1 (6 seconds, 230 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer10 (3 seconds, 61 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer11 (2 seconds, 330 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- correlationoptimizer12 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmp
[0m[[0minfo[0m] [0m[32m- correlationoptimizer13 (1 second, 630 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer14 (2 seconds, 607 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer15 (1 second, 512 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer2 (3 seconds, 907 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer3 (2 seconds, 536 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer4 (4 seconds, 283 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- correlationoptimizer5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer6 (7 seconds, 831 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer7 (2 seconds, 2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- correlationoptimizer8 (3 seconds, 552 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmp
[0m[[0minfo[0m] [0m[32m- correlationoptimizer9 (2 seconds, 728 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- count (1 second, 362 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_six_columns
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_two_columns
[0m[[0minfo[0m] [0m[32m- cp_mj_rc (1 second, 589 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- create_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- create_big_view !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- create_escape !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- create_func1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- create_genericudaf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- create_genericudf !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_test_output_format
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_test_output_format_sequencefile
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_test_output_format_hivesequencefile
[0m[[0minfo[0m] [0m[32m- create_insert_outputformat (1 second, 363 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- create_like !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- create_like_tbl_props (1 second, 154 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table2
[0m[[0minfo[0m] [0m[32m- create_like_view (2 seconds, 348 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table1
[0m[[0minfo[0m] [0m[32m- create_nested_type (854 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- create_or_replace_view !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- create_skewed_table1 (3 seconds, 447 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/abc
[0m[[0minfo[0m] [0m[32m- create_struct_table (852 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- create_udaf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- create_union_table !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- create_view !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- cross_join (931 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/a
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/b
[0m[[0minfo[0m] [0m[32m- cross_product_check_1 (1 second, 281 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/a
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/b
[0m[[0minfo[0m] [0m[32m- cross_product_check_2 (1 second, 443 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ct_case_insensitive (1 second, 190 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ctas_char !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ctas_colname !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ctas_date !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ctas_uses_database_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ctas_varchar !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- cte_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- cte_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- custom_input_output_format !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- date_1 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_2
[0m[[0minfo[0m] [0m[32m- date_2 (1 second, 148 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_3
[0m[[0minfo[0m] [0m[32m- date_3 (982 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_4
[0m[[0minfo[0m] [0m[32m- date_4 (1 second, 59 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- date_comparison (1 second, 85 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_join1
[0m[[0minfo[0m] [0m[32m- date_join1 (964 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_serde_regex
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_serde_lb
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_serde_ls
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_serde_c
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_serde_lbc
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_serde_orc
[0m[[0minfo[0m] [0m[32m- date_serde (3 seconds, 909 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_udf
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_udf_string
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/date_udf_flight
[0m[[0minfo[0m] [0m[32m- date_udf (2 seconds, 690 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_compact1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_compact2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_compact3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_ddl1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_query1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_query2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_query3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_query4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_query5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dbtxnmgr_showlocks !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ddltime !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/decimal_1
[0m[[0minfo[0m] [0m[32m- decimal_1 (1 second, 441 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- decimal_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- decimal_3 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/decimal_4_2
[0m[[0minfo[0m] [0m[32m- decimal_4 (1 second, 325 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- decimal_5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- decimal_6 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- decimal_join (1 second, 216 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- decimal_precision !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- decimal_serde !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- decimal_udf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- default_partition_name (976 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- delimiter (1 second, 272 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- desc_non_existent_tbl (627 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- desc_tbl_part_cols !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- describe_formatted_view_partitioned (907 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- describe_table !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- describe_xpath !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- diff_part_input_formats (1 second, 105 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/kv_fileformat_check_txt
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/kv_fileformat_check_seq
[0m[[0minfo[0m] [0m[32m- disable_file_format_check (1 second, 20 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- disallow_incompatible_type_change_off (1 second, 341 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- distinct_stats (1 second, 126 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- driverhook !!! IGNORED !!![0m[0m
21:34:07.620 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/tmp/testTempFiles6167202715430906972spark.hive.tmp/drop_database_removes_partition_dirs_table specified for non-external table:test_table
Deleted file:///tmp/testTempFiles6167202715430906972spark.hive.tmp/drop_database_removes_partition_dirs_table2/part=1
[0m[[0minfo[0m] [0m[32m- drop_database_removes_partition_dirs (1 second, 220 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- drop_function (747 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- drop_index (1 second, 28 milliseconds)[0m[0m
21:34:10.588 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/tmp/testTempFiles6167202715430906972spark.hive.tmp/drop_database_removes_partition_dirs_table specified for non-external table:test_table
[0m[[0minfo[0m] [0m[32m- drop_index_removes_partition_dirs (918 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- drop_multi_partitions (1 second, 190 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- drop_partitions_filter (2 seconds, 174 milliseconds)[0m[0m
21:34:15.457 ERROR org.apache.hadoop.hive.metastore.ObjectStore: Direct SQL failed, falling back to ORM
javax.jdo.JDODataStoreException: Error executing SQL query "select "PARTITIONS"."PART_ID" from "PARTITIONS"  inner join "TBLS" on "PARTITIONS"."TBL_ID" = "TBLS"."TBL_ID"     and "TBLS"."TBL_NAME" = ?   inner join "DBS" on "TBLS"."DB_ID" = "DBS"."DB_ID"      and "DBS"."NAME" = ? inner join "PARTITION_KEY_VALS" "FILTER0" on "FILTER0"."PART_ID" = "PARTITIONS"."PART_ID" and "FILTER0"."INTEGER_IDX" = 0 inner join "PARTITION_KEY_VALS" "FILTER1" on "FILTER1"."PART_ID" = "PARTITIONS"."PART_ID" and "FILTER1"."INTEGER_IDX" = 1 where ( (((case when "TBLS"."TBL_NAME" = ? and "DBS"."NAME" = ? then cast("FILTER0"."PART_KEY_VAL" as decimal(21,0)) else null end) = ?) and ((case when "TBLS"."TBL_NAME" = ? and "DBS"."NAME" = ? then cast("FILTER1"."PART_KEY_VAL" as decimal(21,0)) else null end) = ?)) )".
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)
	at sun.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy13.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3779)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy14.get_partitions_by_expr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByExpr(HiveMetaStoreClient.java:922)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy15.listPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:1979)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTableDropPartsOutputs(DDLSemanticAnalyzer.java:3084)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableDropParts(DDLSemanticAnalyzer.java:2579)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:396)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:422)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:975)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.executeCollect(commands.scala:59)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:383)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:341)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:339)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:339)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
NestedThrowablesStackTrace:
java.sql.SQLDataException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeQuery(Unknown Source)
	at com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)
	at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:280)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1786)
	at org.datanucleus.store.query.AbstractSQLQuery.executeWithArray(AbstractSQLQuery.java:339)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:312)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)
	at sun.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy13.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3779)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy14.get_partitions_by_expr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByExpr(HiveMetaStoreClient.java:922)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy15.listPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:1979)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTableDropPartsOutputs(DDLSemanticAnalyzer.java:3084)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableDropParts(DDLSemanticAnalyzer.java:2579)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:396)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:422)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:975)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.executeCollect(commands.scala:59)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:383)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:341)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:339)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:339)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.sql.SQLException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 118 more
Caused by: ERROR 22018: Invalid character string format for type DECIMAL.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.types.DataType.invalidFormat(Unknown Source)
	at org.apache.derby.iapi.types.DataType.setValue(Unknown Source)
	at org.apache.derby.exe.ac5e52817cx014ax8becx98c1x00000b5602b0ec8.e6(Unknown Source)
	at org.apache.derby.impl.services.reflect.DirectCall.invoke(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.NestedLoopJoinResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.JoinResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.JoinResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 112 more
21:34:15.484 ERROR org.apache.hadoop.hive.metastore.ObjectStore: Direct SQL failed, falling back to ORM
javax.jdo.JDODataStoreException: Error executing SQL query "select "PARTITIONS"."PART_ID" from "PARTITIONS"  inner join "TBLS" on "PARTITIONS"."TBL_ID" = "TBLS"."TBL_ID"     and "TBLS"."TBL_NAME" = ?   inner join "DBS" on "TBLS"."DB_ID" = "DBS"."DB_ID"      and "DBS"."NAME" = ? inner join "PARTITION_KEY_VALS" "FILTER0" on "FILTER0"."PART_ID" = "PARTITIONS"."PART_ID" and "FILTER0"."INTEGER_IDX" = 0 inner join "PARTITION_KEY_VALS" "FILTER1" on "FILTER1"."PART_ID" = "PARTITIONS"."PART_ID" and "FILTER1"."INTEGER_IDX" = 1 where ( (((case when "TBLS"."TBL_NAME" = ? and "DBS"."NAME" = ? then cast("FILTER0"."PART_KEY_VAL" as decimal(21,0)) else null end) = ?) and ((case when "TBLS"."TBL_NAME" = ? and "DBS"."NAME" = ? then cast("FILTER1"."PART_KEY_VAL" as decimal(21,0)) else null end) = ?)) )".
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)
	at sun.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy13.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2318)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy14.drop_partitions_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(HiveMetaStoreClient.java:709)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy15.dropPartitions(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(Hive.java:1696)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(Hive.java:1681)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:3860)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3854)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:306)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.executeCollect(commands.scala:59)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:383)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:341)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:339)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:339)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
NestedThrowablesStackTrace:
java.sql.SQLDataException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeQuery(Unknown Source)
	at com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)
	at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:280)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1786)
	at org.datanucleus.store.query.AbstractSQLQuery.executeWithArray(AbstractSQLQuery.java:339)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:312)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)
	at sun.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy13.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2318)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy14.drop_partitions_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(HiveMetaStoreClient.java:709)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy15.dropPartitions(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(Hive.java:1696)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(Hive.java:1681)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:3860)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3854)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:306)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.executeCollect(commands.scala:59)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:383)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:341)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:339)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:339)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.sql.SQLException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 119 more
Caused by: ERROR 22018: Invalid character string format for type DECIMAL.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.types.DataType.invalidFormat(Unknown Source)
	at org.apache.derby.iapi.types.DataType.setValue(Unknown Source)
	at org.apache.derby.exe.ac5e52817cx014ax8becx98c1x00000b5602b0ec8.e6(Unknown Source)
	at org.apache.derby.impl.services.reflect.DirectCall.invoke(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.NestedLoopJoinResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.JoinResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.JoinResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 113 more
21:34:15.618 ERROR org.apache.hadoop.hive.metastore.ObjectStore: Direct SQL failed, falling back to ORM
javax.jdo.JDODataStoreException: Error executing SQL query "select "PARTITIONS"."PART_ID" from "PARTITIONS"  inner join "TBLS" on "PARTITIONS"."TBL_ID" = "TBLS"."TBL_ID"     and "TBLS"."TBL_NAME" = ?   inner join "DBS" on "TBLS"."DB_ID" = "DBS"."DB_ID"      and "DBS"."NAME" = ? inner join "PARTITION_KEY_VALS" "FILTER0" on "FILTER0"."PART_ID" = "PARTITIONS"."PART_ID" and "FILTER0"."INTEGER_IDX" = 0 where (((case when "TBLS"."TBL_NAME" = ? and "DBS"."NAME" = ? then cast("FILTER0"."PART_KEY_VAL" as decimal(21,0)) else null end) = ?))".
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)
	at sun.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy13.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3779)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy14.get_partitions_by_expr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByExpr(HiveMetaStoreClient.java:922)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy15.listPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:1979)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTableDropPartsOutputs(DDLSemanticAnalyzer.java:3084)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableDropParts(DDLSemanticAnalyzer.java:2579)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:396)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:422)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:975)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.executeCollect(commands.scala:59)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:383)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:341)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:339)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:339)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
NestedThrowablesStackTrace:
java.sql.SQLDataException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeQuery(Unknown Source)
	at com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)
	at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:280)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1786)
	at org.datanucleus.store.query.AbstractSQLQuery.executeWithArray(AbstractSQLQuery.java:339)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:312)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)
	at sun.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy13.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3779)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy14.get_partitions_by_expr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByExpr(HiveMetaStoreClient.java:922)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy15.listPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:1979)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTableDropPartsOutputs(DDLSemanticAnalyzer.java:3084)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableDropParts(DDLSemanticAnalyzer.java:2579)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:396)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:422)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:975)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.executeCollect(commands.scala:59)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:383)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:341)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:339)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:339)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.sql.SQLException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 118 more
Caused by: ERROR 22018: Invalid character string format for type DECIMAL.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.types.DataType.invalidFormat(Unknown Source)
	at org.apache.derby.iapi.types.DataType.setValue(Unknown Source)
	at org.apache.derby.exe.ac5e52817cx014ax8becx98c1x00000b5602b0ece.e6(Unknown Source)
	at org.apache.derby.impl.services.reflect.DirectCall.invoke(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.NestedLoopJoinResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.JoinResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 112 more
21:34:15.635 ERROR org.apache.hadoop.hive.metastore.ObjectStore: Direct SQL failed, falling back to ORM
javax.jdo.JDODataStoreException: Error executing SQL query "select "PARTITIONS"."PART_ID" from "PARTITIONS"  inner join "TBLS" on "PARTITIONS"."TBL_ID" = "TBLS"."TBL_ID"     and "TBLS"."TBL_NAME" = ?   inner join "DBS" on "TBLS"."DB_ID" = "DBS"."DB_ID"      and "DBS"."NAME" = ? inner join "PARTITION_KEY_VALS" "FILTER0" on "FILTER0"."PART_ID" = "PARTITIONS"."PART_ID" and "FILTER0"."INTEGER_IDX" = 0 where (((case when "TBLS"."TBL_NAME" = ? and "DBS"."NAME" = ? then cast("FILTER0"."PART_KEY_VAL" as decimal(21,0)) else null end) = ?))".
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)
	at sun.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy13.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2318)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy14.drop_partitions_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(HiveMetaStoreClient.java:709)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy15.dropPartitions(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(Hive.java:1696)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(Hive.java:1681)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:3860)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3854)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:306)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.executeCollect(commands.scala:59)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:383)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:341)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:339)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:339)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
NestedThrowablesStackTrace:
java.sql.SQLDataException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeQuery(Unknown Source)
	at com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)
	at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:280)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1786)
	at org.datanucleus.store.query.AbstractSQLQuery.executeWithArray(AbstractSQLQuery.java:339)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:312)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:300)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1915)
	at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1909)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1882)
	at sun.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy13.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2318)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy14.drop_partitions_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(HiveMetaStoreClient.java:709)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy15.dropPartitions(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(Hive.java:1696)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(Hive.java:1681)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:3860)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3854)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:306)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:98)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
	at org.apache.spark.sql.execution.ExecutedCommand.executeCollect(commands.scala:59)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:383)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:341)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$31.apply(HiveComparisonTest.scala:339)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:339)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:236)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:40)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.sql.SQLException: Invalid character string format for type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 119 more
Caused by: ERROR 22018: Invalid character string format for type DECIMAL.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.types.DataType.invalidFormat(Unknown Source)
	at org.apache.derby.iapi.types.DataType.setValue(Unknown Source)
	at org.apache.derby.exe.ac5e52817cx014ax8becx98c1x00000b5602b0ece.e6(Unknown Source)
	at org.apache.derby.impl.services.reflect.DirectCall.invoke(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.NestedLoopJoinResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.JoinResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.openCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 113 more
[0m[[0minfo[0m] [0m[32m- drop_partitions_filter2 (1 second, 725 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- drop_partitions_filter3 (1 second, 459 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- drop_partitions_ignore_protection (1 second, 250 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- drop_table (904 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- drop_table2 (1 second, 138 milliseconds)[0m[0m
21:34:21.586 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/tmp/testTempFiles6167202715430906972spark.hive.tmp/drop_table_removes_partition_dirs_table specified for non-external table:test_table
Deleted file:///tmp/testTempFiles6167202715430906972spark.hive.tmp/drop_table_removes_partition_dirs_table2/part=1
[0m[[0minfo[0m] [0m[32m- drop_table_removes_partition_dirs (1 second, 228 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- drop_udf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- drop_view (1 second, 4 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- drop_with_concurrency !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- dynamic_partition_skip_default (1 second, 361 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- dynpart_sort_opt_vectorization !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- dynpart_sort_optimization !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- enforce_order !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- escape1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- escape2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- escape_clusterby1 (845 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- escape_distributeby1 (769 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- escape_orderby1 (623 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- escape_sortby1 (671 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- exchange_partition !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exchange_partition2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exchange_partition3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_00_nonpart_empty !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_01_nonpart !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_02_00_part_empty !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_02_part !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_03_nonpart_over_compat !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_04_all_part !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_04_evolved_parts !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_05_some_part !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_06_one_part !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_07_all_part_over_nonoverlap !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_08_nonpart_rename !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_09_part_spec_nonoverlap !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_10_external_managed !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_11_managed_external !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_12_external_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_13_managed_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_14_managed_location_over_existing !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_15_external_part !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_16_part_external !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_17_part_managed !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_18_part_external !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_19_00_part_external_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_19_part_external_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_20_part_managed_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_21_export_authsuccess !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_22_import_exist_authsuccess !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_23_import_part_authsuccess !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_24_import_nonexist_authsuccess !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- exim_hidden_files !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- explain_dependency !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- explain_dependency2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- explain_logical !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- explain_rearrange (1 second, 59 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- explode_null !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- external_table_with_space_in_location_path !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- fetch_aggregation (1 second, 576 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- file_with_header_footer !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- fileformat_mix (1 second, 182 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- fileformat_sequencefile (955 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- fileformat_text (973 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- filter_join_breaktask (1 second, 631 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- filter_join_breaktask2 (2 seconds, 113 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- filter_numeric !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- global_limit !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_g1
[0m[[0minfo[0m] [0m[32m- groupby1 (3 seconds, 928 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby10 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- groupby11 (2 seconds, 439 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby12 (1 second, 543 milliseconds)[0m[0m
21:34:45.512 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby1_limit (1 second, 413 milliseconds)[0m[0m
21:34:47.247 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby1_map (2 seconds, 230 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby1_map_nomap (1 second, 527 milliseconds)[0m[0m
21:34:50.493 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby1_map_skew (1 second, 631 milliseconds)[0m[0m
21:34:52.334 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_g1
[0m[[0minfo[0m] [0m[32m- groupby1_noskew (1 second, 878 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_g2
[0m[[0minfo[0m] [0m[32m- groupby2 (1 second, 848 milliseconds)[0m[0m
21:34:55.908 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
[0m[[0minfo[0m] [0m[32m- groupby2_limit (878 milliseconds)[0m[0m
21:34:57.662 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby2_map (2 seconds, 357 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby2_map_multi_distinct !!! IGNORED !!![0m[0m
21:35:01.128 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby2_map_skew (3 seconds, 437 milliseconds)[0m[0m
21:35:03.863 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_g2
[0m[[0minfo[0m] [0m[32m- groupby2_noskew (3 seconds, 610 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby2_noskew_multi_distinct !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby3_map !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby3_map_multi_distinct !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby3_map_skew !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby3_noskew !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby3_noskew_multi_distinct !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby4 (1 second, 252 milliseconds)[0m[0m
21:35:07.354 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby4_map (865 milliseconds)[0m[0m
21:35:08.320 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby4_map_skew (979 milliseconds)[0m[0m
21:35:09.382 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby4_noskew (1 second, 583 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby5 (1 second, 278 milliseconds)[0m[0m
21:35:12.071 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby5_map (901 milliseconds)[0m[0m
21:35:13.035 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby5_map_skew (976 milliseconds)[0m[0m
21:35:14.324 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby5_noskew (2 seconds, 24 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby6 (1 second, 504 milliseconds)[0m[0m
21:35:17.464 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby6_map (1 second, 406 milliseconds)[0m[0m
21:35:18.945 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby6_map_skew (1 second, 527 milliseconds)[0m[0m
21:35:20.449 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby6_noskew (1 second, 366 milliseconds)[0m[0m
21:35:22.074 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.074 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.091 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.096 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.111 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.116 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.128 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.131 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.147 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.151 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.167 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.169 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.188 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.188 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.203 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.204 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.222 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.222 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.243 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.243 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.257 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.257 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.276 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.276 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.291 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.291 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.312 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.312 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.353 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.353 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.370 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
21:35:22.489 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.490 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.512 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.512 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.530 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.530 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.545 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.549 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.566 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.566 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.585 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.585 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.604 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.604 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.620 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.621 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.641 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.641 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.657 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.684 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.699 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.700 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.717 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.717 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.745 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.745 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.762 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.762 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.782 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.785 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:22.801 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby7 (1 second, 995 milliseconds)[0m[0m
21:35:23.834 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
21:35:24.100 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.101 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.117 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.118 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.138 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.139 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.155 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.155 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.174 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.177 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.191 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.201 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.214 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.221 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.237 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.245 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.258 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.265 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.276 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.280 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.294 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.300 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.315 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.315 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.359 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.360 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.374 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.374 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.392 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.392 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.406 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
21:35:24.535 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.535 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.553 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.553 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.571 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.571 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.588 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.590 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.609 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.609 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.627 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.629 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.649 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.650 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.666 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.691 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.707 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.707 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.726 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.726 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.743 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.743 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.758 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.760 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.783 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.784 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.800 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.804 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.822 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.822 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:24.840 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby7_map (2 seconds, 688 milliseconds)[0m[0m
21:35:26.540 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
21:35:26.786 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.786 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.804 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.805 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.823 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.823 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.843 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.845 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.862 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.867 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.877 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.881 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.896 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.916 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.930 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.933 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.945 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.950 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.962 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.966 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.978 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.984 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:26.997 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.003 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.017 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.024 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.037 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.048 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.059 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.067 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.081 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
21:35:27.200 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.200 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.214 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.214 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.257 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.257 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.277 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.277 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.291 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.292 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.311 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.313 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.331 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.333 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.348 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.349 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.366 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.370 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.384 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.389 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.407 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.411 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.430 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.434 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.452 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.454 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.471 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.472 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.488 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.489 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:27.509 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby7_map_multi_single_reducer (2 seconds, 620 milliseconds)[0m[0m
21:35:29.171 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
21:35:29.540 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.541 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.578 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.581 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.596 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.596 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.613 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.613 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.631 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.631 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.650 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.650 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.669 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.669 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.689 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.689 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.707 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.707 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.722 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.722 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.741 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.741 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.755 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.755 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.783 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.783 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.805 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.806 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.826 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.830 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:29.868 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
21:35:30.020 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.020 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.035 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.042 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.052 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.067 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.071 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.089 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.089 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.106 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.107 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.125 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.126 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.146 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.146 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.171 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.171 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.191 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.192 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.218 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.218 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.261 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.262 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.282 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.283 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.301 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.304 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.322 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.322 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.341 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:30.342 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby7_map_skew (2 seconds, 847 milliseconds)[0m[0m
21:35:32.020 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
21:35:32.318 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.318 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.335 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.335 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.355 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.356 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.371 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.371 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.389 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.393 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.407 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.410 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.428 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.428 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.441 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.442 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.461 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.463 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.498 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.499 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.518 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.519 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.544 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.544 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.563 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.563 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.584 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.584 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.602 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.603 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.620 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
21:35:32.760 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.760 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.774 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.774 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.796 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.797 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.815 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.816 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.835 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.835 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.872 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.876 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.889 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.893 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.913 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.916 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.938 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.938 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.952 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.957 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.973 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.982 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:32.997 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:33.006 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:33.021 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:33.030 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:33.037 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:33.050 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:33.055 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:33.071 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:35:33.074 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby7_noskew (2 seconds, 680 milliseconds)[0m[0m
21:35:35.241 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
21:35:35.600 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
21:35:35.797 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby7_noskew_multi_single_reducer (2 seconds, 32 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby8 (3 seconds, 512 milliseconds)[0m[0m
21:35:40.215 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby8_map (2 seconds, 436 milliseconds)[0m[0m
21:35:42.695 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby8_map_skew (2 seconds, 550 milliseconds)[0m[0m
21:35:45.254 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby8_noskew (2 seconds, 621 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby9 (6 seconds, 710 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_bigdata !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_complex_types !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_complex_types_multi_single_reducer !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_cube1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_distinct_samekey (2 seconds, 112 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_grouping_id1 (1 second, 25 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_grouping_id2 (1 second, 767 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_grouping_sets1 (1 second, 498 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t2
[0m[[0minfo[0m] [0m[32m- groupby_grouping_sets2 (1 second, 431 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_grouping_sets3 (1 second, 638 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_grouping_sets4 (1 second, 549 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_grouping_sets5 (1 second, 354 milliseconds)[0m[0m
21:36:06.957 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby_map_ppr (1 second, 393 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_map_ppr_multi_distinct !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- groupby_multi_insert_common_distinct (3 seconds, 337 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_multi_single_reducer !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_g2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_g3
[0m[[0minfo[0m] [0m[32m- groupby_multi_single_reducer2 (2 seconds, 167 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e2
[0m[[0minfo[0m] [0m[32m- groupby_multi_single_reducer3 (5 seconds, 180 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_mutli_insert_common_distinct (740 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_neg_float (1 second, 511 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_position !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_ppd (818 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- groupby_ppr (1 second, 334 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_ppr_multi_distinct !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_resolution !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_rollup1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_sort_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- groupby_sort_10 (1 second, 722 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_sort_11 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
[0m[[0minfo[0m] [0m[32m- groupby_sort_2 (1 second, 594 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl2
[0m[[0minfo[0m] [0m[32m- groupby_sort_3 (2 seconds, 597 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl2
[0m[[0minfo[0m] [0m[32m- groupby_sort_4 (2 seconds, 361 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl2
[0m[[0minfo[0m] [0m[32m- groupby_sort_5 (3 seconds, 705 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
[0m[[0minfo[0m] [0m[32m- groupby_sort_6 (2 seconds, 882 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1/ds=1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
[0m[[0minfo[0m] [0m[32m- groupby_sort_7 (2 seconds, 551 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1/ds=1
[0m[[0minfo[0m] [0m[32m- groupby_sort_8 (1 second, 266 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1/ds=1
[0m[[0minfo[0m] [0m[32m- groupby_sort_9 (1 second, 728 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- groupby_sort_skew_1 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
[0m[[0minfo[0m] [0m[32m- groupby_sort_test_1 (1 second, 59 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- having (1 second, 303 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- implicit_cast1 (895 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- import_exported_table !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_auto !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_bitmap !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_bitmap1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_bitmap2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_bitmap3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_bitmap_auto !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_bitmap_rc !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_compact !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_compact_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_compact_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_compact_3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- index_creation !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- infer_const_type !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- init_file !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- innerjoin (1 second, 521 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- inoutdriver (835 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input (801 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input0 (686 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input1 (801 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input10 (678 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- input11 (883 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- input11_limit (1 second, 339 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- input12 (1 second, 382 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- input12_hadoop20 (1 second, 545 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input13 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- input14 (1 second, 581 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input14_limit !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- input15 (864 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input16_cc !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input17 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input18 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- input19 (877 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- input1_limit (1 second, 220 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input2 (1 second, 227 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- input21 (1 second, 228 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input22 (864 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input23 (781 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input24 (1 second, 169 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input25 (997 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input26 (886 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tst/d=2009-01-01
[0m[[0minfo[0m] [0m[32m- input28 (1 second, 528 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input2_limit (793 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input3 (1 second, 204 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input30 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input31 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input32 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input33 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input34 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input35 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input36 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input37 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input38 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input39 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input39_hadoop20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input3_limit !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- input4 (1 second, 52 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input40 (2 seconds, 286 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_sp
[0m[[0minfo[0m] [0m[32m- input41 (1 second, 71 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input43 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input45 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input46 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/intable
[0m[[0minfo[0m] [0m[32m- input49 (1 second, 2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input4_cb_delim (771 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input4_limit !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input5 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- input6 (1 second, 472 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- input7 (1 second, 62 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- input8 (1 second, 156 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- input9 (1 second, 246 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input_columnarserde !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input_dynamicserde !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input_lazyserde !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- input_limit (1 second, 106 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part0 (880 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- input_part1 (1 second, 571 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part10 (1 second, 10 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part10_win (1 second, 96 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- input_part2 (1 second, 414 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part3 (1 second, 316 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part4 (1 second, 728 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
[0m[[0minfo[0m] [0m[32m- input_part5 (943 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part6 (950 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part7 (732 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part8 (702 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- input_part9 (831 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest4_sequencefile
[0m[[0minfo[0m] [0m[32m- input_testsequencefile (902 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- input_testxpath !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input_testxpath2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input_testxpath3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- input_testxpath4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- inputddl1 (915 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- inputddl2 (2 seconds, 409 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- inputddl3 (660 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- inputddl4 (819 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- inputddl5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- inputddl6 (1 second, 310 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- inputddl7 (3 seconds, 334 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- inputddl8 (3 seconds, 508 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/insert1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/db2.db/result
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/db1.db/result
[0m[[0minfo[0m] [0m[32m- insert1 (2 seconds, 286 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- insert1_overwrite_partitions !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/db2.db/destintable/ds=2011-11-11
[0m[[0minfo[0m] [0m[32m- insert2_overwrite_partitions (4 seconds, 370 milliseconds)[0m[0m
21:37:58.947 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/insert_compressed
21:37:59.113 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:37:59.290 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
[0m[[0minfo[0m] [0m[32m- insert_compressed (1 second, 400 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- insert_into1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- insert_into2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- insert_into3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- insert_into4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- insert_into5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- insert_into6 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- insert_overwrite_local_directory_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- insertexternal1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- join0 (728 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join1 (1 second, 72 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join10 (903 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join11 (801 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join12 (1 second, 161 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join13 (1 second, 21 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join14 (1 second, 406 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join14_hadoop20 (1 second, 414 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join15 (909 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join16 (847 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join17 (957 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join18 (1 second, 338 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join19 (863 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j2
[0m[[0minfo[0m] [0m[32m- join2 (1 second, 63 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join20 (1 second, 190 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join21 (1 second, 213 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join22 (948 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join23 (1 second, 65 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tst1
[0m[[0minfo[0m] [0m[32m- join24 (2 seconds, 46 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join25 (1 second, 581 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join26 (1 second, 498 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join27 (1 second, 769 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join28 (1 second, 865 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join29 (1 second, 834 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join3 (1 second, 355 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join30 (1 second, 572 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join31 (1 second, 645 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join32 (1 second, 715 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j2
[0m[[0minfo[0m] [0m[32m- join32_lessSize (3 seconds, 47 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join33 (1 second, 434 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join34 (1 second, 328 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join35 (2 seconds, 6 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmp1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmp2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join36 (2 seconds, 773 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join37 (1 second, 293 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmp
[0m[[0minfo[0m] [0m[32m- join38 (1 second, 203 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join39 (1 second, 602 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join4 (1 second, 468 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join40 (2 seconds, 323 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/s1
[0m[[0minfo[0m] [0m[32m- join41 (1 second, 666 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join5 (1 second, 370 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join6 (1 second, 571 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join7 (1 second, 581 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join8 (1 second, 464 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- join9 (1 second, 38 milliseconds)[0m[0m
21:39:02.456 WARN org.apache.spark.sql.hive.execution.HiveCompatibilitySuite: Simplifications made on unsupported operations for test join_1to1
[0m[[0minfo[0m] [0m[32m- join_1to1 (9 seconds, 893 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- join_alt_syntax !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- join_array (1 second, 315 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join_casesensitive (1 second, 306 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- join_cond_pushdown_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- join_cond_pushdown_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- join_cond_pushdown_3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- join_cond_pushdown_4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- join_cond_pushdown_unqual1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- join_cond_pushdown_unqual2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- join_cond_pushdown_unqual3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- join_cond_pushdown_unqual4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- join_empty (1 second, 102 milliseconds)[0m[0m
21:39:16.078 WARN org.apache.spark.sql.hive.execution.HiveCompatibilitySuite: Simplifications made on unsupported operations for test join_filters
[0m[[0minfo[0m] [0m[32m- join_filters (20 seconds, 426 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- join_filters_overlap !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hive_foo
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hive_bar
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hive_count
[0m[[0minfo[0m] [0m[32m- join_hive_626 (1 second, 283 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- join_literals !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_copy
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src1_copy
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_j1
[0m[[0minfo[0m] [0m[32m- join_map_ppr (2 seconds, 141 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- join_merging !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- join_nulls (8 seconds, 917 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join_nullsafe (5 seconds, 728 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/join_rc1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/join_rc2
[0m[[0minfo[0m] [0m[32m- join_rc (1 second, 256 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- join_reorder !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- join_reorder2 (1 second, 652 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join_reorder3 (2 seconds, 44 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join_reorder4 (1 second, 634 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- join_star (3 seconds, 366 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- join_thrift !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- join_vc !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmp_pyang_lv
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmp_pyang_src_rcfile
[0m[[0minfo[0m] [0m[32m- lateral_view (2 seconds, 688 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/array_valued_src
[0m[[0minfo[0m] [0m[32m- lateral_view_cp (1 second, 37 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- lateral_view_noalias !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- lateral_view_ppd (1 second, 345 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- lb_fs_stats !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- leadlag !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- leadlag_queries !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- leftsemijoin (1 second, 950 milliseconds)[0m[0m
21:40:12.617 WARN org.apache.spark.sql.execution.SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.
[0m[[0minfo[0m] [0m[32m- leftsemijoin_mr (1 second, 222 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- limit_partition_metadataonly !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- limit_pushdown !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- limit_pushdown_negative (1 second, 127 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_l1
[0m[[0minfo[0m] [0m[32m- lineage1 (1 second, 420 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- literal_decimal !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- literal_double (784 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- literal_ints (732 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- literal_string (737 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- load_binary_data !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part1 (1 second, 484 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part10 (1 second, 576 milliseconds)[0m[0m
21:40:21.635 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:40:21.635 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:40:21.665 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
21:40:21.665 WARN org.apache.hadoop.io.compress.DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.
[0m[[0minfo[0m] [0m[32m- load_dyn_part11 (1 second, 225 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part12 (1 second, 229 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part13 (1 second, 184 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part14 (1 second, 390 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part14_win (1 second, 551 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- load_dyn_part15 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part2 (1 second, 292 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part3 (1 second, 404 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part4 (1 second, 585 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part5 (14 seconds, 550 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part6 (7 seconds, 923 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part7 (2 seconds, 413 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part8 (2 seconds, 30 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- load_dyn_part9 (1 second, 176 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- load_exist_part_authsuccess !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- load_file_with_space_in_the_name (1 second, 575 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- load_fs !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- load_fs2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- load_fs_overwrite !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- load_hdfs_file_with_space_in_the_name !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- load_nonpart_authsuccess !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- load_part_authsuccess !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hive_test_src
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/hive_test_dst/pcol1=test_part/pcol2=test_Part
[0m[[0minfo[0m] [0m[32m- loadpart1 (1 second, 826 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- loadpart2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- loadpart_err !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- lock1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- lock2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- lock3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- lock4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- louter_join_ppr (1 second, 398 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- macro !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- mapjoin1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- mapjoin_addjar !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- mapjoin_decimal !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- mapjoin_distinct (1 second, 301 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- mapjoin_filter_on_outerjoin (1 second, 293 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- mapjoin_mapjoin (1 second, 455 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- mapjoin_memcheck !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- mapjoin_subquery (1 second, 264 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- mapjoin_subquery2 (1 second, 478 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest_1
[0m[[0minfo[0m] [0m[32m- mapjoin_test_outer (2 seconds, 507 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- mapreduce1 (1 second, 53 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- mapreduce2 (1 second, 689 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- mapreduce3 (899 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- mapreduce4 (1 second, 217 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- mapreduce5 (867 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- mapreduce6 (959 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- mapreduce7 (872 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- mapreduce8 (937 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- merge1 (1 second, 645 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test1
[0m[[0minfo[0m] [0m[32m- merge2 (1 second, 772 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- merge3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- merge4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- merge_dynamic_partition !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- merge_dynamic_partition2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- merge_dynamic_partition3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- merge_dynamic_partition4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- merge_dynamic_partition5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- mergejoins (1 second, 122 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- metadata_only_queries !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- metadata_only_queries_with_filters !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- metadataonly1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- mi !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- mrr !!! IGNORED !!![0m[0m
21:41:26.777 WARN org.apache.spark.sql.hive.execution.HiveCompatibilitySuite: Simplifications made on unsupported operations for test multiMapJoin1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smalltbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smalltbl2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smalltbl3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smalltbl4
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/bigtbl
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/bigtbl
[0m[[0minfo[0m] [0m[32m- multiMapJoin1 (4 seconds, 161 milliseconds)[0m[0m
21:41:30.940 WARN org.apache.spark.sql.hive.execution.HiveCompatibilitySuite: Simplifications made on unsupported operations for test multiMapJoin2
[0m[[0minfo[0m] [0m[32m- multiMapJoin2 (3 seconds, 492 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- multi_insert !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e2
[0m[[0minfo[0m] [0m[32m- multi_insert_gby (3 seconds, 321 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- multi_insert_gby2 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/e1
[0m[[0minfo[0m] [0m[32m- multi_insert_gby3 (1 second, 800 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_10
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_lv4
[0m[[0minfo[0m] [0m[32m- multi_insert_lateral_view (4 seconds, 316 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- multi_insert_move_tasks_share_dependencies !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src11
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src12
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src13
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src14
[0m[[0minfo[0m] [0m[32m- multi_join_union (1 second, 515 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- multigroupby_singlemr (1 second, 238 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- nested_complex !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- nestedvirtual !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- newline !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- no_hooks !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- noalias_subq1 (935 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ambiguous
[0m[[0minfo[0m] [0m[32m- nomore_ambiguous_table_col (922 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nonblock_op_deduplicate (1 second, 17 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- nonmr_fetch !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- nonmr_fetch_threshold !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- nonreserved_keywords_input37 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- nonreserved_keywords_insert_into1 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- notable_alias1 (1 second, 628 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- notable_alias2 (1 second, 291 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- notable_alias3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- null_cast !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- null_column !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- nullformat !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- nullformatCTAS !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- nullformatdir !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- nullgroup (985 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nullgroup2 (986 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nullgroup3 (2 seconds, 147 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nullgroup4 (1 second, 141 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nullgroup4_multi_distinct (893 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nullgroup5 (1 second, 149 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nullinput (1 second, 76 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nullinput2 (702 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- nullscript (898 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- num_op_type_conv !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- optional_outer (1 second, 674 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- orc_analyze !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_create !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_createas1 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_orc
21:42:05.492 WARN org.apache.spark.scheduler.TaskSetManager: Stage 5620 contains a task of very large size (249 KB). The maximum recommended task size is 100 KB.
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_orc
[0m[[0minfo[0m] [0m[32m- orc_dictionary_threshold (1 second, 599 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- orc_diff_part_cols !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_diff_part_cols2 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_orc
[0m[[0minfo[0m] [0m[32m- orc_empty_files (917 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- orc_empty_strings !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_min_max !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_ppd_char !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_ppd_date !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_ppd_decimal !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_ppd_varchar !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_split_elimination !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- orc_vectorization_ppd !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- order (804 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- order2 (634 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- order_within_subquery !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- outer_join_ppr (1 second, 52 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_a
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_b
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_a
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src_b
[0m[[0minfo[0m] [0m[32m- parallel (1 second, 558 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- parallel_orderby !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- parenthesis_star_by (1 second, 75 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- parquet_create !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- parquet_ctas !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- parquet_partitioned !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- parquet_types !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partInit !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- part_inherit_tbl_props (824 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- part_inherit_tbl_props_empty (918 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- part_inherit_tbl_props_with_star (934 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- partcols1 (1 second, 552 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- partition_date (2 seconds, 933 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- partition_date2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_decode_name !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- partition_schema1 (1 second, 96 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- partition_serde_format (1 second, 139 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- partition_special_char !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tab1/month=June/day=2008-01-01
[0m[[0minfo[0m] [0m[32m- partition_type_check (1 second, 638 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- partition_varchar1 (2 seconds, 756 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- partition_varchar2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_vs_table_metadata !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat10 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat11 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat12 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat13 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat14 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat15 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat16 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat17 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat18 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- partition_wise_fileformat4 (1 second, 311 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- partition_wise_fileformat5 (1 second, 722 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- partition_wise_fileformat6 (1 second, 532 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- partition_wise_fileformat7 (1 second, 763 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- partition_wise_fileformat8 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- partition_wise_fileformat9 (1 second, 335 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- pcr !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- plan_json (758 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd1 (727 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd2 (1 second, 7 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_clusterby (1 second, 91 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppd_constant_expr
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppd_constant_expr
[0m[[0minfo[0m] [0m[32m- ppd_constant_expr (1 second, 101 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_constant_where (915 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_gby (773 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_gby2 (875 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_gby_join (717 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_join (1 second, 151 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_join2 (1 second, 152 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_join3 (1 second, 47 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ppd_join4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_join_filter (1 second, 989 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ppd_multi_insert !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_outer_join1 (973 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_outer_join2 (797 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_outer_join3 (1 second, 33 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_outer_join4 (1 second, 26 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_outer_join5 (1 second, 202 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_random (726 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_repeated_alias (1 second, 108 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ppd_transform !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ppd_udf_case !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_udf_col (868 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ppd_udtf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- ppd_union (1 second, 711 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ppd_union_view !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ppd_vc !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- ppr_allchildsarenull (815 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppr_test/ds=1234
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppr_test/ds=1224
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppr_test/ds=1214
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppr_test/ds=12+4
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppr_test/ds=12.4
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppr_test/ds=12%3A4
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppr_test/ds=12%254
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/ppr_test/ds=12%2A4
[0m[[0minfo[0m] [0m[32m- ppr_pushdown (3 seconds, 280 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppr_pushdown2 (3 seconds, 781 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- ppr_pushdown3 (1 second, 177 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- print_header !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- progress_1 (810 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
[0m[[0minfo[0m] [0m[32m- protectmode (2 seconds, 557 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- ptf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ptf_decimal !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ptf_general_queries !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ptf_matchpath !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ptf_rcfile !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ptf_register_tblfn !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- ptf_seqfile !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- push_or (1 second, 210 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- query_result_fileformat !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- query_with_semi (1 second, 498 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- quote1 (4 seconds, 635 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- quote2 (1 second, 180 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- quotedid_alter !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- quotedid_basic !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- quotedid_partition !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- quotedid_skew !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- quotedid_smb !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- quotedid_tblproperty !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rand_partitionpruner1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rand_partitionpruner2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rand_partitionpruner3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rcfile_bigdata !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/columntable
[0m[[0minfo[0m] [0m[32m- rcfile_columnar (870 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- rcfile_createas1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rcfile_default_format !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/rcfiletablelazydecompress
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/rcfiletablelazydecompress
[0m[[0minfo[0m] [0m[32m- rcfile_lazydecompress (1 second, 433 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- rcfile_merge1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rcfile_merge2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rcfile_merge3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rcfile_merge4 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src1_rc
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1_rc
[0m[[0minfo[0m] [0m[32m- rcfile_null_value (1 second, 526 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_src
[0m[[0minfo[0m] [0m[32m- rcfile_toleratecorruptions (1 second, 66 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/rcfile_uniontable
[0m[[0minfo[0m] [0m[32m- rcfile_union (913 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- recursive_dir !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/bucket5_1
[0m[[0minfo[0m] [0m[32m- reduce_deduplicate (1 second, 70 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- reduce_deduplicate_exclude_gby (940 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- reduce_deduplicate_exclude_join (633 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- reduce_deduplicate_extended (2 seconds, 387 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- reducesink_dedup (931 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- regex_col !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- regexp_extract !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- remote_script !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- rename_column (2 seconds, 318 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- rename_external_partition_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rename_partition_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- rename_table_location !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- reset_conf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- root_dir_external_table !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- router_join_ppr (1 second, 710 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- sample1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample6 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample7 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample8 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample9 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample_islocalmode_hook !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- sample_islocalmode_hook_hadoop20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- schemeAuthority !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- schemeAuthority2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- script_env_var1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- script_env_var2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- script_pipe !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- scriptfile1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- scriptfile1_win !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- select_as_omitted (825 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- select_dummy_source !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- select_transform_hint !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- select_unquote_and (1 second, 105 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- select_unquote_not (1 second, 892 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- select_unquote_or (1 second, 333 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- semicolon !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t3
[0m[[0minfo[0m] [0m[32m- semijoin (4 seconds, 528 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- serde_regex (1 second, 421 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- serde_reported_schema (701 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- serde_user_properties !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- set_processor_namespaces !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- set_variable_sub (980 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- show_columns (1 second, 474 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- show_create_table_alter (1 second, 219 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- show_create_table_db_table (902 milliseconds)[0m[0m
21:43:49.858 WARN org.apache.hadoop.hive.metastore.HiveMetaStore: Location: file:/tmp/testTempFiles6167202715430906972spark.hive.tmp/tmp_showcrt1 specified for non-external table:tmp_showcrt1
[0m[[0minfo[0m] [0m[32m- show_create_table_delimited (783 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- show_create_table_partitioned (808 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- show_create_table_serde (941 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- show_create_table_view (800 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- show_describe_func_quotes (789 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- show_functions (815 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- show_indexes_edge_cases !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- show_indexes_syntax !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- show_partitions (778 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- show_roles !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- show_tables !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- show_tablestatus !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- show_tblproperties (959 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- showparts !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoin_noskew !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoin_union_remove_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoin_union_remove_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt10 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt11 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt12 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- skewjoinopt13 (2 seconds, 420 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt14 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt15 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt16 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt17 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
[0m[[0minfo[0m] [0m[32m- skewjoinopt18 (1 second, 802 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt19 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt6 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt7 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- skewjoinopt8 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- skewjoinopt9 (1 second, 356 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_mapjoin9_results
[0m[[0minfo[0m] [0m[32m- smb_mapjoin9 (2 seconds, 170 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_3
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_1 (2 seconds, 97 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_10 (1 second, 339 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- smb_mapjoin_11 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- smb_mapjoin_12 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table4
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_13 (1 second, 622 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tbl2
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_14 (4 seconds, 50 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table2
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_15 (2 seconds, 544 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table2
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_16 (1 second, 141 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table4
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table5
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table6
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table7
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_table8
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_17 (3 seconds, 495 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- smb_mapjoin_18 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- smb_mapjoin_19 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_3
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_2 (2 seconds, 512 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- smb_mapjoin_20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_21 (1 second, 786 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- smb_mapjoin_22 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_3
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_25 (1 second, 489 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_3
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_3 (2 seconds, 92 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_3
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_4 (3 seconds, 601 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket_3
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_5 (3 seconds, 274 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_join_results
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/normal_join_results
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_join_results
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_join_results
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/normal_join_results
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_join_results
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_join_results
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_6 (3 seconds, 355 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_join_results_empty_bigtable
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_join_results_empty_bigtable
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_join_results
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/normal_join_results
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_7 (2 seconds, 473 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/smb_bucket4_3
[0m[[0minfo[0m] [0m[32m- smb_mapjoin_8 (5 seconds, 989 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sort (824 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_desc1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_desc2
[0m[[0minfo[0m] [0m[32m- sort_merge_join_desc_1 (1 second, 183 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_desc1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_desc2
[0m[[0minfo[0m] [0m[32m- sort_merge_join_desc_2 (1 second, 623 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_desc1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_desc2
[0m[[0minfo[0m] [0m[32m- sort_merge_join_desc_3 (1 second, 295 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_desc1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_desc2
[0m[[0minfo[0m] [0m[32m- sort_merge_join_desc_4 (1 second, 166 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sort_merge_join_desc_5 (1 second, 247 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sort_merge_join_desc_6 (1 second, 443 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- sort_merge_join_desc_7 (1 second, 741 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- source !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- split !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- split_sample !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/stats_non_partitioned
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/stats_non_partitioned
[0m[[0minfo[0m] [0m[32m- stats0 (2 seconds, 205 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- stats2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats6 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats7 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats8 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats9 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
[0m[[0minfo[0m] [0m[32m- stats_aggregator_error_1 (1 second, 520 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- stats_counter !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats_counter_partitioned !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats_empty_dyn_part !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- stats_empty_partition (982 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- stats_invalidation !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats_noscan_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats_noscan_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats_only_null !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats_partscan_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- stats_partscan_1_23 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
[0m[[0minfo[0m] [0m[32m- stats_publisher_error_1 (1 second, 796 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- statsfs !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- str_to_map !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subq !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- subq2 (782 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- subq_where_serialization !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_alias !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_exists !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_exists_having !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_in !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_in_having !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_multiinsert !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_notexists !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_notexists_having !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_notin !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_notin_having !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_unqualcolumnrefs !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- subquery_views !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- table_access_keys_stats !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmp_select
[0m[[0minfo[0m] [0m[32m- tablename_with_select (822 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- test_boolean_whereclause !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- tez_dml !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- tez_fsstat !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- tez_insert_overwrite_local_directory_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- tez_join_tests !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- tez_joins_explain !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- tez_schema_evolution !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- tez_union !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_1
[0m[[0minfo[0m] [0m[32m- timestamp_1 (3 seconds, 957 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_2
[0m[[0minfo[0m] [0m[32m- timestamp_2 (3 seconds, 966 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_3
[0m[[0minfo[0m] [0m[32m- timestamp_3 (1 second, 309 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- timestamp_comparison (921 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_lazy
[0m[[0minfo[0m] [0m[32m- timestamp_lazy (1 second, 423 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_null
[0m[[0minfo[0m] [0m[32m- timestamp_null (921 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_udf
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/timestamp_udf_string
[0m[[0minfo[0m] [0m[32m- timestamp_udf (1 second, 985 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tstsrc
[0m[[0minfo[0m] [0m[32m- touch (1 second, 810 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- transform1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- transform2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- transform_ppr1 (750 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- transform_ppr2 (694 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- truncate_column !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- truncate_column_merge !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- truncate_table (1 second, 787 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- type_cast_1 (879 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- type_conversions_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- type_widening (813 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udaf_collect_set (993 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udaf_context_ngrams !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/covar_tab
[0m[[0minfo[0m] [0m[32m- udaf_corr (1 second, 276 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/covar_tab
[0m[[0minfo[0m] [0m[32m- udaf_covar_pop (1 second, 193 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/covar_tab
[0m[[0minfo[0m] [0m[32m- udaf_covar_samp (1 second, 115 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udaf_histogram_numeric (827 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udaf_ngrams !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udaf_number_format !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udaf_percentile !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udaf_percentile_approx_20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udaf_percentile_approx_23 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udaf_sum_list !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf1 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf2 (1 second, 211 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf4 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf5 (944 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf6 (1 second, 87 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf7 (945 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf8 (1 second, 92 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf9 (756 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf_10_trims (810 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_E (973 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_PI (947 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_abs (821 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_acos (3 seconds, 443 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_add (2 seconds, 407 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_array (2 seconds, 65 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_array_contains (747 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_ascii (1 second, 256 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_asin (877 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_atan (944 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_avg (602 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_between !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_bigint (750 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_bin (752 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/bitmap_test
[0m[[0minfo[0m] [0m[32m- udf_bitmap_and (997 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_bitmap_empty (725 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/bitmap_test
[0m[[0minfo[0m] [0m[32m- udf_bitmap_or (1 second, 699 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_bitwise_and (814 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_bitwise_not (1 second, 100 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_bitwise_or (652 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_bitwise_xor (725 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_boolean (662 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_case_column_pruning !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_case_thrift !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_ceil (695 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_ceiling (648 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_coalesce !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_compare_java_string !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_concat (887 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf_concat_insert1 (871 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf_concat_insert2 (915 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf_concat_ws (1 second, 96 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_context_aware !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_conv (1 second, 217 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_cos (804 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_count (1 second, 269 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_current_database !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_date_add (1 second, 52 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_date_sub (699 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_datediff (680 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_day (639 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_dayofmonth (651 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_degrees (1 second, 37 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_div (668 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_divide !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_double (706 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_elt (893 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_equal (1 second, 400 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_exp (705 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_explode !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_field (1 second, 240 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_find_in_set (2 seconds, 609 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_float (743 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_floor (687 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_format_number (1 second, 488 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_from_unixtime (622 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_get_json_object !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_greaterthan (798 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_greaterthanorequal (775 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_hash (764 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_hex (869 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_hour !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_if (830 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_in !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_in_file !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_index (628 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_inline !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_instr (838 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_int (1 second, 41 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_isnotnull (646 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_isnull (605 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_isnull_isnotnull !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_java_method (792 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_lcase (714 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
[0m[[0minfo[0m] [0m[32m- udf_length (1 second, 267 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_lessthan (864 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_lessthanorequal (728 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_like (793 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_ln (730 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_locate (747 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_log (721 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_log10 (1 second, 11 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_log2 (696 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_logic_java_boolean !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_lower (689 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_lpad (935 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_ltrim (743 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_map (796 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_map_keys !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_map_values !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_max !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_min !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_minute (721 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_modulo (692 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_month (578 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_named_struct (788 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_negative (974 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_not (823 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_notequal (823 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_notop (1 second, 95 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_nvl (746 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_or (654 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_parse_url (735 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_percentile !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_pmod (1 second, 210 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_positive (710 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_pow (742 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_power (591 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_printf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_radians (1 second, 5 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_rand (981 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_reflect !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_reflect2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_regexp (1 second, 84 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_regexp_extract (1 second, 367 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_regexp_replace (1 second, 529 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_repeat (958 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_reverse !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_rlike (734 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_round_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_round_3 (881 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_rpad (831 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_rtrim (608 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_second (813 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_sentences !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_sign (1 second, 45 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_sin (792 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_size !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_smallint (652 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_space (810 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_split !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_sqrt (730 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_std (983 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_stddev (789 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_stddev_pop (737 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_stddev_samp (656 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_string (672 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_struct (725 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_substr !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_substring (677 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_subtract (658 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_sum (715 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_tan (908 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_testlength !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_testlength2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_tinyint (703 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_to_boolean !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_to_byte (1 second, 48 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_to_date (781 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_to_double (1 second, 597 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_to_float (1 second, 110 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_to_long (1 second, 34 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_to_short (1 second, 157 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_to_string !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_input
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/table_translate
[0m[[0minfo[0m] [0m[32m- udf_translate (1 second, 441 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_trim (804 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_ucase (603 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_unhex !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udf_union !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_unix_timestamp (1 second, 6 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_upper (797 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udf_using !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- udf_var_pop (3 seconds, 398 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_var_samp (1 second, 435 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_variance (894 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_weekofyear (1 second, 33 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_xpath (991 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_xpath_boolean (974 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_xpath_double (1 second, 224 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_xpath_float (1 second, 163 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_xpath_int (1 second, 424 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_xpath_long (1 second, 36 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_xpath_short (1 second, 197 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- udf_xpath_string (1 second, 90 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- udtf_explode !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udtf_json_tuple !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udtf_parse_url_tuple !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udtf_posexplode !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- udtf_stack !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- unicode_notation (1 second, 148 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
[0m[[0minfo[0m] [0m[32m- union10 (1 second, 8 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union11 (938 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union12 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- union13 (714 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union14 (1 second, 342 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union15 (1 second, 48 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union16 (1 second, 90 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- union17 (1 second, 243 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- union18 (1 second, 352 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dest2
[0m[[0minfo[0m] [0m[32m- union19 (1 second, 312 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union2 (848 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union20 (830 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union21 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- union22 (1 second, 558 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union23 (1 second, 86 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src4
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src5
[0m[[0minfo[0m] [0m[32m- union24 (2 seconds, 962 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union25 (1 second, 19 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union26 (1 second, 426 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/jackson_sev_same
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/dim_pho
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/jackson_sev_add
[0m[[0minfo[0m] [0m[32m- union27 (1 second, 299 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/union_subq_union
[0m[[0minfo[0m] [0m[32m- union28 (1 second, 90 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/union_subq_union
[0m[[0minfo[0m] [0m[32m- union29 (909 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/union_out
[0m[[0minfo[0m] [0m[32m- union3 (1 second, 87 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/union_subq_union
[0m[[0minfo[0m] [0m[32m- union30 (1 second, 188 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t4
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t5
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t6
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t7
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/t8
[0m[[0minfo[0m] [0m[32m- union31 (3 seconds, 557 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union32 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_src
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_src
[0m[[0minfo[0m] [0m[32m- union33 (1 second, 496 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src10_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src10_2
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src10_3
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/src10_4
[0m[[0minfo[0m] [0m[32m- union34 (2 seconds, 174 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
[0m[[0minfo[0m] [0m[32m- union4 (1 second, 133 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union5 (809 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/tmptable
[0m[[0minfo[0m] [0m[32m- union6 (1 second, 308 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union7 (1 second, 371 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union8 (853 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- union9 (860 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/union_date_1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/union_date_2
[0m[[0minfo[0m] [0m[32m- union_date (1 second, 100 milliseconds)[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/test_union_lateral_view
[0m[[0minfo[0m] [0m[32m- union_lateralview (1 second, 96 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union_null !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- union_ppr (778 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_10 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
[0m[[0minfo[0m] [0m[32m- union_remove_11 (1 second, 575 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_12 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_13 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_14 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_15 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_16 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_17 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_18 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_19 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_20 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_21 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_22 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_23 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_24 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
[0m[[0minfo[0m] [0m[32m- union_remove_3 (1 second, 168 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_5 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/outputtbl2
[0m[[0minfo[0m] [0m[32m- union_remove_6 (1 second, 599 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_7 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_8 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_remove_9 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- union_script (921 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- union_top_level !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- union_view !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- unset_table_view_property !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- varchar_1 !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/varchar_2
[0m[[0minfo[0m] [0m[32m- varchar_2 (1 second, 567 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- varchar_cast !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- varchar_comparison !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- varchar_join1 (2 seconds, 558 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- varchar_nested_types !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- varchar_serde !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- varchar_udf1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- varchar_union1 (1 second, 549 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- vector_between_in !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vector_coalesce !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vector_decimal_aggregate !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vector_decimal_cast !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vector_decimal_expressions !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vector_decimal_mapjoin !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vector_decimal_math_funcs !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vector_left_outer_join !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vector_non_string_partition !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_0 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_1 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_10 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_11 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_12 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_13 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_14 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_15 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_16 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_2 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_3 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_4 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_5 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_6 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_7 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_8 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_9 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_decimal_date !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_div0 !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_limit !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_nested_udf !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_not !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_part !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_part_project !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_pushdown !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorization_short_regress !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_case !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_casts !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_context !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_date_funcs !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_distinct_gby !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_mapjoin !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_math_funcs !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_nested_mapjoin !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_rcfile_columnar !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_shufflejoin !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_string_funcs !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[33m- vectorized_timestamp_funcs !!! IGNORED !!![0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/db1.db/table1
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/db1.db/table2
[0m[[0minfo[0m] [0m[32m- view (2 seconds, 160 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- view_cast (1 second, 945 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- view_inputs (1 second, 492 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- virtual_column !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32mPlanTest:[0m[0m
[0m[[0minfo[0m] [0m[32mPruningSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - with partitioned table - pruning test (63 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - with partitioned table - query test (637 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - with non-partitioned table - pruning test (45 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - with non-partitioned table - query test (558 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - with multiple projects - pruning test (34 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - with multiple projects - query test (601 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - projects alias substituting - pruning test (34 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - projects alias substituting - query test (631 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - filter alias in-lining - pruning test (40 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - filter alias in-lining - query test (644 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - without filters - pruning test (36 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - without filters - query test (657 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - simple top project without aliases - pruning test (45 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - simple top project without aliases - query test (625 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - non-trivial top project with aliases - pruning test (34 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Column pruning - non-trivial top project with aliases - query test (606 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - non-partitioned, non-trivial project - pruning test (38 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - non-partitioned, non-trivial project - query test (651 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - non-partitioned table - pruning test (41 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - non-partitioned table - query test (569 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - with filter on string partition key - pruning test (390 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - with filter on string partition key - query test (1 second, 443 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - with filter on int partition key - pruning test (51 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - with filter on int partition key - query test (1 second, 66 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - left only 1 partition - pruning test (42 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - left only 1 partition - query test (1 second, 35 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - all partitions pruned - pruning test (40 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - all partitions pruned - query test (1 second, 119 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - pruning with both column key and partition key - pruning test (36 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Partition pruning - pruning with both column key and partition key - query test (1 second, 172 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveSerDeSuite:[0m[0m
Deleted file:///tmp/sparkHiveWarehouse7773807525406879524/serdeins
[0m[[0minfo[0m] [0m[32m- Read and write with LazySimpleSerDe (tab separated) (904 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Read with RegexSerDe (762 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Read with AvroSerDe (1 second, 809 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Read Partitioned with AvroSerDe (1 second, 767 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveResolutionSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-3698: case insensitive test for nested data (50 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- table.attr (927 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- database.table (574 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- database.table table.attr (1 second, 2 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- database.table table.attr case insensitive (862 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alias.attr (592 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- subquery-alias.attr (664 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- quoted alias.attr (612 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- attr (658 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- alias.star (621 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- case insensitivity with scala reflection (72 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[33m- case insensitivity with scala reflection joins !!! IGNORED !!![0m[0m
[0m[[0minfo[0m] [0m[32m- nested repeated resolution (41 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHivePlanTest:[0m[0m
[0m[[0minfo[0m] [0m[32m- udf constant folding (66 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveUdfSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- spark sql udf test that returns a struct (80 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-4785 When called with arguments referring column fields, PMOD throws NPE (61 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- hive struct udf (217 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SPARK-2693 udaf aggregates test (238 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Generic UDAF aggregates (242 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- UDFIntegerToString (1 second, 431 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- UDFListListInt (1 second, 26 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- UDFListString (676 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- UDFStringString (683 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- UDFTwoListList (686 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveExplainSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- explain extended command (116 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- explain create table command (89 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveTableScanSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- partition_based_table_scan_with_different_serde (1 second, 169 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- file_split_for_small_table (755 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Spark-4041: lowercase issue (341 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Spark-4077: timestamp query for null value (283 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32mHiveParquetSuite:[0m[0m
[0m[[0minfo[0m] [0m[32m- Case insensitive attribute names (213 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- SELECT on Parquet table (115 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Simple column projection + filter on Parquet table (165 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- Converting Hive to Parquet Table via saveAsParquetFile (790 milliseconds)[0m[0m
[0m[[0minfo[0m] [0m[32m- INSERT OVERWRITE TABLE Parquet table (511 milliseconds)[0m[0m
[0m[[0minfo[0m] [0mScalaTest[0m
[0m[[0minfo[0m] [0m[36mRun completed in 23 minutes, 42 seconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 1021[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 24, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 1021, failed 0, canceled 0, ignored 602, pending 0[0m[0m
[0m[[0minfo[0m] [0m[32mAll tests passed.[0m[0m
[0m[[0minfo[0m] [0mPassed: Total 1021, Failed 0, Errors 0, Passed 1021, Ignored 602[0m
[0m[[32msuccess[0m] [0mTotal time: 1430 s, completed Dec 27, 2014 9:49:25 PM[0m
