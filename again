diff --cc R/pkg/DESCRIPTION
index 3e49eac,dfb7e22..0000000
--- a/R/pkg/DESCRIPTION
+++ b/R/pkg/DESCRIPTION
diff --cc R/pkg/NAMESPACE
index 4c77d95,62c33a7..0000000
--- a/R/pkg/NAMESPACE
+++ b/R/pkg/NAMESPACE
diff --cc R/pkg/inst/tests/testthat/test_sparkSQL.R
index cdb8ff6,ef6cab1..0000000
--- a/R/pkg/inst/tests/testthat/test_sparkSQL.R
+++ b/R/pkg/inst/tests/testthat/test_sparkSQL.R
diff --cc R/pkg/vignettes/sparkr-vignettes.Rmd
index 5156c9e,babfb71..0000000
--- a/R/pkg/vignettes/sparkr-vignettes.Rmd
+++ b/R/pkg/vignettes/sparkr-vignettes.Rmd
@@@ -640,4 -649,4 +649,4 @@@ env | ma
  
  ```{r, echo=FALSE}
  sparkR.session.stop()
--```
++```
diff --cc assembly/pom.xml
index 6db3a59,58feedc..0000000
--- a/assembly/pom.xml
+++ b/assembly/pom.xml
diff --cc common/network-common/pom.xml
index 269b845,a75d222..0000000
--- a/common/network-common/pom.xml
+++ b/common/network-common/pom.xml
diff --cc common/network-shuffle/pom.xml
index 20cf29e,828a407..0000000
--- a/common/network-shuffle/pom.xml
+++ b/common/network-shuffle/pom.xml
diff --cc common/network-yarn/pom.xml
index 25cc328,30891f3..0000000
--- a/common/network-yarn/pom.xml
+++ b/common/network-yarn/pom.xml
diff --cc common/sketch/pom.xml
index 37a5d09,ea2f4f5..0000000
--- a/common/sketch/pom.xml
+++ b/common/sketch/pom.xml
diff --cc common/tags/pom.xml
index ab287f3,ed31f25..0000000
--- a/common/tags/pom.xml
+++ b/common/tags/pom.xml
diff --cc common/unsafe/pom.xml
index 45831ce,0553a48..0000000
--- a/common/unsafe/pom.xml
+++ b/common/unsafe/pom.xml
diff --cc core/pom.xml
index 2d19e5b,f154df2..0000000
--- a/core/pom.xml
+++ b/core/pom.xml
diff --cc core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala
index 52a3499,47aec44..0000000
--- a/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala
+++ b/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala
@@@ -305,39 -303,3 +303,36 @@@ private[spark] object TaskMetrics exten
      tm
    }
  }
 +
- 
 +private[spark] class BlockStatusesAccumulator
-   extends AccumulatorV2[(BlockId, BlockStatus), java.util.List[(BlockId, BlockStatus)]] {
-   private val _seq = Collections.synchronizedList(new ArrayList[(BlockId, BlockStatus)]())
++  extends AccumulatorV2[(BlockId, BlockStatus), Seq[(BlockId, BlockStatus)]] {
++  private var _seq = ArrayBuffer.empty[(BlockId, BlockStatus)]
 +
 +  override def isZero(): Boolean = _seq.isEmpty
 +
 +  override def copyAndReset(): BlockStatusesAccumulator = new BlockStatusesAccumulator
 +
 +  override def copy(): BlockStatusesAccumulator = {
 +    val newAcc = new BlockStatusesAccumulator
-     newAcc._seq.addAll(_seq)
++    newAcc._seq = _seq.clone()
 +    newAcc
 +  }
 +
 +  override def reset(): Unit = _seq.clear()
 +
-   override def add(v: (BlockId, BlockStatus)): Unit = _seq.add(v)
++  override def add(v: (BlockId, BlockStatus)): Unit = _seq += v
 +
-   override def merge(
-     other: AccumulatorV2[(BlockId, BlockStatus), java.util.List[(BlockId, BlockStatus)]]): Unit = {
-     other match {
-       case o: BlockStatusesAccumulator => _seq.addAll(o.value)
-       case _ => throw new UnsupportedOperationException(
-         s"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}")
-     }
++  override def merge(other: AccumulatorV2[(BlockId, BlockStatus), Seq[(BlockId, BlockStatus)]])
++  : Unit = other match {
++    case o: BlockStatusesAccumulator => _seq ++= o.value
++    case _ => throw new UnsupportedOperationException(
++      s"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}")
 +  }
 +
-   override def value: java.util.List[(BlockId, BlockStatus)] = _seq
++  override def value: Seq[(BlockId, BlockStatus)] = _seq
 +
-   def setValue(newValue: java.util.List[(BlockId, BlockStatus)]): Unit = {
++  def setValue(newValue: Seq[(BlockId, BlockStatus)]): Unit = {
 +    _seq.clear()
-     _seq.addAll(newValue)
++    _seq ++= newValue
 +  }
 +}
diff --cc core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
index 3243b94,9b87c42..0000000
--- a/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
+++ b/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
diff --cc core/src/main/scala/org/apache/spark/util/Utils.scala
index 7d41458,b9cf721..0000000
--- a/core/src/main/scala/org/apache/spark/util/Utils.scala
+++ b/core/src/main/scala/org/apache/spark/util/Utils.scala
@@@ -73,7 -57,7 +75,8 @@@ import org.apache.spark.internal.Loggin
  import org.apache.spark.internal.config.{DYN_ALLOCATION_INITIAL_EXECUTORS, DYN_ALLOCATION_MIN_EXECUTORS, EXECUTOR_INSTANCES}
  import org.apache.spark.network.util.JavaUtils
  import org.apache.spark.serializer.{DeserializationStream, SerializationStream, SerializerInstance}
- import org.apache.spark.storage.StorageUtils
++
+ import org.apache.spark.util.logging.RollingFileAppender
  
  /** CallSite represents a place in user code. It can have a short and a long form. */
  private[spark] case class CallSite(shortForm: String, longForm: String)
diff --cc core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala
index e93eee2,1b3197a..0000000
--- a/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala
+++ b/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala
diff --cc dev/deps/spark-deps-hadoop-2.2
index 8c9e559,34cd4e6..0000000
--- a/dev/deps/spark-deps-hadoop-2.2
+++ b/dev/deps/spark-deps-hadoop-2.2
diff --cc dev/deps/spark-deps-hadoop-2.3
index 839e084,8ae3c5e..0000000
--- a/dev/deps/spark-deps-hadoop-2.3
+++ b/dev/deps/spark-deps-hadoop-2.3
diff --cc dev/deps/spark-deps-hadoop-2.4
index ed84de7,7c69102..0000000
--- a/dev/deps/spark-deps-hadoop-2.4
+++ b/dev/deps/spark-deps-hadoop-2.4
diff --cc dev/deps/spark-deps-hadoop-2.6
index 6e7c9cb,041e01e..0000000
--- a/dev/deps/spark-deps-hadoop-2.6
+++ b/dev/deps/spark-deps-hadoop-2.6
diff --cc dev/deps/spark-deps-hadoop-2.7
index a61f31e,4f70bff..0000000
--- a/dev/deps/spark-deps-hadoop-2.7
+++ b/dev/deps/spark-deps-hadoop-2.7
diff --cc docs/_config.yml
index 75c89bd,824197b..0000000
--- a/docs/_config.yml
+++ b/docs/_config.yml
diff --cc examples/pom.xml
index 89e0c61,a571062..0000000
--- a/examples/pom.xml
+++ b/examples/pom.xml
diff --cc external/docker-integration-tests/pom.xml
index 8c6e221,46b1410..0000000
--- a/external/docker-integration-tests/pom.xml
+++ b/external/docker-integration-tests/pom.xml
diff --cc external/flume-assembly/pom.xml
index dd45935,451b101..0000000
--- a/external/flume-assembly/pom.xml
+++ b/external/flume-assembly/pom.xml
diff --cc external/flume-sink/pom.xml
index ba97794,d48a09b..0000000
--- a/external/flume-sink/pom.xml
+++ b/external/flume-sink/pom.xml
diff --cc external/flume/pom.xml
index 8f8bde7,1d9c63a..0000000
--- a/external/flume/pom.xml
+++ b/external/flume/pom.xml
diff --cc external/java8-tests/pom.xml
index f7d8ef7,27a1f53..0000000
--- a/external/java8-tests/pom.xml
+++ b/external/java8-tests/pom.xml
diff --cc external/kafka-0-10-assembly/pom.xml
index 260969f,c5e4289..0000000
--- a/external/kafka-0-10-assembly/pom.xml
+++ b/external/kafka-0-10-assembly/pom.xml
diff --cc external/kafka-0-10/pom.xml
index 1ae1d0e,86bfa38..0000000
--- a/external/kafka-0-10/pom.xml
+++ b/external/kafka-0-10/pom.xml
diff --cc external/kafka-0-8-assembly/pom.xml
index a4b14f8,ac1d289..0000000
--- a/external/kafka-0-8-assembly/pom.xml
+++ b/external/kafka-0-8-assembly/pom.xml
diff --cc external/kafka-0-8/pom.xml
index 9964b22,3ea78f2..0000000
--- a/external/kafka-0-8/pom.xml
+++ b/external/kafka-0-8/pom.xml
diff --cc external/kinesis-asl-assembly/pom.xml
index b5d90b1,052ffdc..0000000
--- a/external/kinesis-asl-assembly/pom.xml
+++ b/external/kinesis-asl-assembly/pom.xml
diff --cc external/kinesis-asl/pom.xml
index f96db65,4304967..0000000
--- a/external/kinesis-asl/pom.xml
+++ b/external/kinesis-asl/pom.xml
diff --cc external/spark-ganglia-lgpl/pom.xml
index 40f2e38,261a2b2..0000000
--- a/external/spark-ganglia-lgpl/pom.xml
+++ b/external/spark-ganglia-lgpl/pom.xml
diff --cc graphx/pom.xml
index 979217e,f7911c7..0000000
--- a/graphx/pom.xml
+++ b/graphx/pom.xml
diff --cc launcher/pom.xml
index 0c0dd0c,944d3bf..0000000
--- a/launcher/pom.xml
+++ b/launcher/pom.xml
diff --cc mllib-local/pom.xml
index 5681b36,760968a..0000000
--- a/mllib-local/pom.xml
+++ b/mllib-local/pom.xml
diff --cc mllib/pom.xml
index 80ab2e0,1087e3b..0000000
--- a/mllib/pom.xml
+++ b/mllib/pom.xml
diff --cc pom.xml
index 79255f9,6ed58ab..0000000
--- a/pom.xml
+++ b/pom.xml
@@@ -26,7 -26,7 +26,13 @@@
    </parent>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +  <version>2.0.1</version>
++||||||| merged common ancestors
++  <version>2.0.1-SNAPSHOT</version>
++=======
+   <version>2.0.2</version>
++>>>>>>> v2.0.2
    <packaging>pom</packaging>
    <name>Spark Project Parent POM</name>
    <url>http://spark.apache.org/</url>
@@@ -744,7 -745,7 +751,14 @@@
        <dependency>
          <groupId>com.spotify</groupId>
          <artifactId>docker-client</artifactId>
++<<<<<<< HEAD
 +        <version>3.6.6</version>
++||||||| merged common ancestors
++        <classifier>shaded</classifier>
++        <version>3.6.6</version>
++=======
+         <version>5.0.2</version>
++>>>>>>> v2.0.2
          <scope>test</scope>
          <exclusions>
            <exclusion>
diff --cc project/MimaExcludes.scala
index 423cbd4,ee6e31a0..0000000
--- a/project/MimaExcludes.scala
+++ b/project/MimaExcludes.scala
diff --cc project/SparkBuild.scala
index 133d3b3,98f1e23..0000000
--- a/project/SparkBuild.scala
+++ b/project/SparkBuild.scala
diff --cc repl/pom.xml
index 4b70d64,fef2c5f..0000000
--- a/repl/pom.xml
+++ b/repl/pom.xml
diff --cc sql/catalyst/pom.xml
index efa327c,ed74cf5..0000000
--- a/sql/catalyst/pom.xml
+++ b/sql/catalyst/pom.xml
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala
index e7430b0,8342892..0000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
index 0db7435,f0992b3..0000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
@@@ -1147,9 -1161,23 +1161,145 @@@ object PushDownPredicate extends Rule[L
        filter
      }
    }
++<<<<<<< HEAD
++}
++
++/**
++||||||| merged common ancestors
++}
++
++/**
++ * Reorder the joins and push all the conditions into join, so that the bottom ones have at least
++ * one condition.
++ *
++ * The order of joins will not be changed if all of them already have at least one condition.
++ */
++object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper {
++
++  /**
++   * Join a list of plans together and push down the conditions into them.
++   *
++   * The joined plan are picked from left to right, prefer those has at least one join condition.
++   *
++   * @param input a list of LogicalPlans to join.
++   * @param conditions a list of condition for join.
++   */
++  @tailrec
++  def createOrderedJoin(input: Seq[LogicalPlan], conditions: Seq[Expression]): LogicalPlan = {
++    assert(input.size >= 2)
++    if (input.size == 2) {
++      val (joinConditions, others) = conditions.partition(
++        e => !SubqueryExpression.hasCorrelatedSubquery(e))
++      val join = Join(input(0), input(1), Inner, joinConditions.reduceLeftOption(And))
++      if (others.nonEmpty) {
++        Filter(others.reduceLeft(And), join)
++      } else {
++        join
++      }
++    } else {
++      val left :: rest = input.toList
++      // find out the first join that have at least one join condition
++      val conditionalJoin = rest.find { plan =>
++        val refs = left.outputSet ++ plan.outputSet
++        conditions.filterNot(canEvaluate(_, left)).filterNot(canEvaluate(_, plan))
++          .exists(_.references.subsetOf(refs))
++      }
++      // pick the next one if no condition left
++      val right = conditionalJoin.getOrElse(rest.head)
++
++      val joinedRefs = left.outputSet ++ right.outputSet
++      val (joinConditions, others) = conditions.partition(
++        e => e.references.subsetOf(joinedRefs) && !SubqueryExpression.hasCorrelatedSubquery(e))
++      val joined = Join(left, right, Inner, joinConditions.reduceLeftOption(And))
++
++      // should not have reference to same logical plan
++      createOrderedJoin(Seq(joined) ++ rest.filterNot(_ eq right), others)
++    }
++  }
++
++  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
++    case j @ ExtractFiltersAndInnerJoins(input, conditions)
++        if input.size > 2 && conditions.nonEmpty =>
++      createOrderedJoin(input, conditions)
++  }
++}
++
++/**
++ * Elimination of outer joins, if the predicates can restrict the result sets so that
++ * all null-supplying rows are eliminated
++ *
++ * - full outer -> inner if both sides have such predicates
++ * - left outer -> inner if the right side has such predicates
++ * - right outer -> inner if the left side has such predicates
++ * - full outer -> left outer if only the left side has such predicates
++ * - full outer -> right outer if only the right side has such predicates
++ *
++ * This rule should be executed before pushing down the Filter
++ */
++object EliminateOuterJoin extends Rule[LogicalPlan] with PredicateHelper {
++
++  /**
++   * Returns whether the expression returns null or false when all inputs are nulls.
++   */
++  private def canFilterOutNull(e: Expression): Boolean = {
++    if (!e.deterministic || SubqueryExpression.hasCorrelatedSubquery(e)) return false
++    val attributes = e.references.toSeq
++    val emptyRow = new GenericInternalRow(attributes.length)
++    val v = BindReferences.bindReference(e, attributes).eval(emptyRow)
++    v == null || v == false
++  }
++
++  private def buildNewJoinType(filter: Filter, join: Join): JoinType = {
++    val splitConjunctiveConditions: Seq[Expression] = splitConjunctivePredicates(filter.condition)
++    val leftConditions = splitConjunctiveConditions
++      .filter(_.references.subsetOf(join.left.outputSet))
++    val rightConditions = splitConjunctiveConditions
++      .filter(_.references.subsetOf(join.right.outputSet))
++
++    val leftHasNonNullPredicate = leftConditions.exists(canFilterOutNull) ||
++      filter.constraints.filter(_.isInstanceOf[IsNotNull])
++        .exists(expr => join.left.outputSet.intersect(expr.references).nonEmpty)
++    val rightHasNonNullPredicate = rightConditions.exists(canFilterOutNull) ||
++      filter.constraints.filter(_.isInstanceOf[IsNotNull])
++        .exists(expr => join.right.outputSet.intersect(expr.references).nonEmpty)
++
++    join.joinType match {
++      case RightOuter if leftHasNonNullPredicate => Inner
++      case LeftOuter if rightHasNonNullPredicate => Inner
++      case FullOuter if leftHasNonNullPredicate && rightHasNonNullPredicate => Inner
++      case FullOuter if leftHasNonNullPredicate => LeftOuter
++      case FullOuter if rightHasNonNullPredicate => RightOuter
++      case o => o
++    }
++  }
++
++  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
++    case f @ Filter(condition, j @ Join(_, _, RightOuter | LeftOuter | FullOuter, _)) =>
++      val newJoinType = buildNewJoinType(f, j)
++      if (j.joinType == newJoinType) f else Filter(condition, j.copy(joinType = newJoinType))
++  }
++}
++
++/**
++=======
+ 
+   /**
+    * Check if we can safely push a filter through a projection, by making sure that predicate
+    * subqueries in the condition do not contain the same attributes as the plan they are moved
+    * into. This can happen when the plan and predicate subquery have the same source.
+    */
+   private def canPushThroughCondition(plan: LogicalPlan, condition: Expression): Boolean = {
+     val attributes = plan.outputSet
+     val matched = condition.find {
+       case PredicateSubquery(p, _, _, _) => p.outputSet.intersect(attributes).nonEmpty
+       case _ => false
+     }
+     matched.isEmpty
+   }
  }
  
  /**
++>>>>>>> v2.0.2
   * Pushes down [[Filter]] operators where the `condition` can be
   * evaluated using only the attributes of the left or right side of a join.  Other
   * [[Filter]] conditions are moved into the `condition` of the [[Join]].
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
index ae4cd8e,08062bd..0000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
@@@ -133,4 -135,4 +135,4 @@@ object EliminateOuterJoin extends Rule[
        val newJoinType = buildNewJoinType(f, j)
        if (j.joinType == newJoinType) f else Filter(condition, j.copy(joinType = newJoinType))
    }
--}
++}
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala
index 00c92fc,1981fd8..0000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala
@@@ -148,14 -127,9 +145,13 @@@ protected[sql] abstract class AtomicTyp
    private[sql] type InternalType
    private[sql] val tag: TypeTag[InternalType]
    private[sql] val ordering: Ordering[InternalType]
 -}
  
-   @transient private[sql] lazy val classTag = ScalaReflectionLock.synchronized {
++  @transient private[sql] val classTag = ScalaReflectionLock.synchronized {
 +    val mirror = runtimeMirror(Utils.getSparkClassLoader)
 +    ClassTag[InternalType](mirror.runtimeClass(tag.tpe))
 +  }
 +}
  
- 
  /**
   * :: DeveloperApi ::
   * Numeric data types.
diff --cc sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
index 4aaae72,15ebc19..0000000
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
diff --cc sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableIdentifierParserSuite.scala
index 4d3ad21,cb57fb6..0000000
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableIdentifierParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableIdentifierParserSuite.scala
diff --cc sql/core/pom.xml
index 347f8a1,c0a355f..0000000
--- a/sql/core/pom.xml
+++ b/sql/core/pom.xml
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala
index 479934a,56bd5c1..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala
index 424a962,d82e54e..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala
index 995feb3,ad0c779..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala
index 7a8b825,2869e80..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala
index 7e2ebe8,acc42a0..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala
index 027b5bb,c14feea..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala
@@@ -1,245 -1,251 +1,500 @@@
++<<<<<<< HEAD
 +/*
 + * Licensed to the Apache Software Foundation (ASF) under one or more
 + * contributor license agreements.  See the NOTICE file distributed with
 + * this work for additional information regarding copyright ownership.
 + * The ASF licenses this file to You under the Apache License, Version 2.0
 + * (the "License"); you may not use this file except in compliance with
 + * the License.  You may obtain a copy of the License at
 + *
 + *    http://www.apache.org/licenses/LICENSE-2.0
 + *
 + * Unless required by applicable law or agreed to in writing, software
 + * distributed under the License is distributed on an "AS IS" BASIS,
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 + * See the License for the specific language governing permissions and
 + * limitations under the License.
 + */
 +
 +package org.apache.spark.sql.execution.streaming
 +
 +import java.io.IOException
 +import java.nio.charset.StandardCharsets.UTF_8
 +
 +import scala.reflect.ClassTag
 +
 +import org.apache.hadoop.fs.{Path, PathFilter}
 +
 +import org.apache.spark.sql.SparkSession
 +
 +/**
 + * An abstract class for compactible metadata logs. It will write one log file for each batch.
 + * The first line of the log file is the version number, and there are multiple serialized
 + * metadata lines following.
 + *
 + * As reading from many small files is usually pretty slow, also too many
 + * small files in one folder will mess the FS, [[CompactibleFileStreamLog]] will
 + * compact log files every 10 batches by default into a big file. When
 + * doing a compaction, it will read all old log files and merge them with the new batch.
 + */
 +abstract class CompactibleFileStreamLog[T: ClassTag](
 +    metadataLogVersion: String,
 +    sparkSession: SparkSession,
 +    path: String)
 +  extends HDFSMetadataLog[Array[T]](sparkSession, path) {
 +
 +  import CompactibleFileStreamLog._
 +
 +  /**
 +   * If we delete the old files after compaction at once, there is a race condition in S3: other
 +   * processes may see the old files are deleted but still cannot see the compaction file using
 +   * "list". The `allFiles` handles this by looking for the next compaction file directly, however,
 +   * a live lock may happen if the compaction happens too frequently: one processing keeps deleting
 +   * old files while another one keeps retrying. Setting a reasonable cleanup delay could avoid it.
 +   */
 +  protected def fileCleanupDelayMs: Long
 +
 +  protected def isDeletingExpiredLog: Boolean
 +
 +  protected def compactInterval: Int
 +
 +  /**
 +   * Serialize the data into encoded string.
 +   */
 +  protected def serializeData(t: T): String
 +
 +  /**
 +   * Deserialize the string into data object.
 +   */
 +  protected def deserializeData(encodedString: String): T
 +
 +  /**
 +   * Filter out the obsolete logs.
 +   */
 +  def compactLogs(logs: Seq[T]): Seq[T]
 +
 +  override def batchIdToPath(batchId: Long): Path = {
 +    if (isCompactionBatch(batchId, compactInterval)) {
 +      new Path(metadataPath, s"$batchId$COMPACT_FILE_SUFFIX")
 +    } else {
 +      new Path(metadataPath, batchId.toString)
 +    }
 +  }
 +
 +  override def pathToBatchId(path: Path): Long = {
 +    getBatchIdFromFileName(path.getName)
 +  }
 +
 +  override def isBatchFile(path: Path): Boolean = {
 +    try {
 +      getBatchIdFromFileName(path.getName)
 +      true
 +    } catch {
 +      case _: NumberFormatException => false
 +    }
 +  }
 +
 +  override def serialize(logData: Array[T]): Array[Byte] = {
 +    (metadataLogVersion +: logData.map(serializeData)).mkString("\n").getBytes(UTF_8)
 +  }
 +
 +  override def deserialize(bytes: Array[Byte]): Array[T] = {
 +    val lines = new String(bytes, UTF_8).split("\n")
 +    if (lines.length == 0) {
 +      throw new IllegalStateException("Incomplete log file")
 +    }
 +    val version = lines(0)
 +    if (version != metadataLogVersion) {
 +      throw new IllegalStateException(s"Unknown log version: ${version}")
 +    }
 +    lines.slice(1, lines.length).map(deserializeData)
 +  }
 +
 +  override def add(batchId: Long, logs: Array[T]): Boolean = {
 +    if (isCompactionBatch(batchId, compactInterval)) {
 +      compact(batchId, logs)
 +    } else {
 +      super.add(batchId, logs)
 +    }
 +  }
 +
 +  /**
 +   * Compacts all logs before `batchId` plus the provided `logs`, and writes them into the
 +   * corresponding `batchId` file. It will delete expired files as well if enabled.
 +   */
 +  private def compact(batchId: Long, logs: Array[T]): Boolean = {
 +    val validBatches = getValidBatchesBeforeCompactionBatch(batchId, compactInterval)
 +    val allLogs = validBatches.flatMap(batchId => super.get(batchId)).flatten ++ logs
 +    if (super.add(batchId, compactLogs(allLogs).toArray)) {
 +      if (isDeletingExpiredLog) {
 +        deleteExpiredLog(batchId)
 +      }
 +      true
 +    } else {
 +      // Return false as there is another writer.
 +      false
 +    }
 +  }
 +
 +  /**
 +   * Returns all files except the deleted ones.
 +   */
 +  def allFiles(): Array[T] = {
 +    var latestId = getLatest().map(_._1).getOrElse(-1L)
 +    // There is a race condition when `FileStreamSink` is deleting old files and `StreamFileCatalog`
 +    // is calling this method. This loop will retry the reading to deal with the
 +    // race condition.
 +    while (true) {
 +      if (latestId >= 0) {
 +        try {
 +          val logs =
 +            getAllValidBatches(latestId, compactInterval).flatMap(id => super.get(id)).flatten
 +          return compactLogs(logs).toArray
 +        } catch {
 +          case e: IOException =>
 +            // Another process using `CompactibleFileStreamLog` may delete the batch files when
 +            // `StreamFileCatalog` are reading. However, it only happens when a compaction is
 +            // deleting old files. If so, let's try the next compaction batch and we should find it.
 +            // Otherwise, this is a real IO issue and we should throw it.
 +            latestId = nextCompactionBatchId(latestId, compactInterval)
 +            super.get(latestId).getOrElse {
 +              throw e
 +            }
 +        }
 +      } else {
 +        return Array.empty
 +      }
 +    }
 +    Array.empty
 +  }
 +
 +  /**
 +   * Since all logs before `compactionBatchId` are compacted and written into the
 +   * `compactionBatchId` log file, they can be removed. However, due to the eventual consistency of
 +   * S3, the compaction file may not be seen by other processes at once. So we only delete files
 +   * created `fileCleanupDelayMs` milliseconds ago.
 +   */
 +  private def deleteExpiredLog(compactionBatchId: Long): Unit = {
 +    val expiredTime = System.currentTimeMillis() - fileCleanupDelayMs
 +    fileManager.list(metadataPath, new PathFilter {
 +      override def accept(path: Path): Boolean = {
 +        try {
 +          val batchId = getBatchIdFromFileName(path.getName)
 +          batchId < compactionBatchId
 +        } catch {
 +          case _: NumberFormatException =>
 +            false
 +        }
 +      }
 +    }).foreach { f =>
 +      if (f.getModificationTime <= expiredTime) {
 +        fileManager.delete(f.getPath)
 +      }
 +    }
 +  }
 +}
 +
 +object CompactibleFileStreamLog {
 +  val COMPACT_FILE_SUFFIX = ".compact"
 +
 +  def getBatchIdFromFileName(fileName: String): Long = {
 +    fileName.stripSuffix(COMPACT_FILE_SUFFIX).toLong
 +  }
 +
 +  /**
 +   * Returns if this is a compaction batch. FileStreamSinkLog will compact old logs every
 +   * `compactInterval` commits.
 +   *
 +   * E.g., if `compactInterval` is 3, then 2, 5, 8, ... are all compaction batches.
 +   */
 +  def isCompactionBatch(batchId: Long, compactInterval: Int): Boolean = {
 +    (batchId + 1) % compactInterval == 0
 +  }
 +
 +  /**
 +   * Returns all valid batches before the specified `compactionBatchId`. They contain all logs we
 +   * need to do a new compaction.
 +   *
 +   * E.g., if `compactInterval` is 3 and `compactionBatchId` is 5, this method should returns
 +   * `Seq(2, 3, 4)` (Note: it includes the previous compaction batch 2).
 +   */
 +  def getValidBatchesBeforeCompactionBatch(
 +      compactionBatchId: Long,
 +      compactInterval: Int): Seq[Long] = {
 +    assert(isCompactionBatch(compactionBatchId, compactInterval),
 +      s"$compactionBatchId is not a compaction batch")
 +    (math.max(0, compactionBatchId - compactInterval)) until compactionBatchId
 +  }
 +
 +  /**
 +   * Returns all necessary logs before `batchId` (inclusive). If `batchId` is a compaction, just
 +   * return itself. Otherwise, it will find the previous compaction batch and return all batches
 +   * between it and `batchId`.
 +   */
 +  def getAllValidBatches(batchId: Long, compactInterval: Long): Seq[Long] = {
 +    assert(batchId >= 0)
 +    val start = math.max(0, (batchId + 1) / compactInterval * compactInterval - 1)
 +    start to batchId
 +  }
 +
 +  /**
 +   * Returns the next compaction batch id after `batchId`.
 +   */
 +  def nextCompactionBatchId(batchId: Long, compactInterval: Long): Long = {
 +    (batchId + compactInterval + 1) / compactInterval * compactInterval - 1
 +  }
 +}
++||||||| merged common ancestors
++=======
+ /*
+  * Licensed to the Apache Software Foundation (ASF) under one or more
+  * contributor license agreements.  See the NOTICE file distributed with
+  * this work for additional information regarding copyright ownership.
+  * The ASF licenses this file to You under the Apache License, Version 2.0
+  * (the "License"); you may not use this file except in compliance with
+  * the License.  You may obtain a copy of the License at
+  *
+  *    http://www.apache.org/licenses/LICENSE-2.0
+  *
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an "AS IS" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+ 
+ package org.apache.spark.sql.execution.streaming
+ 
+ import java.io.{InputStream, IOException, OutputStream}
+ import java.nio.charset.StandardCharsets.UTF_8
+ 
+ import scala.io.{Source => IOSource}
+ import scala.reflect.ClassTag
+ 
+ import org.apache.hadoop.fs.{Path, PathFilter}
+ 
+ import org.apache.spark.sql.SparkSession
+ 
+ /**
+  * An abstract class for compactible metadata logs. It will write one log file for each batch.
+  * The first line of the log file is the version number, and there are multiple serialized
+  * metadata lines following.
+  *
+  * As reading from many small files is usually pretty slow, also too many
+  * small files in one folder will mess the FS, [[CompactibleFileStreamLog]] will
+  * compact log files every 10 batches by default into a big file. When
+  * doing a compaction, it will read all old log files and merge them with the new batch.
+  */
+ abstract class CompactibleFileStreamLog[T: ClassTag](
+     metadataLogVersion: String,
+     sparkSession: SparkSession,
+     path: String)
+   extends HDFSMetadataLog[Array[T]](sparkSession, path) {
+ 
+   import CompactibleFileStreamLog._
+ 
+   /**
+    * If we delete the old files after compaction at once, there is a race condition in S3: other
+    * processes may see the old files are deleted but still cannot see the compaction file using
+    * "list". The `allFiles` handles this by looking for the next compaction file directly, however,
+    * a live lock may happen if the compaction happens too frequently: one processing keeps deleting
+    * old files while another one keeps retrying. Setting a reasonable cleanup delay could avoid it.
+    */
+   protected def fileCleanupDelayMs: Long
+ 
+   protected def isDeletingExpiredLog: Boolean
+ 
+   protected def compactInterval: Int
+ 
+   /**
+    * Serialize the data into encoded string.
+    */
+   protected def serializeData(t: T): String
+ 
+   /**
+    * Deserialize the string into data object.
+    */
+   protected def deserializeData(encodedString: String): T
+ 
+   /**
+    * Filter out the obsolete logs.
+    */
+   def compactLogs(logs: Seq[T]): Seq[T]
+ 
+   override def batchIdToPath(batchId: Long): Path = {
+     if (isCompactionBatch(batchId, compactInterval)) {
+       new Path(metadataPath, s"$batchId$COMPACT_FILE_SUFFIX")
+     } else {
+       new Path(metadataPath, batchId.toString)
+     }
+   }
+ 
+   override def pathToBatchId(path: Path): Long = {
+     getBatchIdFromFileName(path.getName)
+   }
+ 
+   override def isBatchFile(path: Path): Boolean = {
+     try {
+       getBatchIdFromFileName(path.getName)
+       true
+     } catch {
+       case _: NumberFormatException => false
+     }
+   }
+ 
+   override def serialize(logData: Array[T], out: OutputStream): Unit = {
+     // called inside a try-finally where the underlying stream is closed in the caller
+     out.write(metadataLogVersion.getBytes(UTF_8))
+     logData.foreach { data =>
+       out.write('\n')
+       out.write(serializeData(data).getBytes(UTF_8))
+     }
+   }
+ 
+   override def deserialize(in: InputStream): Array[T] = {
+     val lines = IOSource.fromInputStream(in, UTF_8.name()).getLines()
+     if (!lines.hasNext) {
+       throw new IllegalStateException("Incomplete log file")
+     }
+     val version = lines.next()
+     if (version != metadataLogVersion) {
+       throw new IllegalStateException(s"Unknown log version: ${version}")
+     }
+     lines.map(deserializeData).toArray
+   }
+ 
+   override def add(batchId: Long, logs: Array[T]): Boolean = {
+     if (isCompactionBatch(batchId, compactInterval)) {
+       compact(batchId, logs)
+     } else {
+       super.add(batchId, logs)
+     }
+   }
+ 
+   /**
+    * Compacts all logs before `batchId` plus the provided `logs`, and writes them into the
+    * corresponding `batchId` file. It will delete expired files as well if enabled.
+    */
+   private def compact(batchId: Long, logs: Array[T]): Boolean = {
+     val validBatches = getValidBatchesBeforeCompactionBatch(batchId, compactInterval)
+     val allLogs = validBatches.flatMap(batchId => super.get(batchId)).flatten ++ logs
+     if (super.add(batchId, compactLogs(allLogs).toArray)) {
+       if (isDeletingExpiredLog) {
+         deleteExpiredLog(batchId)
+       }
+       true
+     } else {
+       // Return false as there is another writer.
+       false
+     }
+   }
+ 
+   /**
+    * Returns all files except the deleted ones.
+    */
+   def allFiles(): Array[T] = {
+     var latestId = getLatest().map(_._1).getOrElse(-1L)
+     // There is a race condition when `FileStreamSink` is deleting old files and `StreamFileCatalog`
+     // is calling this method. This loop will retry the reading to deal with the
+     // race condition.
+     while (true) {
+       if (latestId >= 0) {
+         try {
+           val logs =
+             getAllValidBatches(latestId, compactInterval).flatMap(id => super.get(id)).flatten
+           return compactLogs(logs).toArray
+         } catch {
+           case e: IOException =>
+             // Another process using `CompactibleFileStreamLog` may delete the batch files when
+             // `StreamFileCatalog` are reading. However, it only happens when a compaction is
+             // deleting old files. If so, let's try the next compaction batch and we should find it.
+             // Otherwise, this is a real IO issue and we should throw it.
+             latestId = nextCompactionBatchId(latestId, compactInterval)
+             super.get(latestId).getOrElse {
+               throw e
+             }
+         }
+       } else {
+         return Array.empty
+       }
+     }
+     Array.empty
+   }
+ 
+   /**
+    * Since all logs before `compactionBatchId` are compacted and written into the
+    * `compactionBatchId` log file, they can be removed. However, due to the eventual consistency of
+    * S3, the compaction file may not be seen by other processes at once. So we only delete files
+    * created `fileCleanupDelayMs` milliseconds ago.
+    */
+   private def deleteExpiredLog(compactionBatchId: Long): Unit = {
+     val expiredTime = System.currentTimeMillis() - fileCleanupDelayMs
+     fileManager.list(metadataPath, new PathFilter {
+       override def accept(path: Path): Boolean = {
+         try {
+           val batchId = getBatchIdFromFileName(path.getName)
+           batchId < compactionBatchId
+         } catch {
+           case _: NumberFormatException =>
+             false
+         }
+       }
+     }).foreach { f =>
+       if (f.getModificationTime <= expiredTime) {
+         fileManager.delete(f.getPath)
+       }
+     }
+   }
+ }
+ 
+ object CompactibleFileStreamLog {
+   val COMPACT_FILE_SUFFIX = ".compact"
+ 
+   def getBatchIdFromFileName(fileName: String): Long = {
+     fileName.stripSuffix(COMPACT_FILE_SUFFIX).toLong
+   }
+ 
+   /**
+    * Returns if this is a compaction batch. FileStreamSinkLog will compact old logs every
+    * `compactInterval` commits.
+    *
+    * E.g., if `compactInterval` is 3, then 2, 5, 8, ... are all compaction batches.
+    */
+   def isCompactionBatch(batchId: Long, compactInterval: Int): Boolean = {
+     (batchId + 1) % compactInterval == 0
+   }
+ 
+   /**
+    * Returns all valid batches before the specified `compactionBatchId`. They contain all logs we
+    * need to do a new compaction.
+    *
+    * E.g., if `compactInterval` is 3 and `compactionBatchId` is 5, this method should returns
+    * `Seq(2, 3, 4)` (Note: it includes the previous compaction batch 2).
+    */
+   def getValidBatchesBeforeCompactionBatch(
+       compactionBatchId: Long,
+       compactInterval: Int): Seq[Long] = {
+     assert(isCompactionBatch(compactionBatchId, compactInterval),
+       s"$compactionBatchId is not a compaction batch")
+     (math.max(0, compactionBatchId - compactInterval)) until compactionBatchId
+   }
+ 
+   /**
+    * Returns all necessary logs before `batchId` (inclusive). If `batchId` is a compaction, just
+    * return itself. Otherwise, it will find the previous compaction batch and return all batches
+    * between it and `batchId`.
+    */
+   def getAllValidBatches(batchId: Long, compactInterval: Long): Seq[Long] = {
+     assert(batchId >= 0)
+     val start = math.max(0, (batchId + 1) / compactInterval * compactInterval - 1)
+     start to batchId
+   }
+ 
+   /**
+    * Returns the next compaction batch id after `batchId`.
+    */
+   def nextCompactionBatchId(batchId: Long, compactInterval: Long): Long = {
+     (batchId + compactInterval + 1) / compactInterval * compactInterval - 1
+   }
+ }
++>>>>>>> v2.0.2
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
index 8c3e718,c47033a..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala
index b7587f2,4707bfb..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 2614032,7598d47..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala
index f70c7d0,b959444..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala
index db606ab,9e311fa..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala
diff --cc sql/core/src/test/resources/sql-tests/inputs/array.sql
index 4038a0d,984321a..0000000
--- a/sql/core/src/test/resources/sql-tests/inputs/array.sql
+++ b/sql/core/src/test/resources/sql-tests/inputs/array.sql
diff --cc sql/core/src/test/resources/sql-tests/inputs/group-by.sql
index 6741703,d950ec8..0000000
--- a/sql/core/src/test/resources/sql-tests/inputs/group-by.sql
+++ b/sql/core/src/test/resources/sql-tests/inputs/group-by.sql
@@@ -1,17 -1,34 +1,34 @@@
- -- Temporary data.
- create temporary view myview as values 128, 256 as v(int_col);
+ -- Test data.
+ CREATE OR REPLACE TEMPORARY VIEW testData AS SELECT * FROM VALUES
+ (1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2), (null, 1), (3, null), (null, null)
+ AS testData(a, b);
  
- -- group by should produce all input rows,
- select int_col, count(*) from myview group by int_col;
+ -- Aggregate with empty GroupBy expressions.
+ SELECT a, COUNT(b) FROM testData;
+ SELECT COUNT(a), COUNT(b) FROM testData;
  
- -- group by should produce a single row.
- select 'foo', count(*) from myview group by 1;
+ -- Aggregate with non-empty GroupBy expressions.
+ SELECT a, COUNT(b) FROM testData GROUP BY a;
+ SELECT a, COUNT(b) FROM testData GROUP BY b;
+ SELECT COUNT(a), COUNT(b) FROM testData GROUP BY a;
  
- -- group-by should not produce any rows (whole stage code generation).
- select 'foo' from myview where int_col == 0 group by 1;
+ -- Aggregate grouped by literals.
+ SELECT 'foo', COUNT(a) FROM testData GROUP BY 1;
  
- -- group-by should not produce any rows (hash aggregate).
- select 'foo', approx_count_distinct(int_col) from myview where int_col == 0 group by 1;
+ -- Aggregate grouped by literals (whole stage code generation).
+ SELECT 'foo' FROM testData WHERE a = 0 GROUP BY 1;
  
- -- group-by should not produce any rows (sort aggregate).
- select 'foo', max(struct(int_col)) from myview where int_col == 0 group by 1;
+ -- Aggregate grouped by literals (hash aggregate).
+ SELECT 'foo', APPROX_COUNT_DISTINCT(a) FROM testData WHERE a = 0 GROUP BY 1;
+ 
+ -- Aggregate grouped by literals (sort aggregate).
+ SELECT 'foo', MAX(STRUCT(a)) FROM testData WHERE a = 0 GROUP BY 1;
+ 
+ -- Aggregate with complex GroupBy expressions.
+ SELECT a + b, COUNT(b) FROM testData GROUP BY a + b;
+ SELECT a + 2, COUNT(b) FROM testData GROUP BY a + 1;
+ SELECT a + 1 + 1, COUNT(b) FROM testData GROUP BY a + 1;
+ 
+ -- Aggregate with nulls.
+ SELECT SKEWNESS(a), KURTOSIS(a), MIN(a), MAX(a), AVG(a), VARIANCE(a), STDDEV(a), SUM(a), COUNT(a)
 -FROM testData;
++FROM testData;
diff --cc sql/core/src/test/resources/sql-tests/results/array.sql.out
index 4a1d149,499a3d5..0000000
--- a/sql/core/src/test/resources/sql-tests/results/array.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/array.sql.out
diff --cc sql/core/src/test/resources/sql-tests/results/group-by.sql.out
index 9127bd4,a91f04e..0000000
--- a/sql/core/src/test/resources/sql-tests/results/group-by.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/group-by.sql.out
@@@ -44,8 -50,84 +50,84 @@@ expression 'testdata.`a`' is neither pr
  
  
  -- !query 5
- select 'foo', max(struct(int_col)) from myview where int_col == 0 group by 1
+ SELECT COUNT(a), COUNT(b) FROM testData GROUP BY a
  -- !query 5 schema
- struct<foo:string,max(struct(int_col)):struct<int_col:int>>
+ struct<count(a):bigint,count(b):bigint>
  -- !query 5 output
+ 0	1
+ 2	2
+ 2	2
+ 3	2
+ 
+ 
+ -- !query 6
+ SELECT 'foo', COUNT(a) FROM testData GROUP BY 1
+ -- !query 6 schema
+ struct<foo:string,count(a):bigint>
+ -- !query 6 output
+ foo	7
+ 
+ 
+ -- !query 7
+ SELECT 'foo' FROM testData WHERE a = 0 GROUP BY 1
+ -- !query 7 schema
+ struct<foo:string>
+ -- !query 7 output
+ 
  
+ 
+ -- !query 8
+ SELECT 'foo', APPROX_COUNT_DISTINCT(a) FROM testData WHERE a = 0 GROUP BY 1
+ -- !query 8 schema
+ struct<foo:string,approx_count_distinct(a):bigint>
+ -- !query 8 output
+ 
+ 
+ 
+ -- !query 9
+ SELECT 'foo', MAX(STRUCT(a)) FROM testData WHERE a = 0 GROUP BY 1
+ -- !query 9 schema
+ struct<foo:string,max(struct(a)):struct<a:int>>
+ -- !query 9 output
+ 
+ 
+ 
+ -- !query 10
+ SELECT a + b, COUNT(b) FROM testData GROUP BY a + b
+ -- !query 10 schema
+ struct<(a + b):int,count(b):bigint>
+ -- !query 10 output
+ 2	1
+ 3	2
+ 4	2
+ 5	1
+ NULL	1
+ 
+ 
+ -- !query 11
+ SELECT a + 2, COUNT(b) FROM testData GROUP BY a + 1
+ -- !query 11 schema
+ struct<>
+ -- !query 11 output
+ org.apache.spark.sql.AnalysisException
+ expression 'testdata.`a`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;
+ 
+ 
+ -- !query 12
+ SELECT a + 1 + 1, COUNT(b) FROM testData GROUP BY a + 1
+ -- !query 12 schema
+ struct<((a + 1) + 1):int,count(b):bigint>
+ -- !query 12 output
+ 3	2
+ 4	2
+ 5	2
+ NULL	1
+ 
+ 
+ -- !query 13
+ SELECT SKEWNESS(a), KURTOSIS(a), MIN(a), MAX(a), AVG(a), VARIANCE(a), STDDEV(a), SUM(a), COUNT(a)
+ FROM testData
+ -- !query 13 schema
+ struct<skewness(CAST(a AS DOUBLE)):double,kurtosis(CAST(a AS DOUBLE)):double,min(a):int,max(a):int,avg(a):double,var_samp(CAST(a AS DOUBLE)):double,stddev_samp(CAST(a AS DOUBLE)):double,sum(a):bigint,count(a):bigint>
+ -- !query 13 output
 --0.2723801058145729	-1.5069204152249134	1	3	2.142857142857143	0.8095238095238094	0.8997354108424372	15	7
++-0.2723801058145729	-1.5069204152249134	1	3	2.142857142857143	0.8095238095238094	0.8997354108424372	15	7
diff --cc sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala
index da5c538,b8becf7..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala
diff --cc sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala
index f897cfb,7a98915..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala
diff --cc sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
index cf25097,c96ba07..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
@@@ -466,20 -463,6 +463,30 @@@ class SQLQuerySuite extends QueryTest w
      )
    }
  
++  test("index into array of arrays") {
++    checkAnswer(
++      sql(
++        "SELECT nestedData, nestedData[0][0], nestedData[0][0] + nestedData[0][1] FROM arrayData"),
++      arrayData.map(d =>
++        Row(d.nestedData,
++         d.nestedData(0)(0),
++         d.nestedData(0)(0) + d.nestedData(0)(1))).collect().toSeq)
++  }
++
 +  test("agg") {
 +    checkAnswer(
 +      sql("SELECT a, SUM(b) FROM testData2 GROUP BY a"),
 +      Seq(Row(1, 3), Row(2, 3), Row(3, 3)))
 +  }
 +
 +  test("aggregates with nulls") {
 +    checkAnswer(
 +      sql("SELECT SKEWNESS(a), KURTOSIS(a), MIN(a), MAX(a)," +
 +        "AVG(a), VARIANCE(a), STDDEV(a), SUM(a), COUNT(a) FROM nullInts"),
 +      Row(0, -1.5, 1, 3, 2, 1.0, 1, 6, 3)
 +    )
 +  }
 +
    test("select *") {
      checkAnswer(
        sql("SELECT * FROM testData"),
diff --cc sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLogSuite.scala
index 41a8cc2,e1bc674..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLogSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLogSuite.scala
@@@ -174,9 -178,9 +178,8 @@@ class FileStreamSinkLogSuite extends Sp
            blockSize = 30000L,
            action = FileStreamSinkLog.ADD_ACTION))
  
-       assert(expected === sinkLog.deserialize(logs.getBytes(UTF_8)))
- 
-       assert(Nil === sinkLog.deserialize(VERSION.getBytes(UTF_8)))
+       assert(expected === sinkLog.deserialize(new ByteArrayInputStream(logs.getBytes(UTF_8))))
 -
+       assert(Nil === sinkLog.deserialize(new ByteArrayInputStream(VERSION.getBytes(UTF_8))))
      }
    }
  
diff --cc sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceSuite.scala
index 1793db0,3bad5bb..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceSuite.scala
diff --cc sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
index 55c95ae,c1adbc9..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
diff --cc sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamTest.scala
index 6c5b170,7428330..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamTest.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamTest.scala
diff --cc sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryListenerSuite.scala
index 831543a,fad24bb..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryListenerSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryListenerSuite.scala
@@@ -258,49 -287,9 +287,8 @@@ class StreamingQueryListenerSuite exten
      val listenerBus = spark.streams invokePrivate listenerBusMethod()
      listenerBus.listeners.toArray.map(_.asInstanceOf[StreamingQueryListener])
    }
- 
-   class QueryStatusCollector extends StreamingQueryListener {
-     // to catch errors in the async listener events
-     @volatile private var asyncTestWaiter = new Waiter
- 
-     @volatile var startStatus: StreamingQueryInfo = null
-     @volatile var terminationStatus: StreamingQueryInfo = null
-     @volatile var terminationException: Option[String] = null
- 
-     val progressStatuses = new ConcurrentLinkedQueue[StreamingQueryInfo]
- 
-     def reset(): Unit = {
-       startStatus = null
-       terminationStatus = null
-       progressStatuses.clear()
-       asyncTestWaiter = new Waiter
-     }
- 
-     def checkAsyncErrors(): Unit = {
-       asyncTestWaiter.await(timeout(streamingTimeout))
-     }
- 
- 
-     override def onQueryStarted(queryStarted: QueryStarted): Unit = {
-       asyncTestWaiter {
-         startStatus = queryStarted.queryInfo
-       }
-     }
- 
-     override def onQueryProgress(queryProgress: QueryProgress): Unit = {
-       asyncTestWaiter {
-         assert(startStatus != null, "onQueryProgress called before onQueryStarted")
-         progressStatuses.add(queryProgress.queryInfo)
-       }
-     }
- 
-     override def onQueryTerminated(queryTerminated: QueryTerminated): Unit = {
-       asyncTestWaiter {
-         assert(startStatus != null, "onQueryTerminated called before onQueryStarted")
-         terminationStatus = queryTerminated.queryInfo
-         terminationException = queryTerminated.exception
-       }
-       asyncTestWaiter.dismiss()
-     }
-   }
+ }
 -
+ object StreamingQueryListenerSuite {
+   // Singleton reference to clock that does not get serialized in task closures
+   @volatile var clock: ManualClock = null
  }
diff --cc sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala
index 88f1f18,31b7fe0..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala
diff --cc sql/hive-thriftserver/pom.xml
index c47d5f0,73271b6..0000000
--- a/sql/hive-thriftserver/pom.xml
+++ b/sql/hive-thriftserver/pom.xml
diff --cc sql/hive/pom.xml
index 2388d5f,bd62d26..0000000
--- a/sql/hive/pom.xml
+++ b/sql/hive/pom.xml
diff --cc sql/hive/src/test/scala/org/apache/spark/sql/catalyst/LogicalPlanToSQLSuite.scala
index ef2f756,3794f63..0000000
--- a/sql/hive/src/test/scala/org/apache/spark/sql/catalyst/LogicalPlanToSQLSuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/catalyst/LogicalPlanToSQLSuite.scala
diff --cc sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveUtilsSuite.scala
index 667a7dd,667a7dd..0000000
deleted file mode 100644,100644
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveUtilsSuite.scala
+++ /dev/null
diff --cc sql/hive/src/test/scala/org/apache/spark/sql/hive/MetastoreRelationSuite.scala
index eec60b4,c6711c3..0000000
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/MetastoreRelationSuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/MetastoreRelationSuite.scala
diff --cc streaming/pom.xml
index 4e0f5b1,b6a1ab7..0000000
--- a/streaming/pom.xml
+++ b/streaming/pom.xml
diff --cc tools/pom.xml
index ddc1b09,474df40..0000000
--- a/tools/pom.xml
+++ b/tools/pom.xml
diff --cc yarn/pom.xml
index b676b6d,063efee..0000000
--- a/yarn/pom.xml
+++ b/yarn/pom.xml
* Unmerged path core/src/main/scala/org/apache/spark/storage/BlockFetchException.scala
* Unmerged path external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/package-info.java
