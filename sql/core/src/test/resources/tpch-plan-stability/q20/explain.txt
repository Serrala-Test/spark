== Physical Plan ==
* Sort (36)
+- Exchange (35)
   +- * Project (34)
      +- * BroadcastHashJoin Inner BuildRight (33)
         :- * Project (27)
         :  +- * BroadcastHashJoin LeftSemi BuildRight (26)
         :     :- * Filter (3)
         :     :  +- * ColumnarToRow (2)
         :     :     +- Scan parquet spark_catalog.default.supplier (1)
         :     +- BroadcastExchange (25)
         :        +- * Project (24)
         :           +- * BroadcastNestedLoopJoin Inner BuildLeft (23)
         :              :- BroadcastExchange (14)
         :              :  +- * Project (13)
         :              :     +- * BroadcastHashJoin LeftSemi BuildRight (12)
         :              :        :- * Filter (6)
         :              :        :  +- * ColumnarToRow (5)
         :              :        :     +- Scan parquet spark_catalog.default.partsupp (4)
         :              :        +- BroadcastExchange (11)
         :              :           +- * Project (10)
         :              :              +- * Filter (9)
         :              :                 +- * ColumnarToRow (8)
         :              :                    +- Scan parquet spark_catalog.default.part (7)
         :              +- * Filter (22)
         :                 +- * HashAggregate (21)
         :                    +- Exchange (20)
         :                       +- * HashAggregate (19)
         :                          +- * Project (18)
         :                             +- * Filter (17)
         :                                +- * ColumnarToRow (16)
         :                                   +- Scan parquet spark_catalog.default.lineitem (15)
         +- BroadcastExchange (32)
            +- * Project (31)
               +- * Filter (30)
                  +- * ColumnarToRow (29)
                     +- Scan parquet spark_catalog.default.nation (28)


(1) Scan parquet spark_catalog.default.supplier
Output [4]: [s_suppkey#1, s_name#2, s_address#3, s_nationkey#4]
Batched: true
Location [not included in comparison]/{warehouse_dir}/supplier]
PushedFilters: [IsNotNull(s_suppkey), IsNotNull(s_nationkey)]
ReadSchema: struct<s_suppkey:bigint,s_name:string,s_address:string,s_nationkey:bigint>

(2) ColumnarToRow [codegen id : 6]
Input [4]: [s_suppkey#1, s_name#2, s_address#3, s_nationkey#4]

(3) Filter [codegen id : 6]
Input [4]: [s_suppkey#1, s_name#2, s_address#3, s_nationkey#4]
Condition : (isnotnull(s_suppkey#1) AND isnotnull(s_nationkey#4))

(4) Scan parquet spark_catalog.default.partsupp
Output [3]: [ps_partkey#5, ps_suppkey#6, ps_availqty#7]
Batched: true
Location [not included in comparison]/{warehouse_dir}/partsupp]
PushedFilters: [IsNotNull(ps_partkey), IsNotNull(ps_availqty), IsNotNull(ps_suppkey)]
ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int>

(5) ColumnarToRow [codegen id : 2]
Input [3]: [ps_partkey#5, ps_suppkey#6, ps_availqty#7]

(6) Filter [codegen id : 2]
Input [3]: [ps_partkey#5, ps_suppkey#6, ps_availqty#7]
Condition : ((isnotnull(ps_partkey#5) AND isnotnull(ps_availqty#7)) AND isnotnull(ps_suppkey#6))

(7) Scan parquet spark_catalog.default.part
Output [2]: [p_partkey#8, p_name#9]
Batched: true
Location [not included in comparison]/{warehouse_dir}/part]
PushedFilters: [IsNotNull(p_name), StringStartsWith(p_name,forest), IsNotNull(p_partkey)]
ReadSchema: struct<p_partkey:bigint,p_name:string>

(8) ColumnarToRow [codegen id : 1]
Input [2]: [p_partkey#8, p_name#9]

(9) Filter [codegen id : 1]
Input [2]: [p_partkey#8, p_name#9]
Condition : ((isnotnull(p_name#9) AND StartsWith(p_name#9, forest)) AND isnotnull(p_partkey#8))

(10) Project [codegen id : 1]
Output [1]: [p_partkey#8]
Input [2]: [p_partkey#8, p_name#9]

(11) BroadcastExchange
Input [1]: [p_partkey#8]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=1]

(12) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [ps_partkey#5]
Right keys [1]: [p_partkey#8]
Join type: LeftSemi
Join condition: None

(13) Project [codegen id : 2]
Output [2]: [ps_suppkey#6, ps_availqty#7]
Input [3]: [ps_partkey#5, ps_suppkey#6, ps_availqty#7]

(14) BroadcastExchange
Input [2]: [ps_suppkey#6, ps_availqty#7]
Arguments: IdentityBroadcastMode, [plan_id=2]

(15) Scan parquet spark_catalog.default.lineitem
Output [4]: [l_partkey#10, l_suppkey#11, l_quantity#12, l_shipdate#13]
Batched: true
Location [not included in comparison]/{warehouse_dir}/lineitem]
PushedFilters: [IsNotNull(l_partkey), IsNotNull(l_suppkey), IsNotNull(l_shipdate), GreaterThanOrEqual(l_shipdate,1994-01-01), LessThan(l_shipdate,1995-01-01)]
ReadSchema: struct<l_partkey:bigint,l_suppkey:bigint,l_quantity:decimal(10,0),l_shipdate:date>

(16) ColumnarToRow [codegen id : 3]
Input [4]: [l_partkey#10, l_suppkey#11, l_quantity#12, l_shipdate#13]

(17) Filter [codegen id : 3]
Input [4]: [l_partkey#10, l_suppkey#11, l_quantity#12, l_shipdate#13]
Condition : ((((((isnotnull(l_partkey#10) AND isnotnull(l_suppkey#11)) AND isnotnull(l_shipdate#13)) AND (l_partkey#10 = outer(ps_partkey#5))) AND (l_suppkey#11 = outer(ps_suppkey#6))) AND (l_shipdate#13 >= 1994-01-01)) AND (l_shipdate#13 < 1995-01-01))

(18) Project [codegen id : 3]
Output [1]: [l_quantity#12]
Input [4]: [l_partkey#10, l_suppkey#11, l_quantity#12, l_shipdate#13]

(19) HashAggregate [codegen id : 3]
Input [1]: [l_quantity#12]
Keys: []
Functions [1]: [partial_sum(l_quantity#12)]
Aggregate Attributes [2]: [sum#14, isEmpty#15]
Results [2]: [sum#16, isEmpty#17]

(20) Exchange
Input [2]: [sum#16, isEmpty#17]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=3]

(21) HashAggregate
Input [2]: [sum#16, isEmpty#17]
Keys: []
Functions [1]: [sum(l_quantity#12)]
Aggregate Attributes [1]: [sum(l_quantity#12)#18]
Results [1]: [(0.5 * sum(l_quantity#12)#18) AS (0.5 * sum(l_quantity))#19]

(22) Filter
Input [1]: [(0.5 * sum(l_quantity))#19]
Condition : isnotnull((0.5 * sum(l_quantity))#19)

(23) BroadcastNestedLoopJoin [codegen id : 4]
Join type: Inner
Join condition: (cast(ps_availqty#7 as decimal(22,1)) > (0.5 * sum(l_quantity))#19)

(24) Project [codegen id : 4]
Output [1]: [ps_suppkey#6]
Input [3]: [ps_suppkey#6, ps_availqty#7, (0.5 * sum(l_quantity))#19]

(25) BroadcastExchange
Input [1]: [ps_suppkey#6]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=4]

(26) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [s_suppkey#1]
Right keys [1]: [ps_suppkey#6]
Join type: LeftSemi
Join condition: None

(27) Project [codegen id : 6]
Output [3]: [s_name#2, s_address#3, s_nationkey#4]
Input [4]: [s_suppkey#1, s_name#2, s_address#3, s_nationkey#4]

(28) Scan parquet spark_catalog.default.nation
Output [2]: [n_nationkey#20, n_name#21]
Batched: true
Location [not included in comparison]/{warehouse_dir}/nation]
PushedFilters: [IsNotNull(n_name), EqualTo(n_name,CANADA), IsNotNull(n_nationkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string>

(29) ColumnarToRow [codegen id : 5]
Input [2]: [n_nationkey#20, n_name#21]

(30) Filter [codegen id : 5]
Input [2]: [n_nationkey#20, n_name#21]
Condition : ((isnotnull(n_name#21) AND (n_name#21 = CANADA)) AND isnotnull(n_nationkey#20))

(31) Project [codegen id : 5]
Output [1]: [n_nationkey#20]
Input [2]: [n_nationkey#20, n_name#21]

(32) BroadcastExchange
Input [1]: [n_nationkey#20]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=5]

(33) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [s_nationkey#4]
Right keys [1]: [n_nationkey#20]
Join type: Inner
Join condition: None

(34) Project [codegen id : 6]
Output [2]: [s_name#2, s_address#3]
Input [4]: [s_name#2, s_address#3, s_nationkey#4, n_nationkey#20]

(35) Exchange
Input [2]: [s_name#2, s_address#3]
Arguments: rangepartitioning(s_name#2 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=6]

(36) Sort [codegen id : 7]
Input [2]: [s_name#2, s_address#3]
Arguments: [s_name#2 ASC NULLS FIRST], true, 0

