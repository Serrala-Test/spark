-- Automatically generated by SQLQueryTestSuite
-- Number of queries: 37


-- !query
CREATE table  explain_temp1 (key int, val int) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp2 (key int, val int) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp3 (key int, val int) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp4 (key int, val string) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
SET spark.sql.codegen.wholeStage = true
-- !query schema
struct<key:string,value:string>
-- !query output
spark.sql.codegen.wholeStage	true


-- !query
EXPLAIN EXTENDED
  SELECT sum(distinct val)
  FROM explain_temp1
-- !query schema
struct<plan:string>
-- !query output
== Parsed Logical Plan ==
'Project [unresolvedalias('sum(distinct 'val), None)]
+- 'UnresolvedRelation [explain_temp1], [], false

== Analyzed Logical Plan ==
sum(DISTINCT val): bigint
Aggregate [sum(distinct val#x) AS sum(DISTINCT val)#xL]
+- SubqueryAlias spark_catalog.default.explain_temp1
   +- Relation default.explain_temp1[key#x,val#x] parquet

== Optimized Logical Plan ==
Aggregate [sum(distinct val#x) AS sum(DISTINCT val)#xL]
+- Project [val#x]
   +- Relation default.explain_temp1[key#x,val#x] parquet

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[sum(distinct val#x)], output=[sum(DISTINCT val)#xL])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#x]
      +- HashAggregate(keys=[], functions=[partial_sum(distinct val#x)], output=[sum#xL])
         +- HashAggregate(keys=[val#x], functions=[], output=[val#x])
            +- Exchange hashpartitioning(val#x, 4), ENSURE_REQUIREMENTS, [id=#x]
               +- HashAggregate(keys=[val#x], functions=[], output=[val#x])
                  +- FileScan parquet default.explain_temp1[val#x] Batched: true, DataFilters: [], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<val:int>


-- !query
EXPLAIN FORMATTED
  SELECT key, max(val) 
  FROM   explain_temp1 
  WHERE  key > 0 
  GROUP  BY key 
  ORDER  BY key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (8)
+- Sort (7)
   +- Exchange (6)
      +- HashAggregate (5)
         +- Exchange (4)
            +- HashAggregate (3)
               +- Filter (2)
                  +- Scan parquet default.explain_temp1 (1)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,0)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 0))

(3) HashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_max(val#x)]
Aggregate Attributes [1]: [max#x]
Results [2]: [key#x, max#x]

(4) Exchange
Input [2]: [key#x, max#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [id=#x]

(5) HashAggregate
Input [2]: [key#x, max#x]
Keys [1]: [key#x]
Functions [1]: [max(val#x)]
Aggregate Attributes [1]: [max(val#x)#x]
Results [2]: [key#x, max(val#x)#x AS max(val)#x]

(6) Exchange
Input [2]: [key#x, max(val)#x]
Arguments: rangepartitioning(key#x ASC NULLS FIRST, 4), ENSURE_REQUIREMENTS, [id=#x]

(7) Sort
Input [2]: [key#x, max(val)#x]
Arguments: [key#x ASC NULLS FIRST], true, 0

(8) AdaptiveSparkPlan
Output [2]: [key#x, max(val)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT key, max(val)
  FROM explain_temp1
  WHERE key > 0
  GROUP BY key
  HAVING max(val) > 0
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (7)
+- Filter (6)
   +- HashAggregate (5)
      +- Exchange (4)
         +- HashAggregate (3)
            +- Filter (2)
               +- Scan parquet default.explain_temp1 (1)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,0)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 0))

(3) HashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_max(val#x)]
Aggregate Attributes [1]: [max#x]
Results [2]: [key#x, max#x]

(4) Exchange
Input [2]: [key#x, max#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [id=#x]

(5) HashAggregate
Input [2]: [key#x, max#x]
Keys [1]: [key#x]
Functions [1]: [max(val#x)]
Aggregate Attributes [1]: [max(val#x)#x]
Results [2]: [key#x, max(val#x)#x AS max(val)#x]

(6) Filter
Input [2]: [key#x, max(val)#x]
Condition : (isnotnull(max(val)#x) AND (max(val)#x > 0))

(7) AdaptiveSparkPlan
Output [2]: [key#x, max(val)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT key, val FROM explain_temp1 WHERE key > 0
  UNION 
  SELECT key, val FROM explain_temp1 WHERE key > 1
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (9)
+- HashAggregate (8)
   +- Exchange (7)
      +- HashAggregate (6)
         +- Union (5)
            :- Filter (2)
            :  +- Scan parquet default.explain_temp1 (1)
            +- Filter (4)
               +- Scan parquet default.explain_temp1 (3)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,0)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 0))

(3) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,1)]
ReadSchema: struct<key:int,val:int>

(4) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 1))

(5) Union

(6) HashAggregate
Input [2]: [key#x, val#x]
Keys [2]: [key#x, val#x]
Functions: []
Aggregate Attributes: []
Results [2]: [key#x, val#x]

(7) Exchange
Input [2]: [key#x, val#x]
Arguments: hashpartitioning(key#x, val#x, 4), ENSURE_REQUIREMENTS, [id=#x]

(8) HashAggregate
Input [2]: [key#x, val#x]
Keys [2]: [key#x, val#x]
Functions: []
Aggregate Attributes: []
Results [2]: [key#x, val#x]

(9) AdaptiveSparkPlan
Output [2]: [key#x, val#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT * 
  FROM   explain_temp1 a, 
         explain_temp2 b 
  WHERE  a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (7)
+- BroadcastHashJoin Inner BuildRight (6)
   :- Filter (2)
   :  +- Scan parquet default.explain_temp1 (1)
   +- BroadcastExchange (5)
      +- Filter (4)
         +- Scan parquet default.explain_temp2 (3)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(3) Scan parquet default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(key)]
ReadSchema: struct<key:int,val:int>

(4) Filter
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(5) BroadcastExchange
Input [2]: [key#x, val#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]

(6) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join condition: None

(7) AdaptiveSparkPlan
Output [4]: [key#x, val#x, key#x, val#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT * 
  FROM   explain_temp1 a 
         LEFT OUTER JOIN explain_temp2 b 
                      ON a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (6)
+- BroadcastHashJoin LeftOuter BuildRight (5)
   :- Scan parquet default.explain_temp1 (1)
   +- BroadcastExchange (4)
      +- Filter (3)
         +- Scan parquet default.explain_temp2 (2)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int,val:int>

(2) Scan parquet default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(key)]
ReadSchema: struct<key:int,val:int>

(3) Filter
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(4) BroadcastExchange
Input [2]: [key#x, val#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]

(5) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join condition: None

(6) AdaptiveSparkPlan
Output [4]: [key#x, val#x, key#x, val#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT * 
  FROM   explain_temp1 
  WHERE  key = (SELECT max(key) 
                FROM   explain_temp2 
                WHERE  key = (SELECT max(key) 
                              FROM   explain_temp3 
                              WHERE  val > 0) 
                       AND val = 2) 
         AND val > 3
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (3)
+- Filter (2)
   +- Scan parquet default.explain_temp1 (1)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), IsNotNull(val), GreaterThan(val,3)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (((isnotnull(key#x) AND isnotnull(val#x)) AND (key#x = Subquery subquery#x, [id=#x])) AND (val#x > 3))

(3) AdaptiveSparkPlan
Output [2]: [key#x, val#x]
Arguments: isFinalPlan=false

===== Subqueries =====

Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (10)
+- HashAggregate (9)
   +- Exchange (8)
      +- HashAggregate (7)
         +- Project (6)
            +- Filter (5)
               +- Scan parquet default.explain_temp2 (4)


(4) Scan parquet default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(key), IsNotNull(val), EqualTo(val,2)]
ReadSchema: struct<key:int,val:int>

(5) Filter
Input [2]: [key#x, val#x]
Condition : (((isnotnull(key#x) AND isnotnull(val#x)) AND (key#x = Subquery subquery#x, [id=#x])) AND (val#x = 2))

(6) Project
Output [1]: [key#x]
Input [2]: [key#x, val#x]

(7) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_max(key#x)]
Aggregate Attributes [1]: [max#x]
Results [1]: [max#x]

(8) Exchange
Input [1]: [max#x]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [id=#x]

(9) HashAggregate
Input [1]: [max#x]
Keys: []
Functions [1]: [max(key#x)]
Aggregate Attributes [1]: [max(key#x)#x]
Results [1]: [max(key#x)#x AS max(key)#x]

(10) AdaptiveSparkPlan
Output [1]: [max(key)#x]
Arguments: isFinalPlan=false

Subquery:2 Hosting operator id = 5 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (17)
+- HashAggregate (16)
   +- Exchange (15)
      +- HashAggregate (14)
         +- Project (13)
            +- Filter (12)
               +- Scan parquet default.explain_temp3 (11)


(11) Scan parquet default.explain_temp3
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp3]
PushedFilters: [IsNotNull(val), GreaterThan(val,0)]
ReadSchema: struct<key:int,val:int>

(12) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(val#x) AND (val#x > 0))

(13) Project
Output [1]: [key#x]
Input [2]: [key#x, val#x]

(14) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_max(key#x)]
Aggregate Attributes [1]: [max#x]
Results [1]: [max#x]

(15) Exchange
Input [1]: [max#x]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [id=#x]

(16) HashAggregate
Input [1]: [max#x]
Keys: []
Functions [1]: [max(key#x)]
Aggregate Attributes [1]: [max(key#x)#x]
Results [1]: [max(key#x)#x AS max(key)#x]

(17) AdaptiveSparkPlan
Output [1]: [max(key)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT * 
  FROM   explain_temp1 
  WHERE  key = (SELECT max(key) 
                FROM   explain_temp2 
                WHERE  val > 0) 
         OR
         key = (SELECT avg(key)
                FROM   explain_temp3
                WHERE  val > 0)
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (3)
+- Filter (2)
   +- Scan parquet default.explain_temp1 (1)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : ((key#x = Subquery subquery#x, [id=#x]) OR (cast(key#x as double) = Subquery subquery#x, [id=#x]))

(3) AdaptiveSparkPlan
Output [2]: [key#x, val#x]
Arguments: isFinalPlan=false

===== Subqueries =====

Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (10)
+- HashAggregate (9)
   +- Exchange (8)
      +- HashAggregate (7)
         +- Project (6)
            +- Filter (5)
               +- Scan parquet default.explain_temp2 (4)


(4) Scan parquet default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(val), GreaterThan(val,0)]
ReadSchema: struct<key:int,val:int>

(5) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(val#x) AND (val#x > 0))

(6) Project
Output [1]: [key#x]
Input [2]: [key#x, val#x]

(7) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_max(key#x)]
Aggregate Attributes [1]: [max#x]
Results [1]: [max#x]

(8) Exchange
Input [1]: [max#x]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [id=#x]

(9) HashAggregate
Input [1]: [max#x]
Keys: []
Functions [1]: [max(key#x)]
Aggregate Attributes [1]: [max(key#x)#x]
Results [1]: [max(key#x)#x AS max(key)#x]

(10) AdaptiveSparkPlan
Output [1]: [max(key)#x]
Arguments: isFinalPlan=false

Subquery:2 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (17)
+- HashAggregate (16)
   +- Exchange (15)
      +- HashAggregate (14)
         +- Project (13)
            +- Filter (12)
               +- Scan parquet default.explain_temp3 (11)


(11) Scan parquet default.explain_temp3
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp3]
PushedFilters: [IsNotNull(val), GreaterThan(val,0)]
ReadSchema: struct<key:int,val:int>

(12) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(val#x) AND (val#x > 0))

(13) Project
Output [1]: [key#x]
Input [2]: [key#x, val#x]

(14) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_avg(key#x)]
Aggregate Attributes [2]: [sum#x, count#xL]
Results [2]: [sum#x, count#xL]

(15) Exchange
Input [2]: [sum#x, count#xL]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [id=#x]

(16) HashAggregate
Input [2]: [sum#x, count#xL]
Keys: []
Functions [1]: [avg(key#x)]
Aggregate Attributes [1]: [avg(key#x)#x]
Results [1]: [avg(key#x)#x AS avg(key)#x]

(17) AdaptiveSparkPlan
Output [1]: [avg(key)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT (SELECT Avg(key) FROM explain_temp1) + (SELECT Avg(key) FROM explain_temp1)
  FROM explain_temp1
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (3)
+- Project (2)
   +- Scan parquet default.explain_temp1 (1)


(1) Scan parquet default.explain_temp1
Output: []
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<>

(2) Project
Output [1]: [(Subquery subquery#x, [id=#x] + Subquery subquery#x, [id=#x]) AS (scalarsubquery() + scalarsubquery())#x]
Input: []

(3) AdaptiveSparkPlan
Output [1]: [(scalarsubquery() + scalarsubquery())#x]
Arguments: isFinalPlan=false

===== Subqueries =====

Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (8)
+- HashAggregate (7)
   +- Exchange (6)
      +- HashAggregate (5)
         +- Scan parquet default.explain_temp1 (4)


(4) Scan parquet default.explain_temp1
Output [1]: [key#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int>

(5) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_avg(key#x)]
Aggregate Attributes [2]: [sum#x, count#xL]
Results [2]: [sum#x, count#xL]

(6) Exchange
Input [2]: [sum#x, count#xL]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [id=#x]

(7) HashAggregate
Input [2]: [sum#x, count#xL]
Keys: []
Functions [1]: [avg(key#x)]
Aggregate Attributes [1]: [avg(key#x)#x]
Results [1]: [avg(key#x)#x AS avg(key)#x]

(8) AdaptiveSparkPlan
Output [1]: [avg(key)#x]
Arguments: isFinalPlan=false

Subquery:2 Hosting operator id = 2 Hosting Expression = Subquery subquery#x, [id=#x]
AdaptiveSparkPlan (13)
+- HashAggregate (12)
   +- Exchange (11)
      +- HashAggregate (10)
         +- Scan parquet default.explain_temp1 (9)


(9) Scan parquet default.explain_temp1
Output [1]: [key#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int>

(10) HashAggregate
Input [1]: [key#x]
Keys: []
Functions [1]: [partial_avg(key#x)]
Aggregate Attributes [2]: [sum#x, count#xL]
Results [2]: [sum#x, count#xL]

(11) Exchange
Input [2]: [sum#x, count#xL]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [id=#x]

(12) HashAggregate
Input [2]: [sum#x, count#xL]
Keys: []
Functions [1]: [avg(key#x)]
Aggregate Attributes [1]: [avg(key#x)#x]
Results [1]: [avg(key#x)#x AS avg(key)#x]

(13) AdaptiveSparkPlan
Output [1]: [avg(key)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  WITH cte1 AS (
    SELECT *
    FROM explain_temp1 
    WHERE key > 10
  )
  SELECT * FROM cte1 a, cte1 b WHERE a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (7)
+- BroadcastHashJoin Inner BuildRight (6)
   :- Filter (2)
   :  +- Scan parquet default.explain_temp1 (1)
   +- BroadcastExchange (5)
      +- Filter (4)
         +- Scan parquet default.explain_temp1 (3)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,10)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 10))

(3) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,10)]
ReadSchema: struct<key:int,val:int>

(4) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 10))

(5) BroadcastExchange
Input [2]: [key#x, val#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]

(6) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join condition: None

(7) AdaptiveSparkPlan
Output [4]: [key#x, val#x, key#x, val#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  WITH cte1 AS (
    SELECT key, max(val)
    FROM explain_temp1 
    WHERE key > 10
    GROUP BY key
  )
  SELECT * FROM cte1 a, cte1 b WHERE a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (13)
+- BroadcastHashJoin Inner BuildRight (12)
   :- HashAggregate (5)
   :  +- Exchange (4)
   :     +- HashAggregate (3)
   :        +- Filter (2)
   :           +- Scan parquet default.explain_temp1 (1)
   +- BroadcastExchange (11)
      +- HashAggregate (10)
         +- Exchange (9)
            +- HashAggregate (8)
               +- Filter (7)
                  +- Scan parquet default.explain_temp1 (6)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,10)]
ReadSchema: struct<key:int,val:int>

(2) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 10))

(3) HashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_max(val#x)]
Aggregate Attributes [1]: [max#x]
Results [2]: [key#x, max#x]

(4) Exchange
Input [2]: [key#x, max#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [id=#x]

(5) HashAggregate
Input [2]: [key#x, max#x]
Keys [1]: [key#x]
Functions [1]: [max(val#x)]
Aggregate Attributes [1]: [max(val#x)#x]
Results [2]: [key#x, max(val#x)#x AS max(val)#x]

(6) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key), GreaterThan(key,10)]
ReadSchema: struct<key:int,val:int>

(7) Filter
Input [2]: [key#x, val#x]
Condition : (isnotnull(key#x) AND (key#x > 10))

(8) HashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_max(val#x)]
Aggregate Attributes [1]: [max#x]
Results [2]: [key#x, max#x]

(9) Exchange
Input [2]: [key#x, max#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [id=#x]

(10) HashAggregate
Input [2]: [key#x, max#x]
Keys [1]: [key#x]
Functions [1]: [max(val#x)]
Aggregate Attributes [1]: [max(val#x)#x]
Results [2]: [key#x, max(val#x)#x AS max(val)#x]

(11) BroadcastExchange
Input [2]: [key#x, max(val)#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#x]

(12) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join condition: None

(13) AdaptiveSparkPlan
Output [4]: [key#x, max(val)#x, key#x, max(val)#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  CREATE VIEW explain_view AS
    SELECT key, val FROM explain_temp1
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
Execute CreateViewCommand (1)
   +- CreateViewCommand (2)
         +- Project (5)
            +- SubqueryAlias (4)
               +- LogicalRelation (3)


(1) Execute CreateViewCommand
Output: []

(2) CreateViewCommand
Arguments: `default`.`explain_view`, SELECT key, val FROM explain_temp1, false, false, PersistedView, true

(3) LogicalRelation
Arguments: parquet, [key#x, val#x], CatalogTable(
Database: default
Table: explain_temp1
Created Time [not included in comparison]
Last Access [not included in comparison]
Created By [not included in comparison]
Type: MANAGED
Provider: PARQUET
Location [not included in comparison]/{warehouse_dir}/explain_temp1
Schema: root
-- key: integer (nullable = true)
-- val: integer (nullable = true)
), false

(4) SubqueryAlias
Arguments: spark_catalog.default.explain_temp1

(5) Project
Arguments: [key#x, val#x]


-- !query
EXPLAIN FORMATTED
  SELECT
    COUNT(val) + SUM(key) as TOTAL,
    COUNT(key) FILTER (WHERE val > 1)
  FROM explain_temp1
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (5)
+- HashAggregate (4)
   +- Exchange (3)
      +- HashAggregate (2)
         +- Scan parquet default.explain_temp1 (1)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
ReadSchema: struct<key:int,val:int>

(2) HashAggregate
Input [2]: [key#x, val#x]
Keys: []
Functions [3]: [partial_count(val#x), partial_sum(key#x), partial_count(key#x) FILTER (WHERE (val#x > 1))]
Aggregate Attributes [3]: [count#xL, sum#xL, count#xL]
Results [3]: [count#xL, sum#xL, count#xL]

(3) Exchange
Input [3]: [count#xL, sum#xL, count#xL]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [id=#x]

(4) HashAggregate
Input [3]: [count#xL, sum#xL, count#xL]
Keys: []
Functions [3]: [count(val#x), sum(key#x), count(key#x)]
Aggregate Attributes [3]: [count(val#x)#xL, sum(key#x)#xL, count(key#x)#xL]
Results [2]: [(count(val#x)#xL + sum(key#x)#xL) AS TOTAL#xL, count(key#x)#xL AS count(key) FILTER (WHERE (val > 1))#xL]

(5) AdaptiveSparkPlan
Output [2]: [TOTAL#xL, count(key) FILTER (WHERE (val > 1))#xL]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT key, sort_array(collect_set(val))[0]
  FROM explain_temp4
  GROUP BY key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (5)
+- ObjectHashAggregate (4)
   +- Exchange (3)
      +- ObjectHashAggregate (2)
         +- Scan parquet default.explain_temp4 (1)


(1) Scan parquet default.explain_temp4
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp4]
ReadSchema: struct<key:int,val:string>

(2) ObjectHashAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_collect_set(val#x, 0, 0)]
Aggregate Attributes [1]: [buf#x]
Results [2]: [key#x, buf#x]

(3) Exchange
Input [2]: [key#x, buf#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [id=#x]

(4) ObjectHashAggregate
Input [2]: [key#x, buf#x]
Keys [1]: [key#x]
Functions [1]: [collect_set(val#x, 0, 0)]
Aggregate Attributes [1]: [collect_set(val#x, 0, 0)#x]
Results [2]: [key#x, sort_array(collect_set(val#x, 0, 0)#x, true)[0] AS sort_array(collect_set(val), true)[0]#x]

(5) AdaptiveSparkPlan
Output [2]: [key#x, sort_array(collect_set(val), true)[0]#x]
Arguments: isFinalPlan=false


-- !query
EXPLAIN FORMATTED
  SELECT key, MIN(val)
  FROM explain_temp4
  GROUP BY key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (7)
+- SortAggregate (6)
   +- Sort (5)
      +- Exchange (4)
         +- SortAggregate (3)
            +- Sort (2)
               +- Scan parquet default.explain_temp4 (1)


(1) Scan parquet default.explain_temp4
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp4]
ReadSchema: struct<key:int,val:string>

(2) Sort
Input [2]: [key#x, val#x]
Arguments: [key#x ASC NULLS FIRST], false, 0

(3) SortAggregate
Input [2]: [key#x, val#x]
Keys [1]: [key#x]
Functions [1]: [partial_min(val#x)]
Aggregate Attributes [1]: [min#x]
Results [2]: [key#x, min#x]

(4) Exchange
Input [2]: [key#x, min#x]
Arguments: hashpartitioning(key#x, 4), ENSURE_REQUIREMENTS, [id=#x]

(5) Sort
Input [2]: [key#x, min#x]
Arguments: [key#x ASC NULLS FIRST], false, 0

(6) SortAggregate
Input [2]: [key#x, min#x]
Keys [1]: [key#x]
Functions [1]: [min(val#x)]
Aggregate Attributes [1]: [min(val#x)#x]
Results [2]: [key#x, min(val#x)#x AS min(val)#x]

(7) AdaptiveSparkPlan
Output [2]: [key#x, min(val)#x]
Arguments: isFinalPlan=false


-- !query
DROP TABLE explain_temp1
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp2
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp3
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp4
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  t(v array<string>) USING PARQUET
-- !query schema
struct<>
-- !query output



-- !query
EXPLAIN SELECT * FROM t  WHERE v IN (array('a'), null)
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
*Filter v#x IN ([a],null)
+- FileScan parquet default.t[v#x] Batched: false, DataFilters: [v#x IN ([a],null)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/t], PartitionFilters: [], PushedFilters: [In(v, [[a],null])], ReadSchema: struct<v:array<string>>


-- !query
DROP TABLE t
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp1 USING PARQUET AS SELECT 1 as key, 2 as val
-- !query schema
struct<>
-- !query output



-- !query
CREATE table  explain_temp2 USING PARQUET AS SELECT 1 as key, 3 as val
-- !query schema
struct<>
-- !query output



-- !query
EXPLAIN FINAL
  SELECT *
  FROM   explain_temp1 a,
         explain_temp2 b
  WHERE  a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   *BroadcastHashJoin [key#x], [key#x], Inner, BuildRight, false
   :- *Filter isnotnull(key#x)
   :  +- *ColumnarToRow
   :     +- FileScan parquet default.explain_temp1[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
   +- BroadcastQueryStage 0
      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]
         +- *Filter isnotnull(key#x)
            +- *ColumnarToRow
               +- FileScan parquet default.explain_temp2[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
+- == Initial Plan ==
   BroadcastHashJoin [key#x], [key#x], Inner, BuildRight, false
   :- Filter isnotnull(key#x)
   :  +- FileScan parquet default.explain_temp1[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]
      +- Filter isnotnull(key#x)
         +- FileScan parquet default.explain_temp2[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>


-- !query
EXPLAIN FINAL EXTENDED
  SELECT *
  FROM   explain_temp1 a,
         explain_temp2 b
  WHERE  a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Parsed Logical Plan ==
'Project [*]
+- 'Filter ('a.key = 'b.key)
   +- 'Join Inner
      :- 'SubqueryAlias a
      :  +- 'UnresolvedRelation [explain_temp1], [], false
      +- 'SubqueryAlias b
         +- 'UnresolvedRelation [explain_temp2], [], false

== Analyzed Logical Plan ==
key: int, val: int, key: int, val: int
Project [key#x, val#x, key#x, val#x]
+- Filter (key#x = key#x)
   +- Join Inner
      :- SubqueryAlias a
      :  +- SubqueryAlias spark_catalog.default.explain_temp1
      :     +- Relation default.explain_temp1[key#x,val#x] parquet
      +- SubqueryAlias b
         +- SubqueryAlias spark_catalog.default.explain_temp2
            +- Relation default.explain_temp2[key#x,val#x] parquet

== Optimized Logical Plan ==
Join Inner, (key#x = key#x)
:- Filter isnotnull(key#x)
:  +- Relation default.explain_temp1[key#x,val#x] parquet
+- Filter isnotnull(key#x)
   +- Relation default.explain_temp2[key#x,val#x] parquet

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   *BroadcastHashJoin [key#x], [key#x], Inner, BuildRight, false
   :- *Filter isnotnull(key#x)
   :  +- *ColumnarToRow
   :     +- FileScan parquet default.explain_temp1[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
   +- BroadcastQueryStage 0
      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]
         +- *Filter isnotnull(key#x)
            +- *ColumnarToRow
               +- FileScan parquet default.explain_temp2[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
+- == Initial Plan ==
   BroadcastHashJoin [key#x], [key#x], Inner, BuildRight, false
   :- Filter isnotnull(key#x)
   :  +- FileScan parquet default.explain_temp1[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]
      +- Filter isnotnull(key#x)
         +- FileScan parquet default.explain_temp2[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>


-- !query
EXPLAIN FINAL FORMATTED
  SELECT *
  FROM   explain_temp1 a,
         explain_temp2 b
  WHERE  a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
AdaptiveSparkPlan (14)
+- == Final Plan ==
   * BroadcastHashJoin Inner BuildRight (9)
   :- * Filter (3)
   :  +- * ColumnarToRow (2)
   :     +- Scan parquet default.explain_temp1 (1)
   +- BroadcastQueryStage (8)
      +- BroadcastExchange (7)
         +- * Filter (6)
            +- * ColumnarToRow (5)
               +- Scan parquet default.explain_temp2 (4)
+- == Initial Plan ==
   BroadcastHashJoin Inner BuildRight (13)
   :- Filter (10)
   :  +- Scan parquet default.explain_temp1 (1)
   +- BroadcastExchange (12)
      +- Filter (11)
         +- Scan parquet default.explain_temp2 (4)


(1) Scan parquet default.explain_temp1
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp1]
PushedFilters: [IsNotNull(key)]
ReadSchema: struct<key:int,val:int>

(2) ColumnarToRow [codegen id : 2]
Input [2]: [key#x, val#x]

(3) Filter [codegen id : 2]
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(4) Scan parquet default.explain_temp2
Output [2]: [key#x, val#x]
Batched: true
Location [not included in comparison]/{warehouse_dir}/explain_temp2]
PushedFilters: [IsNotNull(key)]
ReadSchema: struct<key:int,val:int>

(5) ColumnarToRow [codegen id : 1]
Input [2]: [key#x, val#x]

(6) Filter [codegen id : 1]
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(7) BroadcastExchange
Input [2]: [key#x, val#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]

(8) BroadcastQueryStage
Output [2]: [key#x, val#x]
Arguments: 0

(9) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join condition: None

(10) Filter
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(11) Filter
Input [2]: [key#x, val#x]
Condition : isnotnull(key#x)

(12) BroadcastExchange
Input [2]: [key#x, val#x]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]

(13) BroadcastHashJoin
Left keys [1]: [key#x]
Right keys [1]: [key#x]
Join condition: None

(14) AdaptiveSparkPlan
Output [4]: [key#x, val#x, key#x, val#x]
Arguments: isFinalPlan=true


-- !query
EXPLAIN FINAL CODEGEN
  SELECT *
  FROM   explain_temp1 a,
         explain_temp2 b
  WHERE  a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
Found 2 WholeStageCodegen subtrees.
== Subtree 1 / 2 (maxMethodCodeSize:277; maxConstantPoolSize:127(0.19% used); numInnerClasses:0) ==
*Filter isnotnull(key#x)
+- *ColumnarToRow
   +- FileScan parquet default.explain_temp2[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private int columnartorow_batchIdx_0;
/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[2];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];
/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];
/* 023 */
/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 025 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 026 */
/* 027 */   }
/* 028 */
/* 029 */   private void columnartorow_nextBatch_0() throws java.io.IOException {
/* 030 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {
/* 031 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();
/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);
/* 033 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());
/* 034 */       columnartorow_batchIdx_0 = 0;
/* 035 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);
/* 036 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);
/* 037 */
/* 038 */     }
/* 039 */   }
/* 040 */
/* 041 */   protected void processNext() throws java.io.IOException {
/* 042 */     if (columnartorow_mutableStateArray_1[0] == null) {
/* 043 */       columnartorow_nextBatch_0();
/* 044 */     }
/* 045 */     while ( columnartorow_mutableStateArray_1[0] != null) {
/* 046 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();
/* 047 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;
/* 048 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {
/* 049 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;
/* 050 */         do {
/* 051 */           boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);
/* 052 */           int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));
/* 053 */
/* 054 */           boolean filter_value_2 = !columnartorow_isNull_0;
/* 055 */           if (!filter_value_2) continue;
/* 056 */
/* 057 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);
/* 058 */
/* 059 */           boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);
/* 060 */           int columnartorow_value_1 = columnartorow_isNull_1 ? -1 : (columnartorow_mutableStateArray_2[1].getInt(columnartorow_rowIdx_0));
/* 061 */           columnartorow_mutableStateArray_3[1].reset();
/* 062 */
/* 063 */           columnartorow_mutableStateArray_3[1].zeroOutNullBytes();
/* 064 */
/* 065 */           columnartorow_mutableStateArray_3[1].write(0, columnartorow_value_0);
/* 066 */
/* 067 */           if (columnartorow_isNull_1) {
/* 068 */             columnartorow_mutableStateArray_3[1].setNullAt(1);
/* 069 */           } else {
/* 070 */             columnartorow_mutableStateArray_3[1].write(1, columnartorow_value_1);
/* 071 */           }
/* 072 */           append((columnartorow_mutableStateArray_3[1].getRow()));
/* 073 */
/* 074 */         } while(false);
/* 075 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }
/* 076 */       }
/* 077 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;
/* 078 */       columnartorow_mutableStateArray_1[0] = null;
/* 079 */       columnartorow_nextBatch_0();
/* 080 */     }
/* 081 */   }
/* 082 */
/* 083 */ }

== Subtree 2 / 2 (maxMethodCodeSize:407; maxConstantPoolSize:161(0.25% used); numInnerClasses:0) ==
*BroadcastHashJoin [key#x], [key#x], Inner, BuildRight, false
:- *Filter isnotnull(key#x)
:  +- *ColumnarToRow
:     +- FileScan parquet default.explain_temp1[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
+- BroadcastQueryStage 0
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]
      +- *Filter isnotnull(key#x)
         +- *ColumnarToRow
            +- FileScan parquet default.explain_temp2[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private int columnartorow_batchIdx_0;
/* 010 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 011 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[2];
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[3];
/* 013 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];
/* 014 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */     columnartorow_mutableStateArray_0[0] = inputs[0];
/* 024 */
/* 025 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 026 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 027 */
/* 028 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[3] /* broadcast */).value()).asReadOnlyCopy();
/* 029 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 030 */
/* 031 */     columnartorow_mutableStateArray_3[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);
/* 032 */
/* 033 */   }
/* 034 */
/* 035 */   private void columnartorow_nextBatch_0() throws java.io.IOException {
/* 036 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {
/* 037 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();
/* 038 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);
/* 039 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());
/* 040 */       columnartorow_batchIdx_0 = 0;
/* 041 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);
/* 042 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);
/* 043 */
/* 044 */     }
/* 045 */   }
/* 046 */
/* 047 */   protected void processNext() throws java.io.IOException {
/* 048 */     if (columnartorow_mutableStateArray_1[0] == null) {
/* 049 */       columnartorow_nextBatch_0();
/* 050 */     }
/* 051 */     while ( columnartorow_mutableStateArray_1[0] != null) {
/* 052 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();
/* 053 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;
/* 054 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {
/* 055 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;
/* 056 */         do {
/* 057 */           boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);
/* 058 */           int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));
/* 059 */
/* 060 */           boolean filter_value_2 = !columnartorow_isNull_0;
/* 061 */           if (!filter_value_2) continue;
/* 062 */
/* 063 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);
/* 064 */
/* 065 */           // generate join key for stream side
/* 066 */           boolean bhj_isNull_0 = false;
/* 067 */           long bhj_value_0 = -1L;
/* 068 */           if (!false) {
/* 069 */             bhj_value_0 = (long) columnartorow_value_0;
/* 070 */           }
/* 071 */           // find matches from HashedRelation
/* 072 */           UnsafeRow bhj_buildRow_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);
/* 073 */           if (bhj_buildRow_0 != null) {
/* 074 */             {
/* 075 */               ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 076 */
/* 077 */               boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);
/* 078 */               int columnartorow_value_1 = columnartorow_isNull_1 ? -1 : (columnartorow_mutableStateArray_2[1].getInt(columnartorow_rowIdx_0));
/* 079 */               int bhj_value_2 = bhj_buildRow_0.getInt(0);
/* 080 */               boolean bhj_isNull_3 = bhj_buildRow_0.isNullAt(1);
/* 081 */               int bhj_value_3 = bhj_isNull_3 ?
/* 082 */               -1 : (bhj_buildRow_0.getInt(1));
/* 083 */               columnartorow_mutableStateArray_3[2].reset();
/* 084 */
/* 085 */               columnartorow_mutableStateArray_3[2].zeroOutNullBytes();
/* 086 */
/* 087 */               columnartorow_mutableStateArray_3[2].write(0, columnartorow_value_0);
/* 088 */
/* 089 */               if (columnartorow_isNull_1) {
/* 090 */                 columnartorow_mutableStateArray_3[2].setNullAt(1);
/* 091 */               } else {
/* 092 */                 columnartorow_mutableStateArray_3[2].write(1, columnartorow_value_1);
/* 093 */               }
/* 094 */
/* 095 */               columnartorow_mutableStateArray_3[2].write(2, bhj_value_2);
/* 096 */
/* 097 */               if (bhj_isNull_3) {
/* 098 */                 columnartorow_mutableStateArray_3[2].setNullAt(3);
/* 099 */               } else {
/* 100 */                 columnartorow_mutableStateArray_3[2].write(3, bhj_value_3);
/* 101 */               }
/* 102 */               append((columnartorow_mutableStateArray_3[2].getRow()));
/* 103 */
/* 104 */             }
/* 105 */           }
/* 106 */
/* 107 */         } while(false);
/* 108 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }
/* 109 */       }
/* 110 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;
/* 111 */       columnartorow_mutableStateArray_1[0] = null;
/* 112 */       columnartorow_nextBatch_0();
/* 113 */     }
/* 114 */   }
/* 115 */
/* 116 */ }


-- !query
EXPLAIN FINAL COST
  SELECT *
  FROM   explain_temp1 a,
         explain_temp2 b
  WHERE  a.key = b.key
-- !query schema
struct<plan:string>
-- !query output
== Optimized Logical Plan ==
Join Inner, (key#x = key#x), Statistics(sizeInBytes=428.6 KiB)
:- Filter isnotnull(key#x), Statistics(sizeInBytes=663.0 B)
:  +- Relation default.explain_temp1[key#x,val#x] parquet, Statistics(sizeInBytes=663.0 B)
+- Filter isnotnull(key#x), Statistics(sizeInBytes=662.0 B)
   +- Relation default.explain_temp2[key#x,val#x] parquet, Statistics(sizeInBytes=662.0 B)

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   *BroadcastHashJoin [key#x], [key#x], Inner, BuildRight, false
   :- *Filter isnotnull(key#x)
   :  +- *ColumnarToRow
   :     +- FileScan parquet default.explain_temp1[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
   +- BroadcastQueryStage 0
      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]
         +- *Filter isnotnull(key#x)
            +- *ColumnarToRow
               +- FileScan parquet default.explain_temp2[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
+- == Initial Plan ==
   BroadcastHashJoin [key#x], [key#x], Inner, BuildRight, false
   :- Filter isnotnull(key#x)
   :  +- FileScan parquet default.explain_temp1[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#x]
      +- Filter isnotnull(key#x)
         +- FileScan parquet default.explain_temp2[key#x,val#x] Batched: true, DataFilters: [isnotnull(key#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>


-- !query
EXPLAIN FINAL FORMATTED
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException

mismatched input '<EOF>' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 23)

== SQL ==
EXPLAIN FINAL FORMATTED
-----------------------^^^


-- !query
DROP TABLE explain_temp1
-- !query schema
struct<>
-- !query output



-- !query
DROP TABLE explain_temp2
-- !query schema
struct<>
-- !query output

