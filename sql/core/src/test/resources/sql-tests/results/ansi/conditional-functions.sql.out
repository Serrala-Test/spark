-- Automatically generated by SQLQueryTestSuite
-- Number of queries: 25


-- !query
CREATE TABLE t USING PARQUET AS SELECT c1, c2 FROM VALUES(1d, 0),(2d, 1),(null, 1),(CAST('NaN' AS DOUBLE), 0) AS t(c1, c2)
-- !query schema
struct<>
-- !query output



-- !query
SELECT nanvl(c1, c1/c2 + c1/c2) FROM t
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 17) ==
SELECT nanvl(c1, c1/c2 + c1/c2) FROM t
                 ^^^^^


-- !query
EXPLAIN SELECT nanvl(c1, c1/c2 + c1/c2) FROM t
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
*Project [nanvl(c1#x, ((c1#x / cast(c2#x as double)) + (c1#x / cast(c2#x as double)))) AS nanvl(c1, ((c1 / c2) + (c1 / c2)))#x]
+- *ColumnarToRow
   +- FileScan parquet default.t[c1#x,c2#x] Batched: true, DataFilters: [], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c1:double,c2:int>


-- !query
SELECT nanvl(c2, c1/c2 + c1/c2) FROM t
-- !query schema
struct<nanvl(c2, ((c1 / c2) + (c1 / c2))):double>
-- !query output
0.0
0.0
1.0
1.0


-- !query
SELECT nanvl(c1, 1/0) FROM t
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 17) ==
SELECT nanvl(c1, 1/0) FROM t
                 ^^^


-- !query
SELECT nanvl(1-0, 1/0) FROM t
-- !query schema
struct<nanvl((1 - 0), (1 / 0)):double>
-- !query output
1.0
1.0
1.0
1.0


-- !query
SELECT nanvl(1/0, 1/0) FROM t
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 13) ==
SELECT nanvl(1/0, 1/0) FROM t
             ^^^


-- !query
SELECT if(c2 > 2, 1-0, 1/0) from t
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 23) ==
SELECT if(c2 > 2, 1-0, 1/0) from t
                       ^^^


-- !query
EXPLAIN SELECT if(c2 > 2, 1-0, 1/0) from t
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
*Project [if ((c2#x > 2)) 1.0 else (cast(1 as double) / cast(0 as double)) AS (IF((c2 > 2), (1 - 0), (1 / 0)))#x]
+- *ColumnarToRow
   +- FileScan parquet default.t[c2#x] Batched: true, DataFilters: [], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c2:int>


-- !query
SELECT if(1 == 1, 1, 1/0)
-- !query schema
struct<(IF((1 = 1), 1, (1 / 0))):double>
-- !query output
1.0


-- !query
SELECT if(1 == 1, 1/0, 1)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 18) ==
SELECT if(1 == 1, 1/0, 1)
                  ^^^


-- !query
SELECT if(1 == 1/0, 1, 1/0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 15) ==
SELECT if(1 == 1/0, 1, 1/0)
               ^^^


-- !query
SELECT coalesce(c1, 1/0) from t
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 20) ==
SELECT coalesce(c1, 1/0) from t
                    ^^^


-- !query
EXPLAIN SELECT coalesce(c1, 1/0) from t
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
*Project [coalesce(c1#x, (cast(1 as double) / cast(0 as double))) AS coalesce(c1, (1 / 0))#x]
+- *ColumnarToRow
   +- FileScan parquet default.t[c1#x] Batched: true, DataFilters: [], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c1:double>


-- !query
SELECT coalesce(1, 1/0)
-- !query schema
struct<coalesce(1, (1 / 0)):double>
-- !query output
1.0


-- !query
SELECT coalesce(null, 1/0)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 22) ==
SELECT coalesce(null, 1/0)
                      ^^^


-- !query
SELECT coalesce(null, 1, 1/0)
-- !query schema
struct<coalesce(NULL, 1, (1 / 0)):double>
-- !query output
1.0


-- !query
SELECT coalesce(null, 1/0, 1)
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 22) ==
SELECT coalesce(null, 1/0, 1)
                      ^^^


-- !query
SELECT case when c2 > 2 then 1 else 1/0 end from t
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 36) ==
...CT case when c2 > 2 then 1 else 1/0 end from t
                                   ^^^


-- !query
EXPLAIN SELECT case when c2 > 2 then 1 else 1/0 end from t
-- !query schema
struct<plan:string>
-- !query output
== Physical Plan ==
*Project [CASE WHEN (c2#x > 2) THEN 1.0 ELSE (cast(1 as double) / cast(0 as double)) END AS CASE WHEN (c2 > 2) THEN 1 ELSE (1 / 0) END#x]
+- *ColumnarToRow
   +- FileScan parquet default.t[c2#x] Batched: true, DataFilters: [], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c2:int>


-- !query
SELECT case when 1 < 2 then 1 else 1/0 end
-- !query schema
struct<CASE WHEN (1 < 2) THEN 1 ELSE (1 / 0) END:double>
-- !query output
1.0


-- !query
SELECT case when 1 < 2/0 then 1 else 1/0 end
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 21) ==
SELECT case when 1 < 2/0 then 1 else 1/0 end
                     ^^^


-- !query
SELECT case when 1 > 2 then 1 else 1/0 end
-- !query schema
struct<>
-- !query output
org.apache.spark.SparkArithmeticException
[DIVIDE_BY_ZERO] Division by zero. To return NULL instead, use `try_divide`. If necessary set "spark.sql.ansi.enabled" to false (except for ANSI interval type) to bypass this error.
== SQL(line 1, position 35) ==
...ECT case when 1 > 2 then 1 else 1/0 end
                                   ^^^


-- !query
SELECT case when 1 > 2 then 1/0 else 1 end
-- !query schema
struct<CASE WHEN (1 > 2) THEN (1 / 0) ELSE 1 END:double>
-- !query output
1.0


-- !query
DROP TABLE IF EXISTS t
-- !query schema
struct<>
-- !query output

