-- Automatically generated by SQLQueryTestSuite
-- Number of queries: 11


-- !query 0
SET spark.sql.caseSensitive=true
-- !query 0 schema
struct<key:string,value:string>
-- !query 0 output
spark.sql.caseSensitive	true


-- !query 1
CREATE TABLE t(c0 STRING, c1 INT, c1 DOUBLE, c0 INT) USING parquet
-- !query 1 schema
struct<>
-- !query 1 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the table definition of `t`: `c1`, `c0`;


-- !query 2
CREATE TABLE t(c0 INT) USING parquet PARTITIONED BY (c0, c0)
-- !query 2 schema
struct<>
-- !query 2 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the partition column(s): `c0`;


-- !query 3
CREATE TABLE t(c0 INT) USING parquet CLUSTERED BY (c0, c0) INTO 2 BUCKETS
-- !query 3 schema
struct<>
-- !query 3 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the bucket column(s): `c0`;


-- !query 4
CREATE TABLE t(c0 INT, c1 INT) USING parquet CLUSTERED BY (c0) SORTED BY (c1, c1) INTO 2 BUCKETS
-- !query 4 schema
struct<>
-- !query 4 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the sort column(s): `c1`;


-- !query 5
SET spark.sql.caseSensitive=false
-- !query 5 schema
struct<key:string,value:string>
-- !query 5 output
spark.sql.caseSensitive	false


-- !query 6
CREATE TABLE t(c0 STRING, c1 INT, c1 DOUBLE, c0 INT) USING parquet
-- !query 6 schema
struct<>
-- !query 6 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the table definition of `t`: `c1`, `c0`;


-- !query 7
CREATE TABLE t(ab STRING, cd INT, ef DOUBLE, Ab INT) USING parquet
-- !query 7 schema
struct<>
-- !query 7 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the table definition of `t`: `ab`;


-- !query 8
CREATE TABLE t(ab INT, cd INT) USING parquet PARTITIONED BY (Ab, aB)
-- !query 8 schema
struct<>
-- !query 8 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the partition column(s): `ab`;


-- !query 9
CREATE TABLE t(ab INT) USING parquet CLUSTERED BY (Ab, aB) INTO 2 BUCKETS
-- !query 9 schema
struct<>
-- !query 9 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the bucket column(s): `ab`;


-- !query 10
CREATE TABLE t(ab INT, cd INT) USING parquet CLUSTERED BY (ab) SORTED BY (cD, Cd) INTO 2 BUCKETS
-- !query 10 schema
struct<>
-- !query 10 output
org.apache.spark.sql.AnalysisException
Found duplicate column(s) in the sort column(s): `cd`;
