== Physical Plan ==
* HashAggregate (47)
+- Exchange (46)
   +- * HashAggregate (45)
      +- * HashAggregate (44)
         +- * HashAggregate (43)
            +- * Project (42)
               +- * BroadcastHashJoin Inner BuildRight (41)
                  :- * Project (35)
                  :  +- * BroadcastHashJoin Inner BuildRight (34)
                  :     :- * Project (28)
                  :     :  +- * BroadcastHashJoin Inner BuildRight (27)
                  :     :     :- * SortMergeJoin LeftAnti (21)
                  :     :     :  :- * Project (14)
                  :     :     :  :  +- * SortMergeJoin LeftSemi (13)
                  :     :     :  :     :- * Sort (6)
                  :     :     :  :     :  +- Exchange (5)
                  :     :     :  :     :     +- * Project (4)
                  :     :     :  :     :        +- * Filter (3)
                  :     :     :  :     :           +- * ColumnarToRow (2)
                  :     :     :  :     :              +- Scan parquet spark_catalog.default.catalog_sales (1)
                  :     :     :  :     +- * Sort (12)
                  :     :     :  :        +- Exchange (11)
                  :     :     :  :           +- * Project (10)
                  :     :     :  :              +- * Filter (9)
                  :     :     :  :                 +- * ColumnarToRow (8)
                  :     :     :  :                    +- Scan parquet spark_catalog.default.catalog_sales (7)
                  :     :     :  +- * Sort (20)
                  :     :     :     +- Exchange (19)
                  :     :     :        +- * Project (18)
                  :     :     :           +- * Filter (17)
                  :     :     :              +- * ColumnarToRow (16)
                  :     :     :                 +- Scan parquet spark_catalog.default.catalog_returns (15)
                  :     :     +- BroadcastExchange (26)
                  :     :        +- * Project (25)
                  :     :           +- * Filter (24)
                  :     :              +- * ColumnarToRow (23)
                  :     :                 +- Scan parquet spark_catalog.default.customer_address (22)
                  :     +- BroadcastExchange (33)
                  :        +- * Project (32)
                  :           +- * Filter (31)
                  :              +- * ColumnarToRow (30)
                  :                 +- Scan parquet spark_catalog.default.call_center (29)
                  +- BroadcastExchange (40)
                     +- * Project (39)
                        +- * Filter (38)
                           +- * ColumnarToRow (37)
                              +- Scan parquet spark_catalog.default.date_dim (36)


(1) Scan parquet spark_catalog.default.catalog_sales
Output [8]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_warehouse_sk#4, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7, cs_sold_date_sk#8]
Batched: true
Location [not included in comparison]/{warehouse_dir}/catalog_sales]
PushedFilters: [IsNotNull(cs_order_number), IsNotNull(cs_warehouse_sk), IsNotNull(cs_ship_date_sk), IsNotNull(cs_ship_addr_sk), IsNotNull(cs_call_center_sk)]
ReadSchema: struct<cs_ship_date_sk:int,cs_ship_addr_sk:int,cs_call_center_sk:int,cs_warehouse_sk:int,cs_order_number:int,cs_ext_ship_cost:decimal(7,2),cs_net_profit:decimal(7,2)>

(2) ColumnarToRow [codegen id : 1]
Input [8]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_warehouse_sk#4, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7, cs_sold_date_sk#8]

(3) Filter [codegen id : 1]
Input [8]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_warehouse_sk#4, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7, cs_sold_date_sk#8]
Condition : (((((((isnotnull(cs_order_number#5) AND isnotnull(cs_warehouse_sk#4)) AND isnotnull(cs_ship_date_sk#1)) AND isnotnull(cs_ship_addr_sk#2)) AND isnotnull(cs_call_center_sk#3)) AND might_contain(Subquery scalar-subquery#9, [id=#10], xxhash64(cs_ship_addr_sk#2, 42))) AND might_contain(Subquery scalar-subquery#11, [id=#12], xxhash64(cs_call_center_sk#3, 42))) AND might_contain(Subquery scalar-subquery#13, [id=#14], xxhash64(cs_ship_date_sk#1, 42)))

(4) Project [codegen id : 1]
Output [7]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_warehouse_sk#4, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]
Input [8]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_warehouse_sk#4, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7, cs_sold_date_sk#8]

(5) Exchange
Input [7]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_warehouse_sk#4, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]
Arguments: hashpartitioning(cs_order_number#5, 5), ENSURE_REQUIREMENTS, [plan_id=1]

(6) Sort [codegen id : 2]
Input [7]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_warehouse_sk#4, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]
Arguments: [cs_order_number#5 ASC NULLS FIRST], false, 0

(7) Scan parquet spark_catalog.default.catalog_sales
Output [3]: [cs_warehouse_sk#15, cs_order_number#16, cs_sold_date_sk#17]
Batched: true
Location [not included in comparison]/{warehouse_dir}/catalog_sales]
PushedFilters: [IsNotNull(cs_order_number), IsNotNull(cs_warehouse_sk)]
ReadSchema: struct<cs_warehouse_sk:int,cs_order_number:int>

(8) ColumnarToRow [codegen id : 3]
Input [3]: [cs_warehouse_sk#15, cs_order_number#16, cs_sold_date_sk#17]

(9) Filter [codegen id : 3]
Input [3]: [cs_warehouse_sk#15, cs_order_number#16, cs_sold_date_sk#17]
Condition : (isnotnull(cs_order_number#16) AND isnotnull(cs_warehouse_sk#15))

(10) Project [codegen id : 3]
Output [2]: [cs_warehouse_sk#15, cs_order_number#16]
Input [3]: [cs_warehouse_sk#15, cs_order_number#16, cs_sold_date_sk#17]

(11) Exchange
Input [2]: [cs_warehouse_sk#15, cs_order_number#16]
Arguments: hashpartitioning(cs_order_number#16, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(12) Sort [codegen id : 4]
Input [2]: [cs_warehouse_sk#15, cs_order_number#16]
Arguments: [cs_order_number#16 ASC NULLS FIRST], false, 0

(13) SortMergeJoin [codegen id : 5]
Left keys [1]: [cs_order_number#5]
Right keys [1]: [cs_order_number#16]
Join type: LeftSemi
Join condition: NOT (cs_warehouse_sk#4 = cs_warehouse_sk#15)

(14) Project [codegen id : 5]
Output [6]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]
Input [7]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_warehouse_sk#4, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]

(15) Scan parquet spark_catalog.default.catalog_returns
Output [2]: [cr_order_number#18, cr_returned_date_sk#19]
Batched: true
Location [not included in comparison]/{warehouse_dir}/catalog_returns]
PushedFilters: [IsNotNull(cr_order_number)]
ReadSchema: struct<cr_order_number:int>

(16) ColumnarToRow [codegen id : 6]
Input [2]: [cr_order_number#18, cr_returned_date_sk#19]

(17) Filter [codegen id : 6]
Input [2]: [cr_order_number#18, cr_returned_date_sk#19]
Condition : isnotnull(cr_order_number#18)

(18) Project [codegen id : 6]
Output [1]: [cr_order_number#18]
Input [2]: [cr_order_number#18, cr_returned_date_sk#19]

(19) Exchange
Input [1]: [cr_order_number#18]
Arguments: hashpartitioning(cr_order_number#18, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(20) Sort [codegen id : 7]
Input [1]: [cr_order_number#18]
Arguments: [cr_order_number#18 ASC NULLS FIRST], false, 0

(21) SortMergeJoin [codegen id : 11]
Left keys [1]: [cs_order_number#5]
Right keys [1]: [cr_order_number#18]
Join type: LeftAnti
Join condition: None

(22) Scan parquet spark_catalog.default.customer_address
Output [2]: [ca_address_sk#20, ca_state#21]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer_address]
PushedFilters: [IsNotNull(ca_state), EqualTo(ca_state,GA), IsNotNull(ca_address_sk)]
ReadSchema: struct<ca_address_sk:int,ca_state:string>

(23) ColumnarToRow [codegen id : 8]
Input [2]: [ca_address_sk#20, ca_state#21]

(24) Filter [codegen id : 8]
Input [2]: [ca_address_sk#20, ca_state#21]
Condition : ((isnotnull(ca_state#21) AND (ca_state#21 = GA)) AND isnotnull(ca_address_sk#20))

(25) Project [codegen id : 8]
Output [1]: [ca_address_sk#20]
Input [2]: [ca_address_sk#20, ca_state#21]

(26) BroadcastExchange
Input [1]: [ca_address_sk#20]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]

(27) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_ship_addr_sk#2]
Right keys [1]: [ca_address_sk#20]
Join type: Inner
Join condition: None

(28) Project [codegen id : 11]
Output [5]: [cs_ship_date_sk#1, cs_call_center_sk#3, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]
Input [7]: [cs_ship_date_sk#1, cs_ship_addr_sk#2, cs_call_center_sk#3, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7, ca_address_sk#20]

(29) Scan parquet spark_catalog.default.call_center
Output [2]: [cc_call_center_sk#22, cc_county#23]
Batched: true
Location [not included in comparison]/{warehouse_dir}/call_center]
PushedFilters: [IsNotNull(cc_county), EqualTo(cc_county,Williamson County), IsNotNull(cc_call_center_sk)]
ReadSchema: struct<cc_call_center_sk:int,cc_county:string>

(30) ColumnarToRow [codegen id : 9]
Input [2]: [cc_call_center_sk#22, cc_county#23]

(31) Filter [codegen id : 9]
Input [2]: [cc_call_center_sk#22, cc_county#23]
Condition : ((isnotnull(cc_county#23) AND (cc_county#23 = Williamson County)) AND isnotnull(cc_call_center_sk#22))

(32) Project [codegen id : 9]
Output [1]: [cc_call_center_sk#22]
Input [2]: [cc_call_center_sk#22, cc_county#23]

(33) BroadcastExchange
Input [1]: [cc_call_center_sk#22]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=5]

(34) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_call_center_sk#3]
Right keys [1]: [cc_call_center_sk#22]
Join type: Inner
Join condition: None

(35) Project [codegen id : 11]
Output [4]: [cs_ship_date_sk#1, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]
Input [6]: [cs_ship_date_sk#1, cs_call_center_sk#3, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7, cc_call_center_sk#22]

(36) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#24, d_date#25]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date), GreaterThanOrEqual(d_date,2002-02-01), LessThanOrEqual(d_date,2002-04-02), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_date:date>

(37) ColumnarToRow [codegen id : 10]
Input [2]: [d_date_sk#24, d_date#25]

(38) Filter [codegen id : 10]
Input [2]: [d_date_sk#24, d_date#25]
Condition : (((isnotnull(d_date#25) AND (d_date#25 >= 2002-02-01)) AND (d_date#25 <= 2002-04-02)) AND isnotnull(d_date_sk#24))

(39) Project [codegen id : 10]
Output [1]: [d_date_sk#24]
Input [2]: [d_date_sk#24, d_date#25]

(40) BroadcastExchange
Input [1]: [d_date_sk#24]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=6]

(41) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_ship_date_sk#1]
Right keys [1]: [d_date_sk#24]
Join type: Inner
Join condition: None

(42) Project [codegen id : 11]
Output [3]: [cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]
Input [5]: [cs_ship_date_sk#1, cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7, d_date_sk#24]

(43) HashAggregate [codegen id : 11]
Input [3]: [cs_order_number#5, cs_ext_ship_cost#6, cs_net_profit#7]
Keys [1]: [cs_order_number#5]
Functions [2]: [partial_sum(UnscaledValue(cs_ext_ship_cost#6)), partial_sum(UnscaledValue(cs_net_profit#7))]
Aggregate Attributes [2]: [sum(UnscaledValue(cs_ext_ship_cost#6))#26, sum(UnscaledValue(cs_net_profit#7))#27]
Results [3]: [cs_order_number#5, sum#28, sum#29]

(44) HashAggregate [codegen id : 11]
Input [3]: [cs_order_number#5, sum#28, sum#29]
Keys [1]: [cs_order_number#5]
Functions [2]: [merge_sum(UnscaledValue(cs_ext_ship_cost#6)), merge_sum(UnscaledValue(cs_net_profit#7))]
Aggregate Attributes [2]: [sum(UnscaledValue(cs_ext_ship_cost#6))#26, sum(UnscaledValue(cs_net_profit#7))#27]
Results [3]: [cs_order_number#5, sum#28, sum#29]

(45) HashAggregate [codegen id : 11]
Input [3]: [cs_order_number#5, sum#28, sum#29]
Keys: []
Functions [3]: [merge_sum(UnscaledValue(cs_ext_ship_cost#6)), merge_sum(UnscaledValue(cs_net_profit#7)), partial_count(distinct cs_order_number#5)]
Aggregate Attributes [3]: [sum(UnscaledValue(cs_ext_ship_cost#6))#26, sum(UnscaledValue(cs_net_profit#7))#27, count(cs_order_number#5)#30]
Results [3]: [sum#28, sum#29, count#31]

(46) Exchange
Input [3]: [sum#28, sum#29, count#31]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=7]

(47) HashAggregate [codegen id : 12]
Input [3]: [sum#28, sum#29, count#31]
Keys: []
Functions [3]: [sum(UnscaledValue(cs_ext_ship_cost#6)), sum(UnscaledValue(cs_net_profit#7)), count(distinct cs_order_number#5)]
Aggregate Attributes [3]: [sum(UnscaledValue(cs_ext_ship_cost#6))#26, sum(UnscaledValue(cs_net_profit#7))#27, count(cs_order_number#5)#30]
Results [3]: [count(cs_order_number#5)#30 AS order count #32, MakeDecimal(sum(UnscaledValue(cs_ext_ship_cost#6))#26,17,2) AS total shipping cost #33, MakeDecimal(sum(UnscaledValue(cs_net_profit#7))#27,17,2) AS total net profit #34]

===== Subqueries =====

Subquery:1 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#9, [id=#10]
ObjectHashAggregate (54)
+- Exchange (53)
   +- ObjectHashAggregate (52)
      +- * Project (51)
         +- * Filter (50)
            +- * ColumnarToRow (49)
               +- Scan parquet spark_catalog.default.customer_address (48)


(48) Scan parquet spark_catalog.default.customer_address
Output [2]: [ca_address_sk#20, ca_state#21]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer_address]
PushedFilters: [IsNotNull(ca_state), EqualTo(ca_state,GA), IsNotNull(ca_address_sk)]
ReadSchema: struct<ca_address_sk:int,ca_state:string>

(49) ColumnarToRow [codegen id : 1]
Input [2]: [ca_address_sk#20, ca_state#21]

(50) Filter [codegen id : 1]
Input [2]: [ca_address_sk#20, ca_state#21]
Condition : ((isnotnull(ca_state#21) AND (ca_state#21 = GA)) AND isnotnull(ca_address_sk#20))

(51) Project [codegen id : 1]
Output [1]: [ca_address_sk#20]
Input [2]: [ca_address_sk#20, ca_state#21]

(52) ObjectHashAggregate
Input [1]: [ca_address_sk#20]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(ca_address_sk#20, 42), 17961, 333176, 0, 0)]
Aggregate Attributes [1]: [buf#35]
Results [1]: [buf#36]

(53) Exchange
Input [1]: [buf#36]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=8]

(54) ObjectHashAggregate
Input [1]: [buf#36]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(ca_address_sk#20, 42), 17961, 333176, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(ca_address_sk#20, 42), 17961, 333176, 0, 0)#37]
Results [1]: [bloom_filter_agg(xxhash64(ca_address_sk#20, 42), 17961, 333176, 0, 0)#37 AS bloomFilter#38]

Subquery:2 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#11, [id=#12]
ObjectHashAggregate (61)
+- Exchange (60)
   +- ObjectHashAggregate (59)
      +- * Project (58)
         +- * Filter (57)
            +- * ColumnarToRow (56)
               +- Scan parquet spark_catalog.default.call_center (55)


(55) Scan parquet spark_catalog.default.call_center
Output [2]: [cc_call_center_sk#22, cc_county#23]
Batched: true
Location [not included in comparison]/{warehouse_dir}/call_center]
PushedFilters: [IsNotNull(cc_county), EqualTo(cc_county,Williamson County), IsNotNull(cc_call_center_sk)]
ReadSchema: struct<cc_call_center_sk:int,cc_county:string>

(56) ColumnarToRow [codegen id : 1]
Input [2]: [cc_call_center_sk#22, cc_county#23]

(57) Filter [codegen id : 1]
Input [2]: [cc_call_center_sk#22, cc_county#23]
Condition : ((isnotnull(cc_county#23) AND (cc_county#23 = Williamson County)) AND isnotnull(cc_call_center_sk#22))

(58) Project [codegen id : 1]
Output [1]: [cc_call_center_sk#22]
Input [2]: [cc_call_center_sk#22, cc_county#23]

(59) ObjectHashAggregate
Input [1]: [cc_call_center_sk#22]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(cc_call_center_sk#22, 42), 4, 144, 0, 0)]
Aggregate Attributes [1]: [buf#39]
Results [1]: [buf#40]

(60) Exchange
Input [1]: [buf#40]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=9]

(61) ObjectHashAggregate
Input [1]: [buf#40]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(cc_call_center_sk#22, 42), 4, 144, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(cc_call_center_sk#22, 42), 4, 144, 0, 0)#41]
Results [1]: [bloom_filter_agg(xxhash64(cc_call_center_sk#22, 42), 4, 144, 0, 0)#41 AS bloomFilter#42]

Subquery:3 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#13, [id=#14]
ObjectHashAggregate (68)
+- Exchange (67)
   +- ObjectHashAggregate (66)
      +- * Project (65)
         +- * Filter (64)
            +- * ColumnarToRow (63)
               +- Scan parquet spark_catalog.default.date_dim (62)


(62) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#24, d_date#25]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date), GreaterThanOrEqual(d_date,2002-02-01), LessThanOrEqual(d_date,2002-04-02), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_date:date>

(63) ColumnarToRow [codegen id : 1]
Input [2]: [d_date_sk#24, d_date#25]

(64) Filter [codegen id : 1]
Input [2]: [d_date_sk#24, d_date#25]
Condition : (((isnotnull(d_date#25) AND (d_date#25 >= 2002-02-01)) AND (d_date#25 <= 2002-04-02)) AND isnotnull(d_date_sk#24))

(65) Project [codegen id : 1]
Output [1]: [d_date_sk#24]
Input [2]: [d_date_sk#24, d_date#25]

(66) ObjectHashAggregate
Input [1]: [d_date_sk#24]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(d_date_sk#24, 42), 73049, 1141755, 0, 0)]
Aggregate Attributes [1]: [buf#43]
Results [1]: [buf#44]

(67) Exchange
Input [1]: [buf#44]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=10]

(68) ObjectHashAggregate
Input [1]: [buf#44]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(d_date_sk#24, 42), 73049, 1141755, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(d_date_sk#24, 42), 73049, 1141755, 0, 0)#45]
Results [1]: [bloom_filter_agg(xxhash64(d_date_sk#24, 42), 73049, 1141755, 0, 0)#45 AS bloomFilter#46]


