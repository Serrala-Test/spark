== Physical Plan ==
TakeOrderedAndProject (40)
+- * HashAggregate (39)
   +- Exchange (38)
      +- * HashAggregate (37)
         +- * Project (36)
            +- * SortMergeJoin Inner (35)
               :- * Sort (26)
               :  +- Exchange (25)
               :     +- * Project (24)
               :        +- * SortMergeJoin Inner (23)
               :           :- * Sort (20)
               :           :  +- Exchange (19)
               :           :     +- * Project (18)
               :           :        +- * SortMergeJoin Inner (17)
               :           :           :- * Sort (14)
               :           :           :  +- Exchange (13)
               :           :           :     +- * Project (12)
               :           :           :        +- * BroadcastHashJoin Inner BuildRight (11)
               :           :           :           :- * Project (6)
               :           :           :           :  +- * BroadcastHashJoin Inner BuildRight (5)
               :           :           :           :     :- * Filter (3)
               :           :           :           :     :  +- * ColumnarToRow (2)
               :           :           :           :     :     +- Scan parquet spark_catalog.default.store_sales (1)
               :           :           :           :     +- ReusedExchange (4)
               :           :           :           +- BroadcastExchange (10)
               :           :           :              +- * Filter (9)
               :           :           :                 +- * ColumnarToRow (8)
               :           :           :                    +- Scan parquet spark_catalog.default.store (7)
               :           :           +- * Sort (16)
               :           :              +- ReusedExchange (15)
               :           +- * Sort (22)
               :              +- ReusedExchange (21)
               +- * Sort (34)
                  +- Exchange (33)
                     +- * Project (32)
                        +- * BroadcastHashJoin Inner BuildRight (31)
                           :- * Filter (29)
                           :  +- * ColumnarToRow (28)
                           :     +- Scan parquet spark_catalog.default.catalog_sales (27)
                           +- ReusedExchange (30)


(1) Scan parquet spark_catalog.default.store_sales
Output [6]: [ss_item_sk#1, ss_customer_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_quantity#5, ss_sold_date_sk#6]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#6), dynamicpruningexpression(ss_sold_date_sk#6 IN dynamicpruning#7)]
PushedFilters: [IsNotNull(ss_customer_sk), IsNotNull(ss_item_sk), IsNotNull(ss_ticket_number), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_item_sk:int,ss_customer_sk:int,ss_store_sk:int,ss_ticket_number:int,ss_quantity:int>

(2) ColumnarToRow [codegen id : 3]
Input [6]: [ss_item_sk#1, ss_customer_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_quantity#5, ss_sold_date_sk#6]

(3) Filter [codegen id : 3]
Input [6]: [ss_item_sk#1, ss_customer_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_quantity#5, ss_sold_date_sk#6]
Condition : ((((((isnotnull(ss_customer_sk#2) AND isnotnull(ss_item_sk#1)) AND isnotnull(ss_ticket_number#4)) AND isnotnull(ss_store_sk#3)) AND might_contain(Subquery scalar-subquery#8, [id=#9], xxhash64(ss_item_sk#1, 42), false)) AND might_contain(Subquery scalar-subquery#10, [id=#11], xxhash64(ss_customer_sk#2, 42), false)) AND might_contain(Subquery scalar-subquery#12, [id=#13], xxhash64(ss_ticket_number#4, 42), false))

(4) ReusedExchange [Reuses operator id: 71]
Output [1]: [d_date_sk#14]

(5) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_sold_date_sk#6]
Right keys [1]: [d_date_sk#14]
Join type: Inner
Join condition: None

(6) Project [codegen id : 3]
Output [5]: [ss_item_sk#1, ss_customer_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_quantity#5]
Input [7]: [ss_item_sk#1, ss_customer_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_quantity#5, ss_sold_date_sk#6, d_date_sk#14]

(7) Scan parquet spark_catalog.default.store
Output [3]: [s_store_sk#15, s_store_id#16, s_store_name#17]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int,s_store_id:string,s_store_name:string>

(8) ColumnarToRow [codegen id : 2]
Input [3]: [s_store_sk#15, s_store_id#16, s_store_name#17]

(9) Filter [codegen id : 2]
Input [3]: [s_store_sk#15, s_store_id#16, s_store_name#17]
Condition : isnotnull(s_store_sk#15)

(10) BroadcastExchange
Input [3]: [s_store_sk#15, s_store_id#16, s_store_name#17]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1]

(11) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_store_sk#3]
Right keys [1]: [s_store_sk#15]
Join type: Inner
Join condition: None

(12) Project [codegen id : 3]
Output [6]: [ss_item_sk#1, ss_customer_sk#2, ss_ticket_number#4, ss_quantity#5, s_store_id#16, s_store_name#17]
Input [8]: [ss_item_sk#1, ss_customer_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_quantity#5, s_store_sk#15, s_store_id#16, s_store_name#17]

(13) Exchange
Input [6]: [ss_item_sk#1, ss_customer_sk#2, ss_ticket_number#4, ss_quantity#5, s_store_id#16, s_store_name#17]
Arguments: hashpartitioning(ss_item_sk#1, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(14) Sort [codegen id : 4]
Input [6]: [ss_item_sk#1, ss_customer_sk#2, ss_ticket_number#4, ss_quantity#5, s_store_id#16, s_store_name#17]
Arguments: [ss_item_sk#1 ASC NULLS FIRST], false, 0

(15) ReusedExchange [Reuses operator id: 44]
Output [3]: [i_item_sk#18, i_item_id#19, i_item_desc#20]

(16) Sort [codegen id : 6]
Input [3]: [i_item_sk#18, i_item_id#19, i_item_desc#20]
Arguments: [i_item_sk#18 ASC NULLS FIRST], false, 0

(17) SortMergeJoin [codegen id : 7]
Left keys [1]: [ss_item_sk#1]
Right keys [1]: [i_item_sk#18]
Join type: Inner
Join condition: None

(18) Project [codegen id : 7]
Output [8]: [ss_item_sk#1, ss_customer_sk#2, ss_ticket_number#4, ss_quantity#5, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20]
Input [9]: [ss_item_sk#1, ss_customer_sk#2, ss_ticket_number#4, ss_quantity#5, s_store_id#16, s_store_name#17, i_item_sk#18, i_item_id#19, i_item_desc#20]

(19) Exchange
Input [8]: [ss_item_sk#1, ss_customer_sk#2, ss_ticket_number#4, ss_quantity#5, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20]
Arguments: hashpartitioning(ss_customer_sk#2, ss_item_sk#1, ss_ticket_number#4, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(20) Sort [codegen id : 8]
Input [8]: [ss_item_sk#1, ss_customer_sk#2, ss_ticket_number#4, ss_quantity#5, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20]
Arguments: [ss_customer_sk#2 ASC NULLS FIRST, ss_item_sk#1 ASC NULLS FIRST, ss_ticket_number#4 ASC NULLS FIRST], false, 0

(21) ReusedExchange [Reuses operator id: 54]
Output [4]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24]

(22) Sort [codegen id : 11]
Input [4]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24]
Arguments: [sr_customer_sk#22 ASC NULLS FIRST, sr_item_sk#21 ASC NULLS FIRST, sr_ticket_number#23 ASC NULLS FIRST], false, 0

(23) SortMergeJoin [codegen id : 12]
Left keys [3]: [ss_customer_sk#2, ss_item_sk#1, ss_ticket_number#4]
Right keys [3]: [sr_customer_sk#22, sr_item_sk#21, sr_ticket_number#23]
Join type: Inner
Join condition: None

(24) Project [codegen id : 12]
Output [8]: [ss_quantity#5, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20, sr_item_sk#21, sr_customer_sk#22, sr_return_quantity#24]
Input [12]: [ss_item_sk#1, ss_customer_sk#2, ss_ticket_number#4, ss_quantity#5, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20, sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24]

(25) Exchange
Input [8]: [ss_quantity#5, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20, sr_item_sk#21, sr_customer_sk#22, sr_return_quantity#24]
Arguments: hashpartitioning(sr_customer_sk#22, sr_item_sk#21, 5), ENSURE_REQUIREMENTS, [plan_id=4]

(26) Sort [codegen id : 13]
Input [8]: [ss_quantity#5, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20, sr_item_sk#21, sr_customer_sk#22, sr_return_quantity#24]
Arguments: [sr_customer_sk#22 ASC NULLS FIRST, sr_item_sk#21 ASC NULLS FIRST], false, 0

(27) Scan parquet spark_catalog.default.catalog_sales
Output [4]: [cs_bill_customer_sk#25, cs_item_sk#26, cs_quantity#27, cs_sold_date_sk#28]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#28), dynamicpruningexpression(cs_sold_date_sk#28 IN dynamicpruning#29)]
PushedFilters: [IsNotNull(cs_bill_customer_sk), IsNotNull(cs_item_sk)]
ReadSchema: struct<cs_bill_customer_sk:int,cs_item_sk:int,cs_quantity:int>

(28) ColumnarToRow [codegen id : 15]
Input [4]: [cs_bill_customer_sk#25, cs_item_sk#26, cs_quantity#27, cs_sold_date_sk#28]

(29) Filter [codegen id : 15]
Input [4]: [cs_bill_customer_sk#25, cs_item_sk#26, cs_quantity#27, cs_sold_date_sk#28]
Condition : (isnotnull(cs_bill_customer_sk#25) AND isnotnull(cs_item_sk#26))

(30) ReusedExchange [Reuses operator id: 76]
Output [1]: [d_date_sk#30]

(31) BroadcastHashJoin [codegen id : 15]
Left keys [1]: [cs_sold_date_sk#28]
Right keys [1]: [d_date_sk#30]
Join type: Inner
Join condition: None

(32) Project [codegen id : 15]
Output [3]: [cs_bill_customer_sk#25, cs_item_sk#26, cs_quantity#27]
Input [5]: [cs_bill_customer_sk#25, cs_item_sk#26, cs_quantity#27, cs_sold_date_sk#28, d_date_sk#30]

(33) Exchange
Input [3]: [cs_bill_customer_sk#25, cs_item_sk#26, cs_quantity#27]
Arguments: hashpartitioning(cs_bill_customer_sk#25, cs_item_sk#26, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(34) Sort [codegen id : 16]
Input [3]: [cs_bill_customer_sk#25, cs_item_sk#26, cs_quantity#27]
Arguments: [cs_bill_customer_sk#25 ASC NULLS FIRST, cs_item_sk#26 ASC NULLS FIRST], false, 0

(35) SortMergeJoin [codegen id : 17]
Left keys [2]: [sr_customer_sk#22, sr_item_sk#21]
Right keys [2]: [cs_bill_customer_sk#25, cs_item_sk#26]
Join type: Inner
Join condition: None

(36) Project [codegen id : 17]
Output [7]: [ss_quantity#5, sr_return_quantity#24, cs_quantity#27, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20]
Input [11]: [ss_quantity#5, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20, sr_item_sk#21, sr_customer_sk#22, sr_return_quantity#24, cs_bill_customer_sk#25, cs_item_sk#26, cs_quantity#27]

(37) HashAggregate [codegen id : 17]
Input [7]: [ss_quantity#5, sr_return_quantity#24, cs_quantity#27, s_store_id#16, s_store_name#17, i_item_id#19, i_item_desc#20]
Keys [4]: [i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17]
Functions [3]: [partial_sum(ss_quantity#5), partial_sum(sr_return_quantity#24), partial_sum(cs_quantity#27)]
Aggregate Attributes [3]: [sum#31, sum#32, sum#33]
Results [7]: [i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17, sum#34, sum#35, sum#36]

(38) Exchange
Input [7]: [i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17, sum#34, sum#35, sum#36]
Arguments: hashpartitioning(i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17, 5), ENSURE_REQUIREMENTS, [plan_id=6]

(39) HashAggregate [codegen id : 18]
Input [7]: [i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17, sum#34, sum#35, sum#36]
Keys [4]: [i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17]
Functions [3]: [sum(ss_quantity#5), sum(sr_return_quantity#24), sum(cs_quantity#27)]
Aggregate Attributes [3]: [sum(ss_quantity#5)#37, sum(sr_return_quantity#24)#38, sum(cs_quantity#27)#39]
Results [7]: [i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17, sum(ss_quantity#5)#37 AS store_sales_quantity#40, sum(sr_return_quantity#24)#38 AS store_returns_quantity#41, sum(cs_quantity#27)#39 AS catalog_sales_quantity#42]

(40) TakeOrderedAndProject
Input [7]: [i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17, store_sales_quantity#40, store_returns_quantity#41, catalog_sales_quantity#42]
Arguments: 100, [i_item_id#19 ASC NULLS FIRST, i_item_desc#20 ASC NULLS FIRST, s_store_id#16 ASC NULLS FIRST, s_store_name#17 ASC NULLS FIRST], [i_item_id#19, i_item_desc#20, s_store_id#16, s_store_name#17, store_sales_quantity#40, store_returns_quantity#41, catalog_sales_quantity#42]

===== Subqueries =====

Subquery:1 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#8, [id=#9]
ObjectHashAggregate (47)
+- Exchange (46)
   +- ObjectHashAggregate (45)
      +- Exchange (44)
         +- * Filter (43)
            +- * ColumnarToRow (42)
               +- Scan parquet spark_catalog.default.item (41)


(41) Scan parquet spark_catalog.default.item
Output [3]: [i_item_sk#18, i_item_id#19, i_item_desc#20]
Batched: true
Location [not included in comparison]/{warehouse_dir}/item]
PushedFilters: [IsNotNull(i_item_sk)]
ReadSchema: struct<i_item_sk:int,i_item_id:string,i_item_desc:string>

(42) ColumnarToRow [codegen id : 1]
Input [3]: [i_item_sk#18, i_item_id#19, i_item_desc#20]

(43) Filter [codegen id : 1]
Input [3]: [i_item_sk#18, i_item_id#19, i_item_desc#20]
Condition : isnotnull(i_item_sk#18)

(44) Exchange
Input [3]: [i_item_sk#18, i_item_id#19, i_item_desc#20]
Arguments: hashpartitioning(i_item_sk#18, 5), ENSURE_REQUIREMENTS, [plan_id=7]

(45) ObjectHashAggregate
Input [3]: [i_item_sk#18, i_item_id#19, i_item_desc#20]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(i_item_sk#18, 42), 204000, 1632000, 0, 0)]
Aggregate Attributes [1]: [buf#43]
Results [1]: [buf#44]

(46) Exchange
Input [1]: [buf#44]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=8]

(47) ObjectHashAggregate
Input [1]: [buf#44]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(i_item_sk#18, 42), 204000, 1632000, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(i_item_sk#18, 42), 204000, 1632000, 0, 0)#45]
Results [1]: [bloom_filter_agg(xxhash64(i_item_sk#18, 42), 204000, 1632000, 0, 0)#45 AS bloomFilter#46]

Subquery:2 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#10, [id=#11]
ObjectHashAggregate (57)
+- Exchange (56)
   +- ObjectHashAggregate (55)
      +- Exchange (54)
         +- * Project (53)
            +- * BroadcastHashJoin Inner BuildRight (52)
               :- * Filter (50)
               :  +- * ColumnarToRow (49)
               :     +- Scan parquet spark_catalog.default.store_returns (48)
               +- ReusedExchange (51)


(48) Scan parquet spark_catalog.default.store_returns
Output [5]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24, sr_returned_date_sk#47]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(sr_returned_date_sk#47), dynamicpruningexpression(sr_returned_date_sk#47 IN dynamicpruning#48)]
PushedFilters: [IsNotNull(sr_customer_sk), IsNotNull(sr_item_sk), IsNotNull(sr_ticket_number)]
ReadSchema: struct<sr_item_sk:int,sr_customer_sk:int,sr_ticket_number:int,sr_return_quantity:int>

(49) ColumnarToRow [codegen id : 2]
Input [5]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24, sr_returned_date_sk#47]

(50) Filter [codegen id : 2]
Input [5]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24, sr_returned_date_sk#47]
Condition : ((isnotnull(sr_customer_sk#22) AND isnotnull(sr_item_sk#21)) AND isnotnull(sr_ticket_number#23))

(51) ReusedExchange [Reuses operator id: 62]
Output [1]: [d_date_sk#49]

(52) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [sr_returned_date_sk#47]
Right keys [1]: [d_date_sk#49]
Join type: Inner
Join condition: None

(53) Project [codegen id : 2]
Output [4]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24]
Input [6]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24, sr_returned_date_sk#47, d_date_sk#49]

(54) Exchange
Input [4]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24]
Arguments: hashpartitioning(sr_customer_sk#22, sr_item_sk#21, sr_ticket_number#23, 5), ENSURE_REQUIREMENTS, [plan_id=9]

(55) ObjectHashAggregate
Input [4]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(sr_customer_sk#22, 42), 1417316, 11338528, 0, 0)]
Aggregate Attributes [1]: [buf#50]
Results [1]: [buf#51]

(56) Exchange
Input [1]: [buf#51]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=10]

(57) ObjectHashAggregate
Input [1]: [buf#51]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(sr_customer_sk#22, 42), 1417316, 11338528, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(sr_customer_sk#22, 42), 1417316, 11338528, 0, 0)#52]
Results [1]: [bloom_filter_agg(xxhash64(sr_customer_sk#22, 42), 1417316, 11338528, 0, 0)#52 AS bloomFilter#53]

Subquery:3 Hosting operator id = 48 Hosting Expression = sr_returned_date_sk#47 IN dynamicpruning#48
BroadcastExchange (62)
+- * Project (61)
   +- * Filter (60)
      +- * ColumnarToRow (59)
         +- Scan parquet spark_catalog.default.date_dim (58)


(58) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#49, d_year#54, d_moy#55]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), GreaterThanOrEqual(d_moy,9), LessThanOrEqual(d_moy,12), EqualTo(d_year,1999), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>

(59) ColumnarToRow [codegen id : 1]
Input [3]: [d_date_sk#49, d_year#54, d_moy#55]

(60) Filter [codegen id : 1]
Input [3]: [d_date_sk#49, d_year#54, d_moy#55]
Condition : (((((isnotnull(d_moy#55) AND isnotnull(d_year#54)) AND (d_moy#55 >= 9)) AND (d_moy#55 <= 12)) AND (d_year#54 = 1999)) AND isnotnull(d_date_sk#49))

(61) Project [codegen id : 1]
Output [1]: [d_date_sk#49]
Input [3]: [d_date_sk#49, d_year#54, d_moy#55]

(62) BroadcastExchange
Input [1]: [d_date_sk#49]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=11]

Subquery:4 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#12, [id=#13]
ObjectHashAggregate (66)
+- Exchange (65)
   +- ObjectHashAggregate (64)
      +- ReusedExchange (63)


(63) ReusedExchange [Reuses operator id: 54]
Output [4]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24]

(64) ObjectHashAggregate
Input [4]: [sr_item_sk#21, sr_customer_sk#22, sr_ticket_number#23, sr_return_quantity#24]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(sr_ticket_number#23, 42), 1417316, 11338528, 0, 0)]
Aggregate Attributes [1]: [buf#56]
Results [1]: [buf#57]

(65) Exchange
Input [1]: [buf#57]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=12]

(66) ObjectHashAggregate
Input [1]: [buf#57]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(sr_ticket_number#23, 42), 1417316, 11338528, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(sr_ticket_number#23, 42), 1417316, 11338528, 0, 0)#58]
Results [1]: [bloom_filter_agg(xxhash64(sr_ticket_number#23, 42), 1417316, 11338528, 0, 0)#58 AS bloomFilter#59]

Subquery:5 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#6 IN dynamicpruning#7
BroadcastExchange (71)
+- * Project (70)
   +- * Filter (69)
      +- * ColumnarToRow (68)
         +- Scan parquet spark_catalog.default.date_dim (67)


(67) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#14, d_year#60, d_moy#61]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,9), EqualTo(d_year,1999), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>

(68) ColumnarToRow [codegen id : 1]
Input [3]: [d_date_sk#14, d_year#60, d_moy#61]

(69) Filter [codegen id : 1]
Input [3]: [d_date_sk#14, d_year#60, d_moy#61]
Condition : ((((isnotnull(d_moy#61) AND isnotnull(d_year#60)) AND (d_moy#61 = 9)) AND (d_year#60 = 1999)) AND isnotnull(d_date_sk#14))

(70) Project [codegen id : 1]
Output [1]: [d_date_sk#14]
Input [3]: [d_date_sk#14, d_year#60, d_moy#61]

(71) BroadcastExchange
Input [1]: [d_date_sk#14]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=13]

Subquery:6 Hosting operator id = 27 Hosting Expression = cs_sold_date_sk#28 IN dynamicpruning#29
BroadcastExchange (76)
+- * Project (75)
   +- * Filter (74)
      +- * ColumnarToRow (73)
         +- Scan parquet spark_catalog.default.date_dim (72)


(72) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#30, d_year#62]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [In(d_year, [1999,2000,2001]), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int>

(73) ColumnarToRow [codegen id : 1]
Input [2]: [d_date_sk#30, d_year#62]

(74) Filter [codegen id : 1]
Input [2]: [d_date_sk#30, d_year#62]
Condition : (d_year#62 IN (1999,2000,2001) AND isnotnull(d_date_sk#30))

(75) Project [codegen id : 1]
Output [1]: [d_date_sk#30]
Input [2]: [d_date_sk#30, d_year#62]

(76) BroadcastExchange
Input [1]: [d_date_sk#30]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=14]


