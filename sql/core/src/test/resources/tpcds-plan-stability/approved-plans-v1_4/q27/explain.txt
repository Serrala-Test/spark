== Physical Plan ==
TakeOrderedAndProject (36)
+- * HashAggregate (35)
   +- Exchange (34)
      +- * HashAggregate (33)
         +- * Expand (32)
            +- * Project (31)
               +- * BroadcastHashJoin Inner BuildRight (30)
                  :- * Project (24)
                  :  +- * BroadcastHashJoin Inner BuildRight (23)
                  :     :- * Project (17)
                  :     :  +- * BroadcastHashJoin Inner BuildRight (16)
                  :     :     :- * Project (10)
                  :     :     :  +- * BroadcastHashJoin Inner BuildRight (9)
                  :     :     :     :- * Filter (3)
                  :     :     :     :  +- * ColumnarToRow (2)
                  :     :     :     :     +- Scan parquet default.store_sales (1)
                  :     :     :     +- BroadcastExchange (8)
                  :     :     :        +- * Project (7)
                  :     :     :           +- * Filter (6)
                  :     :     :              +- * ColumnarToRow (5)
                  :     :     :                 +- Scan parquet default.customer_demographics (4)
                  :     :     +- BroadcastExchange (15)
                  :     :        +- * Project (14)
                  :     :           +- * Filter (13)
                  :     :              +- * ColumnarToRow (12)
                  :     :                 +- Scan parquet default.date_dim (11)
                  :     +- BroadcastExchange (22)
                  :        +- * Project (21)
                  :           +- * Filter (20)
                  :              +- * ColumnarToRow (19)
                  :                 +- Scan parquet default.store (18)
                  +- BroadcastExchange (29)
                     +- * Project (28)
                        +- * Filter (27)
                           +- * ColumnarToRow (26)
                              +- Scan parquet default.item (25)


(1) Scan parquet default.store_sales
Output [8]: [ss_sold_date_sk#1, ss_item_sk#2, ss_cdemo_sk#3, ss_store_sk#4, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_cdemo_sk), IsNotNull(ss_sold_date_sk), IsNotNull(ss_store_sk), IsNotNull(ss_item_sk)]
ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_cdemo_sk:int,ss_store_sk:int,ss_quantity:int,ss_list_price:decimal(7,2),ss_sales_price:decimal(7,2),ss_coupon_amt:decimal(7,2)>

(2) ColumnarToRow [codegen id : 5]
Input [8]: [ss_sold_date_sk#1, ss_item_sk#2, ss_cdemo_sk#3, ss_store_sk#4, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8]

(3) Filter [codegen id : 5]
Input [8]: [ss_sold_date_sk#1, ss_item_sk#2, ss_cdemo_sk#3, ss_store_sk#4, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8]
Condition : (((isnotnull(ss_cdemo_sk#3) AND isnotnull(ss_sold_date_sk#1)) AND isnotnull(ss_store_sk#4)) AND isnotnull(ss_item_sk#2))

(4) Scan parquet default.customer_demographics
Output [4]: [cd_demo_sk#9, cd_gender#10, cd_marital_status#11, cd_education_status#12]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer_demographics]
PushedFilters: [IsNotNull(cd_demo_sk)]
ReadSchema: struct<cd_demo_sk:int,cd_gender:string,cd_marital_status:string,cd_education_status:string>

(5) ColumnarToRow [codegen id : 1]
Input [4]: [cd_demo_sk#9, cd_gender#10, cd_marital_status#11, cd_education_status#12]

(6) Filter [codegen id : 1]
Input [4]: [cd_demo_sk#9, cd_gender#10, cd_marital_status#11, cd_education_status#12]
Condition : ((((staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, paddingWithLengthCheck, cd_gender#10, 1, false, true) = M) AND (staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, paddingWithLengthCheck, cd_marital_status#11, 1, false, true) = S)) AND (staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, paddingWithLengthCheck, cd_education_status#12, 20, false, true) = College             )) AND isnotnull(cd_demo_sk#9))

(7) Project [codegen id : 1]
Output [1]: [cd_demo_sk#9]
Input [4]: [cd_demo_sk#9, cd_gender#10, cd_marital_status#11, cd_education_status#12]

(8) BroadcastExchange
Input [1]: [cd_demo_sk#9]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#13]

(9) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [ss_cdemo_sk#3]
Right keys [1]: [cd_demo_sk#9]
Join condition: None

(10) Project [codegen id : 5]
Output [7]: [ss_sold_date_sk#1, ss_item_sk#2, ss_store_sk#4, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8]
Input [9]: [ss_sold_date_sk#1, ss_item_sk#2, ss_cdemo_sk#3, ss_store_sk#4, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, cd_demo_sk#9]

(11) Scan parquet default.date_dim
Output [2]: [d_date_sk#14, d_year#15]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int>

(12) ColumnarToRow [codegen id : 2]
Input [2]: [d_date_sk#14, d_year#15]

(13) Filter [codegen id : 2]
Input [2]: [d_date_sk#14, d_year#15]
Condition : ((isnotnull(d_year#15) AND (d_year#15 = 2002)) AND isnotnull(d_date_sk#14))

(14) Project [codegen id : 2]
Output [1]: [d_date_sk#14]
Input [2]: [d_date_sk#14, d_year#15]

(15) BroadcastExchange
Input [1]: [d_date_sk#14]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#16]

(16) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [ss_sold_date_sk#1]
Right keys [1]: [d_date_sk#14]
Join condition: None

(17) Project [codegen id : 5]
Output [6]: [ss_item_sk#2, ss_store_sk#4, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8]
Input [8]: [ss_sold_date_sk#1, ss_item_sk#2, ss_store_sk#4, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, d_date_sk#14]

(18) Scan parquet default.store
Output [2]: [s_store_sk#17, s_state#18]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int,s_state:string>

(19) ColumnarToRow [codegen id : 3]
Input [2]: [s_store_sk#17, s_state#18]

(20) Filter [codegen id : 3]
Input [2]: [s_store_sk#17, s_state#18]
Condition : ((staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, paddingWithLengthCheck, s_state#18, 2, false, true) = TN) AND isnotnull(s_store_sk#17))

(21) Project [codegen id : 3]
Output [2]: [s_store_sk#17, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, paddingWithLengthCheck, s_state#18, 2, false, true) AS s_state#19]
Input [2]: [s_store_sk#17, s_state#18]

(22) BroadcastExchange
Input [2]: [s_store_sk#17, s_state#19]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#20]

(23) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [ss_store_sk#4]
Right keys [1]: [s_store_sk#17]
Join condition: None

(24) Project [codegen id : 5]
Output [6]: [ss_item_sk#2, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, s_state#19]
Input [8]: [ss_item_sk#2, ss_store_sk#4, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, s_store_sk#17, s_state#19]

(25) Scan parquet default.item
Output [2]: [i_item_sk#21, i_item_id#22]
Batched: true
Location [not included in comparison]/{warehouse_dir}/item]
PushedFilters: [IsNotNull(i_item_sk)]
ReadSchema: struct<i_item_sk:int,i_item_id:string>

(26) ColumnarToRow [codegen id : 4]
Input [2]: [i_item_sk#21, i_item_id#22]

(27) Filter [codegen id : 4]
Input [2]: [i_item_sk#21, i_item_id#22]
Condition : isnotnull(i_item_sk#21)

(28) Project [codegen id : 4]
Output [2]: [i_item_sk#21, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, paddingWithLengthCheck, i_item_id#22, 16, false, true) AS i_item_id#23]
Input [2]: [i_item_sk#21, i_item_id#22]

(29) BroadcastExchange
Input [2]: [i_item_sk#21, i_item_id#23]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#24]

(30) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [ss_item_sk#2]
Right keys [1]: [i_item_sk#21]
Join condition: None

(31) Project [codegen id : 5]
Output [6]: [ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, i_item_id#23 AS i_item_id#25, s_state#19 AS s_state#26]
Input [8]: [ss_item_sk#2, ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, s_state#19, i_item_sk#21, i_item_id#23]

(32) Expand [codegen id : 5]
Input [6]: [ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, i_item_id#25, s_state#26]
Arguments: [List(ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, i_item_id#25, s_state#26, 0), List(ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, i_item_id#25, null, 1), List(ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, null, null, 3)], [ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, i_item_id#27, s_state#28, spark_grouping_id#29]

(33) HashAggregate [codegen id : 5]
Input [7]: [ss_quantity#5, ss_list_price#6, ss_sales_price#7, ss_coupon_amt#8, i_item_id#27, s_state#28, spark_grouping_id#29]
Keys [3]: [i_item_id#27, s_state#28, spark_grouping_id#29]
Functions [4]: [partial_avg(cast(ss_quantity#5 as bigint)), partial_avg(UnscaledValue(ss_list_price#6)), partial_avg(UnscaledValue(ss_coupon_amt#8)), partial_avg(UnscaledValue(ss_sales_price#7))]
Aggregate Attributes [8]: [sum#30, count#31, sum#32, count#33, sum#34, count#35, sum#36, count#37]
Results [11]: [i_item_id#27, s_state#28, spark_grouping_id#29, sum#38, count#39, sum#40, count#41, sum#42, count#43, sum#44, count#45]

(34) Exchange
Input [11]: [i_item_id#27, s_state#28, spark_grouping_id#29, sum#38, count#39, sum#40, count#41, sum#42, count#43, sum#44, count#45]
Arguments: hashpartitioning(i_item_id#27, s_state#28, spark_grouping_id#29, 5), ENSURE_REQUIREMENTS, [id=#46]

(35) HashAggregate [codegen id : 6]
Input [11]: [i_item_id#27, s_state#28, spark_grouping_id#29, sum#38, count#39, sum#40, count#41, sum#42, count#43, sum#44, count#45]
Keys [3]: [i_item_id#27, s_state#28, spark_grouping_id#29]
Functions [4]: [avg(cast(ss_quantity#5 as bigint)), avg(UnscaledValue(ss_list_price#6)), avg(UnscaledValue(ss_coupon_amt#8)), avg(UnscaledValue(ss_sales_price#7))]
Aggregate Attributes [4]: [avg(cast(ss_quantity#5 as bigint))#47, avg(UnscaledValue(ss_list_price#6))#48, avg(UnscaledValue(ss_coupon_amt#8))#49, avg(UnscaledValue(ss_sales_price#7))#50]
Results [7]: [i_item_id#27, s_state#28, cast((shiftright(spark_grouping_id#29, 0) & 1) as tinyint) AS g_state#51, avg(cast(ss_quantity#5 as bigint))#47 AS agg1#52, cast((avg(UnscaledValue(ss_list_price#6))#48 / 100.0) as decimal(11,6)) AS agg2#53, cast((avg(UnscaledValue(ss_coupon_amt#8))#49 / 100.0) as decimal(11,6)) AS agg3#54, cast((avg(UnscaledValue(ss_sales_price#7))#50 / 100.0) as decimal(11,6)) AS agg4#55]

(36) TakeOrderedAndProject
Input [7]: [i_item_id#27, s_state#28, g_state#51, agg1#52, agg2#53, agg3#54, agg4#55]
Arguments: 100, [i_item_id#27 ASC NULLS FIRST, s_state#28 ASC NULLS FIRST], [i_item_id#27, s_state#28, g_state#51, agg1#52, agg2#53, agg3#54, agg4#55]

