== Physical Plan ==
TakeOrderedAndProject (46)
+- * Project (45)
   +- * BroadcastHashJoin Inner BuildRight (44)
      :- * Project (30)
      :  +- * BroadcastHashJoin Inner BuildRight (29)
      :     :- * HashAggregate (15)
      :     :  +- Exchange (14)
      :     :     +- * HashAggregate (13)
      :     :        +- * Project (12)
      :     :           +- * BroadcastHashJoin Inner BuildRight (11)
      :     :              :- * Project (6)
      :     :              :  +- * BroadcastHashJoin Inner BuildRight (5)
      :     :              :     :- * Filter (3)
      :     :              :     :  +- * ColumnarToRow (2)
      :     :              :     :     +- Scan parquet spark_catalog.default.store_returns (1)
      :     :              :     +- ReusedExchange (4)
      :     :              +- BroadcastExchange (10)
      :     :                 +- * Filter (9)
      :     :                    +- * ColumnarToRow (8)
      :     :                       +- Scan parquet spark_catalog.default.item (7)
      :     +- BroadcastExchange (28)
      :        +- * HashAggregate (27)
      :           +- Exchange (26)
      :              +- * HashAggregate (25)
      :                 +- * Project (24)
      :                    +- * BroadcastHashJoin Inner BuildRight (23)
      :                       :- * Project (21)
      :                       :  +- * BroadcastHashJoin Inner BuildRight (20)
      :                       :     :- * Filter (18)
      :                       :     :  +- * ColumnarToRow (17)
      :                       :     :     +- Scan parquet spark_catalog.default.catalog_returns (16)
      :                       :     +- ReusedExchange (19)
      :                       +- ReusedExchange (22)
      +- BroadcastExchange (43)
         +- * HashAggregate (42)
            +- Exchange (41)
               +- * HashAggregate (40)
                  +- * Project (39)
                     +- * BroadcastHashJoin Inner BuildRight (38)
                        :- * Project (36)
                        :  +- * BroadcastHashJoin Inner BuildRight (35)
                        :     :- * Filter (33)
                        :     :  +- * ColumnarToRow (32)
                        :     :     +- Scan parquet spark_catalog.default.web_returns (31)
                        :     +- ReusedExchange (34)
                        +- ReusedExchange (37)


(1) Scan parquet spark_catalog.default.store_returns
Output [3]: [sr_item_sk#1, sr_return_quantity#2, sr_returned_date_sk#3]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(sr_returned_date_sk#3), dynamicpruningexpression(sr_returned_date_sk#3 IN dynamicpruning#4)]
PushedFilters: [IsNotNull(sr_item_sk)]
ReadSchema: struct<sr_item_sk:int,sr_return_quantity:int>

(2) ColumnarToRow [codegen id : 5]
Input [3]: [sr_item_sk#1, sr_return_quantity#2, sr_returned_date_sk#3]

(3) Filter [codegen id : 5]
Input [3]: [sr_item_sk#1, sr_return_quantity#2, sr_returned_date_sk#3]
Condition : isnotnull(sr_item_sk#1)

(4) ReusedExchange [Reuses operator id: 62]
Output [1]: [d_date_sk#5]

(5) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [sr_returned_date_sk#3]
Right keys [1]: [d_date_sk#5]
Join type: Inner
Join condition: None

(6) Project [codegen id : 5]
Output [2]: [sr_item_sk#1, sr_return_quantity#2]
Input [4]: [sr_item_sk#1, sr_return_quantity#2, sr_returned_date_sk#3, d_date_sk#5]

(7) Scan parquet spark_catalog.default.item
Output [2]: [i_item_sk#6, i_item_id#7]
Batched: true
Location [not included in comparison]/{warehouse_dir}/item]
PushedFilters: [IsNotNull(i_item_sk), IsNotNull(i_item_id)]
ReadSchema: struct<i_item_sk:int,i_item_id:string>

(8) ColumnarToRow [codegen id : 4]
Input [2]: [i_item_sk#6, i_item_id#7]

(9) Filter [codegen id : 4]
Input [2]: [i_item_sk#6, i_item_id#7]
Condition : (isnotnull(i_item_sk#6) AND isnotnull(i_item_id#7))

(10) BroadcastExchange
Input [2]: [i_item_sk#6, i_item_id#7]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1]

(11) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [sr_item_sk#1]
Right keys [1]: [i_item_sk#6]
Join type: Inner
Join condition: None

(12) Project [codegen id : 5]
Output [2]: [sr_return_quantity#2, i_item_id#7]
Input [4]: [sr_item_sk#1, sr_return_quantity#2, i_item_sk#6, i_item_id#7]

(13) HashAggregate [codegen id : 5]
Input [2]: [sr_return_quantity#2, i_item_id#7]
Keys [1]: [i_item_id#7]
Functions [1]: [partial_sum(sr_return_quantity#2)]
Aggregate Attributes [1]: [sum#8]
Results [2]: [i_item_id#7, sum#9]

(14) Exchange
Input [2]: [i_item_id#7, sum#9]
Arguments: hashpartitioning(i_item_id#7, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(15) HashAggregate [codegen id : 18]
Input [2]: [i_item_id#7, sum#9]
Keys [1]: [i_item_id#7]
Functions [1]: [sum(sr_return_quantity#2)]
Aggregate Attributes [1]: [sum(sr_return_quantity#2)#10]
Results [2]: [i_item_id#7 AS item_id#11, sum(sr_return_quantity#2)#10 AS sr_item_qty#12]

(16) Scan parquet spark_catalog.default.catalog_returns
Output [3]: [cr_item_sk#13, cr_return_quantity#14, cr_returned_date_sk#15]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cr_returned_date_sk#15), dynamicpruningexpression(cr_returned_date_sk#15 IN dynamicpruning#4)]
PushedFilters: [IsNotNull(cr_item_sk)]
ReadSchema: struct<cr_item_sk:int,cr_return_quantity:int>

(17) ColumnarToRow [codegen id : 10]
Input [3]: [cr_item_sk#13, cr_return_quantity#14, cr_returned_date_sk#15]

(18) Filter [codegen id : 10]
Input [3]: [cr_item_sk#13, cr_return_quantity#14, cr_returned_date_sk#15]
Condition : isnotnull(cr_item_sk#13)

(19) ReusedExchange [Reuses operator id: 62]
Output [1]: [d_date_sk#5]

(20) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [cr_returned_date_sk#15]
Right keys [1]: [d_date_sk#5]
Join type: Inner
Join condition: None

(21) Project [codegen id : 10]
Output [2]: [cr_item_sk#13, cr_return_quantity#14]
Input [4]: [cr_item_sk#13, cr_return_quantity#14, cr_returned_date_sk#15, d_date_sk#5]

(22) ReusedExchange [Reuses operator id: 10]
Output [2]: [i_item_sk#6, i_item_id#7]

(23) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [cr_item_sk#13]
Right keys [1]: [i_item_sk#6]
Join type: Inner
Join condition: None

(24) Project [codegen id : 10]
Output [2]: [cr_return_quantity#14, i_item_id#7]
Input [4]: [cr_item_sk#13, cr_return_quantity#14, i_item_sk#6, i_item_id#7]

(25) HashAggregate [codegen id : 10]
Input [2]: [cr_return_quantity#14, i_item_id#7]
Keys [1]: [i_item_id#7]
Functions [1]: [partial_sum(cr_return_quantity#14)]
Aggregate Attributes [1]: [sum#16]
Results [2]: [i_item_id#7, sum#17]

(26) Exchange
Input [2]: [i_item_id#7, sum#17]
Arguments: hashpartitioning(i_item_id#7, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(27) HashAggregate [codegen id : 11]
Input [2]: [i_item_id#7, sum#17]
Keys [1]: [i_item_id#7]
Functions [1]: [sum(cr_return_quantity#14)]
Aggregate Attributes [1]: [sum(cr_return_quantity#14)#18]
Results [2]: [i_item_id#7 AS item_id#19, sum(cr_return_quantity#14)#18 AS cr_item_qty#20]

(28) BroadcastExchange
Input [2]: [item_id#19, cr_item_qty#20]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=4]

(29) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [item_id#11]
Right keys [1]: [item_id#19]
Join type: Inner
Join condition: None

(30) Project [codegen id : 18]
Output [3]: [item_id#11, sr_item_qty#12, cr_item_qty#20]
Input [4]: [item_id#11, sr_item_qty#12, item_id#19, cr_item_qty#20]

(31) Scan parquet spark_catalog.default.web_returns
Output [3]: [wr_item_sk#21, wr_return_quantity#22, wr_returned_date_sk#23]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(wr_returned_date_sk#23), dynamicpruningexpression(wr_returned_date_sk#23 IN dynamicpruning#4)]
PushedFilters: [IsNotNull(wr_item_sk)]
ReadSchema: struct<wr_item_sk:int,wr_return_quantity:int>

(32) ColumnarToRow [codegen id : 16]
Input [3]: [wr_item_sk#21, wr_return_quantity#22, wr_returned_date_sk#23]

(33) Filter [codegen id : 16]
Input [3]: [wr_item_sk#21, wr_return_quantity#22, wr_returned_date_sk#23]
Condition : isnotnull(wr_item_sk#21)

(34) ReusedExchange [Reuses operator id: 62]
Output [1]: [d_date_sk#5]

(35) BroadcastHashJoin [codegen id : 16]
Left keys [1]: [wr_returned_date_sk#23]
Right keys [1]: [d_date_sk#5]
Join type: Inner
Join condition: None

(36) Project [codegen id : 16]
Output [2]: [wr_item_sk#21, wr_return_quantity#22]
Input [4]: [wr_item_sk#21, wr_return_quantity#22, wr_returned_date_sk#23, d_date_sk#5]

(37) ReusedExchange [Reuses operator id: 10]
Output [2]: [i_item_sk#6, i_item_id#7]

(38) BroadcastHashJoin [codegen id : 16]
Left keys [1]: [wr_item_sk#21]
Right keys [1]: [i_item_sk#6]
Join type: Inner
Join condition: None

(39) Project [codegen id : 16]
Output [2]: [wr_return_quantity#22, i_item_id#7]
Input [4]: [wr_item_sk#21, wr_return_quantity#22, i_item_sk#6, i_item_id#7]

(40) HashAggregate [codegen id : 16]
Input [2]: [wr_return_quantity#22, i_item_id#7]
Keys [1]: [i_item_id#7]
Functions [1]: [partial_sum(wr_return_quantity#22)]
Aggregate Attributes [1]: [sum#24]
Results [2]: [i_item_id#7, sum#25]

(41) Exchange
Input [2]: [i_item_id#7, sum#25]
Arguments: hashpartitioning(i_item_id#7, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(42) HashAggregate [codegen id : 17]
Input [2]: [i_item_id#7, sum#25]
Keys [1]: [i_item_id#7]
Functions [1]: [sum(wr_return_quantity#22)]
Aggregate Attributes [1]: [sum(wr_return_quantity#22)#26]
Results [2]: [i_item_id#7 AS item_id#27, sum(wr_return_quantity#22)#26 AS wr_item_qty#28]

(43) BroadcastExchange
Input [2]: [item_id#27, wr_item_qty#28]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=6]

(44) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [item_id#11]
Right keys [1]: [item_id#27]
Join type: Inner
Join condition: None

(45) Project [codegen id : 18]
Output [8]: [item_id#11, sr_item_qty#12, (((cast(sr_item_qty#12 as double) / cast(((sr_item_qty#12 + cr_item_qty#20) + wr_item_qty#28) as double)) / 3.0) * 100.0) AS sr_dev#29, cr_item_qty#20, (((cast(cr_item_qty#20 as double) / cast(((sr_item_qty#12 + cr_item_qty#20) + wr_item_qty#28) as double)) / 3.0) * 100.0) AS cr_dev#30, wr_item_qty#28, (((cast(wr_item_qty#28 as double) / cast(((sr_item_qty#12 + cr_item_qty#20) + wr_item_qty#28) as double)) / 3.0) * 100.0) AS wr_dev#31, (cast(((sr_item_qty#12 + cr_item_qty#20) + wr_item_qty#28) as decimal(20,0)) / 3.0) AS average#32]
Input [5]: [item_id#11, sr_item_qty#12, cr_item_qty#20, item_id#27, wr_item_qty#28]

(46) TakeOrderedAndProject
Input [8]: [item_id#11, sr_item_qty#12, sr_dev#29, cr_item_qty#20, cr_dev#30, wr_item_qty#28, wr_dev#31, average#32]
Arguments: 100, [item_id#11 ASC NULLS FIRST, sr_item_qty#12 ASC NULLS FIRST], [item_id#11, sr_item_qty#12, sr_dev#29, cr_item_qty#20, cr_dev#30, wr_item_qty#28, wr_dev#31, average#32]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = sr_returned_date_sk#3 IN dynamicpruning#4
BroadcastExchange (62)
+- * Project (61)
   +- * BroadcastHashJoin LeftSemi BuildRight (60)
      :- * Filter (49)
      :  +- * ColumnarToRow (48)
      :     +- Scan parquet spark_catalog.default.date_dim (47)
      +- BroadcastExchange (59)
         +- * Project (58)
            +- * BroadcastHashJoin LeftSemi BuildRight (57)
               :- * ColumnarToRow (51)
               :  +- Scan parquet spark_catalog.default.date_dim (50)
               +- BroadcastExchange (56)
                  +- * Project (55)
                     +- * Filter (54)
                        +- * ColumnarToRow (53)
                           +- Scan parquet spark_catalog.default.date_dim (52)


(47) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#5, d_date#33]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_date:date>

(48) ColumnarToRow [codegen id : 3]
Input [2]: [d_date_sk#5, d_date#33]

(49) Filter [codegen id : 3]
Input [2]: [d_date_sk#5, d_date#33]
Condition : isnotnull(d_date_sk#5)

(50) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date#34, d_week_seq#35]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
ReadSchema: struct<d_date:date,d_week_seq:int>

(51) ColumnarToRow [codegen id : 2]
Input [2]: [d_date#34, d_week_seq#35]

(52) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date#36, d_week_seq#37]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [In(d_date, [2000-06-30,2000-09-27,2000-11-17])]
ReadSchema: struct<d_date:date,d_week_seq:int>

(53) ColumnarToRow [codegen id : 1]
Input [2]: [d_date#36, d_week_seq#37]

(54) Filter [codegen id : 1]
Input [2]: [d_date#36, d_week_seq#37]
Condition : d_date#36 IN (2000-06-30,2000-09-27,2000-11-17)

(55) Project [codegen id : 1]
Output [1]: [d_week_seq#37]
Input [2]: [d_date#36, d_week_seq#37]

(56) BroadcastExchange
Input [1]: [d_week_seq#37]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=7]

(57) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [d_week_seq#35]
Right keys [1]: [d_week_seq#37]
Join type: LeftSemi
Join condition: None

(58) Project [codegen id : 2]
Output [1]: [d_date#34]
Input [2]: [d_date#34, d_week_seq#35]

(59) BroadcastExchange
Input [1]: [d_date#34]
Arguments: HashedRelationBroadcastMode(List(input[0, date, true]),false), [plan_id=8]

(60) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [d_date#33]
Right keys [1]: [d_date#34]
Join type: LeftSemi
Join condition: None

(61) Project [codegen id : 3]
Output [1]: [d_date_sk#5]
Input [2]: [d_date_sk#5, d_date#33]

(62) BroadcastExchange
Input [1]: [d_date_sk#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=9]

Subquery:2 Hosting operator id = 16 Hosting Expression = cr_returned_date_sk#15 IN dynamicpruning#4

Subquery:3 Hosting operator id = 31 Hosting Expression = wr_returned_date_sk#23 IN dynamicpruning#4


