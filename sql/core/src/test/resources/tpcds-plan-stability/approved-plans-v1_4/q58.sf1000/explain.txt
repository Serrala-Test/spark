== Physical Plan ==
TakeOrderedAndProject (58)
+- * Project (57)
   +- * BroadcastHashJoin Inner BuildRight (56)
      :- * Project (38)
      :  +- * BroadcastHashJoin Inner BuildRight (37)
      :     :- * Filter (19)
      :     :  +- * HashAggregate (18)
      :     :     +- Exchange (17)
      :     :        +- * HashAggregate (16)
      :     :           +- * Project (15)
      :     :              +- * SortMergeJoin Inner (14)
      :     :                 :- * Sort (8)
      :     :                 :  +- Exchange (7)
      :     :                 :     +- * Project (6)
      :     :                 :        +- * BroadcastHashJoin Inner BuildRight (5)
      :     :                 :           :- * Filter (3)
      :     :                 :           :  +- * ColumnarToRow (2)
      :     :                 :           :     +- Scan parquet spark_catalog.default.store_sales (1)
      :     :                 :           +- ReusedExchange (4)
      :     :                 +- * Sort (13)
      :     :                    +- Exchange (12)
      :     :                       +- * Filter (11)
      :     :                          +- * ColumnarToRow (10)
      :     :                             +- Scan parquet spark_catalog.default.item (9)
      :     +- BroadcastExchange (36)
      :        +- * Filter (35)
      :           +- * HashAggregate (34)
      :              +- Exchange (33)
      :                 +- * HashAggregate (32)
      :                    +- * Project (31)
      :                       +- * SortMergeJoin Inner (30)
      :                          :- * Sort (27)
      :                          :  +- Exchange (26)
      :                          :     +- * Project (25)
      :                          :        +- * BroadcastHashJoin Inner BuildRight (24)
      :                          :           :- * Filter (22)
      :                          :           :  +- * ColumnarToRow (21)
      :                          :           :     +- Scan parquet spark_catalog.default.catalog_sales (20)
      :                          :           +- ReusedExchange (23)
      :                          +- * Sort (29)
      :                             +- ReusedExchange (28)
      +- BroadcastExchange (55)
         +- * Filter (54)
            +- * HashAggregate (53)
               +- Exchange (52)
                  +- * HashAggregate (51)
                     +- * Project (50)
                        +- * SortMergeJoin Inner (49)
                           :- * Sort (46)
                           :  +- Exchange (45)
                           :     +- * Project (44)
                           :        +- * BroadcastHashJoin Inner BuildRight (43)
                           :           :- * Filter (41)
                           :           :  +- * ColumnarToRow (40)
                           :           :     +- Scan parquet spark_catalog.default.web_sales (39)
                           :           +- ReusedExchange (42)
                           +- * Sort (48)
                              +- ReusedExchange (47)


(1) Scan parquet spark_catalog.default.store_sales
Output [3]: [ss_item_sk#1, ss_ext_sales_price#2, ss_sold_date_sk#3]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#3), dynamicpruningexpression(ss_sold_date_sk#3 IN dynamicpruning#4)]
PushedFilters: [IsNotNull(ss_item_sk)]
ReadSchema: struct<ss_item_sk:int,ss_ext_sales_price:decimal(7,2)>

(2) ColumnarToRow [codegen id : 3]
Input [3]: [ss_item_sk#1, ss_ext_sales_price#2, ss_sold_date_sk#3]

(3) Filter [codegen id : 3]
Input [3]: [ss_item_sk#1, ss_ext_sales_price#2, ss_sold_date_sk#3]
Condition : isnotnull(ss_item_sk#1)

(4) ReusedExchange [Reuses operator id: 69]
Output [1]: [d_date_sk#5]

(5) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_sold_date_sk#3]
Right keys [1]: [d_date_sk#5]
Join type: Inner
Join condition: None

(6) Project [codegen id : 3]
Output [2]: [ss_item_sk#1, ss_ext_sales_price#2]
Input [4]: [ss_item_sk#1, ss_ext_sales_price#2, ss_sold_date_sk#3, d_date_sk#5]

(7) Exchange
Input [2]: [ss_item_sk#1, ss_ext_sales_price#2]
Arguments: hashpartitioning(ss_item_sk#1, 5), ENSURE_REQUIREMENTS, [plan_id=1]

(8) Sort [codegen id : 4]
Input [2]: [ss_item_sk#1, ss_ext_sales_price#2]
Arguments: [ss_item_sk#1 ASC NULLS FIRST], false, 0

(9) Scan parquet spark_catalog.default.item
Output [2]: [i_item_sk#6, i_item_id#7]
Batched: true
Location [not included in comparison]/{warehouse_dir}/item]
PushedFilters: [IsNotNull(i_item_sk), IsNotNull(i_item_id)]
ReadSchema: struct<i_item_sk:int,i_item_id:string>

(10) ColumnarToRow [codegen id : 5]
Input [2]: [i_item_sk#6, i_item_id#7]

(11) Filter [codegen id : 5]
Input [2]: [i_item_sk#6, i_item_id#7]
Condition : (isnotnull(i_item_sk#6) AND isnotnull(i_item_id#7))

(12) Exchange
Input [2]: [i_item_sk#6, i_item_id#7]
Arguments: hashpartitioning(i_item_sk#6, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(13) Sort [codegen id : 6]
Input [2]: [i_item_sk#6, i_item_id#7]
Arguments: [i_item_sk#6 ASC NULLS FIRST], false, 0

(14) SortMergeJoin [codegen id : 7]
Left keys [1]: [ss_item_sk#1]
Right keys [1]: [i_item_sk#6]
Join type: Inner
Join condition: None

(15) Project [codegen id : 7]
Output [2]: [ss_ext_sales_price#2, i_item_id#7]
Input [4]: [ss_item_sk#1, ss_ext_sales_price#2, i_item_sk#6, i_item_id#7]

(16) HashAggregate [codegen id : 7]
Input [2]: [ss_ext_sales_price#2, i_item_id#7]
Keys [1]: [i_item_id#7]
Functions [1]: [partial_sum(UnscaledValue(ss_ext_sales_price#2))]
Aggregate Attributes [1]: [sum#8]
Results [2]: [i_item_id#7, sum#9]

(17) Exchange
Input [2]: [i_item_id#7, sum#9]
Arguments: hashpartitioning(i_item_id#7, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(18) HashAggregate [codegen id : 24]
Input [2]: [i_item_id#7, sum#9]
Keys [1]: [i_item_id#7]
Functions [1]: [sum(UnscaledValue(ss_ext_sales_price#2))]
Aggregate Attributes [1]: [sum(UnscaledValue(ss_ext_sales_price#2))#10]
Results [2]: [i_item_id#7 AS item_id#11, MakeDecimal(sum(UnscaledValue(ss_ext_sales_price#2))#10,17,2) AS ss_item_rev#12]

(19) Filter [codegen id : 24]
Input [2]: [item_id#11, ss_item_rev#12]
Condition : isnotnull(ss_item_rev#12)

(20) Scan parquet spark_catalog.default.catalog_sales
Output [3]: [cs_item_sk#13, cs_ext_sales_price#14, cs_sold_date_sk#15]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#15), dynamicpruningexpression(cs_sold_date_sk#15 IN dynamicpruning#4)]
PushedFilters: [IsNotNull(cs_item_sk)]
ReadSchema: struct<cs_item_sk:int,cs_ext_sales_price:decimal(7,2)>

(21) ColumnarToRow [codegen id : 10]
Input [3]: [cs_item_sk#13, cs_ext_sales_price#14, cs_sold_date_sk#15]

(22) Filter [codegen id : 10]
Input [3]: [cs_item_sk#13, cs_ext_sales_price#14, cs_sold_date_sk#15]
Condition : isnotnull(cs_item_sk#13)

(23) ReusedExchange [Reuses operator id: 69]
Output [1]: [d_date_sk#16]

(24) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [cs_sold_date_sk#15]
Right keys [1]: [d_date_sk#16]
Join type: Inner
Join condition: None

(25) Project [codegen id : 10]
Output [2]: [cs_item_sk#13, cs_ext_sales_price#14]
Input [4]: [cs_item_sk#13, cs_ext_sales_price#14, cs_sold_date_sk#15, d_date_sk#16]

(26) Exchange
Input [2]: [cs_item_sk#13, cs_ext_sales_price#14]
Arguments: hashpartitioning(cs_item_sk#13, 5), ENSURE_REQUIREMENTS, [plan_id=4]

(27) Sort [codegen id : 11]
Input [2]: [cs_item_sk#13, cs_ext_sales_price#14]
Arguments: [cs_item_sk#13 ASC NULLS FIRST], false, 0

(28) ReusedExchange [Reuses operator id: 12]
Output [2]: [i_item_sk#17, i_item_id#18]

(29) Sort [codegen id : 13]
Input [2]: [i_item_sk#17, i_item_id#18]
Arguments: [i_item_sk#17 ASC NULLS FIRST], false, 0

(30) SortMergeJoin [codegen id : 14]
Left keys [1]: [cs_item_sk#13]
Right keys [1]: [i_item_sk#17]
Join type: Inner
Join condition: None

(31) Project [codegen id : 14]
Output [2]: [cs_ext_sales_price#14, i_item_id#18]
Input [4]: [cs_item_sk#13, cs_ext_sales_price#14, i_item_sk#17, i_item_id#18]

(32) HashAggregate [codegen id : 14]
Input [2]: [cs_ext_sales_price#14, i_item_id#18]
Keys [1]: [i_item_id#18]
Functions [1]: [partial_sum(UnscaledValue(cs_ext_sales_price#14))]
Aggregate Attributes [1]: [sum#19]
Results [2]: [i_item_id#18, sum#20]

(33) Exchange
Input [2]: [i_item_id#18, sum#20]
Arguments: hashpartitioning(i_item_id#18, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(34) HashAggregate [codegen id : 15]
Input [2]: [i_item_id#18, sum#20]
Keys [1]: [i_item_id#18]
Functions [1]: [sum(UnscaledValue(cs_ext_sales_price#14))]
Aggregate Attributes [1]: [sum(UnscaledValue(cs_ext_sales_price#14))#21]
Results [2]: [i_item_id#18 AS item_id#22, MakeDecimal(sum(UnscaledValue(cs_ext_sales_price#14))#21,17,2) AS cs_item_rev#23]

(35) Filter [codegen id : 15]
Input [2]: [item_id#22, cs_item_rev#23]
Condition : isnotnull(cs_item_rev#23)

(36) BroadcastExchange
Input [2]: [item_id#22, cs_item_rev#23]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=6]

(37) BroadcastHashJoin [codegen id : 24]
Left keys [1]: [item_id#11]
Right keys [1]: [item_id#22]
Join type: Inner
Join condition: ((((cast(ss_item_rev#12 as decimal(19,3)) >= (0.9 * cs_item_rev#23)) AND (cast(ss_item_rev#12 as decimal(20,3)) <= (1.1 * cs_item_rev#23))) AND (cast(cs_item_rev#23 as decimal(19,3)) >= (0.9 * ss_item_rev#12))) AND (cast(cs_item_rev#23 as decimal(20,3)) <= (1.1 * ss_item_rev#12)))

(38) Project [codegen id : 24]
Output [3]: [item_id#11, ss_item_rev#12, cs_item_rev#23]
Input [4]: [item_id#11, ss_item_rev#12, item_id#22, cs_item_rev#23]

(39) Scan parquet spark_catalog.default.web_sales
Output [3]: [ws_item_sk#24, ws_ext_sales_price#25, ws_sold_date_sk#26]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ws_sold_date_sk#26), dynamicpruningexpression(ws_sold_date_sk#26 IN dynamicpruning#4)]
PushedFilters: [IsNotNull(ws_item_sk)]
ReadSchema: struct<ws_item_sk:int,ws_ext_sales_price:decimal(7,2)>

(40) ColumnarToRow [codegen id : 18]
Input [3]: [ws_item_sk#24, ws_ext_sales_price#25, ws_sold_date_sk#26]

(41) Filter [codegen id : 18]
Input [3]: [ws_item_sk#24, ws_ext_sales_price#25, ws_sold_date_sk#26]
Condition : isnotnull(ws_item_sk#24)

(42) ReusedExchange [Reuses operator id: 69]
Output [1]: [d_date_sk#27]

(43) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ws_sold_date_sk#26]
Right keys [1]: [d_date_sk#27]
Join type: Inner
Join condition: None

(44) Project [codegen id : 18]
Output [2]: [ws_item_sk#24, ws_ext_sales_price#25]
Input [4]: [ws_item_sk#24, ws_ext_sales_price#25, ws_sold_date_sk#26, d_date_sk#27]

(45) Exchange
Input [2]: [ws_item_sk#24, ws_ext_sales_price#25]
Arguments: hashpartitioning(ws_item_sk#24, 5), ENSURE_REQUIREMENTS, [plan_id=7]

(46) Sort [codegen id : 19]
Input [2]: [ws_item_sk#24, ws_ext_sales_price#25]
Arguments: [ws_item_sk#24 ASC NULLS FIRST], false, 0

(47) ReusedExchange [Reuses operator id: 12]
Output [2]: [i_item_sk#28, i_item_id#29]

(48) Sort [codegen id : 21]
Input [2]: [i_item_sk#28, i_item_id#29]
Arguments: [i_item_sk#28 ASC NULLS FIRST], false, 0

(49) SortMergeJoin [codegen id : 22]
Left keys [1]: [ws_item_sk#24]
Right keys [1]: [i_item_sk#28]
Join type: Inner
Join condition: None

(50) Project [codegen id : 22]
Output [2]: [ws_ext_sales_price#25, i_item_id#29]
Input [4]: [ws_item_sk#24, ws_ext_sales_price#25, i_item_sk#28, i_item_id#29]

(51) HashAggregate [codegen id : 22]
Input [2]: [ws_ext_sales_price#25, i_item_id#29]
Keys [1]: [i_item_id#29]
Functions [1]: [partial_sum(UnscaledValue(ws_ext_sales_price#25))]
Aggregate Attributes [1]: [sum#30]
Results [2]: [i_item_id#29, sum#31]

(52) Exchange
Input [2]: [i_item_id#29, sum#31]
Arguments: hashpartitioning(i_item_id#29, 5), ENSURE_REQUIREMENTS, [plan_id=8]

(53) HashAggregate [codegen id : 23]
Input [2]: [i_item_id#29, sum#31]
Keys [1]: [i_item_id#29]
Functions [1]: [sum(UnscaledValue(ws_ext_sales_price#25))]
Aggregate Attributes [1]: [sum(UnscaledValue(ws_ext_sales_price#25))#32]
Results [2]: [i_item_id#29 AS item_id#33, MakeDecimal(sum(UnscaledValue(ws_ext_sales_price#25))#32,17,2) AS ws_item_rev#34]

(54) Filter [codegen id : 23]
Input [2]: [item_id#33, ws_item_rev#34]
Condition : isnotnull(ws_item_rev#34)

(55) BroadcastExchange
Input [2]: [item_id#33, ws_item_rev#34]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=9]

(56) BroadcastHashJoin [codegen id : 24]
Left keys [1]: [item_id#11]
Right keys [1]: [item_id#33]
Join type: Inner
Join condition: ((((((((cast(ss_item_rev#12 as decimal(19,3)) >= (0.9 * ws_item_rev#34)) AND (cast(ss_item_rev#12 as decimal(20,3)) <= (1.1 * ws_item_rev#34))) AND (cast(cs_item_rev#23 as decimal(19,3)) >= (0.9 * ws_item_rev#34))) AND (cast(cs_item_rev#23 as decimal(20,3)) <= (1.1 * ws_item_rev#34))) AND (cast(ws_item_rev#34 as decimal(19,3)) >= (0.9 * ss_item_rev#12))) AND (cast(ws_item_rev#34 as decimal(20,3)) <= (1.1 * ss_item_rev#12))) AND (cast(ws_item_rev#34 as decimal(19,3)) >= (0.9 * cs_item_rev#23))) AND (cast(ws_item_rev#34 as decimal(20,3)) <= (1.1 * cs_item_rev#23)))

(57) Project [codegen id : 24]
Output [8]: [item_id#11, ss_item_rev#12, (((ss_item_rev#12 / ((ss_item_rev#12 + cs_item_rev#23) + ws_item_rev#34)) / 3) * 100) AS ss_dev#35, cs_item_rev#23, (((cs_item_rev#23 / ((ss_item_rev#12 + cs_item_rev#23) + ws_item_rev#34)) / 3) * 100) AS cs_dev#36, ws_item_rev#34, (((ws_item_rev#34 / ((ss_item_rev#12 + cs_item_rev#23) + ws_item_rev#34)) / 3) * 100) AS ws_dev#37, (((ss_item_rev#12 + cs_item_rev#23) + ws_item_rev#34) / 3) AS average#38]
Input [5]: [item_id#11, ss_item_rev#12, cs_item_rev#23, item_id#33, ws_item_rev#34]

(58) TakeOrderedAndProject
Input [8]: [item_id#11, ss_item_rev#12, ss_dev#35, cs_item_rev#23, cs_dev#36, ws_item_rev#34, ws_dev#37, average#38]
Arguments: 100, [item_id#11 ASC NULLS FIRST, ss_item_rev#12 ASC NULLS FIRST], [item_id#11, ss_item_rev#12, ss_dev#35, cs_item_rev#23, cs_dev#36, ws_item_rev#34, ws_dev#37, average#38]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#3 IN dynamicpruning#4
BroadcastExchange (69)
+- * Project (68)
   +- * BroadcastHashJoin LeftSemi BuildRight (67)
      :- * Filter (61)
      :  +- * ColumnarToRow (60)
      :     +- Scan parquet spark_catalog.default.date_dim (59)
      +- BroadcastExchange (66)
         +- * Project (65)
            +- * Filter (64)
               +- * ColumnarToRow (63)
                  +- Scan parquet spark_catalog.default.date_dim (62)


(59) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#5, d_date#39]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_date:date>

(60) ColumnarToRow [codegen id : 2]
Input [2]: [d_date_sk#5, d_date#39]

(61) Filter [codegen id : 2]
Input [2]: [d_date_sk#5, d_date#39]
Condition : isnotnull(d_date_sk#5)

(62) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date#40, d_week_seq#41]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_week_seq)]
ReadSchema: struct<d_date:date,d_week_seq:int>

(63) ColumnarToRow [codegen id : 1]
Input [2]: [d_date#40, d_week_seq#41]

(64) Filter [codegen id : 1]
Input [2]: [d_date#40, d_week_seq#41]
Condition : (isnotnull(d_week_seq#41) AND (d_week_seq#41 = Subquery scalar-subquery#42, [id=#43]))

(65) Project [codegen id : 1]
Output [1]: [d_date#40]
Input [2]: [d_date#40, d_week_seq#41]

(66) BroadcastExchange
Input [1]: [d_date#40]
Arguments: HashedRelationBroadcastMode(List(input[0, date, true]),false), [plan_id=10]

(67) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [d_date#39]
Right keys [1]: [d_date#40]
Join type: LeftSemi
Join condition: None

(68) Project [codegen id : 2]
Output [1]: [d_date_sk#5]
Input [2]: [d_date_sk#5, d_date#39]

(69) BroadcastExchange
Input [1]: [d_date_sk#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=11]

Subquery:2 Hosting operator id = 64 Hosting Expression = Subquery scalar-subquery#42, [id=#43]
* Project (73)
+- * Filter (72)
   +- * ColumnarToRow (71)
      +- Scan parquet spark_catalog.default.date_dim (70)


(70) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date#44, d_week_seq#45]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date), EqualTo(d_date,2000-01-03)]
ReadSchema: struct<d_date:date,d_week_seq:int>

(71) ColumnarToRow [codegen id : 1]
Input [2]: [d_date#44, d_week_seq#45]

(72) Filter [codegen id : 1]
Input [2]: [d_date#44, d_week_seq#45]
Condition : (isnotnull(d_date#44) AND (d_date#44 = 2000-01-03))

(73) Project [codegen id : 1]
Output [1]: [d_week_seq#45]
Input [2]: [d_date#44, d_week_seq#45]

Subquery:3 Hosting operator id = 20 Hosting Expression = cs_sold_date_sk#15 IN dynamicpruning#4

Subquery:4 Hosting operator id = 39 Hosting Expression = ws_sold_date_sk#26 IN dynamicpruning#4


