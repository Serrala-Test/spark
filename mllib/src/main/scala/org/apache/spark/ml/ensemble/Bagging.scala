/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.ml.ensemble

import org.apache.spark.Logging
import org.apache.spark.annotation.Experimental
import org.apache.spark.ml._
import org.apache.spark.ml.param._
import org.apache.spark.ml.param.shared.{HasPredictionCol, HasSeed}
import org.apache.spark.ml.util.Identifiable
import org.apache.spark.sql.{Row, DataFrame}
import org.apache.spark.sql.functions.{col, udf}
import org.apache.spark.sql.types.{LongType, StructField, StructType}

import scala.util.Random

/**
 * Params for [[Bagging]] and [[BaggingModel]].
 */
private[ml] trait BaggingParams[M <: Model[M] with PredictorParams]
  extends Params with HasSeed {
  /**
   * Param for the [[estimator]] to be validated.
   * @group param
   */
  val estimator: Param[Estimator[M]] = new Param(this, "estimator", "estimator for bagging")

  /** @group getParam */
  def getEstimator: Estimator[M] = $(estimator)

  /**
   * Param for indicating whether bagged model is a classifier (true) or regressor (false).
   * This parameter affects how models are aggregated: voting is used for classification (with ties
   * broken arbitrarily) and averaging is used for regression.
   * Default: true (classification)
   * @group param
   */
  val isClassifier: BooleanParam = new BooleanParam(this, "isClassification",
    "indicates if bagged model is a classifier or regressor")

  /** @group getParam */
  def getIsClassifier: Boolean = $(isClassifier)

  /**
   * Param for number of bootstraped models.
   * Default: 3
   * @group param
   */
  val numModels: IntParam = new IntParam(this, "numModels",
    "number of models to train on bootstrapped samples (>=1)", ParamValidators.gtEq(1))

  /** @group getParam */
  def getNumModels: Int = $(numModels)

  setDefault(numModels-> 3, isClassifier->true)
}

/**
 * :: Experimental ::
 * Trains an ensemble of models using bootstrap aggregation. Given a dataset with N points,
 * the traditional bootstrap sample consists of N points sampled with replacement from the original
 * dataset. This class generates `numModels` bootstrap samples and uses `estimator` to train a model
 * on each sample. The predictions generated by the trained models are then aggregated to generate
 * the ensemble prediction.
 */
@Experimental
class Bagging[M <: PredictionModel[_,M]](override val uid: String)
  extends Estimator[BaggingModel[M]] with BaggingParams[M] with Logging {

  def this() = this(Identifiable.randomUID("bagging"))

  /** @group setParam */
  def setSeed(value: Long): this.type = set(seed, value)

  /** @group setParam */
  def setEstimator(value: Estimator[M]): this.type = set(estimator, value)

  /** @group setParam */
  def setNumModels(value: Int): this.type = set(numModels, value)

  /** @group setParam */
  def setIsClassifier(value: Boolean): this.type = set(isClassifier, value)

  override def fit(dataset: DataFrame): BaggingModel[M] = {
    Random.setSeed($(seed))
    val models = (0 until $(numModels)).map { _ =>
      val bootstrapSample = dataset.sample(true, 1.0, Random.nextLong())
      $(estimator).fit(bootstrapSample)
    }
    copyValues(new BaggingModel[M](uid, models).setParent(this))
  }

  override def transformSchema(schema: StructType): StructType = {
    $(estimator).transformSchema(schema)
  }

  override def copy(extra: ParamMap): Bagging[M] = {
    val copied = defaultCopy(extra).asInstanceOf[Bagging[M]]
    if (copied.isDefined(estimator)) {
      copied.setEstimator(copied.getEstimator.copy(extra))
    }
    copied
  }
}

/**
 * :: Experimental ::
 * Model from bootstrap aggregating (bagging).
 *
 * TODO: type-safe way to ensure models has at least one
 */
@Experimental
class BaggingModel[M <: PredictionModel[_,M]] private[ml] (
    override val uid: String,
    val models: Seq[M])
    extends Model[BaggingModel[M]] with BaggingParams[M] {

  assert(models.size > 0,
    s"BaggingModel requires > 0 models to aggregate over, got ${models.size}")

  override def transform(dataset: DataFrame): DataFrame = {
    transformSchema(dataset.schema, logging = true)

    // these are constant across models since the estimator unchanged
    val predictionCol = models.head.getPredictionCol // head is safe because models.size > 0
    val instanceIdCol = "instanceId"
    val modelIdCol = "modelId"
    val predictionCountsCol = "predCounts"

    val numberedDataset = Bagging.dfZipWithIndex(dataset, colName = instanceIdCol)
    val predictions = models.zipWithIndex.map { case (model, modelId: Int) =>
      val toModelId = udf { (x: Any) => modelId }
      // cast is required because of erasure
      model.asInstanceOf[M].transform(numberedDataset)
        .withColumn(modelIdCol, toModelId(col(instanceIdCol)))
    }.reduce[DataFrame] { case (a: DataFrame, b: DataFrame) =>
      a.unionAll(b)
    }
    val aggregatedPrediction = if (this.getIsClassifier) { // aggregate by voting
      // counts number of models voting for each (instance, prediction) pair
      val predictionCounts = predictions
        .groupBy(instanceIdCol, predictionCol)
        .agg(modelIdCol -> "count")
        .withColumnRenamed("count(" + modelIdCol + ")", predictionCountsCol)

      // gets the counts for the most predicted prediction
      val maxPredictionCounts = predictionCounts
        .groupBy(instanceIdCol)
        .agg(predictionCountsCol -> "max")
        .withColumnRenamed("max(" + predictionCountsCol + ")", predictionCountsCol)

      // join and project to recover actual prediction
      maxPredictionCounts
        .join(predictionCounts, Seq(instanceIdCol, predictionCountsCol))
        .drop(predictionCountsCol)
    } else { // aggregate by averaging
      predictions.groupBy(instanceIdCol)
        .agg(predictionCol -> "avg")
        .withColumnRenamed("avg(" + predictionCol + ")", predictionCol)
    }

    numberedDataset.join(aggregatedPrediction, instanceIdCol).drop(instanceIdCol)
  }

  override def transformSchema(schema: StructType): StructType = {
    models.head.transformSchema(schema)
  }

  override def copy(extra: ParamMap): BaggingModel[M] = {
    val copied = new BaggingModel[M](
      uid,
      models.map(_.copy(extra)))
    copyValues(copied, extra).setParent(parent)
  }
}

private object Bagging {
 def dfZipWithIndex(
    df: DataFrame,
    offset: Int = 1,
    colName: String = "id",
     inFront: Boolean = true) : DataFrame = {
   df.sqlContext.createDataFrame(
     df.rdd.zipWithIndex.map(ln =>
       Row.fromSeq(
         (if (inFront) Seq(ln._2 + offset) else Seq())
           ++ ln._1.toSeq ++
           (if (inFront) Seq() else Seq(ln._2 + offset))
       )
     ),
     StructType(
       (if (inFront) Array(StructField(colName,LongType,false)) else Array[StructField]())
         ++ df.schema.fields ++
         (if (inFront) Array[StructField]() else Array(StructField(colName,LongType,false)))
     )
   )
 }
}
