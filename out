[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Spark Project Parent POM
[INFO] Spark Project Core
[INFO] Spark Project Bagel
[INFO] Spark Project GraphX
[INFO] Spark Project ML Library
[INFO] Spark Project Streaming
[INFO] Spark Project Tools
[INFO] Spark Project Catalyst
[INFO] Spark Project SQL
[INFO] Spark Project Hive
[INFO] Spark Project REPL
[INFO] Spark Project YARN Parent POM
[INFO] Spark Project YARN Stable API
[INFO] Spark Project Assembly
[INFO] Spark Project External Twitter
[INFO] Spark Project External Kafka
[INFO] Spark Project External Flume Sink
[INFO] Spark Project External Flume
[INFO] Spark Project External ZeroMQ
[INFO] Spark Project External MQTT
[INFO] Spark Project Examples
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Parent POM 1.1.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @ spark-parent ---
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-parent ---
[INFO] Source directory: /shared/strlen/src/main/scala added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-parent ---
[INFO] 
[INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @ spark-parent ---
[INFO] No sources to compile
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-test-source (add-scala-test-sources) @ spark-parent ---
[INFO] Test Source directory: /shared/strlen/src/test/scala added.
[INFO] 
[INFO] --- scala-maven-plugin:3.1.6:testCompile (scala-test-compile-first) @ spark-parent ---
[INFO] No sources to compile
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Core 1.1.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @ spark-core_2.10 ---
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-core_2.10 ---
[INFO] Source directory: /shared/strlen/core/src/main/scala added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-core_2.10 ---
[INFO] 
[INFO] --- exec-maven-plugin:1.2.1:exec (default) @ spark-core_2.10 ---
Archive:  lib/py4j-0.8.2.1-src.zip
 extracting: build/py4j/__init__.py  
  inflating: build/py4j/compat.py    
  inflating: build/py4j/finalizer.py  
  inflating: build/py4j/java_collections.py  
  inflating: build/py4j/java_gateway.py  
  inflating: build/py4j/protocol.py  
 extracting: build/py4j/tests/__init__.py  
  inflating: build/py4j/tests/byte_string_test.py  
  inflating: build/py4j/tests/finalizer_test.py  
  inflating: build/py4j/tests/java_array_test.py  
  inflating: build/py4j/tests/java_callback_test.py  
  inflating: build/py4j/tests/java_gateway_test.py  
  inflating: build/py4j/tests/java_list_test.py  
  inflating: build/py4j/tests/java_map_test.py  
  inflating: build/py4j/tests/java_set_test.py  
  inflating: build/py4j/tests/multithreadtest.py  
  inflating: build/py4j/tests/py4j_callback_example.py  
  inflating: build/py4j/tests/py4j_callback_example2.py  
  inflating: build/py4j/tests/py4j_example.py  
  inflating: build/py4j/version.py   
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ spark-core_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 9 resources
[INFO] Copying 21 resources
[INFO] Copying 7 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @ spark-core_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[0m[[0minfo[0m] [0mCompile success at Aug 1, 2014 7:55:25 PM [0.258s][0m
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ spark-core_2.10 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 35 source files to /shared/strlen/core/target/scala-2.10/classes
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-test-source (add-scala-test-sources) @ spark-core_2.10 ---
[INFO] Test Source directory: /shared/strlen/core/src/test/scala added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ spark-core_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.1.6:testCompile (scala-test-compile-first) @ spark-core_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[0m[[0minfo[0m] [0mCompiling 1 Scala source and 1 Java source to /shared/strlen/core/target/scala-2.10/test-classes...[0m
[0m[[33mwarn[0m] [0mNote: /shared/strlen/core/src/test/java/org/apache/spark/JavaAPISuite.java uses or overrides a deprecated API.[0m
[0m[[33mwarn[0m] [0mNote: Recompile with -Xlint:deprecation for details.[0m
[0m[[33mwarn[0m] [0mNote: /shared/strlen/core/src/test/java/org/apache/spark/JavaAPISuite.java uses unchecked or unsafe operations.[0m
[0m[[33mwarn[0m] [0mNote: Recompile with -Xlint:unchecked for details.[0m
[0m[[0minfo[0m] [0mCompile success at Aug 1, 2014 7:55:32 PM [4.592s][0m
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ spark-core_2.10 ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.17:test (default-test) @ spark-core_2.10 ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- scalatest-maven-plugin:1.0-RC2:test (test) @ spark-core_2.10 ---
[36mDiscovery starting.[0m
[36mDiscovery completed in 6 seconds, 462 milliseconds.[0m
[36mRun starting. Expected test count is: 724[0m
[32mExternalSorterSuite:[0m
[32m- empty data stream[0m
[32m- few elements per partition[0m
[32m- empty partitions with spilling[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- spilling in local cluster[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- spilling in local cluster with many reduce tasks[0m
[32m- cleanup of intermediate files in sorter[0m
[32m- cleanup of intermediate files in sorter if there are errors[0m
[32m- cleanup of intermediate files in shuffle[0m
[32m- cleanup of intermediate files in shuffle with errors[0m
[32m- no partial aggregation or sorting[0m
[32m- partial aggregation without spill[0m
[32m- partial aggregation with spill, no ordering[0m
[32m- partial aggregation with spill, with ordering[0m
[32m- sorting without aggregation, no spill[0m
[32m- sorting without aggregation, with spill[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- spilling with hash collisions[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- spilling with many hash collisions[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- spilling with hash collisions using the Int.MaxValue key[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- spilling with null keys and values[0m
[32mDAGSchedulerSuite:[0m
[32m- zero split job[0m
[32m- run trivial job[0m
[32m- local job[0m
[32m- local job oom[0m
[32m- run trivial job w/ dependency[0m
[32m- cache location preferences w/ dependency[0m
[32m- avoid exponential blowup when getting preferred locs list[0m
[32m- unserializable task[0m
[32m- trivial job failure[0m
[32m- trivial job cancellation[0m
[32m- job cancellation no-kill backend[0m
[32m- run trivial shuffle[0m
[32m- run trivial shuffle with fetch failure[0m
[32m- ignore late map task completions[0m
[32m- run shuffle with map stage failure[0m
[32m- failure of stage used by two jobs[0m
[32m- run trivial shuffle with out-of-band failure and retry[0m
[32m- recursive shuffle failures[0m
[32m- cached post-shuffle[0m
[33m- misbehaved accumulator should not crash DAGScheduler and SparkContext !!! IGNORED !!![0m
[32m- misbehaved resultHandler should not crash DAGScheduler and SparkContext[0m
[ERROR] [08/01/2014 19:57:21.310] [test-akka.actor.default-dispatcher-3] [akka://test/user/dagSupervisor/$a] error
org.apache.spark.SparkException: error
	at org.apache.spark.scheduler.BuggyDAGEventProcessActor$$anonfun$receive$1.applyOrElse(DAGSchedulerSuite.scala:39)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

[32m- DAGSchedulerActorSupervisor closes the SparkContext when EventProcessActor crashes[0m
[ERROR] [08/01/2014 19:57:21.321] [DAGSchedulerSuite-akka.actor.default-dispatcher-3] [akka://DAGSchedulerSuite/user/$$a] Job cancelled because SparkContext was shut down
org.apache.spark.SparkException: Job cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:688)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:687)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:687)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.postStop(DAGScheduler.scala:1342)
	at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:201)
	at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:163)
	at akka.actor.ActorCell.terminate(ActorCell.scala:338)
	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431)
	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)
	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)
	at akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:244)
	at akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:284)
	at akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:192)
	at akka.actor.dungeon.Dispatch$class.stop(Dispatch.scala:106)
	at akka.actor.ActorCell.stop(ActorCell.scala:338)
	at akka.actor.LocalActorRef.stop(ActorRef.scala:340)
	at akka.actor.dungeon.Children$class.stop(Children.scala:66)
	at akka.actor.ActorCell.stop(ActorCell.scala:338)
	at akka.actor.dungeon.FaultHandling$$anonfun$terminate$1.apply(FaultHandling.scala:149)
	at akka.actor.dungeon.FaultHandling$$anonfun$terminate$1.apply(FaultHandling.scala:149)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at akka.util.Collections$PartialImmutableValuesIterable$$anon$1.foreach(Collections.scala:27)
	at akka.util.Collections$PartialImmutableValuesIterable.foreach(Collections.scala:52)
	at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:149)
	at akka.actor.ActorCell.terminate(ActorCell.scala:338)
	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431)
	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)
	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)
	at akka.dispatch.Mailbox.run(Mailbox.scala:218)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

[32mRDDSuite:[0m
[32m- basic operations[0m
[32m- serialization[0m
[32m- countApproxDistinct[0m
[32m- SparkContext.union[0m
[32m- partitioner aware union[0m
[32m- UnionRDD partition serialized size should be small[0m
[32m- aggregate[0m
[32m- basic caching[0m
[32m- caching with failures[0m
[32m- empty RDD[0m
[32m- repartitioned RDDs[0m
[32m- repartitioned RDDs perform load balancing[0m
[32m- coalesced RDDs[0m
[32m- coalesced RDDs with locality[0m
[32m- coalesced RDDs with locality, large scale (10K partitions)[0m
[32m- coalesced RDDs with locality, fail first pass[0m
[32m- zipped RDDs[0m
[32m- partition pruning[0m
[32m- mapWith[0m
[32m- flatMapWith[0m
[32m- filterWith[0m
[32m- take[0m
[32m- top with predefined ordering[0m
[32m- top with custom ordering[0m
[32m- takeOrdered with predefined ordering[0m
[32m- takeOrdered with custom ordering[0m
[32m- sample preserves partitioner[0m
[32m- takeSample[0m
[32m- takeSample from an empty rdd[0m
[32m- randomSplit[0m
[32m- runJob on an invalid partition[0m
[32m- sort an empty RDD[0m
[32m- sortByKey[0m
[32m- sortByKey ascending parameter[0m
[32m- sortByKey with explicit ordering[0m
[32m- intersection[0m
[32m- intersection strips duplicates in an input[0m
[32m- zipWithIndex[0m
[32m- zipWithIndex with a single partition[0m
[32m- zipWithUniqueId[0m
[32m- retag with implicit ClassTag[0m
[32m- getNarrowAncestors[0m
[32m- getNarrowAncestors with multiple parents[0m
[32m- getNarrowAncestors with cycles[0m
[32mUtilsSuite:[0m
[32m- bytesToString[0m
[32m- copyStream[0m
[32m- memoryStringToMb[0m
[32m- splitCommandString[0m
[32m- string formatting of time durations[0m
[32m- reading offset bytes of a file[0m
[32m- reading offset bytes across multiple files[0m
[32m- deserialize long value[0m
[32m- get iterator size[0m
[32m- findOldFiles[0m
[32m- resolveURI[0m
[32m- nonLocalPaths[0m
[32mSortingSuite:[0m
[32m- sortByKey[0m
[32m- large array[0m
[32m- large array with one split[0m
[32m- large array with many partitions[0m
[32m- sort descending[0m
[32m- sort descending with one split[0m
[32m- sort descending with many partitions[0m
[32m- more partitions than elements[0m
[32m- empty RDD[0m
[32m- partition balancing[0m
[32m- partition balancing for descending sort[0m
[32mTaskContextSuite:[0m
[32m- Calls executeOnCompleteCallbacks after failure[0m
[32mNextIteratorSuite:[0m
[32m- one iteration[0m
[32m- two iterations[0m
[32m- empty iteration[0m
[32m- close is called once for empty iterations[0m
[32m- close is called once for non-empty iterations[0m
[32mParallelCollectionSplitSuite:[0m
[32m- one element per slice[0m
[32m- one slice[0m
[32m- equal slices[0m
[32m- non-equal slices[0m
[32m- splitting exclusive range[0m
[32m- splitting inclusive range[0m
[32m- empty data[0m
[32m- zero slices[0m
[32m- negative number of slices[0m
[32m- exclusive ranges sliced into ranges[0m
[32m- inclusive ranges sliced into ranges[0m
[32m- identical slice sizes between Range and NumericRange[0m
[32m- identical slice sizes between List and NumericRange[0m
[32m- large ranges don't overflow[0m
[32m- random array tests[0m
[32m- random exclusive range tests[0m
[32m- random inclusive range tests[0m
[32m- exclusive ranges of longs[0m
[32m- inclusive ranges of longs[0m
[32m- exclusive ranges of doubles[0m
[32m- inclusive ranges of doubles[0m
[32mExecutorURLClassLoaderSuite:[0m
[32m- child first[0m
[32m- parent first[0m
[32m- child first can fall back[0m
[32m- child first can fail[0m
[32m- driver sets context class loader in local mode[0m
[32mExecutorRunnerTest:[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- command includes appId[0m
[32mEventLoggingListenerSuite:[0m
[32m- Parse names of special files[0m
[32m- Verify special files exist[0m
[32m- Verify special files exist with compression[0m
[32m- Parse event logging info[0m
[32m- Parse event logging info with compression[0m
[32m- Basic event logging[0m
[32m- Basic event logging with compression[0m
[32m- End-to-end event logging[0m
[32m- End-to-end event logging with compression[0m
[32mDriverRunnerTest:[0m
[32m- Process succeeds instantly[0m
[32m- Process failing several times and then succeeding[0m
[32m- Process doesn't restart if not supervised[0m
[32m- Process doesn't restart if killed[0m
[32m- Reset of backoff counter[0m
[32mPairRDDFunctionsSuite:[0m
[32m- aggregateByKey[0m
[32m- groupByKey[0m
[32m- groupByKey with duplicates[0m
[32m- groupByKey with negative key hash codes[0m
[32m- groupByKey with many output partitions[0m
[32m- sampleByKey[0m
[32m- reduceByKey[0m
[32m- reduceByKey with collectAsMap[0m
[32m- reduceByKey with many output partitons[0m
[32m- reduceByKey with partitioner[0m
[32m- countApproxDistinctByKey[0m
[32m- join[0m
[32m- join all-to-all[0m
[32m- leftOuterJoin[0m
[32m- rightOuterJoin[0m
[32m- join with no matches[0m
[32m- join with many output partitions[0m
[32m- groupWith[0m
[32m- groupWith3[0m
[32m- groupWith4[0m
[32m- zero-partition RDD[0m
[32m- keys and values[0m
[32m- default partitioner uses partition size[0m
[32m- default partitioner uses largest partitioner[0m
[32m- subtract[0m
[32m- subtract with narrow dependency[0m
[32m- subtractByKey[0m
[32m- subtractByKey with narrow dependency[0m
[32m- foldByKey[0m
[32m- foldByKey with mutable result type[0m
[32m- saveNewAPIHadoopFile should call setConf if format is configurable[0m
[32m- lookup[0m
[32m- lookup with partitioner[0m
[32m- lookup with bad partitioner[0m
[32mPrimitiveVectorSuite:[0m
[32m- primitive value[0m
[32m- non-primitive value[0m
[32m- ideal growth[0m
[32m- ideal size[0m
[32m- resizing[0m
[32mMetricsConfigSuite:[0m
[32m- MetricsConfig with default properties[0m
[32m- MetricsConfig with properties set[0m
[32m- MetricsConfig with subProperties[0m
[32mSparkContextSchedulerCreationSuite:[0m
[32m- bad-master[0m
[32m- local[0m
[32m- local-*[0m
[32m- local-n[0m
[32m- local-*-n-failures[0m
[32m- local-n-failures[0m
[32m- bad-local-n[0m
[32m- bad-local-n-failures[0m
[32m- local-default-parallelism[0m
[32m- simr[0m
[32m- local-cluster[0m
[32m- yarn-cluster[0m
[32m- yarn-standalone[0m
[32m- yarn-client[0m
Failed to load native Mesos library from /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64:/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[32m- mesos fine-grained[0m
Failed to load native Mesos library from /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64:/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[32m- mesos coarse-grained[0m
Failed to load native Mesos library from /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/amd64:/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[32m- mesos with zookeeper[0m
[32mSamplingUtilsSuite:[0m
[32m- reservoirSampleAndCount[0m
[32m- computeFraction[0m
[32mTimeStampedHashMapSuite:[0m
[32m- HashMap - basic test[0m
[32m- TimeStampedHashMap - basic test[0m
[32m- TimeStampedHashMap - threading safety test[0m
[32m- TimeStampedWeakValueHashMap - basic test[0m
[32m- TimeStampedWeakValueHashMap - threading safety test[0m
[32m- TimeStampedHashMap - clearing by timestamp[0m
[32m- TimeStampedWeakValueHashMap - clearing by timestamp[0m
[32m- TimeStampedWeakValueHashMap - clearing weak references[0m
[32mRandomSamplerSuite:[0m
[32m- BernoulliSamplerWithRange[0m
[32m- BernoulliSamplerWithRangeInverse[0m
[32m- BernoulliSamplerWithRatio[0m
[32m- BernoulliSamplerWithComplement[0m
[32m- BernoulliSamplerSetSeed[0m
[32m- PoissonSampler[0m
[32mImplicitOrderingSuite:[0m
[32m- basic inference of Orderings[0m
[32mClosureCleanerSuite:[0m
[32m- closures inside an object[0m
[32m- closures inside a class[0m
[32m- closures inside a class with no default constructor[0m
[32m- closures that don't use fields of the outer class[0m
[32m- nested closures inside an object[0m
[32m- nested closures inside a class[0m
[32m- toplevel return statements in closures are identified at cleaning time[0m
[32m- return statements from named functions nested in closures don't raise exceptions[0m
[32mUnpersistSuite:[0m
[32m- unpersist RDD[0m
[32mTaskSetManagerSuite:[0m
[32m- TaskSet with no preferences[0m
[32m- multiple offers with no preferences[0m
[32m- skip unsatisfiable locality levels[0m
[32m- basic delay scheduling[0m
[32m- delay scheduling with fallback[0m
[32m- delay scheduling with failed hosts[0m
[32m- task result lost[0m
[32m- repeated failures lead to task set abortion[0m
[32m- executors should be blacklisted after task failure, in spite of locality preferences[0m
[32m- new executors get added[0m
[32m- test RACK_LOCAL tasks[0m
[32m- do not emit warning when serialized task is small[0m
[32m- emit warning when serialized task is large[0m
[32mDriverSuite:[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- driver should exit after finishing[0m
[32mCompactBufferSuite:[0m
[32m- empty buffer[0m
[32m- basic inserts[0m
[32m- adding sequences[0m
[32m- adding the same buffer to itself[0m
[32mCacheManagerSuite:[0m
[32m- get uncached rdd[0m
[32m- get cached rdd[0m
[32m- get uncached local rdd[0m
[32mTaskSchedulerImplSuite:[0m
[32m- FIFO Scheduler Test[0m
[32m- Fair Scheduler Test[0m
[32m- Nested Pool Test[0m
[32m- Scheduler does not always schedule tasks on the same workers[0m
[32m- Scheduler correctly accounts for multiple CPUs per task[0m
[32mSparkConfSuite:[0m
[32m- loading from system properties[0m
[32m- initializing without loading defaults[0m
[32m- named set methods[0m
[32m- basic get and set[0m
[32m- creating SparkContext without master and app name[0m
[32m- creating SparkContext without master[0m
[32m- creating SparkContext without app name[0m
[32m- creating SparkContext with both master and app name[0m
[32m- SparkContext property overriding[0m
[32m- nested property names[0m
[32mBlockManagerSuite:[0m
[32m- StorageLevel object caching[0m
[32m- BlockManagerId object caching[0m
[32m- master + 1 manager interaction[0m
[32m- master + 2 managers interaction[0m
[32m- removing block[0m
[32m- removing rdd[0m
[32m- removing broadcast[0m
[32m- reregistration on heart beat[0m
[32m- reregistration on block update[0m
[32m- reregistration doesn't dead lock[0m
Some(org.apache.spark.storage.BlockResult@12a52bbe)
[32m- correct BlockResult returned from get() calls[0m
[32m- in-memory LRU storage[0m
[32m- in-memory LRU storage with serialization[0m
[32m- in-memory LRU for partitions of same RDD[0m
[32m- in-memory LRU for partitions of multiple RDDs[0m
[32m- tachyon storage[0m
[32m  + tachyon storage test disabled. [0m
[32m- on-disk storage[0m
[32m- disk and memory storage[0m
[32m- disk and memory storage with getLocalBytes[0m
[32m- disk and memory storage with serialization[0m
[32m- disk and memory storage with serialization and getLocalBytes[0m
[32m- LRU with mixed storage levels[0m
[32m- in-memory LRU with streams[0m
[32m- LRU with mixed storage levels and streams[0m
[32m- negative byte values in ByteBufferInputStream[0m
[32m- overly large block[0m
[32m- block compression[0m
[32m- block store put failure[0m
[32m- reads of memory-mapped and non memory-mapped files are equivalent[0m
[32m- updated block statuses[0m
[32m- query block statuses[0m
[32m- get matching blocks[0m
[32m- SPARK-1194 regression: fix the same-RDD rule for cache replacement[0m
[32m- reserve/release unroll memory[0m
[32m- safely unroll blocks[0m
[32m- safely unroll blocks through putIterator[0m
[32m- safely unroll blocks through putIterator (disk)[0m
[32m- multiple unrolls by the same thread[0m
[32mPythonRunnerSuite:[0m
[32m- format path[0m
[32m- format paths[0m
[32mBitSetSuite:[0m
[32m- basic set and get[0m
[32m- 100% full bit set[0m
[32m- nextSetBit[0m
[32m- xor len(bitsetX) < len(bitsetY)[0m
[32m- xor len(bitsetX) > len(bitsetY)[0m
[32m- andNot len(bitsetX) < len(bitsetY)[0m
[32m- andNot len(bitsetX) > len(bitsetY)[0m
[32mAsyncRDDActionsSuite:[0m
[32m- countAsync[0m
[32m- collectAsync[0m
[32m- foreachAsync[0m
[32m- foreachPartitionAsync[0m
[32m- takeAsync[0m
[32m- async success handling[0m
[32m- async failure handling[0m
[32m- FutureAction result, infinite wait[0m
[32m- FutureAction result, finite wait[0m
[32m- FutureAction result, timeout[0m
[32mMetricsSystemSuite:[0m
[32m- MetricsSystem with default config[0m
[32m- MetricsSystem with sources add[0m
[32mJobCancellationSuite:[0m
[32m- local mode, FIFO scheduler[0m
[32m- local mode, fair scheduler[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- cluster mode, FIFO scheduler[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- cluster mode, fair scheduler[0m
[32m- do not put partially executed partitions into cache[0m
[32m- job group[0m
[32m- job group with interruption[0m
[33m- two jobs sharing the same stage !!! IGNORED !!![0m
[32mPartitioningSuite:[0m
[32m- HashPartitioner equality[0m
[32m- RangePartitioner equality[0m
[32m- RangePartitioner getPartition[0m
[32m- RangePartitioner for keys that are not Comparable (but with Ordering)[0m
[32m- RangPartitioner.sketch[0m
[32m- RangePartitioner.determineBounds[0m
[32m- RangePartitioner should run only one job if data is roughly balanced[0m
[32m- RangePartitioner should work well on unbalanced data[0m
[32m- RangePartitioner should return a single partition for empty RDDs[0m
[32m- HashPartitioner not equal to RangePartitioner[0m
[32m- partitioner preservation[0m
[32m- partitioning Java arrays should fail[0m
[32m- zero-length partitions should be correctly handled[0m
[32mSecurityManagerSuite:[0m
[32m- set security with conf[0m
[32m- set security with api[0m
[32mUISuite:[0m
[33m- basic ui visibility !!! IGNORED !!![0m
[33m- visibility at localhost:4040 !!! IGNORED !!![0m
[33m- attaching a new tab !!! IGNORED !!![0m
[32m- jetty selects different port under contention[0m
[32m- jetty binds to port 0 correctly[0m
[32m- verify appUIAddress contains the scheme[0m
[32m- verify appUIAddress contains the port[0m
[32mSortShuffleSuite:[0m
[32m- groupByKey without compression[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- shuffle non-zero block size[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- shuffle serializer[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- zero sized blocks[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- zero sized blocks without kryo[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- shuffle on mutable pairs[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- sorting on mutable pairs[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- cogroup using mutable pairs[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- subtract mutable pairs[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- sort with Java non serializable class - Kryo[0m
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
[32m- sort with Java non serializable class - Java[0m
/bin/sh: line 1:  1895 Killed                  java -Dbasedir=/shared/strlen/core -Xmx3g -XX:MaxPermSize=512m -XX:ReservedCodeCacheSize=512m org.scalatest.tools.Runner -R '/shared/strlen/core/target/scala-2.10/classes /shared/strlen/core/target/scala-2.10/test-classes' -o -f /shared/strlen/core/target/surefire-reports/shared/strlen/core/target/SparkTestSuite.txt -u /shared/strlen/core/target/surefire-reports/.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM .......................... SUCCESS [2.021s]
[INFO] Spark Project Core ................................ FAILURE [8:09.635s]
[INFO] Spark Project Bagel ............................... SKIPPED
[INFO] Spark Project GraphX .............................. SKIPPED
[INFO] Spark Project ML Library .......................... SKIPPED
[INFO] Spark Project Streaming ........................... SKIPPED
[INFO] Spark Project Tools ............................... SKIPPED
[INFO] Spark Project Catalyst ............................ SKIPPED
[INFO] Spark Project SQL ................................. SKIPPED
[INFO] Spark Project Hive ................................ SKIPPED
[INFO] Spark Project REPL ................................ SKIPPED
[INFO] Spark Project YARN Parent POM ..................... SKIPPED
[INFO] Spark Project YARN Stable API ..................... SKIPPED
[INFO] Spark Project Assembly ............................ SKIPPED
[INFO] Spark Project External Twitter .................... SKIPPED
[INFO] Spark Project External Kafka ...................... SKIPPED
[INFO] Spark Project External Flume Sink ................. SKIPPED
[INFO] Spark Project External Flume ...................... SKIPPED
[INFO] Spark Project External ZeroMQ ..................... SKIPPED
[INFO] Spark Project External MQTT ....................... SKIPPED
[INFO] Spark Project Examples ............................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 8:12.473s
[INFO] Finished at: Fri Aug 01 20:03:30 PDT 2014
[INFO] Final Memory: 30M/697M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.scalatest:scalatest-maven-plugin:1.0-RC2:test (test) on project spark-core_2.10: There are test failures -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :spark-core_2.10
