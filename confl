diff --cc R/pkg/DESCRIPTION
index 3e49eac,dfb7e22..0000000
--- a/R/pkg/DESCRIPTION
+++ b/R/pkg/DESCRIPTION
diff --cc R/pkg/NAMESPACE
index 4c77d95,62c33a7..0000000
--- a/R/pkg/NAMESPACE
+++ b/R/pkg/NAMESPACE
@@@ -3,7 -3,7 +3,8 @@@
  importFrom("methods", "setGeneric", "setMethod", "setOldClass")
  importFrom("methods", "is", "new", "signature", "show")
  importFrom("stats", "gaussian", "setNames")
- importFrom("utils", "download.file", "packageVersion", "untar")
+ importFrom("utils", "download.file", "object.size", "packageVersion", "untar")
++>>>>>>> v2.0.2
  
  # Disable native libraries till we figure out how to package it
  # See SPARKR-7839
diff --cc R/pkg/inst/tests/testthat/test_sparkSQL.R
index cdb8ff6,ef6cab1..0000000
--- a/R/pkg/inst/tests/testthat/test_sparkSQL.R
+++ b/R/pkg/inst/tests/testthat/test_sparkSQL.R
diff --cc R/pkg/vignettes/sparkr-vignettes.Rmd
index 5156c9e,babfb71..0000000
--- a/R/pkg/vignettes/sparkr-vignettes.Rmd
+++ b/R/pkg/vignettes/sparkr-vignettes.Rmd
@@@ -640,4 -649,4 +649,4 @@@ env | ma
  
  ```{r, echo=FALSE}
  sparkR.session.stop()
--```
++```
diff --cc assembly/pom.xml
index 6db3a59,58feedc..0000000
--- a/assembly/pom.xml
+++ b/assembly/pom.xml
diff --cc common/network-common/pom.xml
index 269b845,a75d222..0000000
--- a/common/network-common/pom.xml
+++ b/common/network-common/pom.xml
diff --cc common/network-shuffle/pom.xml
index 20cf29e,828a407..0000000
--- a/common/network-shuffle/pom.xml
+++ b/common/network-shuffle/pom.xml
diff --cc common/network-yarn/pom.xml
index 25cc328,30891f3..0000000
--- a/common/network-yarn/pom.xml
+++ b/common/network-yarn/pom.xml
diff --cc common/sketch/pom.xml
index 37a5d09,ea2f4f5..0000000
--- a/common/sketch/pom.xml
+++ b/common/sketch/pom.xml
diff --cc common/tags/pom.xml
index ab287f3,ed31f25..0000000
--- a/common/tags/pom.xml
+++ b/common/tags/pom.xml
diff --cc common/unsafe/pom.xml
index 45831ce,0553a48..0000000
--- a/common/unsafe/pom.xml
+++ b/common/unsafe/pom.xml
diff --cc core/pom.xml
index 2d19e5b,f154df2..0000000
--- a/core/pom.xml
+++ b/core/pom.xml
diff --cc core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala
index 52a3499,47aec44..0000000
--- a/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala
+++ b/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala
@@@ -17,9 -17,7 +17,14 @@@
  
  package org.apache.spark.executor
  
++<<<<<<< HEAD
 +import java.util.{ArrayList, Collections}
 +
 +import scala.collection.JavaConverters._
++||||||| merged common ancestors
++=======
+ import scala.collection.JavaConverters._
++>>>>>>> v2.0.2
  import scala.collection.mutable.{ArrayBuffer, LinkedHashMap}
  
  import org.apache.spark._
@@@ -305,39 -303,3 +310,77 @@@ private[spark] object TaskMetrics exten
      tm
    }
  }
++<<<<<<< HEAD
 +
 +
 +private[spark] class BlockStatusesAccumulator
 +  extends AccumulatorV2[(BlockId, BlockStatus), java.util.List[(BlockId, BlockStatus)]] {
 +  private val _seq = Collections.synchronizedList(new ArrayList[(BlockId, BlockStatus)]())
 +
 +  override def isZero(): Boolean = _seq.isEmpty
 +
 +  override def copyAndReset(): BlockStatusesAccumulator = new BlockStatusesAccumulator
 +
 +  override def copy(): BlockStatusesAccumulator = {
 +    val newAcc = new BlockStatusesAccumulator
 +    newAcc._seq.addAll(_seq)
 +    newAcc
 +  }
 +
 +  override def reset(): Unit = _seq.clear()
 +
 +  override def add(v: (BlockId, BlockStatus)): Unit = _seq.add(v)
 +
 +  override def merge(
 +    other: AccumulatorV2[(BlockId, BlockStatus), java.util.List[(BlockId, BlockStatus)]]): Unit = {
 +    other match {
 +      case o: BlockStatusesAccumulator => _seq.addAll(o.value)
 +      case _ => throw new UnsupportedOperationException(
 +        s"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}")
 +    }
 +  }
 +
 +  override def value: java.util.List[(BlockId, BlockStatus)] = _seq
 +
 +  def setValue(newValue: java.util.List[(BlockId, BlockStatus)]): Unit = {
 +    _seq.clear()
 +    _seq.addAll(newValue)
 +  }
 +}
++||||||| merged common ancestors
++
++
++private[spark] class BlockStatusesAccumulator
++  extends AccumulatorV2[(BlockId, BlockStatus), Seq[(BlockId, BlockStatus)]] {
++  private var _seq = ArrayBuffer.empty[(BlockId, BlockStatus)]
++
++  override def isZero(): Boolean = _seq.isEmpty
++
++  override def copyAndReset(): BlockStatusesAccumulator = new BlockStatusesAccumulator
++
++  override def copy(): BlockStatusesAccumulator = {
++    val newAcc = new BlockStatusesAccumulator
++    newAcc._seq = _seq.clone()
++    newAcc
++  }
++
++  override def reset(): Unit = _seq.clear()
++
++  override def add(v: (BlockId, BlockStatus)): Unit = _seq += v
++
++  override def merge(other: AccumulatorV2[(BlockId, BlockStatus), Seq[(BlockId, BlockStatus)]])
++  : Unit = other match {
++    case o: BlockStatusesAccumulator => _seq ++= o.value
++    case _ => throw new UnsupportedOperationException(
++      s"Cannot merge ${this.getClass.getName} with ${other.getClass.getName}")
++  }
++
++  override def value: Seq[(BlockId, BlockStatus)] = _seq
++
++  def setValue(newValue: Seq[(BlockId, BlockStatus)]): Unit = {
++    _seq.clear()
++    _seq ++= newValue
++  }
++}
++=======
++>>>>>>> v2.0.2
diff --cc core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
index 3243b94,9b87c42..0000000
--- a/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
+++ b/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala
@@@ -708,16 -690,13 +708,29 @@@ private[storage] class PartiallyUnrolle
      }
    }
  
++<<<<<<< HEAD
 +  override def next(): T = {
 +    if (unrolled == null) {
 +      rest.next()
 +    } else if (!unrolled.hasNext) {
 +      releaseUnrollMemory()
 +      rest.next
 +    } else {
 +      unrolled.next()
 +    }
 +  }
++||||||| merged common ancestors
++  override def hasNext: Boolean = iter.hasNext
++  override def next(): T = iter.next()
++=======
+   override def next(): T = {
+     if (unrolled == null) {
+       rest.next()
+     } else {
+       unrolled.next()
+     }
+   }
++>>>>>>> v2.0.2
  
    /**
     * Called to dispose of this iterator and free its memory.
diff --cc core/src/main/scala/org/apache/spark/util/Utils.scala
index 7d41458,b9cf721..0000000
--- a/core/src/main/scala/org/apache/spark/util/Utils.scala
+++ b/core/src/main/scala/org/apache/spark/util/Utils.scala
@@@ -73,7 -57,7 +75,12 @@@ import org.apache.spark.internal.Loggin
  import org.apache.spark.internal.config.{DYN_ALLOCATION_INITIAL_EXECUTORS, DYN_ALLOCATION_MIN_EXECUTORS, EXECUTOR_INSTANCES}
  import org.apache.spark.network.util.JavaUtils
  import org.apache.spark.serializer.{DeserializationStream, SerializationStream, SerializerInstance}
++<<<<<<< HEAD
 +import org.apache.spark.storage.StorageUtils
++||||||| merged common ancestors
++=======
+ import org.apache.spark.util.logging.RollingFileAppender
++>>>>>>> v2.0.2
  
  /** CallSite represents a place in user code. It can have a short and a long form. */
  private[spark] case class CallSite(shortForm: String, longForm: String)
diff --cc core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala
index e93eee2,1b3197a..0000000
--- a/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala
+++ b/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala
@@@ -107,7 -107,10 +107,15 @@@ class BlockManagerSuite extends SparkFu
      rpcEnv = RpcEnv.create("test", "localhost", 0, conf, securityMgr)
      conf.set("spark.driver.port", rpcEnv.address.port.toString)
  
++<<<<<<< HEAD
 +    sc = new SparkContext("local", "test", conf)
++||||||| merged common ancestors
++=======
+     // Mock SparkContext to reduce the memory usage of tests. It's fine since the only reason we
+     // need to create a SparkContext is to initialize LiveListenerBus.
+     sc = mock(classOf[SparkContext])
+     when(sc.conf).thenReturn(conf)
++>>>>>>> v2.0.2
      master = new BlockManagerMaster(rpcEnv.setupEndpoint("blockmanager",
        new BlockManagerMasterEndpoint(rpcEnv, true, conf,
          new LiveListenerBus(sc))), conf, true)
diff --cc dev/deps/spark-deps-hadoop-2.2
index 8c9e559,34cd4e6..0000000
--- a/dev/deps/spark-deps-hadoop-2.2
+++ b/dev/deps/spark-deps-hadoop-2.2
@@@ -140,8 -140,8 +140,16 @@@ parquet-jackson-1.7.0.ja
  pmml-model-1.2.15.jar
  pmml-schema-1.2.15.jar
  protobuf-java-2.5.0.jar
++<<<<<<< HEAD
 +py4j-0.10.3.jar
 +pyrolite-4.9.jar
++||||||| merged common ancestors
++py4j-0.10.1.jar
++pyrolite-4.9.jar
++=======
+ py4j-0.10.3.jar
+ pyrolite-4.13.jar
++>>>>>>> v2.0.2
  scala-compiler-2.11.8.jar
  scala-library-2.11.8.jar
  scala-parser-combinators_2.11-1.0.4.jar
diff --cc dev/deps/spark-deps-hadoop-2.3
index 839e084,8ae3c5e..0000000
--- a/dev/deps/spark-deps-hadoop-2.3
+++ b/dev/deps/spark-deps-hadoop-2.3
@@@ -147,8 -147,8 +147,16 @@@ parquet-jackson-1.7.0.ja
  pmml-model-1.2.15.jar
  pmml-schema-1.2.15.jar
  protobuf-java-2.5.0.jar
++<<<<<<< HEAD
 +py4j-0.10.3.jar
 +pyrolite-4.9.jar
++||||||| merged common ancestors
++py4j-0.10.1.jar
++pyrolite-4.9.jar
++=======
+ py4j-0.10.3.jar
+ pyrolite-4.13.jar
++>>>>>>> v2.0.2
  scala-compiler-2.11.8.jar
  scala-library-2.11.8.jar
  scala-parser-combinators_2.11-1.0.4.jar
diff --cc dev/deps/spark-deps-hadoop-2.4
index ed84de7,7c69102..0000000
--- a/dev/deps/spark-deps-hadoop-2.4
+++ b/dev/deps/spark-deps-hadoop-2.4
@@@ -147,8 -147,8 +147,16 @@@ parquet-jackson-1.7.0.ja
  pmml-model-1.2.15.jar
  pmml-schema-1.2.15.jar
  protobuf-java-2.5.0.jar
++<<<<<<< HEAD
 +py4j-0.10.3.jar
 +pyrolite-4.9.jar
++||||||| merged common ancestors
++py4j-0.10.1.jar
++pyrolite-4.9.jar
++=======
+ py4j-0.10.3.jar
+ pyrolite-4.13.jar
++>>>>>>> v2.0.2
  scala-compiler-2.11.8.jar
  scala-library-2.11.8.jar
  scala-parser-combinators_2.11-1.0.4.jar
diff --cc dev/deps/spark-deps-hadoop-2.6
index 6e7c9cb,041e01e..0000000
--- a/dev/deps/spark-deps-hadoop-2.6
+++ b/dev/deps/spark-deps-hadoop-2.6
@@@ -155,8 -155,8 +155,16 @@@ parquet-jackson-1.7.0.ja
  pmml-model-1.2.15.jar
  pmml-schema-1.2.15.jar
  protobuf-java-2.5.0.jar
++<<<<<<< HEAD
 +py4j-0.10.3.jar
 +pyrolite-4.9.jar
++||||||| merged common ancestors
++py4j-0.10.1.jar
++pyrolite-4.9.jar
++=======
+ py4j-0.10.3.jar
+ pyrolite-4.13.jar
++>>>>>>> v2.0.2
  scala-compiler-2.11.8.jar
  scala-library-2.11.8.jar
  scala-parser-combinators_2.11-1.0.4.jar
diff --cc dev/deps/spark-deps-hadoop-2.7
index a61f31e,4f70bff..0000000
--- a/dev/deps/spark-deps-hadoop-2.7
+++ b/dev/deps/spark-deps-hadoop-2.7
@@@ -156,8 -156,8 +156,16 @@@ parquet-jackson-1.7.0.ja
  pmml-model-1.2.15.jar
  pmml-schema-1.2.15.jar
  protobuf-java-2.5.0.jar
++<<<<<<< HEAD
 +py4j-0.10.3.jar
 +pyrolite-4.9.jar
++||||||| merged common ancestors
++py4j-0.10.1.jar
++pyrolite-4.9.jar
++=======
+ py4j-0.10.3.jar
+ pyrolite-4.13.jar
++>>>>>>> v2.0.2
  scala-compiler-2.11.8.jar
  scala-library-2.11.8.jar
  scala-parser-combinators_2.11-1.0.4.jar
diff --cc docs/_config.yml
index 75c89bd,824197b..0000000
--- a/docs/_config.yml
+++ b/docs/_config.yml
@@@ -14,8 -14,8 +14,16 @@@ include
  
  # These allow the documentation to be updated with newer releases
  # of Spark, Scala, and Mesos.
++<<<<<<< HEAD
 +SPARK_VERSION: 2.0.1
 +SPARK_VERSION_SHORT: 2.0.1
++||||||| merged common ancestors
++SPARK_VERSION: 2.0.0
++SPARK_VERSION_SHORT: 2.0.0
++=======
+ SPARK_VERSION: 2.0.2
+ SPARK_VERSION_SHORT: 2.0.2
++>>>>>>> v2.0.2
  SCALA_BINARY_VERSION: "2.11"
  SCALA_VERSION: "2.11.7"
  MESOS_VERSION: 0.21.0
diff --cc examples/pom.xml
index 89e0c61,a571062..0000000
--- a/examples/pom.xml
+++ b/examples/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../pom.xml</relativePath>
    </parent>
  
diff --cc external/docker-integration-tests/pom.xml
index 8c6e221,46b1410..0000000
--- a/external/docker-integration-tests/pom.xml
+++ b/external/docker-integration-tests/pom.xml
@@@ -22,7 -22,7 +22,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/flume-assembly/pom.xml
index dd45935,451b101..0000000
--- a/external/flume-assembly/pom.xml
+++ b/external/flume-assembly/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/flume-sink/pom.xml
index ba97794,d48a09b..0000000
--- a/external/flume-sink/pom.xml
+++ b/external/flume-sink/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/flume/pom.xml
index 8f8bde7,1d9c63a..0000000
--- a/external/flume/pom.xml
+++ b/external/flume/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/java8-tests/pom.xml
index f7d8ef7,27a1f53..0000000
--- a/external/java8-tests/pom.xml
+++ b/external/java8-tests/pom.xml
@@@ -20,7 -20,7 +20,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/kafka-0-10-assembly/pom.xml
index 260969f,c5e4289..0000000
--- a/external/kafka-0-10-assembly/pom.xml
+++ b/external/kafka-0-10-assembly/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/kafka-0-10/pom.xml
index 1ae1d0e,86bfa38..0000000
--- a/external/kafka-0-10/pom.xml
+++ b/external/kafka-0-10/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/kafka-0-8-assembly/pom.xml
index a4b14f8,ac1d289..0000000
--- a/external/kafka-0-8-assembly/pom.xml
+++ b/external/kafka-0-8-assembly/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/kafka-0-8/pom.xml
index 9964b22,3ea78f2..0000000
--- a/external/kafka-0-8/pom.xml
+++ b/external/kafka-0-8/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/kinesis-asl-assembly/pom.xml
index b5d90b1,052ffdc..0000000
--- a/external/kinesis-asl-assembly/pom.xml
+++ b/external/kinesis-asl-assembly/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/kinesis-asl/pom.xml
index f96db65,4304967..0000000
--- a/external/kinesis-asl/pom.xml
+++ b/external/kinesis-asl/pom.xml
@@@ -20,7 -20,7 +20,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc external/spark-ganglia-lgpl/pom.xml
index 40f2e38,261a2b2..0000000
--- a/external/spark-ganglia-lgpl/pom.xml
+++ b/external/spark-ganglia-lgpl/pom.xml
@@@ -20,7 -20,7 +20,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc graphx/pom.xml
index 979217e,f7911c7..0000000
--- a/graphx/pom.xml
+++ b/graphx/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../pom.xml</relativePath>
    </parent>
  
diff --cc launcher/pom.xml
index 0c0dd0c,944d3bf..0000000
--- a/launcher/pom.xml
+++ b/launcher/pom.xml
@@@ -22,7 -22,7 +22,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../pom.xml</relativePath>
    </parent>
  
diff --cc mllib-local/pom.xml
index 5681b36,760968a..0000000
--- a/mllib-local/pom.xml
+++ b/mllib-local/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../pom.xml</relativePath>
    </parent>
  
diff --cc mllib/pom.xml
index 80ab2e0,1087e3b..0000000
--- a/mllib/pom.xml
+++ b/mllib/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../pom.xml</relativePath>
    </parent>
  
diff --cc pom.xml
index 79255f9,6ed58ab..0000000
--- a/pom.xml
+++ b/pom.xml
@@@ -26,7 -26,7 +26,13 @@@
    </parent>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +  <version>2.0.1</version>
++||||||| merged common ancestors
++  <version>2.0.1-SNAPSHOT</version>
++=======
+   <version>2.0.2</version>
++>>>>>>> v2.0.2
    <packaging>pom</packaging>
    <name>Spark Project Parent POM</name>
    <url>http://spark.apache.org/</url>
@@@ -744,7 -745,7 +751,14 @@@
        <dependency>
          <groupId>com.spotify</groupId>
          <artifactId>docker-client</artifactId>
++<<<<<<< HEAD
 +        <version>3.6.6</version>
++||||||| merged common ancestors
++        <classifier>shaded</classifier>
++        <version>3.6.6</version>
++=======
+         <version>5.0.2</version>
++>>>>>>> v2.0.2
          <scope>test</scope>
          <exclusions>
            <exclusion>
diff --cc project/MimaExcludes.scala
index 423cbd4,ee6e31a0..0000000
--- a/project/MimaExcludes.scala
+++ b/project/MimaExcludes.scala
diff --cc project/SparkBuild.scala
index 133d3b3,98f1e23..0000000
--- a/project/SparkBuild.scala
+++ b/project/SparkBuild.scala
diff --cc repl/pom.xml
index 4b70d64,fef2c5f..0000000
--- a/repl/pom.xml
+++ b/repl/pom.xml
@@@ -21,7 -21,7 +21,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../pom.xml</relativePath>
    </parent>
  
diff --cc sql/catalyst/pom.xml
index efa327c,ed74cf5..0000000
--- a/sql/catalyst/pom.xml
+++ b/sql/catalyst/pom.xml
@@@ -22,7 -22,7 +22,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala
index e7430b0,8342892..0000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
index 0db7435,f0992b3..0000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
@@@ -1147,9 -1161,23 +1161,145 @@@ object PushDownPredicate extends Rule[L
        filter
      }
    }
++<<<<<<< HEAD
++}
++
++/**
++||||||| merged common ancestors
++}
++
++/**
++ * Reorder the joins and push all the conditions into join, so that the bottom ones have at least
++ * one condition.
++ *
++ * The order of joins will not be changed if all of them already have at least one condition.
++ */
++object ReorderJoin extends Rule[LogicalPlan] with PredicateHelper {
++
++  /**
++   * Join a list of plans together and push down the conditions into them.
++   *
++   * The joined plan are picked from left to right, prefer those has at least one join condition.
++   *
++   * @param input a list of LogicalPlans to join.
++   * @param conditions a list of condition for join.
++   */
++  @tailrec
++  def createOrderedJoin(input: Seq[LogicalPlan], conditions: Seq[Expression]): LogicalPlan = {
++    assert(input.size >= 2)
++    if (input.size == 2) {
++      val (joinConditions, others) = conditions.partition(
++        e => !SubqueryExpression.hasCorrelatedSubquery(e))
++      val join = Join(input(0), input(1), Inner, joinConditions.reduceLeftOption(And))
++      if (others.nonEmpty) {
++        Filter(others.reduceLeft(And), join)
++      } else {
++        join
++      }
++    } else {
++      val left :: rest = input.toList
++      // find out the first join that have at least one join condition
++      val conditionalJoin = rest.find { plan =>
++        val refs = left.outputSet ++ plan.outputSet
++        conditions.filterNot(canEvaluate(_, left)).filterNot(canEvaluate(_, plan))
++          .exists(_.references.subsetOf(refs))
++      }
++      // pick the next one if no condition left
++      val right = conditionalJoin.getOrElse(rest.head)
++
++      val joinedRefs = left.outputSet ++ right.outputSet
++      val (joinConditions, others) = conditions.partition(
++        e => e.references.subsetOf(joinedRefs) && !SubqueryExpression.hasCorrelatedSubquery(e))
++      val joined = Join(left, right, Inner, joinConditions.reduceLeftOption(And))
++
++      // should not have reference to same logical plan
++      createOrderedJoin(Seq(joined) ++ rest.filterNot(_ eq right), others)
++    }
++  }
++
++  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
++    case j @ ExtractFiltersAndInnerJoins(input, conditions)
++        if input.size > 2 && conditions.nonEmpty =>
++      createOrderedJoin(input, conditions)
++  }
++}
++
++/**
++ * Elimination of outer joins, if the predicates can restrict the result sets so that
++ * all null-supplying rows are eliminated
++ *
++ * - full outer -> inner if both sides have such predicates
++ * - left outer -> inner if the right side has such predicates
++ * - right outer -> inner if the left side has such predicates
++ * - full outer -> left outer if only the left side has such predicates
++ * - full outer -> right outer if only the right side has such predicates
++ *
++ * This rule should be executed before pushing down the Filter
++ */
++object EliminateOuterJoin extends Rule[LogicalPlan] with PredicateHelper {
++
++  /**
++   * Returns whether the expression returns null or false when all inputs are nulls.
++   */
++  private def canFilterOutNull(e: Expression): Boolean = {
++    if (!e.deterministic || SubqueryExpression.hasCorrelatedSubquery(e)) return false
++    val attributes = e.references.toSeq
++    val emptyRow = new GenericInternalRow(attributes.length)
++    val v = BindReferences.bindReference(e, attributes).eval(emptyRow)
++    v == null || v == false
++  }
++
++  private def buildNewJoinType(filter: Filter, join: Join): JoinType = {
++    val splitConjunctiveConditions: Seq[Expression] = splitConjunctivePredicates(filter.condition)
++    val leftConditions = splitConjunctiveConditions
++      .filter(_.references.subsetOf(join.left.outputSet))
++    val rightConditions = splitConjunctiveConditions
++      .filter(_.references.subsetOf(join.right.outputSet))
++
++    val leftHasNonNullPredicate = leftConditions.exists(canFilterOutNull) ||
++      filter.constraints.filter(_.isInstanceOf[IsNotNull])
++        .exists(expr => join.left.outputSet.intersect(expr.references).nonEmpty)
++    val rightHasNonNullPredicate = rightConditions.exists(canFilterOutNull) ||
++      filter.constraints.filter(_.isInstanceOf[IsNotNull])
++        .exists(expr => join.right.outputSet.intersect(expr.references).nonEmpty)
++
++    join.joinType match {
++      case RightOuter if leftHasNonNullPredicate => Inner
++      case LeftOuter if rightHasNonNullPredicate => Inner
++      case FullOuter if leftHasNonNullPredicate && rightHasNonNullPredicate => Inner
++      case FullOuter if leftHasNonNullPredicate => LeftOuter
++      case FullOuter if rightHasNonNullPredicate => RightOuter
++      case o => o
++    }
++  }
++
++  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
++    case f @ Filter(condition, j @ Join(_, _, RightOuter | LeftOuter | FullOuter, _)) =>
++      val newJoinType = buildNewJoinType(f, j)
++      if (j.joinType == newJoinType) f else Filter(condition, j.copy(joinType = newJoinType))
++  }
++}
++
++/**
++=======
+ 
+   /**
+    * Check if we can safely push a filter through a projection, by making sure that predicate
+    * subqueries in the condition do not contain the same attributes as the plan they are moved
+    * into. This can happen when the plan and predicate subquery have the same source.
+    */
+   private def canPushThroughCondition(plan: LogicalPlan, condition: Expression): Boolean = {
+     val attributes = plan.outputSet
+     val matched = condition.find {
+       case PredicateSubquery(p, _, _, _) => p.outputSet.intersect(attributes).nonEmpty
+       case _ => false
+     }
+     matched.isEmpty
+   }
  }
  
  /**
++>>>>>>> v2.0.2
   * Pushes down [[Filter]] operators where the `condition` can be
   * evaluated using only the attributes of the left or right side of a join.  Other
   * [[Filter]] conditions are moved into the `condition` of the [[Join]].
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
index ae4cd8e,08062bd..0000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
@@@ -133,4 -135,4 +135,4 @@@ object EliminateOuterJoin extends Rule[
        val newJoinType = buildNewJoinType(f, j)
        if (j.joinType == newJoinType) f else Filter(condition, j.copy(joinType = newJoinType))
    }
--}
++}
diff --cc sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala
index 00c92fc,1981fd8..0000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala
@@@ -148,14 -127,9 +145,13 @@@ protected[sql] abstract class AtomicTyp
    private[sql] type InternalType
    private[sql] val tag: TypeTag[InternalType]
    private[sql] val ordering: Ordering[InternalType]
 -}
  
-   @transient private[sql] lazy val classTag = ScalaReflectionLock.synchronized {
++  @transient private[sql] val classTag = ScalaReflectionLock.synchronized {
 +    val mirror = runtimeMirror(Utils.getSparkClassLoader)
 +    ClassTag[InternalType](mirror.runtimeClass(tag.tpe))
 +  }
 +}
  
- 
  /**
   * :: DeveloperApi ::
   * Numeric data types.
diff --cc sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
index 4aaae72,15ebc19..0000000
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
diff --cc sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableIdentifierParserSuite.scala
index 4d3ad21,cb57fb6..0000000
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableIdentifierParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableIdentifierParserSuite.scala
diff --cc sql/core/pom.xml
index 347f8a1,c0a355f..0000000
--- a/sql/core/pom.xml
+++ b/sql/core/pom.xml
@@@ -22,7 -22,7 +22,13 @@@
    <parent>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-parent_2.11</artifactId>
++<<<<<<< HEAD
 +    <version>2.0.1</version>
++||||||| merged common ancestors
++    <version>2.0.1-SNAPSHOT</version>
++=======
+     <version>2.0.2</version>
++>>>>>>> v2.0.2
      <relativePath>../../pom.xml</relativePath>
    </parent>
  
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala
index 479934a,56bd5c1..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala
@@@ -62,9 -60,8 +60,18 @@@ case class InMemoryRelation
      storageLevel: StorageLevel,
      @transient child: SparkPlan,
      tableName: Option[String])(
++<<<<<<< HEAD
 +    @transient var _cachedColumnBuffers: RDD[CachedBatch] = null,
 +    val batchStats: CollectionAccumulator[InternalRow] =
 +      child.sqlContext.sparkContext.collectionAccumulator[InternalRow])
++||||||| merged common ancestors
++    @transient private[sql] var _cachedColumnBuffers: RDD[CachedBatch] = null,
++    private[sql] val batchStats: CollectionAccumulator[InternalRow] =
++      child.sqlContext.sparkContext.collectionAccumulator[InternalRow])
++=======
+     @transient var _cachedColumnBuffers: RDD[CachedBatch] = null,
+     val batchStats: LongAccumulator = child.sqlContext.sparkContext.longAccumulator)
++>>>>>>> v2.0.2
    extends logical.LeafNode with MultiInstanceRelation {
  
    override protected def innerChildren: Seq[QueryPlan[_]] = Seq(child)
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala
index 424a962,d82e54e..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala
@@@ -35,9 -35,7 +35,17 @@@ import org.apache.spark.sql.types.
   * A logical command that is executed for its side-effects.  `RunnableCommand`s are
   * wrapped in `ExecutedCommand` during execution.
   */
++<<<<<<< HEAD
 +trait RunnableCommand extends LogicalPlan with logical.Command {
 +  override def output: Seq[Attribute] = Seq.empty
 +  final override def children: Seq[LogicalPlan] = Seq.empty
++||||||| merged common ancestors
++private[sql] trait RunnableCommand extends LogicalPlan with logical.Command {
++  override def output: Seq[Attribute] = Seq.empty
++  override def children: Seq[LogicalPlan] = Seq.empty
++=======
+ trait RunnableCommand extends logical.Command {
++>>>>>>> v2.0.2
    def run(sparkSession: SparkSession): Seq[Row]
  }
  
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala
index 995feb3,ad0c779..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala
@@@ -33,8 -33,9 +33,16 @@@ import org.apache.spark.sql.catalyst.ca
  import org.apache.spark.sql.catalyst.catalog.CatalogTableType._
  import org.apache.spark.sql.catalyst.catalog.CatalogTypes.TablePartitionSpec
  import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference}
++<<<<<<< HEAD
 +import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
 +import org.apache.spark.sql.catalyst.plans.logical.{Command, LogicalPlan, UnaryNode}
++||||||| merged common ancestors
++import org.apache.spark.sql.catalyst.plans.logical.{Command, LogicalPlan, UnaryNode}
++=======
+ import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
+ import org.apache.spark.sql.catalyst.plans.QueryPlan
+ import org.apache.spark.sql.catalyst.plans.logical.{Command, LogicalPlan}
++>>>>>>> v2.0.2
  import org.apache.spark.sql.catalyst.util.quoteIdentifier
  import org.apache.spark.sql.execution.command.CreateDataSourceTableUtils._
  import org.apache.spark.sql.execution.datasources.PartitioningUtils
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala
index 7a8b825,2869e80..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala
@@@ -233,17 -233,17 +233,32 @@@ object JdbcUtils extends Logging 
          conn.commit()
        }
        committed = true
++<<<<<<< HEAD
 +    } catch {
 +      case e: SQLException =>
 +        val cause = e.getNextException
 +        if (e.getCause != cause) {
 +          if (e.getCause == null) {
 +            e.initCause(cause)
 +          } else {
 +            e.addSuppressed(cause)
 +          }
 +        }
 +        throw e
++||||||| merged common ancestors
++=======
+     } catch {
+       case e: SQLException =>
+         val cause = e.getNextException
+         if (cause != null && e.getCause != cause) {
+           if (e.getCause == null) {
+             e.initCause(cause)
+           } else {
+             e.addSuppressed(cause)
+           }
+         }
+         throw e
++>>>>>>> v2.0.2
      } finally {
        if (!committed) {
          // The stage must fail.  We got here through an exception path, so
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala
index 7e2ebe8,acc42a0..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala
@@@ -336,11 -336,13 +336,24 @@@ object StatFunctions extends Logging 
        }
        res.prepend(head)
        // If necessary, add the minimum element:
++<<<<<<< HEAD
 +      val currHead = currentSamples.head
 +      if (currHead.value < head.value) {
 +        res.prepend(currentSamples.head)
 +      }
 +      res.toArray
++||||||| merged common ancestors
++      res.prepend(currentSamples.head)
++      res
++=======
+       val currHead = currentSamples.head
+       // don't add the minimum element if `currentSamples` has only one element (both `currHead` and
+       // `head` point to the same element)
+       if (currHead.value <= head.value && currentSamples.length > 1) {
+         res.prepend(currentSamples.head)
+       }
+       res.toArray
++>>>>>>> v2.0.2
      }
    }
  
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala
index 027b5bb,c14feea..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala
@@@ -1,245 -1,251 +1,500 @@@
++<<<<<<< HEAD
 +/*
 + * Licensed to the Apache Software Foundation (ASF) under one or more
 + * contributor license agreements.  See the NOTICE file distributed with
 + * this work for additional information regarding copyright ownership.
 + * The ASF licenses this file to You under the Apache License, Version 2.0
 + * (the "License"); you may not use this file except in compliance with
 + * the License.  You may obtain a copy of the License at
 + *
 + *    http://www.apache.org/licenses/LICENSE-2.0
 + *
 + * Unless required by applicable law or agreed to in writing, software
 + * distributed under the License is distributed on an "AS IS" BASIS,
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 + * See the License for the specific language governing permissions and
 + * limitations under the License.
 + */
 +
 +package org.apache.spark.sql.execution.streaming
 +
 +import java.io.IOException
 +import java.nio.charset.StandardCharsets.UTF_8
 +
 +import scala.reflect.ClassTag
 +
 +import org.apache.hadoop.fs.{Path, PathFilter}
 +
 +import org.apache.spark.sql.SparkSession
 +
 +/**
 + * An abstract class for compactible metadata logs. It will write one log file for each batch.
 + * The first line of the log file is the version number, and there are multiple serialized
 + * metadata lines following.
 + *
 + * As reading from many small files is usually pretty slow, also too many
 + * small files in one folder will mess the FS, [[CompactibleFileStreamLog]] will
 + * compact log files every 10 batches by default into a big file. When
 + * doing a compaction, it will read all old log files and merge them with the new batch.
 + */
 +abstract class CompactibleFileStreamLog[T: ClassTag](
 +    metadataLogVersion: String,
 +    sparkSession: SparkSession,
 +    path: String)
 +  extends HDFSMetadataLog[Array[T]](sparkSession, path) {
 +
 +  import CompactibleFileStreamLog._
 +
 +  /**
 +   * If we delete the old files after compaction at once, there is a race condition in S3: other
 +   * processes may see the old files are deleted but still cannot see the compaction file using
 +   * "list". The `allFiles` handles this by looking for the next compaction file directly, however,
 +   * a live lock may happen if the compaction happens too frequently: one processing keeps deleting
 +   * old files while another one keeps retrying. Setting a reasonable cleanup delay could avoid it.
 +   */
 +  protected def fileCleanupDelayMs: Long
 +
 +  protected def isDeletingExpiredLog: Boolean
 +
 +  protected def compactInterval: Int
 +
 +  /**
 +   * Serialize the data into encoded string.
 +   */
 +  protected def serializeData(t: T): String
 +
 +  /**
 +   * Deserialize the string into data object.
 +   */
 +  protected def deserializeData(encodedString: String): T
 +
 +  /**
 +   * Filter out the obsolete logs.
 +   */
 +  def compactLogs(logs: Seq[T]): Seq[T]
 +
 +  override def batchIdToPath(batchId: Long): Path = {
 +    if (isCompactionBatch(batchId, compactInterval)) {
 +      new Path(metadataPath, s"$batchId$COMPACT_FILE_SUFFIX")
 +    } else {
 +      new Path(metadataPath, batchId.toString)
 +    }
 +  }
 +
 +  override def pathToBatchId(path: Path): Long = {
 +    getBatchIdFromFileName(path.getName)
 +  }
 +
 +  override def isBatchFile(path: Path): Boolean = {
 +    try {
 +      getBatchIdFromFileName(path.getName)
 +      true
 +    } catch {
 +      case _: NumberFormatException => false
 +    }
 +  }
 +
 +  override def serialize(logData: Array[T]): Array[Byte] = {
 +    (metadataLogVersion +: logData.map(serializeData)).mkString("\n").getBytes(UTF_8)
 +  }
 +
 +  override def deserialize(bytes: Array[Byte]): Array[T] = {
 +    val lines = new String(bytes, UTF_8).split("\n")
 +    if (lines.length == 0) {
 +      throw new IllegalStateException("Incomplete log file")
 +    }
 +    val version = lines(0)
 +    if (version != metadataLogVersion) {
 +      throw new IllegalStateException(s"Unknown log version: ${version}")
 +    }
 +    lines.slice(1, lines.length).map(deserializeData)
 +  }
 +
 +  override def add(batchId: Long, logs: Array[T]): Boolean = {
 +    if (isCompactionBatch(batchId, compactInterval)) {
 +      compact(batchId, logs)
 +    } else {
 +      super.add(batchId, logs)
 +    }
 +  }
 +
 +  /**
 +   * Compacts all logs before `batchId` plus the provided `logs`, and writes them into the
 +   * corresponding `batchId` file. It will delete expired files as well if enabled.
 +   */
 +  private def compact(batchId: Long, logs: Array[T]): Boolean = {
 +    val validBatches = getValidBatchesBeforeCompactionBatch(batchId, compactInterval)
 +    val allLogs = validBatches.flatMap(batchId => super.get(batchId)).flatten ++ logs
 +    if (super.add(batchId, compactLogs(allLogs).toArray)) {
 +      if (isDeletingExpiredLog) {
 +        deleteExpiredLog(batchId)
 +      }
 +      true
 +    } else {
 +      // Return false as there is another writer.
 +      false
 +    }
 +  }
 +
 +  /**
 +   * Returns all files except the deleted ones.
 +   */
 +  def allFiles(): Array[T] = {
 +    var latestId = getLatest().map(_._1).getOrElse(-1L)
 +    // There is a race condition when `FileStreamSink` is deleting old files and `StreamFileCatalog`
 +    // is calling this method. This loop will retry the reading to deal with the
 +    // race condition.
 +    while (true) {
 +      if (latestId >= 0) {
 +        try {
 +          val logs =
 +            getAllValidBatches(latestId, compactInterval).flatMap(id => super.get(id)).flatten
 +          return compactLogs(logs).toArray
 +        } catch {
 +          case e: IOException =>
 +            // Another process using `CompactibleFileStreamLog` may delete the batch files when
 +            // `StreamFileCatalog` are reading. However, it only happens when a compaction is
 +            // deleting old files. If so, let's try the next compaction batch and we should find it.
 +            // Otherwise, this is a real IO issue and we should throw it.
 +            latestId = nextCompactionBatchId(latestId, compactInterval)
 +            super.get(latestId).getOrElse {
 +              throw e
 +            }
 +        }
 +      } else {
 +        return Array.empty
 +      }
 +    }
 +    Array.empty
 +  }
 +
 +  /**
 +   * Since all logs before `compactionBatchId` are compacted and written into the
 +   * `compactionBatchId` log file, they can be removed. However, due to the eventual consistency of
 +   * S3, the compaction file may not be seen by other processes at once. So we only delete files
 +   * created `fileCleanupDelayMs` milliseconds ago.
 +   */
 +  private def deleteExpiredLog(compactionBatchId: Long): Unit = {
 +    val expiredTime = System.currentTimeMillis() - fileCleanupDelayMs
 +    fileManager.list(metadataPath, new PathFilter {
 +      override def accept(path: Path): Boolean = {
 +        try {
 +          val batchId = getBatchIdFromFileName(path.getName)
 +          batchId < compactionBatchId
 +        } catch {
 +          case _: NumberFormatException =>
 +            false
 +        }
 +      }
 +    }).foreach { f =>
 +      if (f.getModificationTime <= expiredTime) {
 +        fileManager.delete(f.getPath)
 +      }
 +    }
 +  }
 +}
 +
 +object CompactibleFileStreamLog {
 +  val COMPACT_FILE_SUFFIX = ".compact"
 +
 +  def getBatchIdFromFileName(fileName: String): Long = {
 +    fileName.stripSuffix(COMPACT_FILE_SUFFIX).toLong
 +  }
 +
 +  /**
 +   * Returns if this is a compaction batch. FileStreamSinkLog will compact old logs every
 +   * `compactInterval` commits.
 +   *
 +   * E.g., if `compactInterval` is 3, then 2, 5, 8, ... are all compaction batches.
 +   */
 +  def isCompactionBatch(batchId: Long, compactInterval: Int): Boolean = {
 +    (batchId + 1) % compactInterval == 0
 +  }
 +
 +  /**
 +   * Returns all valid batches before the specified `compactionBatchId`. They contain all logs we
 +   * need to do a new compaction.
 +   *
 +   * E.g., if `compactInterval` is 3 and `compactionBatchId` is 5, this method should returns
 +   * `Seq(2, 3, 4)` (Note: it includes the previous compaction batch 2).
 +   */
 +  def getValidBatchesBeforeCompactionBatch(
 +      compactionBatchId: Long,
 +      compactInterval: Int): Seq[Long] = {
 +    assert(isCompactionBatch(compactionBatchId, compactInterval),
 +      s"$compactionBatchId is not a compaction batch")
 +    (math.max(0, compactionBatchId - compactInterval)) until compactionBatchId
 +  }
 +
 +  /**
 +   * Returns all necessary logs before `batchId` (inclusive). If `batchId` is a compaction, just
 +   * return itself. Otherwise, it will find the previous compaction batch and return all batches
 +   * between it and `batchId`.
 +   */
 +  def getAllValidBatches(batchId: Long, compactInterval: Long): Seq[Long] = {
 +    assert(batchId >= 0)
 +    val start = math.max(0, (batchId + 1) / compactInterval * compactInterval - 1)
 +    start to batchId
 +  }
 +
 +  /**
 +   * Returns the next compaction batch id after `batchId`.
 +   */
 +  def nextCompactionBatchId(batchId: Long, compactInterval: Long): Long = {
 +    (batchId + compactInterval + 1) / compactInterval * compactInterval - 1
 +  }
 +}
++||||||| merged common ancestors
++=======
+ /*
+  * Licensed to the Apache Software Foundation (ASF) under one or more
+  * contributor license agreements.  See the NOTICE file distributed with
+  * this work for additional information regarding copyright ownership.
+  * The ASF licenses this file to You under the Apache License, Version 2.0
+  * (the "License"); you may not use this file except in compliance with
+  * the License.  You may obtain a copy of the License at
+  *
+  *    http://www.apache.org/licenses/LICENSE-2.0
+  *
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an "AS IS" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+ 
+ package org.apache.spark.sql.execution.streaming
+ 
+ import java.io.{InputStream, IOException, OutputStream}
+ import java.nio.charset.StandardCharsets.UTF_8
+ 
+ import scala.io.{Source => IOSource}
+ import scala.reflect.ClassTag
+ 
+ import org.apache.hadoop.fs.{Path, PathFilter}
+ 
+ import org.apache.spark.sql.SparkSession
+ 
+ /**
+  * An abstract class for compactible metadata logs. It will write one log file for each batch.
+  * The first line of the log file is the version number, and there are multiple serialized
+  * metadata lines following.
+  *
+  * As reading from many small files is usually pretty slow, also too many
+  * small files in one folder will mess the FS, [[CompactibleFileStreamLog]] will
+  * compact log files every 10 batches by default into a big file. When
+  * doing a compaction, it will read all old log files and merge them with the new batch.
+  */
+ abstract class CompactibleFileStreamLog[T: ClassTag](
+     metadataLogVersion: String,
+     sparkSession: SparkSession,
+     path: String)
+   extends HDFSMetadataLog[Array[T]](sparkSession, path) {
+ 
+   import CompactibleFileStreamLog._
+ 
+   /**
+    * If we delete the old files after compaction at once, there is a race condition in S3: other
+    * processes may see the old files are deleted but still cannot see the compaction file using
+    * "list". The `allFiles` handles this by looking for the next compaction file directly, however,
+    * a live lock may happen if the compaction happens too frequently: one processing keeps deleting
+    * old files while another one keeps retrying. Setting a reasonable cleanup delay could avoid it.
+    */
+   protected def fileCleanupDelayMs: Long
+ 
+   protected def isDeletingExpiredLog: Boolean
+ 
+   protected def compactInterval: Int
+ 
+   /**
+    * Serialize the data into encoded string.
+    */
+   protected def serializeData(t: T): String
+ 
+   /**
+    * Deserialize the string into data object.
+    */
+   protected def deserializeData(encodedString: String): T
+ 
+   /**
+    * Filter out the obsolete logs.
+    */
+   def compactLogs(logs: Seq[T]): Seq[T]
+ 
+   override def batchIdToPath(batchId: Long): Path = {
+     if (isCompactionBatch(batchId, compactInterval)) {
+       new Path(metadataPath, s"$batchId$COMPACT_FILE_SUFFIX")
+     } else {
+       new Path(metadataPath, batchId.toString)
+     }
+   }
+ 
+   override def pathToBatchId(path: Path): Long = {
+     getBatchIdFromFileName(path.getName)
+   }
+ 
+   override def isBatchFile(path: Path): Boolean = {
+     try {
+       getBatchIdFromFileName(path.getName)
+       true
+     } catch {
+       case _: NumberFormatException => false
+     }
+   }
+ 
+   override def serialize(logData: Array[T], out: OutputStream): Unit = {
+     // called inside a try-finally where the underlying stream is closed in the caller
+     out.write(metadataLogVersion.getBytes(UTF_8))
+     logData.foreach { data =>
+       out.write('\n')
+       out.write(serializeData(data).getBytes(UTF_8))
+     }
+   }
+ 
+   override def deserialize(in: InputStream): Array[T] = {
+     val lines = IOSource.fromInputStream(in, UTF_8.name()).getLines()
+     if (!lines.hasNext) {
+       throw new IllegalStateException("Incomplete log file")
+     }
+     val version = lines.next()
+     if (version != metadataLogVersion) {
+       throw new IllegalStateException(s"Unknown log version: ${version}")
+     }
+     lines.map(deserializeData).toArray
+   }
+ 
+   override def add(batchId: Long, logs: Array[T]): Boolean = {
+     if (isCompactionBatch(batchId, compactInterval)) {
+       compact(batchId, logs)
+     } else {
+       super.add(batchId, logs)
+     }
+   }
+ 
+   /**
+    * Compacts all logs before `batchId` plus the provided `logs`, and writes them into the
+    * corresponding `batchId` file. It will delete expired files as well if enabled.
+    */
+   private def compact(batchId: Long, logs: Array[T]): Boolean = {
+     val validBatches = getValidBatchesBeforeCompactionBatch(batchId, compactInterval)
+     val allLogs = validBatches.flatMap(batchId => super.get(batchId)).flatten ++ logs
+     if (super.add(batchId, compactLogs(allLogs).toArray)) {
+       if (isDeletingExpiredLog) {
+         deleteExpiredLog(batchId)
+       }
+       true
+     } else {
+       // Return false as there is another writer.
+       false
+     }
+   }
+ 
+   /**
+    * Returns all files except the deleted ones.
+    */
+   def allFiles(): Array[T] = {
+     var latestId = getLatest().map(_._1).getOrElse(-1L)
+     // There is a race condition when `FileStreamSink` is deleting old files and `StreamFileCatalog`
+     // is calling this method. This loop will retry the reading to deal with the
+     // race condition.
+     while (true) {
+       if (latestId >= 0) {
+         try {
+           val logs =
+             getAllValidBatches(latestId, compactInterval).flatMap(id => super.get(id)).flatten
+           return compactLogs(logs).toArray
+         } catch {
+           case e: IOException =>
+             // Another process using `CompactibleFileStreamLog` may delete the batch files when
+             // `StreamFileCatalog` are reading. However, it only happens when a compaction is
+             // deleting old files. If so, let's try the next compaction batch and we should find it.
+             // Otherwise, this is a real IO issue and we should throw it.
+             latestId = nextCompactionBatchId(latestId, compactInterval)
+             super.get(latestId).getOrElse {
+               throw e
+             }
+         }
+       } else {
+         return Array.empty
+       }
+     }
+     Array.empty
+   }
+ 
+   /**
+    * Since all logs before `compactionBatchId` are compacted and written into the
+    * `compactionBatchId` log file, they can be removed. However, due to the eventual consistency of
+    * S3, the compaction file may not be seen by other processes at once. So we only delete files
+    * created `fileCleanupDelayMs` milliseconds ago.
+    */
+   private def deleteExpiredLog(compactionBatchId: Long): Unit = {
+     val expiredTime = System.currentTimeMillis() - fileCleanupDelayMs
+     fileManager.list(metadataPath, new PathFilter {
+       override def accept(path: Path): Boolean = {
+         try {
+           val batchId = getBatchIdFromFileName(path.getName)
+           batchId < compactionBatchId
+         } catch {
+           case _: NumberFormatException =>
+             false
+         }
+       }
+     }).foreach { f =>
+       if (f.getModificationTime <= expiredTime) {
+         fileManager.delete(f.getPath)
+       }
+     }
+   }
+ }
+ 
+ object CompactibleFileStreamLog {
+   val COMPACT_FILE_SUFFIX = ".compact"
+ 
+   def getBatchIdFromFileName(fileName: String): Long = {
+     fileName.stripSuffix(COMPACT_FILE_SUFFIX).toLong
+   }
+ 
+   /**
+    * Returns if this is a compaction batch. FileStreamSinkLog will compact old logs every
+    * `compactInterval` commits.
+    *
+    * E.g., if `compactInterval` is 3, then 2, 5, 8, ... are all compaction batches.
+    */
+   def isCompactionBatch(batchId: Long, compactInterval: Int): Boolean = {
+     (batchId + 1) % compactInterval == 0
+   }
+ 
+   /**
+    * Returns all valid batches before the specified `compactionBatchId`. They contain all logs we
+    * need to do a new compaction.
+    *
+    * E.g., if `compactInterval` is 3 and `compactionBatchId` is 5, this method should returns
+    * `Seq(2, 3, 4)` (Note: it includes the previous compaction batch 2).
+    */
+   def getValidBatchesBeforeCompactionBatch(
+       compactionBatchId: Long,
+       compactInterval: Int): Seq[Long] = {
+     assert(isCompactionBatch(compactionBatchId, compactInterval),
+       s"$compactionBatchId is not a compaction batch")
+     (math.max(0, compactionBatchId - compactInterval)) until compactionBatchId
+   }
+ 
+   /**
+    * Returns all necessary logs before `batchId` (inclusive). If `batchId` is a compaction, just
+    * return itself. Otherwise, it will find the previous compaction batch and return all batches
+    * between it and `batchId`.
+    */
+   def getAllValidBatches(batchId: Long, compactInterval: Long): Seq[Long] = {
+     assert(batchId >= 0)
+     val start = math.max(0, (batchId + 1) / compactInterval * compactInterval - 1)
+     start to batchId
+   }
+ 
+   /**
+    * Returns the next compaction batch id after `batchId`.
+    */
+   def nextCompactionBatchId(batchId: Long, compactInterval: Long): Long = {
+     (batchId + compactInterval + 1) / compactInterval * compactInterval - 1
+   }
+ }
++>>>>>>> v2.0.2
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
index 8c3e718,c47033a..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
@@@ -38,28 -39,35 +39,72 @@@ class FileStreamSource
      metadataPath: String,
      options: Map[String, String]) extends Source with Logging {
  
++<<<<<<< HEAD
 +  import FileStreamSource._
 +
 +  private val sourceOptions = new FileStreamOptions(options)
 +
 +  private val qualifiedBasePath: Path = {
 +    val fs = new Path(path).getFileSystem(sparkSession.sessionState.newHadoopConf())
 +    fs.makeQualified(new Path(path))  // can contains glob patterns
 +  }
 +
 +  private val metadataLog =
 +    new FileStreamSourceLog(FileStreamSourceLog.VERSION, sparkSession, metadataPath)
++||||||| merged common ancestors
++  private val fs = new Path(path).getFileSystem(sparkSession.sessionState.newHadoopConf())
++  private val qualifiedBasePath = fs.makeQualified(new Path(path)) // can contains glob patterns
++  private val metadataLog = new HDFSMetadataLog[Seq[String]](sparkSession, metadataPath)
++=======
+   import FileStreamSource._
+ 
+   private val sourceOptions = new FileStreamOptions(options)
+ 
+   private val qualifiedBasePath: Path = {
+     val fs = new Path(path).getFileSystem(sparkSession.sessionState.newHadoopConf())
+     fs.makeQualified(new Path(path))  // can contains glob patterns
+   }
+ 
+   private val optionsWithPartitionBasePath = sourceOptions.optionMapWithoutPath ++ {
+     if (!SparkHadoopUtil.get.isGlobPath(new Path(path)) && options.contains("path")) {
+       Map("basePath" -> path)
+     } else {
+       Map()
+     }}
+ 
+   private val metadataLog =
+     new FileStreamSourceLog(FileStreamSourceLog.VERSION, sparkSession, metadataPath)
++>>>>>>> v2.0.2
    private var maxBatchId = metadataLog.getLatest().map(_._1).getOrElse(-1L)
  
    /** Maximum number of new files to be considered in each batch */
++<<<<<<< HEAD
 +  private val maxFilesPerBatch = sourceOptions.maxFilesPerTrigger
 +
 +  /** A mapping from a file that we have processed to some timestamp it was last modified. */
 +  // Visible for testing and debugging in production.
 +  val seenFiles = new SeenFilesMap(sourceOptions.maxFileAgeMs)
++||||||| merged common ancestors
++  private val maxFilesPerBatch = getMaxFilesPerBatch()
++=======
+   private val maxFilesPerBatch = sourceOptions.maxFilesPerTrigger
++>>>>>>> v2.0.2
+ 
++<<<<<<< HEAD
++  metadataLog.allFiles().foreach { entry =>
++    seenFiles.add(entry.path, entry.timestamp)
++||||||| merged common ancestors
++  private val seenFiles = new OpenHashSet[String]
++  metadataLog.get(None, Some(maxBatchId)).foreach { case (batchId, files) =>
++    files.foreach(seenFiles.add)
++=======
+   /** A mapping from a file that we have processed to some timestamp it was last modified. */
+   // Visible for testing and debugging in production.
+   val seenFiles = new SeenFilesMap(sourceOptions.maxFileAgeMs)
  
    metadataLog.allFiles().foreach { entry =>
      seenFiles.add(entry.path, entry.timestamp)
++>>>>>>> v2.0.2
    }
    seenFiles.purge()
  
@@@ -135,10 -143,11 +180,20 @@@
          sparkSession,
          paths = files.map(_.path),
          userSpecifiedSchema = Some(schema),
+         partitionColumns = partitionColumns,
          className = fileFormatClassName,
++<<<<<<< HEAD
 +        options = sourceOptions.optionMapWithoutPath)
 +    Dataset.ofRows(sparkSession, LogicalRelation(newDataSource.resolveRelation(
 +      checkPathExist = false)))
++||||||| merged common ancestors
++        options = newOptions)
++    Dataset.ofRows(sparkSession, LogicalRelation(newDataSource.resolveRelation()))
++=======
+         options = optionsWithPartitionBasePath)
+     Dataset.ofRows(sparkSession, LogicalRelation(newDataSource.resolveRelation(
+       checkPathExist = false)))
++>>>>>>> v2.0.2
    }
  
    /**
diff --cc sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala
index b7587f2,4707bfb..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala
@@@ -105,7 -112,7 +112,13 @@@ class StreamExecution
    var lastExecution: QueryExecution = null
  
    @volatile
++<<<<<<< HEAD
 +  var streamDeathCause: StreamingQueryException = null
++||||||| merged common ancestors
++  private[sql] var streamDeathCause: StreamingQueryException = null
++=======
+   private var streamDeathCause: StreamingQueryException = null
++>>>>>>> v2.0.2
  
    /* Get the call site in the caller thread; will pass this into the micro batch thread */
    private val callSite = Utils.getCallSite()
@@@ -115,8 -133,8 +139,16 @@@
     * [[org.apache.spark.util.UninterruptibleThread]] to avoid potential deadlocks in using
     * [[HDFSMetadataLog]]. See SPARK-14131 for more details.
     */
++<<<<<<< HEAD
 +  val microBatchThread =
 +    new UninterruptibleThread(s"stream execution thread for $name") {
++||||||| merged common ancestors
++  private[sql] val microBatchThread =
++    new UninterruptibleThread(s"stream execution thread for $name") {
++=======
+   val microBatchThread =
+     new StreamExecutionThread(s"stream execution thread for $name") {
++>>>>>>> v2.0.2
        override def run(): Unit = {
          // To fix call site like "run at <unknown>:0", we bridge the call site from the caller
          // thread to this micro batch thread
@@@ -214,10 -251,23 +265,32 @@@
            e,
            Some(committedOffsets.toCompositeOffset(sources)))
          logError(s"Query $name terminated with error", e)
+         // Rethrow the fatal errors to allow the user using `Thread.UncaughtExceptionHandler` to
+         // handle them
+         if (!NonFatal(e)) {
+           throw e
+         }
      } finally {
        state = TERMINATED
+ 
+       // Update metrics and status
+       streamMetrics.stop()
+       sparkSession.sparkContext.env.metricsSystem.removeSource(streamMetrics)
+       updateStatus()
+ 
+       // Notify others
        sparkSession.streams.notifyQueryTermination(StreamExecution.this)
++<<<<<<< HEAD
 +      postEvent(new QueryTerminated(this.toInfo, exception.map(_.cause).map(Utils.exceptionString)))
++||||||| merged common ancestors
++      postEvent(new QueryTerminated(
++        this.toInfo,
++        exception.map(_.getMessage),
++        exception.map(_.getStackTrace.toSeq).getOrElse(Nil)))
++=======
+       postEvent(
+         new QueryTerminatedEvent(currentStatus, exception.map(_.cause).map(Utils.exceptionString)))
++>>>>>>> v2.0.2
        terminationLatch.countDown()
      }
    }
@@@ -287,16 -342,29 +365,46 @@@
        }
      }
      if (hasNewData) {
++<<<<<<< HEAD
 +      assert(offsetLog.add(currentBatchId, availableOffsets.toCompositeOffset(sources)),
 +        s"Concurrent update to the log. Multiple streaming jobs detected for $currentBatchId")
 +      logInfo(s"Committed offsets for batch $currentBatchId.")
 +
 +      // Now that we have logged the new batch, no further processing will happen for
 +      // the previous batch, and it is safe to discard the old metadata.
 +      // Note that purge is exclusive, i.e. it purges everything before currentBatchId.
 +      // NOTE: If StreamExecution implements pipeline parallelism (multiple batches in
 +      // flight at the same time), this cleanup logic will need to change.
 +      offsetLog.purge(currentBatchId)
++||||||| merged common ancestors
++      assert(offsetLog.add(currentBatchId, availableOffsets.toCompositeOffset(sources)),
++        s"Concurrent update to the log. Multiple streaming jobs detected for $currentBatchId")
++      logInfo(s"Committed offsets for batch $currentBatchId.")
++=======
+       reportTimeTaken(OFFSET_WAL_WRITE_LATENCY) {
+         assert(offsetLog.add(currentBatchId, availableOffsets.toCompositeOffset(sources)),
+           s"Concurrent update to the log. Multiple streaming jobs detected for $currentBatchId")
+         logInfo(s"Committed offsets for batch $currentBatchId.")
+ 
+         // NOTE: The following code is correct because runBatches() processes exactly one
+         // batch at a time. If we add pipeline parallelism (multiple batches in flight at
+         // the same time), this cleanup logic will need to change.
+ 
+         // Now that we've updated the scheduler's persistent checkpoint, it is safe for the
+         // sources to discard data from the previous batch.
+         val prevBatchOff = offsetLog.get(currentBatchId - 1)
+         if (prevBatchOff.isDefined) {
+           prevBatchOff.get.toStreamProgress(sources).foreach {
+             case (src, off) => src.commit(off)
+           }
+         }
+ 
+         // Now that we have logged the new batch, no further processing will happen for
+         // the batch before the previous batch, and it is safe to discard the old metadata.
+         // Note that purge is exclusive, i.e. it purges everything before the target ID.
+         offsetLog.purge(currentBatchId - 1)
+       }
++>>>>>>> v2.0.2
      } else {
        awaitBatchLock.lock()
        try {
diff --cc sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 2614032,7598d47..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@@ -56,7 -57,7 +57,13 @@@ object SQLConf 
    val WAREHOUSE_PATH = SQLConfigBuilder("spark.sql.warehouse.dir")
      .doc("The default location for managed databases and tables.")
      .stringConf
++<<<<<<< HEAD
 +    .createWithDefault("file:${system:user.dir}/spark-warehouse")
++||||||| merged common ancestors
++    .createWithDefault("${system:user.dir}/spark-warehouse")
++=======
+     .createWithDefault(Utils.resolveURI("spark-warehouse").toString)
++>>>>>>> v2.0.2
  
    val OPTIMIZER_MAX_ITERATIONS = SQLConfigBuilder("spark.sql.optimizer.maxIterations")
      .internal()
diff --cc sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala
index f70c7d0,b959444..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala
diff --cc sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala
index db606ab,9e311fa..0000000
--- a/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala
@@@ -104,9 -104,7 +104,18 @@@ object StreamingQueryListener 
     * @since 2.0.0
     */
    @Experimental
++<<<<<<< HEAD
 +  class QueryTerminated private[sql](
 +      val queryInfo: StreamingQueryInfo,
 +      val exception: Option[String]) extends Event
++||||||| merged common ancestors
++  class QueryTerminated private[sql](
++      val queryInfo: StreamingQueryInfo,
++      val exception: Option[String],
++      val stackTrace: Seq[StackTraceElement]) extends Event
++=======
+   class QueryTerminatedEvent private[sql](
+       val queryStatus: StreamingQueryStatus,
+       val exception: Option[String]) extends Event
++>>>>>>> v2.0.2
  }
diff --cc sql/core/src/test/resources/sql-tests/inputs/array.sql
index 4038a0d,984321a..0000000
--- a/sql/core/src/test/resources/sql-tests/inputs/array.sql
+++ b/sql/core/src/test/resources/sql-tests/inputs/array.sql
@@@ -1,86 -1,92 +1,182 @@@
++<<<<<<< HEAD
 +-- test cases for array functions
 +
 +create temporary view data as select * from values
 +  ("one", array(11, 12, 13), array(array(111, 112, 113), array(121, 122, 123))),
 +  ("two", array(21, 22, 23), array(array(211, 212, 213), array(221, 222, 223)))
 +  as data(a, b, c);
 +
 +select * from data;
 +
 +-- index into array
 +select a, b[0], b[0] + b[1] from data;
 +
 +-- index into array of arrays
 +select a, c[0][0] + c[0][0 + 1] from data;
 +
 +
 +create temporary view primitive_arrays as select * from values (
 +  array(true),
 +  array(2Y, 1Y),
 +  array(2S, 1S),
 +  array(2, 1),
 +  array(2L, 1L),
 +  array(9223372036854775809, 9223372036854775808),
 +  array(2.0D, 1.0D),
 +  array(float(2.0), float(1.0)),
 +  array(date '2016-03-14', date '2016-03-13'),
 +  array(timestamp '2016-11-15 20:54:00.000',  timestamp '2016-11-12 20:54:00.000')
 +) as primitive_arrays(
 +  boolean_array,
 +  tinyint_array,
 +  smallint_array,
 +  int_array,
 +  bigint_array,
 +  decimal_array,
 +  double_array,
 +  float_array,
 +  date_array,
 +  timestamp_array
 +);
 +
 +select * from primitive_arrays;
 +
 +-- array_contains on all primitive types: result should alternate between true and false
 +select
 +  array_contains(boolean_array, true), array_contains(boolean_array, false),
 +  array_contains(tinyint_array, 2Y), array_contains(tinyint_array, 0Y),
 +  array_contains(smallint_array, 2S), array_contains(smallint_array, 0S),
 +  array_contains(int_array, 2), array_contains(int_array, 0),
 +  array_contains(bigint_array, 2L), array_contains(bigint_array, 0L),
 +  array_contains(decimal_array, 9223372036854775809), array_contains(decimal_array, 1),
 +  array_contains(double_array, 2.0D), array_contains(double_array, 0.0D),
 +  array_contains(float_array, float(2.0)), array_contains(float_array, float(0.0)),
 +  array_contains(date_array, date '2016-03-14'), array_contains(date_array, date '2016-01-01'),
 +  array_contains(timestamp_array, timestamp '2016-11-15 20:54:00.000'), array_contains(timestamp_array, timestamp '2016-01-01 20:54:00.000')
 +from primitive_arrays;
 +
 +-- array_contains on nested arrays
 +select array_contains(b, 11), array_contains(c, array(111, 112, 113)) from data;
 +
 +-- sort_array
 +select
 +  sort_array(boolean_array),
 +  sort_array(tinyint_array),
 +  sort_array(smallint_array),
 +  sort_array(int_array),
 +  sort_array(bigint_array),
 +  sort_array(decimal_array),
 +  sort_array(double_array),
 +  sort_array(float_array),
 +  sort_array(date_array),
 +  sort_array(timestamp_array)
 +from primitive_arrays;
 +
 +-- size
 +select
 +  size(boolean_array),
 +  size(tinyint_array),
 +  size(smallint_array),
 +  size(int_array),
 +  size(bigint_array),
 +  size(decimal_array),
 +  size(double_array),
 +  size(float_array),
 +  size(date_array),
 +  size(timestamp_array)
 +from primitive_arrays;
++||||||| merged common ancestors
++=======
+ -- test cases for array functions
+ 
+ create temporary view data as select * from values
+   ("one", array(11, 12, 13), array(array(111, 112, 113), array(121, 122, 123))),
+   ("two", array(21, 22, 23), array(array(211, 212, 213), array(221, 222, 223)))
+   as data(a, b, c);
+ 
+ select * from data;
+ 
+ -- index into array
+ select a, b[0], b[0] + b[1] from data;
+ 
+ -- index into array of arrays
+ select a, c[0][0] + c[0][0 + 1] from data;
+ 
+ 
+ create temporary view primitive_arrays as select * from values (
+   array(true),
+   array(2Y, 1Y),
+   array(2S, 1S),
+   array(2, 1),
+   array(2L, 1L),
+   array(9223372036854775809, 9223372036854775808),
+   array(2.0D, 1.0D),
+   array(float(2.0), float(1.0)),
+   array(date '2016-03-14', date '2016-03-13'),
+   array(timestamp '2016-11-15 20:54:00.000',  timestamp '2016-11-12 20:54:00.000')
+ ) as primitive_arrays(
+   boolean_array,
+   tinyint_array,
+   smallint_array,
+   int_array,
+   bigint_array,
+   decimal_array,
+   double_array,
+   float_array,
+   date_array,
+   timestamp_array
+ );
+ 
+ select * from primitive_arrays;
+ 
+ -- array_contains on all primitive types: result should alternate between true and false
+ select
+   array_contains(boolean_array, true), array_contains(boolean_array, false),
+   array_contains(tinyint_array, 2Y), array_contains(tinyint_array, 0Y),
+   array_contains(smallint_array, 2S), array_contains(smallint_array, 0S),
+   array_contains(int_array, 2), array_contains(int_array, 0),
+   array_contains(bigint_array, 2L), array_contains(bigint_array, 0L),
+   array_contains(decimal_array, 9223372036854775809), array_contains(decimal_array, 1),
+   array_contains(double_array, 2.0D), array_contains(double_array, 0.0D),
+   array_contains(float_array, float(2.0)), array_contains(float_array, float(0.0)),
+   array_contains(date_array, date '2016-03-14'), array_contains(date_array, date '2016-01-01'),
+   array_contains(timestamp_array, timestamp '2016-11-15 20:54:00.000'), array_contains(timestamp_array, timestamp '2016-01-01 20:54:00.000')
+ from primitive_arrays;
+ 
+ -- array_contains on nested arrays
+ select array_contains(b, 11), array_contains(c, array(111, 112, 113)) from data;
+ 
+ -- sort_array
+ select
+   sort_array(boolean_array),
+   sort_array(tinyint_array),
+   sort_array(smallint_array),
+   sort_array(int_array),
+   sort_array(bigint_array),
+   sort_array(decimal_array),
+   sort_array(double_array),
+   sort_array(float_array),
+   sort_array(date_array),
+   sort_array(timestamp_array)
+ from primitive_arrays;
+ 
+ -- sort_array with an invalid string literal for the argument of sort order.
+ select sort_array(array('b', 'd'), '1');
+ 
+ -- sort_array with an invalid null literal casted as boolean for the argument of sort order.
+ select sort_array(array('b', 'd'), cast(NULL as boolean));
+ 
+ -- size
+ select
+   size(boolean_array),
+   size(tinyint_array),
+   size(smallint_array),
+   size(int_array),
+   size(bigint_array),
+   size(decimal_array),
+   size(double_array),
+   size(float_array),
+   size(date_array),
+   size(timestamp_array)
+ from primitive_arrays;
++>>>>>>> v2.0.2
diff --cc sql/core/src/test/resources/sql-tests/inputs/group-by.sql
index 6741703,d950ec8..0000000
--- a/sql/core/src/test/resources/sql-tests/inputs/group-by.sql
+++ b/sql/core/src/test/resources/sql-tests/inputs/group-by.sql
@@@ -1,17 -1,34 +1,55 @@@
++<<<<<<< HEAD
 +-- Temporary data.
 +create temporary view myview as values 128, 256 as v(int_col);
 +
 +-- group by should produce all input rows,
 +select int_col, count(*) from myview group by int_col;
 +
 +-- group by should produce a single row.
 +select 'foo', count(*) from myview group by 1;
 +
 +-- group-by should not produce any rows (whole stage code generation).
 +select 'foo' from myview where int_col == 0 group by 1;
 +
 +-- group-by should not produce any rows (hash aggregate).
 +select 'foo', approx_count_distinct(int_col) from myview where int_col == 0 group by 1;
 +
 +-- group-by should not produce any rows (sort aggregate).
 +select 'foo', max(struct(int_col)) from myview where int_col == 0 group by 1;
++||||||| merged common ancestors
++=======
+ -- Test data.
+ CREATE OR REPLACE TEMPORARY VIEW testData AS SELECT * FROM VALUES
+ (1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2), (null, 1), (3, null), (null, null)
+ AS testData(a, b);
+ 
+ -- Aggregate with empty GroupBy expressions.
+ SELECT a, COUNT(b) FROM testData;
+ SELECT COUNT(a), COUNT(b) FROM testData;
+ 
+ -- Aggregate with non-empty GroupBy expressions.
+ SELECT a, COUNT(b) FROM testData GROUP BY a;
+ SELECT a, COUNT(b) FROM testData GROUP BY b;
+ SELECT COUNT(a), COUNT(b) FROM testData GROUP BY a;
+ 
+ -- Aggregate grouped by literals.
+ SELECT 'foo', COUNT(a) FROM testData GROUP BY 1;
+ 
+ -- Aggregate grouped by literals (whole stage code generation).
+ SELECT 'foo' FROM testData WHERE a = 0 GROUP BY 1;
+ 
+ -- Aggregate grouped by literals (hash aggregate).
+ SELECT 'foo', APPROX_COUNT_DISTINCT(a) FROM testData WHERE a = 0 GROUP BY 1;
+ 
+ -- Aggregate grouped by literals (sort aggregate).
+ SELECT 'foo', MAX(STRUCT(a)) FROM testData WHERE a = 0 GROUP BY 1;
+ 
+ -- Aggregate with complex GroupBy expressions.
+ SELECT a + b, COUNT(b) FROM testData GROUP BY a + b;
+ SELECT a + 2, COUNT(b) FROM testData GROUP BY a + 1;
+ SELECT a + 1 + 1, COUNT(b) FROM testData GROUP BY a + 1;
+ 
+ -- Aggregate with nulls.
+ SELECT SKEWNESS(a), KURTOSIS(a), MIN(a), MAX(a), AVG(a), VARIANCE(a), STDDEV(a), SUM(a), COUNT(a)
+ FROM testData;
++>>>>>>> v2.0.2
diff --cc sql/core/src/test/resources/sql-tests/results/array.sql.out
index 4a1d149,499a3d5..0000000
--- a/sql/core/src/test/resources/sql-tests/results/array.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/array.sql.out
@@@ -1,144 -1,159 +1,307 @@@
++<<<<<<< HEAD
 +-- Automatically generated by SQLQueryTestSuite
 +-- Number of queries: 10
 +
 +
 +-- !query 0
 +create temporary view data as select * from values
 +  ("one", array(11, 12, 13), array(array(111, 112, 113), array(121, 122, 123))),
 +  ("two", array(21, 22, 23), array(array(211, 212, 213), array(221, 222, 223)))
 +  as data(a, b, c)
 +-- !query 0 schema
 +struct<>
 +-- !query 0 output
 +
 +
 +
 +-- !query 1
 +select * from data
 +-- !query 1 schema
 +struct<a:string,b:array<int>,c:array<array<int>>>
 +-- !query 1 output
 +one	[11,12,13]	[[111,112,113],[121,122,123]]
 +two	[21,22,23]	[[211,212,213],[221,222,223]]
 +
 +
 +-- !query 2
 +select a, b[0], b[0] + b[1] from data
 +-- !query 2 schema
 +struct<a:string,b[0]:int,(b[0] + b[1]):int>
 +-- !query 2 output
 +one	11	23
 +two	21	43
 +
 +
 +-- !query 3
 +select a, c[0][0] + c[0][0 + 1] from data
 +-- !query 3 schema
 +struct<a:string,(c[0][0] + c[0][(0 + 1)]):int>
 +-- !query 3 output
 +one	223
 +two	423
 +
 +
 +-- !query 4
 +create temporary view primitive_arrays as select * from values (
 +  array(true),
 +  array(2Y, 1Y),
 +  array(2S, 1S),
 +  array(2, 1),
 +  array(2L, 1L),
 +  array(9223372036854775809, 9223372036854775808),
 +  array(2.0D, 1.0D),
 +  array(float(2.0), float(1.0)),
 +  array(date '2016-03-14', date '2016-03-13'),
 +  array(timestamp '2016-11-15 20:54:00.000',  timestamp '2016-11-12 20:54:00.000')
 +) as primitive_arrays(
 +  boolean_array,
 +  tinyint_array,
 +  smallint_array,
 +  int_array,
 +  bigint_array,
 +  decimal_array,
 +  double_array,
 +  float_array,
 +  date_array,
 +  timestamp_array
 +)
 +-- !query 4 schema
 +struct<>
 +-- !query 4 output
 +
 +
 +
 +-- !query 5
 +select * from primitive_arrays
 +-- !query 5 schema
 +struct<boolean_array:array<boolean>,tinyint_array:array<tinyint>,smallint_array:array<smallint>,int_array:array<int>,bigint_array:array<bigint>,decimal_array:array<decimal(19,0)>,double_array:array<double>,float_array:array<float>,date_array:array<date>,timestamp_array:array<timestamp>>
 +-- !query 5 output
 +[true]	[2,1]	[2,1]	[2,1]	[2,1]	[9223372036854775809,9223372036854775808]	[2.0,1.0]	[2.0,1.0]	[2016-03-14,2016-03-13]	[2016-11-15 20:54:00.0,2016-11-12 20:54:00.0]
 +
 +
 +-- !query 6
 +select
 +  array_contains(boolean_array, true), array_contains(boolean_array, false),
 +  array_contains(tinyint_array, 2Y), array_contains(tinyint_array, 0Y),
 +  array_contains(smallint_array, 2S), array_contains(smallint_array, 0S),
 +  array_contains(int_array, 2), array_contains(int_array, 0),
 +  array_contains(bigint_array, 2L), array_contains(bigint_array, 0L),
 +  array_contains(decimal_array, 9223372036854775809), array_contains(decimal_array, 1),
 +  array_contains(double_array, 2.0D), array_contains(double_array, 0.0D),
 +  array_contains(float_array, float(2.0)), array_contains(float_array, float(0.0)),
 +  array_contains(date_array, date '2016-03-14'), array_contains(date_array, date '2016-01-01'),
 +  array_contains(timestamp_array, timestamp '2016-11-15 20:54:00.000'), array_contains(timestamp_array, timestamp '2016-01-01 20:54:00.000')
 +from primitive_arrays
 +-- !query 6 schema
 +struct<array_contains(boolean_array, true):boolean,array_contains(boolean_array, false):boolean,array_contains(tinyint_array, 2):boolean,array_contains(tinyint_array, 0):boolean,array_contains(smallint_array, 2):boolean,array_contains(smallint_array, 0):boolean,array_contains(int_array, 2):boolean,array_contains(int_array, 0):boolean,array_contains(bigint_array, 2):boolean,array_contains(bigint_array, 0):boolean,array_contains(decimal_array, 9223372036854775809):boolean,array_contains(decimal_array, CAST(1 AS DECIMAL(19,0))):boolean,array_contains(double_array, 2.0):boolean,array_contains(double_array, 0.0):boolean,array_contains(float_array, CAST(2.0 AS FLOAT)):boolean,array_contains(float_array, CAST(0.0 AS FLOAT)):boolean,array_contains(date_array, DATE '2016-03-14'):boolean,array_contains(date_array, DATE '2016-01-01'):boolean,array_contains(timestamp_array, TIMESTAMP('2016-11-15 20:54:00.0')):boolean,array_contains(timestamp_array, TIMESTAMP('2016-01-01 20:54:00.0')):boolean>
 +-- !query 6 output
 +true	false	true	false	true	false	true	false	true	false	true	false	true	false	true	false	true	false	true	false
 +
 +
 +-- !query 7
 +select array_contains(b, 11), array_contains(c, array(111, 112, 113)) from data
 +-- !query 7 schema
 +struct<array_contains(b, 11):boolean,array_contains(c, array(111, 112, 113)):boolean>
 +-- !query 7 output
 +false	false
 +true	true
 +
 +
 +-- !query 8
 +select
 +  sort_array(boolean_array),
 +  sort_array(tinyint_array),
 +  sort_array(smallint_array),
 +  sort_array(int_array),
 +  sort_array(bigint_array),
 +  sort_array(decimal_array),
 +  sort_array(double_array),
 +  sort_array(float_array),
 +  sort_array(date_array),
 +  sort_array(timestamp_array)
 +from primitive_arrays
 +-- !query 8 schema
 +struct<sort_array(boolean_array, true):array<boolean>,sort_array(tinyint_array, true):array<tinyint>,sort_array(smallint_array, true):array<smallint>,sort_array(int_array, true):array<int>,sort_array(bigint_array, true):array<bigint>,sort_array(decimal_array, true):array<decimal(19,0)>,sort_array(double_array, true):array<double>,sort_array(float_array, true):array<float>,sort_array(date_array, true):array<date>,sort_array(timestamp_array, true):array<timestamp>>
 +-- !query 8 output
 +[true]	[1,2]	[1,2]	[1,2]	[1,2]	[9223372036854775808,9223372036854775809]	[1.0,2.0]	[1.0,2.0]	[2016-03-13,2016-03-14]	[2016-11-12 20:54:00.0,2016-11-15 20:54:00.0]
 +
 +
 +-- !query 9
 +select
 +  size(boolean_array),
 +  size(tinyint_array),
 +  size(smallint_array),
 +  size(int_array),
 +  size(bigint_array),
 +  size(decimal_array),
 +  size(double_array),
 +  size(float_array),
 +  size(date_array),
 +  size(timestamp_array)
 +from primitive_arrays
 +-- !query 9 schema
 +struct<size(boolean_array):int,size(tinyint_array):int,size(smallint_array):int,size(int_array):int,size(bigint_array):int,size(decimal_array):int,size(double_array):int,size(float_array):int,size(date_array):int,size(timestamp_array):int>
 +-- !query 9 output
 +1	2	2	2	2	2	2	2	2	2
++||||||| merged common ancestors
++=======
+ -- Automatically generated by SQLQueryTestSuite
+ -- Number of queries: 10
+ 
+ 
+ -- !query 0
+ create temporary view data as select * from values
+   ("one", array(11, 12, 13), array(array(111, 112, 113), array(121, 122, 123))),
+   ("two", array(21, 22, 23), array(array(211, 212, 213), array(221, 222, 223)))
+   as data(a, b, c)
+ -- !query 0 schema
+ struct<>
+ -- !query 0 output
+ 
+ 
+ 
+ -- !query 1
+ select * from data
+ -- !query 1 schema
+ struct<a:string,b:array<int>,c:array<array<int>>>
+ -- !query 1 output
+ one	[11,12,13]	[[111,112,113],[121,122,123]]
+ two	[21,22,23]	[[211,212,213],[221,222,223]]
+ 
+ 
+ -- !query 2
+ select a, b[0], b[0] + b[1] from data
+ -- !query 2 schema
+ struct<a:string,b[0]:int,(b[0] + b[1]):int>
+ -- !query 2 output
+ one	11	23
+ two	21	43
+ 
+ 
+ -- !query 3
+ select a, c[0][0] + c[0][0 + 1] from data
+ -- !query 3 schema
+ struct<a:string,(c[0][0] + c[0][(0 + 1)]):int>
+ -- !query 3 output
+ one	223
+ two	423
+ 
+ 
+ -- !query 4
+ create temporary view primitive_arrays as select * from values (
+   array(true),
+   array(2Y, 1Y),
+   array(2S, 1S),
+   array(2, 1),
+   array(2L, 1L),
+   array(9223372036854775809, 9223372036854775808),
+   array(2.0D, 1.0D),
+   array(float(2.0), float(1.0)),
+   array(date '2016-03-14', date '2016-03-13'),
+   array(timestamp '2016-11-15 20:54:00.000',  timestamp '2016-11-12 20:54:00.000')
+ ) as primitive_arrays(
+   boolean_array,
+   tinyint_array,
+   smallint_array,
+   int_array,
+   bigint_array,
+   decimal_array,
+   double_array,
+   float_array,
+   date_array,
+   timestamp_array
+ )
+ -- !query 4 schema
+ struct<>
+ -- !query 4 output
+ 
+ 
+ 
+ -- !query 5
+ select * from primitive_arrays
+ -- !query 5 schema
+ struct<boolean_array:array<boolean>,tinyint_array:array<tinyint>,smallint_array:array<smallint>,int_array:array<int>,bigint_array:array<bigint>,decimal_array:array<decimal(19,0)>,double_array:array<double>,float_array:array<float>,date_array:array<date>,timestamp_array:array<timestamp>>
+ -- !query 5 output
+ [true]	[2,1]	[2,1]	[2,1]	[2,1]	[9223372036854775809,9223372036854775808]	[2.0,1.0]	[2.0,1.0]	[2016-03-14,2016-03-13]	[2016-11-15 20:54:00.0,2016-11-12 20:54:00.0]
+ 
+ 
+ -- !query 6
+ select
+   array_contains(boolean_array, true), array_contains(boolean_array, false),
+   array_contains(tinyint_array, 2Y), array_contains(tinyint_array, 0Y),
+   array_contains(smallint_array, 2S), array_contains(smallint_array, 0S),
+   array_contains(int_array, 2), array_contains(int_array, 0),
+   array_contains(bigint_array, 2L), array_contains(bigint_array, 0L),
+   array_contains(decimal_array, 9223372036854775809), array_contains(decimal_array, 1),
+   array_contains(double_array, 2.0D), array_contains(double_array, 0.0D),
+   array_contains(float_array, float(2.0)), array_contains(float_array, float(0.0)),
+   array_contains(date_array, date '2016-03-14'), array_contains(date_array, date '2016-01-01'),
+   array_contains(timestamp_array, timestamp '2016-11-15 20:54:00.000'), array_contains(timestamp_array, timestamp '2016-01-01 20:54:00.000')
+ from primitive_arrays
+ -- !query 6 schema
+ struct<array_contains(boolean_array, true):boolean,array_contains(boolean_array, false):boolean,array_contains(tinyint_array, 2):boolean,array_contains(tinyint_array, 0):boolean,array_contains(smallint_array, 2):boolean,array_contains(smallint_array, 0):boolean,array_contains(int_array, 2):boolean,array_contains(int_array, 0):boolean,array_contains(bigint_array, 2):boolean,array_contains(bigint_array, 0):boolean,array_contains(decimal_array, 9223372036854775809):boolean,array_contains(decimal_array, CAST(1 AS DECIMAL(19,0))):boolean,array_contains(double_array, 2.0):boolean,array_contains(double_array, 0.0):boolean,array_contains(float_array, CAST(2.0 AS FLOAT)):boolean,array_contains(float_array, CAST(0.0 AS FLOAT)):boolean,array_contains(date_array, DATE '2016-03-14'):boolean,array_contains(date_array, DATE '2016-01-01'):boolean,array_contains(timestamp_array, TIMESTAMP('2016-11-15 20:54:00.0')):boolean,array_contains(timestamp_array, TIMESTAMP('2016-01-01 20:54:00.0')):boolean>
+ -- !query 6 output
+ true	false	true	false	true	false	true	false	true	false	true	false	true	false	true	false	true	false	true	false
+ 
+ 
+ -- !query 7
+ select array_contains(b, 11), array_contains(c, array(111, 112, 113)) from data
+ -- !query 7 schema
+ struct<array_contains(b, 11):boolean,array_contains(c, array(111, 112, 113)):boolean>
+ -- !query 7 output
+ false	false
+ true	true
+ 
+ 
+ -- !query 8
+ select
+   sort_array(boolean_array),
+   sort_array(tinyint_array),
+   sort_array(smallint_array),
+   sort_array(int_array),
+   sort_array(bigint_array),
+   sort_array(decimal_array),
+   sort_array(double_array),
+   sort_array(float_array),
+   sort_array(date_array),
+   sort_array(timestamp_array)
+ from primitive_arrays
+ -- !query 8 schema
+ struct<sort_array(boolean_array, true):array<boolean>,sort_array(tinyint_array, true):array<tinyint>,sort_array(smallint_array, true):array<smallint>,sort_array(int_array, true):array<int>,sort_array(bigint_array, true):array<bigint>,sort_array(decimal_array, true):array<decimal(19,0)>,sort_array(double_array, true):array<double>,sort_array(float_array, true):array<float>,sort_array(date_array, true):array<date>,sort_array(timestamp_array, true):array<timestamp>>
+ -- !query 8 output
+ [true]	[1,2]	[1,2]	[1,2]	[1,2]	[9223372036854775808,9223372036854775809]	[1.0,2.0]	[1.0,2.0]	[2016-03-13,2016-03-14]	[2016-11-12 20:54:00.0,2016-11-15 20:54:00.0]
+ 
+ -- !query 9
+ select sort_array(array('b', 'd'), '1')
+ -- !query 9 schema
+ struct<>
+ -- !query 9 output
+ org.apache.spark.sql.AnalysisException
+ cannot resolve 'sort_array(array('b', 'd'), '1')' due to data type mismatch: Sort order in second argument requires a boolean literal.; line 1 pos 7
+ 
+ -- !query 10
+ select sort_array(array('b', 'd'), cast(NULL as boolean))
+ -- !query 10 schema
+ struct<>
+ -- !query 10 output
+ org.apache.spark.sql.AnalysisException
+ cannot resolve 'sort_array(array('b', 'd'), CAST(NULL AS BOOLEAN))' due to data type mismatch: Sort order in second argument requires a boolean literal.; line 1 pos 7
+ 
+ -- !query 11
+ select
+   size(boolean_array),
+   size(tinyint_array),
+   size(smallint_array),
+   size(int_array),
+   size(bigint_array),
+   size(decimal_array),
+   size(double_array),
+   size(float_array),
+   size(date_array),
+   size(timestamp_array)
+ from primitive_arrays
+ -- !query 11 schema
+ struct<size(boolean_array):int,size(tinyint_array):int,size(smallint_array):int,size(int_array):int,size(bigint_array):int,size(decimal_array):int,size(double_array):int,size(float_array):int,size(date_array):int,size(timestamp_array):int>
+ -- !query 11 output
+ 1	2	2	2	2	2	2	2	2	2
++>>>>>>> v2.0.2
diff --cc sql/core/src/test/resources/sql-tests/results/group-by.sql.out
index 9127bd4,a91f04e..0000000
--- a/sql/core/src/test/resources/sql-tests/results/group-by.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/group-by.sql.out
@@@ -1,51 -1,133 +1,188 @@@
++<<<<<<< HEAD
 +-- Automatically generated by SQLQueryTestSuite
 +-- Number of queries: 6
 +
 +
 +-- !query 0
 +create temporary view myview as values 128, 256 as v(int_col)
 +-- !query 0 schema
 +struct<>
 +-- !query 0 output
 +
 +
 +
 +-- !query 1
 +select int_col, count(*) from myview group by int_col
 +-- !query 1 schema
 +struct<int_col:int,count(1):bigint>
 +-- !query 1 output
 +128	1
 +256	1
 +
 +
 +-- !query 2
 +select 'foo', count(*) from myview group by 1
 +-- !query 2 schema
 +struct<foo:string,count(1):bigint>
 +-- !query 2 output
 +foo	2
 +
 +
 +-- !query 3
 +select 'foo' from myview where int_col == 0 group by 1
 +-- !query 3 schema
 +struct<foo:string>
 +-- !query 3 output
 +
 +
 +
 +-- !query 4
 +select 'foo', approx_count_distinct(int_col) from myview where int_col == 0 group by 1
 +-- !query 4 schema
 +struct<foo:string,approx_count_distinct(int_col):bigint>
 +-- !query 4 output
 +
 +
 +
 +-- !query 5
 +select 'foo', max(struct(int_col)) from myview where int_col == 0 group by 1
 +-- !query 5 schema
 +struct<foo:string,max(struct(int_col)):struct<int_col:int>>
 +-- !query 5 output
 +
++||||||| merged common ancestors
++=======
+ -- Automatically generated by SQLQueryTestSuite
+ -- Number of queries: 14
+ 
+ 
+ -- !query 0
+ CREATE OR REPLACE TEMPORARY VIEW testData AS SELECT * FROM VALUES
+ (1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2), (null, 1), (3, null), (null, null)
+ AS testData(a, b)
+ -- !query 0 schema
+ struct<>
+ -- !query 0 output
+ 
+ 
+ 
+ -- !query 1
+ SELECT a, COUNT(b) FROM testData
+ -- !query 1 schema
+ struct<>
+ -- !query 1 output
+ org.apache.spark.sql.AnalysisException
+ grouping expressions sequence is empty, and 'testdata.`a`' is not an aggregate function. Wrap '(count(testdata.`b`) AS `count(b)`)' in windowing function(s) or wrap 'testdata.`a`' in first() (or first_value) if you don't care which value you get.;
+ 
+ 
+ -- !query 2
+ SELECT COUNT(a), COUNT(b) FROM testData
+ -- !query 2 schema
+ struct<count(a):bigint,count(b):bigint>
+ -- !query 2 output
+ 7	7
+ 
+ 
+ -- !query 3
+ SELECT a, COUNT(b) FROM testData GROUP BY a
+ -- !query 3 schema
+ struct<a:int,count(b):bigint>
+ -- !query 3 output
+ 1	2
+ 2	2
+ 3	2
+ NULL	1
+ 
+ 
+ -- !query 4
+ SELECT a, COUNT(b) FROM testData GROUP BY b
+ -- !query 4 schema
+ struct<>
+ -- !query 4 output
+ org.apache.spark.sql.AnalysisException
+ expression 'testdata.`a`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;
+ 
+ 
+ -- !query 5
+ SELECT COUNT(a), COUNT(b) FROM testData GROUP BY a
+ -- !query 5 schema
+ struct<count(a):bigint,count(b):bigint>
+ -- !query 5 output
+ 0	1
+ 2	2
+ 2	2
+ 3	2
+ 
+ 
+ -- !query 6
+ SELECT 'foo', COUNT(a) FROM testData GROUP BY 1
+ -- !query 6 schema
+ struct<foo:string,count(a):bigint>
+ -- !query 6 output
+ foo	7
+ 
+ 
+ -- !query 7
+ SELECT 'foo' FROM testData WHERE a = 0 GROUP BY 1
+ -- !query 7 schema
+ struct<foo:string>
+ -- !query 7 output
+ 
+ 
+ 
+ -- !query 8
+ SELECT 'foo', APPROX_COUNT_DISTINCT(a) FROM testData WHERE a = 0 GROUP BY 1
+ -- !query 8 schema
+ struct<foo:string,approx_count_distinct(a):bigint>
+ -- !query 8 output
+ 
+ 
+ 
+ -- !query 9
+ SELECT 'foo', MAX(STRUCT(a)) FROM testData WHERE a = 0 GROUP BY 1
+ -- !query 9 schema
+ struct<foo:string,max(struct(a)):struct<a:int>>
+ -- !query 9 output
+ 
+ 
+ 
+ -- !query 10
+ SELECT a + b, COUNT(b) FROM testData GROUP BY a + b
+ -- !query 10 schema
+ struct<(a + b):int,count(b):bigint>
+ -- !query 10 output
+ 2	1
+ 3	2
+ 4	2
+ 5	1
+ NULL	1
+ 
+ 
+ -- !query 11
+ SELECT a + 2, COUNT(b) FROM testData GROUP BY a + 1
+ -- !query 11 schema
+ struct<>
+ -- !query 11 output
+ org.apache.spark.sql.AnalysisException
+ expression 'testdata.`a`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;
+ 
+ 
+ -- !query 12
+ SELECT a + 1 + 1, COUNT(b) FROM testData GROUP BY a + 1
+ -- !query 12 schema
+ struct<((a + 1) + 1):int,count(b):bigint>
+ -- !query 12 output
+ 3	2
+ 4	2
+ 5	2
+ NULL	1
+ 
+ 
+ -- !query 13
+ SELECT SKEWNESS(a), KURTOSIS(a), MIN(a), MAX(a), AVG(a), VARIANCE(a), STDDEV(a), SUM(a), COUNT(a)
+ FROM testData
+ -- !query 13 schema
+ struct<skewness(CAST(a AS DOUBLE)):double,kurtosis(CAST(a AS DOUBLE)):double,min(a):int,max(a):int,avg(a):double,var_samp(CAST(a AS DOUBLE)):double,stddev_samp(CAST(a AS DOUBLE)):double,sum(a):bigint,count(a):bigint>
+ -- !query 13 output
+ -0.2723801058145729	-1.5069204152249134	1	3	2.142857142857143	0.8095238095238094	0.8997354108424372	15	7
++>>>>>>> v2.0.2
diff --cc sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala
index da5c538,b8becf7..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala
@@@ -1558,11 -1561,123 +1561,134 @@@ class DataFrameSuite extends QueryTest 
      val df = spark.createDataFrame(rdd, StructType(schemas), false)
      assert(df.persist.take(1).apply(0).toSeq(100).asInstanceOf[Long] == 100)
    }
++<<<<<<< HEAD
 +
 +  test("copy results for sampling with replacement") {
 +    val df = Seq((1, 0), (2, 0), (3, 0)).toDF("a", "b")
 +    val sampleDf = df.sample(true, 2.00)
 +    val d = sampleDf.withColumn("c", monotonically_increasing_id).select($"c").collect
 +    assert(d.size == d.distinct.size)
 +  }
++||||||| merged common ancestors
++=======
+ 
+   test("copy results for sampling with replacement") {
+     val df = Seq((1, 0), (2, 0), (3, 0)).toDF("a", "b")
+     val sampleDf = df.sample(true, 2.00)
+     val d = sampleDf.withColumn("c", monotonically_increasing_id).select($"c").collect
+     assert(d.size == d.distinct.size)
+   }
+ 
+   test("SPARK-17409: Do Not Optimize Query in CTAS (Data source tables) More Than Once") {
+     withTable("bar") {
+       withTempView("foo") {
+         withSQLConf(SQLConf.DEFAULT_DATA_SOURCE_NAME.key -> "json") {
+           sql("select 0 as id").createOrReplaceTempView("foo")
+           val df = sql("select * from foo group by id")
+           // If we optimize the query in CTAS more than once, the following saveAsTable will fail
+           // with the error: `GROUP BY position 0 is not in select list (valid range is [1, 1])`
+           df.write.mode("overwrite").saveAsTable("bar")
+           checkAnswer(spark.table("bar"), Row(0) :: Nil)
+           val tableMetadata = spark.sessionState.catalog.getTableMetadata(TableIdentifier("bar"))
+           assert(tableMetadata.properties(DATASOURCE_PROVIDER) == "json",
+             "the expected table is a data source table using json")
+         }
+       }
+     }
+   }
+ 
+   private def verifyNullabilityInFilterExec(
+       df: DataFrame,
+       expr: String,
+       expectedNonNullableColumns: Seq[String]): Unit = {
+     val dfWithFilter = df.where(s"isnotnull($expr)").selectExpr(expr)
+     // In the logical plan, all the output columns of input dataframe are nullable
+     dfWithFilter.queryExecution.optimizedPlan.collect {
+       case e: Filter => assert(e.output.forall(_.nullable))
+     }
+ 
+     dfWithFilter.queryExecution.executedPlan.collect {
+       // When the child expression in isnotnull is null-intolerant (i.e. any null input will
+       // result in null output), the involved columns are converted to not nullable;
+       // otherwise, no change should be made.
+       case e: FilterExec =>
+         assert(e.output.forall { o =>
+           if (expectedNonNullableColumns.contains(o.name)) !o.nullable else o.nullable
+         })
+     }
+   }
+ 
+   test("SPARK-17957: no change on nullability in FilterExec output") {
+     val df = sparkContext.parallelize(Seq(
+       null.asInstanceOf[java.lang.Integer] -> new java.lang.Integer(3),
+       new java.lang.Integer(1) -> null.asInstanceOf[java.lang.Integer],
+       new java.lang.Integer(2) -> new java.lang.Integer(4))).toDF()
+ 
+     verifyNullabilityInFilterExec(df,
+       expr = "Rand()", expectedNonNullableColumns = Seq.empty[String])
+     verifyNullabilityInFilterExec(df,
+       expr = "coalesce(_1, _2)", expectedNonNullableColumns = Seq.empty[String])
+     verifyNullabilityInFilterExec(df,
+       expr = "coalesce(_1, 0) + Rand()", expectedNonNullableColumns = Seq.empty[String])
+     verifyNullabilityInFilterExec(df,
+       expr = "cast(coalesce(cast(coalesce(_1, _2) as double), 0.0) as int)",
+       expectedNonNullableColumns = Seq.empty[String])
+   }
+ 
+   test("SPARK-17957: set nullability to false in FilterExec output") {
+     val df = sparkContext.parallelize(Seq(
+       null.asInstanceOf[java.lang.Integer] -> new java.lang.Integer(3),
+       new java.lang.Integer(1) -> null.asInstanceOf[java.lang.Integer],
+       new java.lang.Integer(2) -> new java.lang.Integer(4))).toDF()
+ 
+     verifyNullabilityInFilterExec(df,
+       expr = "_1 + _2 * 3", expectedNonNullableColumns = Seq("_1", "_2"))
+     verifyNullabilityInFilterExec(df,
+       expr = "_1 + _2", expectedNonNullableColumns = Seq("_1", "_2"))
+     verifyNullabilityInFilterExec(df,
+       expr = "_1", expectedNonNullableColumns = Seq("_1"))
+     // `constructIsNotNullConstraints` infers the IsNotNull(_2) from IsNotNull(_2 + Rand())
+     // Thus, we are able to set nullability of _2 to false.
+     // If IsNotNull(_2) is not given from `constructIsNotNullConstraints`, the impl of
+     // isNullIntolerant in `FilterExec` needs an update for more advanced inference.
+     verifyNullabilityInFilterExec(df,
+       expr = "_2 + Rand()", expectedNonNullableColumns = Seq("_2"))
+     verifyNullabilityInFilterExec(df,
+       expr = "_2 * 3 + coalesce(_1, 0)", expectedNonNullableColumns = Seq("_2"))
+     verifyNullabilityInFilterExec(df,
+       expr = "cast((_1 + _2) as boolean)", expectedNonNullableColumns = Seq("_1", "_2"))
+   }
+ 
+   test("SPARK-17957: outer join + na.fill") {
+     val df1 = Seq((1, 2), (2, 3)).toDF("a", "b")
+     val df2 = Seq((2, 5), (3, 4)).toDF("a", "c")
+     val joinedDf = df1.join(df2, Seq("a"), "outer").na.fill(0)
+     val df3 = Seq((3, 1)).toDF("a", "d")
+     checkAnswer(joinedDf.join(df3, "a"), Row(3, 0, 4, 1))
+   }
+ 
+   test("SPARK-17123: Performing set operations that combine non-scala native types") {
+     val dates = Seq(
+       (BigDecimal.valueOf(1), new Timestamp(2)),
+       (BigDecimal.valueOf(4), new Timestamp(5))
+     ).toDF("decimal", "timestamp")
+ 
+     val widenTypedRows = Seq(
+       (10.5D, "string")
+     ).toDF("decimal", "timestamp")
+ 
+     dates.union(widenTypedRows).collect()
+     dates.except(widenTypedRows).collect()
+     dates.intersect(widenTypedRows).collect()
+   }
+ 
+   test("SPARK-18070 binary operator should not consider nullability when comparing input types") {
+     val rows = Seq(Row(Seq(1), Seq(1)))
+     val schema = new StructType()
+       .add("array1", ArrayType(IntegerType))
+       .add("array2", ArrayType(IntegerType, containsNull = false))
+     val df = spark.createDataFrame(spark.sparkContext.makeRDD(rows), schema)
+     assert(df.filter($"array1" === $"array2").count() == 1)
+   }
++>>>>>>> v2.0.2
  }
diff --cc sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala
index f897cfb,7a98915..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala
@@@ -869,19 -869,66 +869,83 @@@ class DatasetSuite extends QueryTest wi
      val ds = spark.createDataset(data)(enc)
      checkDataset(ds, (("a", "b"), "c"), (null, "d"))
    }
++<<<<<<< HEAD
 +
 +  test("SPARK-16995: flat mapping on Dataset containing a column created with lit/expr") {
 +    val df = Seq("1").toDF("a")
 +
 +    import df.sparkSession.implicits._
 +
 +    checkDataset(
 +      df.withColumn("b", lit(0)).as[ClassData]
 +        .groupByKey(_.a).flatMapGroups { case (x, iter) => List[Int]() })
 +    checkDataset(
 +      df.withColumn("b", expr("0")).as[ClassData]
 +        .groupByKey(_.a).flatMapGroups { case (x, iter) => List[Int]() })
 +  }
++||||||| merged common ancestors
++=======
+ 
+   test("SPARK-16995: flat mapping on Dataset containing a column created with lit/expr") {
+     val df = Seq("1").toDF("a")
+ 
+     import df.sparkSession.implicits._
+ 
+     checkDataset(
+       df.withColumn("b", lit(0)).as[ClassData]
+         .groupByKey(_.a).flatMapGroups { case (x, iter) => List[Int]() })
+     checkDataset(
+       df.withColumn("b", expr("0")).as[ClassData]
+         .groupByKey(_.a).flatMapGroups { case (x, iter) => List[Int]() })
+   }
+ 
+   // This is moved from ReplSuite to prevent java.lang.ClassCircularityError.
+   test("SPARK-18189: Fix serialization issue in KeyValueGroupedDataset") {
+     val resultValue = 12345
+     val keyValueGrouped = Seq((1, 2), (3, 4)).toDS().groupByKey(_._1)
+     val mapGroups = keyValueGrouped.mapGroups((k, v) => (k, 1))
+     val broadcasted = spark.sparkContext.broadcast(resultValue)
+ 
+     // Using broadcast triggers serialization issue in KeyValueGroupedDataset
+     val dataset = mapGroups.map(_ => broadcasted.value)
+ 
+     assert(dataset.collect() sameElements Array(resultValue, resultValue))
+   }
+ 
+   test("SPARK-18125: Spark generated code causes CompileException") {
+     val data = Array(
+       Route("a", "b", 1),
+       Route("a", "b", 2),
+       Route("a", "c", 2),
+       Route("a", "d", 10),
+       Route("b", "a", 1),
+       Route("b", "a", 5),
+       Route("b", "c", 6))
+     val ds = sparkContext.parallelize(data).toDF.as[Route]
+ 
+     val grped = ds.map(r => GroupedRoutes(r.src, r.dest, Seq(r)))
+       .groupByKey(r => (r.src, r.dest))
+       .reduceGroups { (g1: GroupedRoutes, g2: GroupedRoutes) =>
+         GroupedRoutes(g1.src, g1.dest, g1.routes ++ g2.routes)
+       }.map(_._2)
+ 
+     val expected = Seq(
+       GroupedRoutes("a", "d", Seq(Route("a", "d", 10))),
+       GroupedRoutes("b", "c", Seq(Route("b", "c", 6))),
+       GroupedRoutes("a", "b", Seq(Route("a", "b", 1), Route("a", "b", 2))),
+       GroupedRoutes("b", "a", Seq(Route("b", "a", 1), Route("b", "a", 5))),
+       GroupedRoutes("a", "c", Seq(Route("a", "c", 2)))
+     )
+ 
+     implicit def ordering[GroupedRoutes]: Ordering[GroupedRoutes] = new Ordering[GroupedRoutes] {
+       override def compare(x: GroupedRoutes, y: GroupedRoutes): Int = {
+         x.toString.compareTo(y.toString)
+       }
+     }
+ 
+     checkDatasetUnorderly(grped, expected: _*)
+   }
++>>>>>>> v2.0.2
  }
  
  case class Generic[T](id: T, value: Double)
diff --cc sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
index cf25097,c96ba07..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
@@@ -466,20 -463,6 +463,48 @@@ class SQLQuerySuite extends QueryTest w
      )
    }
  
++<<<<<<< HEAD
 +  test("agg") {
 +    checkAnswer(
 +      sql("SELECT a, SUM(b) FROM testData2 GROUP BY a"),
 +      Seq(Row(1, 3), Row(2, 3), Row(3, 3)))
 +  }
 +
 +  test("aggregates with nulls") {
 +    checkAnswer(
 +      sql("SELECT SKEWNESS(a), KURTOSIS(a), MIN(a), MAX(a)," +
 +        "AVG(a), VARIANCE(a), STDDEV(a), SUM(a), COUNT(a) FROM nullInts"),
 +      Row(0, -1.5, 1, 3, 2, 1.0, 1, 6, 3)
 +    )
 +  }
 +
++||||||| merged common ancestors
++  test("index into array of arrays") {
++    checkAnswer(
++      sql(
++        "SELECT nestedData, nestedData[0][0], nestedData[0][0] + nestedData[0][1] FROM arrayData"),
++      arrayData.map(d =>
++        Row(d.nestedData,
++         d.nestedData(0)(0),
++         d.nestedData(0)(0) + d.nestedData(0)(1))).collect().toSeq)
++  }
++
++  test("agg") {
++    checkAnswer(
++      sql("SELECT a, SUM(b) FROM testData2 GROUP BY a"),
++      Seq(Row(1, 3), Row(2, 3), Row(3, 3)))
++  }
++
++  test("aggregates with nulls") {
++    checkAnswer(
++      sql("SELECT SKEWNESS(a), KURTOSIS(a), MIN(a), MAX(a)," +
++        "AVG(a), VARIANCE(a), STDDEV(a), SUM(a), COUNT(a) FROM nullInts"),
++      Row(0, -1.5, 1, 3, 2, 1.0, 1, 6, 3)
++    )
++  }
++
++=======
++>>>>>>> v2.0.2
    test("select *") {
      checkAnswer(
        sql("SELECT * FROM testData"),
diff --cc sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLogSuite.scala
index 41a8cc2,e1bc674..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLogSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLogSuite.scala
@@@ -133,9 -134,12 +134,22 @@@ class FileStreamSinkLogSuite extends Sp
            |{"path":"/a/b/y","size":200,"isDir":false,"modificationTime":2000,"blockReplication":2,"blockSize":20000,"action":"delete"}
            |{"path":"/a/b/z","size":300,"isDir":false,"modificationTime":3000,"blockReplication":3,"blockSize":30000,"action":"add"}""".stripMargin
        // scalastyle:on
++<<<<<<< HEAD
 +      assert(expected === new String(sinkLog.serialize(logs), UTF_8))
 +
 +      assert(VERSION === new String(sinkLog.serialize(Array()), UTF_8))
++||||||| merged common ancestors
++      assert(expected === new String(sinkLog.serialize(logs), UTF_8))
++
++      assert(FileStreamSinkLog.VERSION === new String(sinkLog.serialize(Nil), UTF_8))
++=======
+       val baos = new ByteArrayOutputStream()
+       sinkLog.serialize(logs, baos)
+       assert(expected === baos.toString(UTF_8.name()))
+       baos.reset()
+       sinkLog.serialize(Array(), baos)
+       assert(VERSION === baos.toString(UTF_8.name()))
++>>>>>>> v2.0.2
      }
    }
  
@@@ -174,9 -178,9 +188,15 @@@
            blockSize = 30000L,
            action = FileStreamSinkLog.ADD_ACTION))
  
-       assert(expected === sinkLog.deserialize(logs.getBytes(UTF_8)))
+       assert(expected === sinkLog.deserialize(new ByteArrayInputStream(logs.getBytes(UTF_8))))
  
++<<<<<<< HEAD
 +      assert(Nil === sinkLog.deserialize(VERSION.getBytes(UTF_8)))
++||||||| merged common ancestors
++      assert(Nil === sinkLog.deserialize(FileStreamSinkLog.VERSION.getBytes(UTF_8)))
++=======
+       assert(Nil === sinkLog.deserialize(new ByteArrayInputStream(VERSION.getBytes(UTF_8))))
++>>>>>>> v2.0.2
      }
    }
  
diff --cc sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceSuite.scala
index 1793db0,3bad5bb..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceSuite.scala
@@@ -1,125 -1,125 +1,254 @@@
++<<<<<<< HEAD
 +/*
 + * Licensed to the Apache Software Foundation (ASF) under one or more
 + * contributor license agreements.  See the NOTICE file distributed with
 + * this work for additional information regarding copyright ownership.
 + * The ASF licenses this file to You under the Apache License, Version 2.0
 + * (the "License"); you may not use this file except in compliance with
 + * the License.  You may obtain a copy of the License at
 + *
 + *    http://www.apache.org/licenses/LICENSE-2.0
 + *
 + * Unless required by applicable law or agreed to in writing, software
 + * distributed under the License is distributed on an "AS IS" BASIS,
 + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 + * See the License for the specific language governing permissions and
 + * limitations under the License.
 + */
 +
 +package org.apache.spark.sql.execution.streaming
 +
 +import java.io.{File, FileNotFoundException}
 +import java.net.URI
 +
 +import scala.util.Random
 +
 +import org.apache.hadoop.fs.{FileStatus, Path, RawLocalFileSystem}
 +
 +import org.apache.spark.SparkFunSuite
 +import org.apache.spark.sql.execution.streaming.ExistsThrowsExceptionFileSystem._
 +import org.apache.spark.sql.test.SharedSQLContext
 +import org.apache.spark.sql.types.StructType
 +
 +class FileStreamSourceSuite extends SparkFunSuite with SharedSQLContext {
 +
 +  import FileStreamSource._
 +
 +  test("SeenFilesMap") {
 +    val map = new SeenFilesMap(maxAgeMs = 10)
 +
 +    map.add("a", 5)
 +    assert(map.size == 1)
 +    map.purge()
 +    assert(map.size == 1)
 +
 +    // Add a new entry and purge should be no-op, since the gap is exactly 10 ms.
 +    map.add("b", 15)
 +    assert(map.size == 2)
 +    map.purge()
 +    assert(map.size == 2)
 +
 +    // Add a new entry that's more than 10 ms than the first entry. We should be able to purge now.
 +    map.add("c", 16)
 +    assert(map.size == 3)
 +    map.purge()
 +    assert(map.size == 2)
 +
 +    // Override existing entry shouldn't change the size
 +    map.add("c", 25)
 +    assert(map.size == 2)
 +
 +    // Not a new file because we have seen c before
 +    assert(!map.isNewFile("c", 20))
 +
 +    // Not a new file because timestamp is too old
 +    assert(!map.isNewFile("d", 5))
 +
 +    // Finally a new file: never seen and not too old
 +    assert(map.isNewFile("e", 20))
 +  }
 +
 +  test("SeenFilesMap should only consider a file old if it is earlier than last purge time") {
 +    val map = new SeenFilesMap(maxAgeMs = 10)
 +
 +    map.add("a", 20)
 +    assert(map.size == 1)
 +
 +    // Timestamp 5 should still considered a new file because purge time should be 0
 +    assert(map.isNewFile("b", 9))
 +    assert(map.isNewFile("b", 10))
 +
 +    // Once purge, purge time should be 10 and then b would be a old file if it is less than 10.
 +    map.purge()
 +    assert(!map.isNewFile("b", 9))
 +    assert(map.isNewFile("b", 10))
 +  }
 +
 +  testWithUninterruptibleThread("do not recheck that files exist during getBatch") {
 +    withTempDir { temp =>
 +      spark.conf.set(
 +        s"fs.$scheme.impl",
 +        classOf[ExistsThrowsExceptionFileSystem].getName)
 +      // add the metadata entries as a pre-req
 +      val dir = new File(temp, "dir") // use non-existent directory to test whether log make the dir
 +      val metadataLog =
 +        new FileStreamSourceLog(FileStreamSourceLog.VERSION, spark, dir.getAbsolutePath)
 +      assert(metadataLog.add(0, Array(FileEntry(s"$scheme:///file1", 100L, 0))))
 +
 +      val newSource = new FileStreamSource(spark, s"$scheme:///", "parquet", StructType(Nil),
 +        dir.getAbsolutePath, Map.empty)
 +      // this method should throw an exception if `fs.exists` is called during resolveRelation
 +      newSource.getBatch(None, LongOffset(1))
 +    }
 +  }
 +}
 +
 +/** Fake FileSystem to test whether the method `fs.exists` is called during
 + * `DataSource.resolveRelation`.
 + */
 +class ExistsThrowsExceptionFileSystem extends RawLocalFileSystem {
 +  override def getUri: URI = {
 +    URI.create(s"$scheme:///")
 +  }
 +
 +  override def exists(f: Path): Boolean = {
 +    throw new IllegalArgumentException("Exists shouldn't have been called!")
 +  }
 +
 +  /** Simply return an empty file for now. */
 +  override def listStatus(file: Path): Array[FileStatus] = {
 +    throw new FileNotFoundException("Folder was suddenly deleted but this should not make it fail!")
 +  }
 +}
 +
 +object ExistsThrowsExceptionFileSystem {
 +  val scheme = s"FileStreamSourceSuite${math.abs(Random.nextInt)}fs"
 +}
++||||||| merged common ancestors
++=======
+ /*
+  * Licensed to the Apache Software Foundation (ASF) under one or more
+  * contributor license agreements.  See the NOTICE file distributed with
+  * this work for additional information regarding copyright ownership.
+  * The ASF licenses this file to You under the Apache License, Version 2.0
+  * (the "License"); you may not use this file except in compliance with
+  * the License.  You may obtain a copy of the License at
+  *
+  *    http://www.apache.org/licenses/LICENSE-2.0
+  *
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an "AS IS" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+ 
+ package org.apache.spark.sql.execution.streaming
+ 
+ import java.io.{File, FileNotFoundException}
+ import java.net.URI
+ 
+ import scala.util.Random
+ 
+ import org.apache.hadoop.fs.{FileStatus, Path, RawLocalFileSystem}
+ 
+ import org.apache.spark.SparkFunSuite
+ import org.apache.spark.sql.execution.streaming.ExistsThrowsExceptionFileSystem._
+ import org.apache.spark.sql.test.SharedSQLContext
+ import org.apache.spark.sql.types.StructType
+ 
+ class FileStreamSourceSuite extends SparkFunSuite with SharedSQLContext {
+ 
+   import FileStreamSource._
+ 
+   test("SeenFilesMap") {
+     val map = new SeenFilesMap(maxAgeMs = 10)
+ 
+     map.add("a", 5)
+     assert(map.size == 1)
+     map.purge()
+     assert(map.size == 1)
+ 
+     // Add a new entry and purge should be no-op, since the gap is exactly 10 ms.
+     map.add("b", 15)
+     assert(map.size == 2)
+     map.purge()
+     assert(map.size == 2)
+ 
+     // Add a new entry that's more than 10 ms than the first entry. We should be able to purge now.
+     map.add("c", 16)
+     assert(map.size == 3)
+     map.purge()
+     assert(map.size == 2)
+ 
+     // Override existing entry shouldn't change the size
+     map.add("c", 25)
+     assert(map.size == 2)
+ 
+     // Not a new file because we have seen c before
+     assert(!map.isNewFile("c", 20))
+ 
+     // Not a new file because timestamp is too old
+     assert(!map.isNewFile("d", 5))
+ 
+     // Finally a new file: never seen and not too old
+     assert(map.isNewFile("e", 20))
+   }
+ 
+   test("SeenFilesMap should only consider a file old if it is earlier than last purge time") {
+     val map = new SeenFilesMap(maxAgeMs = 10)
+ 
+     map.add("a", 20)
+     assert(map.size == 1)
+ 
+     // Timestamp 5 should still considered a new file because purge time should be 0
+     assert(map.isNewFile("b", 9))
+     assert(map.isNewFile("b", 10))
+ 
+     // Once purge, purge time should be 10 and then b would be a old file if it is less than 10.
+     map.purge()
+     assert(!map.isNewFile("b", 9))
+     assert(map.isNewFile("b", 10))
+   }
+ 
+   testWithUninterruptibleThread("do not recheck that files exist during getBatch") {
+     withTempDir { temp =>
+       spark.conf.set(
+         s"fs.$scheme.impl",
+         classOf[ExistsThrowsExceptionFileSystem].getName)
+       // add the metadata entries as a pre-req
+       val dir = new File(temp, "dir") // use non-existent directory to test whether log make the dir
+       val metadataLog =
+         new FileStreamSourceLog(FileStreamSourceLog.VERSION, spark, dir.getAbsolutePath)
+       assert(metadataLog.add(0, Array(FileEntry(s"$scheme:///file1", 100L, 0))))
+ 
+       val newSource = new FileStreamSource(spark, s"$scheme:///", "parquet", StructType(Nil), Nil,
+         dir.getAbsolutePath, Map.empty)
+       // this method should throw an exception if `fs.exists` is called during resolveRelation
+       newSource.getBatch(None, LongOffset(1))
+     }
+   }
+ }
+ 
+ /** Fake FileSystem to test whether the method `fs.exists` is called during
+  * `DataSource.resolveRelation`.
+  */
+ class ExistsThrowsExceptionFileSystem extends RawLocalFileSystem {
+   override def getUri: URI = {
+     URI.create(s"$scheme:///")
+   }
+ 
+   override def exists(f: Path): Boolean = {
+     throw new IllegalArgumentException("Exists shouldn't have been called!")
+   }
+ 
+   /** Simply return an empty file for now. */
+   override def listStatus(file: Path): Array[FileStatus] = {
+     throw new FileNotFoundException("Folder was suddenly deleted but this should not make it fail!")
+   }
+ }
+ 
+ object ExistsThrowsExceptionFileSystem {
+   val scheme = s"FileStreamSourceSuite${math.abs(Random.nextInt)}fs"
+ }
++>>>>>>> v2.0.2
diff --cc sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
index 55c95ae,c1adbc9..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
@@@ -768,137 -839,151 +839,286 @@@ class FileStreamSourceSuite extends Fil
        }
      }
    }
++<<<<<<< HEAD
 +
 +  test("SPARK-17372 - write file names to WAL as Array[String]") {
 +    // Note: If this test takes longer than the timeout, then its likely that this is actually
 +    // running a Spark job with 10000 tasks. This test tries to avoid that by
 +    // 1. Setting the threshold for parallel file listing to very high
 +    // 2. Using a query that should use constant folding to eliminate reading of the files
 +
 +    val numFiles = 10000
 +
 +    // This is to avoid running a spark job to list of files in parallel
 +    // by the ListingFileCatalog.
 +    spark.sessionState.conf.setConf(SQLConf.PARALLEL_PARTITION_DISCOVERY_THRESHOLD, numFiles * 2)
 +
 +    withTempDirs { case (root, tmp) =>
 +      val src = new File(root, "a=1")
 +      src.mkdirs()
 +
 +      (1 to numFiles).map { _.toString }.foreach { i =>
 +        val tempFile = Utils.tempFileWith(new File(tmp, "text"))
 +        val finalFile = new File(src, tempFile.getName)
 +        stringToFile(finalFile, i)
 +      }
 +      assert(src.listFiles().size === numFiles)
 +
 +      val files = spark.readStream.text(root.getCanonicalPath).as[String]
 +
 +      // Note this query will use constant folding to eliminate the file scan.
 +      // This is to avoid actually running a Spark job with 10000 tasks
 +      val df = files.filter("1 == 0").groupBy().count()
 +
 +      testStream(df, InternalOutputModes.Complete)(
 +        AddTextFileData("0", src, tmp),
 +        CheckAnswer(0)
 +      )
 +    }
 +  }
 +
 +  test("compacat metadata log") {
 +    val _sources = PrivateMethod[Seq[Source]]('sources)
 +    val _metadataLog = PrivateMethod[FileStreamSourceLog]('metadataLog)
 +
 +    def verify(execution: StreamExecution)
 +      (batchId: Long, expectedBatches: Int): Boolean = {
 +      import CompactibleFileStreamLog._
 +
 +      val fileSource = (execution invokePrivate _sources()).head.asInstanceOf[FileStreamSource]
 +      val metadataLog = fileSource invokePrivate _metadataLog()
 +
 +      if (isCompactionBatch(batchId, 2)) {
 +        val path = metadataLog.batchIdToPath(batchId)
 +
 +        // Assert path name should be ended with compact suffix.
 +        assert(path.getName.endsWith(COMPACT_FILE_SUFFIX))
 +
 +        // Compacted batch should include all entries from start.
 +        val entries = metadataLog.get(batchId)
 +        assert(entries.isDefined)
 +        assert(entries.get.length === metadataLog.allFiles().length)
 +        assert(metadataLog.get(None, Some(batchId)).flatMap(_._2).length === entries.get.length)
 +      }
 +
 +      assert(metadataLog.allFiles().sortBy(_.batchId) ===
 +        metadataLog.get(None, Some(batchId)).flatMap(_._2).sortBy(_.batchId))
 +
 +      metadataLog.get(None, Some(batchId)).flatMap(_._2).length === expectedBatches
 +    }
 +
 +    withTempDirs { case (src, tmp) =>
 +      withSQLConf(
 +        SQLConf.FILE_SOURCE_LOG_COMPACT_INTERVAL.key -> "2"
 +      ) {
 +        val fileStream = createFileStream("text", src.getCanonicalPath)
 +        val filtered = fileStream.filter($"value" contains "keep")
 +
 +        testStream(filtered)(
 +          AddTextFileData("drop1\nkeep2\nkeep3", src, tmp),
 +          CheckAnswer("keep2", "keep3"),
 +          AssertOnQuery(verify(_)(0L, 1)),
 +          AddTextFileData("drop4\nkeep5\nkeep6", src, tmp),
 +          CheckAnswer("keep2", "keep3", "keep5", "keep6"),
 +          AssertOnQuery(verify(_)(1L, 2)),
 +          AddTextFileData("drop7\nkeep8\nkeep9", src, tmp),
 +          CheckAnswer("keep2", "keep3", "keep5", "keep6", "keep8", "keep9"),
 +          AssertOnQuery(verify(_)(2L, 3)),
 +          StopStream,
 +          StartStream(),
 +          AssertOnQuery(verify(_)(2L, 3)),
 +          AddTextFileData("drop10\nkeep11", src, tmp),
 +          CheckAnswer("keep2", "keep3", "keep5", "keep6", "keep8", "keep9", "keep11"),
 +          AssertOnQuery(verify(_)(3L, 4)),
 +          AddTextFileData("drop12\nkeep13", src, tmp),
 +          CheckAnswer("keep2", "keep3", "keep5", "keep6", "keep8", "keep9", "keep11", "keep13"),
 +          AssertOnQuery(verify(_)(4L, 5))
 +        )
 +      }
 +    }
 +  }
 +
 +  test("get arbitrary batch from FileStreamSource") {
 +    withTempDirs { case (src, tmp) =>
 +      withSQLConf(
 +        SQLConf.FILE_SOURCE_LOG_COMPACT_INTERVAL.key -> "2",
 +        // Force deleting the old logs
 +        SQLConf.FILE_SOURCE_LOG_CLEANUP_DELAY.key -> "1"
 +      ) {
 +        val fileStream = createFileStream("text", src.getCanonicalPath)
 +        val filtered = fileStream.filter($"value" contains "keep")
 +
 +        testStream(filtered)(
 +          AddTextFileData("keep1", src, tmp),
 +          CheckAnswer("keep1"),
 +          AddTextFileData("keep2", src, tmp),
 +          CheckAnswer("keep1", "keep2"),
 +          AddTextFileData("keep3", src, tmp),
 +          CheckAnswer("keep1", "keep2", "keep3"),
 +          AssertOnQuery("check getBatch") { execution: StreamExecution =>
 +            val _sources = PrivateMethod[Seq[Source]]('sources)
 +            val fileSource =
 +              (execution invokePrivate _sources()).head.asInstanceOf[FileStreamSource]
 +            assert(fileSource.getBatch(None, LongOffset(2)).as[String].collect() ===
 +              List("keep1", "keep2", "keep3"))
 +            assert(fileSource.getBatch(Some(LongOffset(0)), LongOffset(2)).as[String].collect() ===
 +              List("keep2", "keep3"))
 +            assert(fileSource.getBatch(Some(LongOffset(1)), LongOffset(2)).as[String].collect() ===
 +              List("keep3"))
 +            true
 +          }
 +        )
 +      }
 +    }
 +  }
++||||||| merged common ancestors
++=======
+ 
+   test("SPARK-17372 - write file names to WAL as Array[String]") {
+     // Note: If this test takes longer than the timeout, then its likely that this is actually
+     // running a Spark job with 10000 tasks. This test tries to avoid that by
+     // 1. Setting the threshold for parallel file listing to very high
+     // 2. Using a query that should use constant folding to eliminate reading of the files
+ 
+     val numFiles = 10000
+ 
+     // This is to avoid running a spark job to list of files in parallel
+     // by the ListingFileCatalog.
+     spark.sessionState.conf.setConf(SQLConf.PARALLEL_PARTITION_DISCOVERY_THRESHOLD, numFiles * 2)
+ 
+     withTempDirs { case (root, tmp) =>
+       val src = new File(root, "a=1")
+       src.mkdirs()
+ 
+       (1 to numFiles).map { _.toString }.foreach { i =>
+         val tempFile = Utils.tempFileWith(new File(tmp, "text"))
+         val finalFile = new File(src, tempFile.getName)
+         stringToFile(finalFile, i)
+       }
+       assert(src.listFiles().size === numFiles)
+ 
+       val files = spark.readStream.text(root.getCanonicalPath).as[(String, Int)]
+ 
+       // Note this query will use constant folding to eliminate the file scan.
+       // This is to avoid actually running a Spark job with 10000 tasks
+       val df = files.filter("1 == 0").groupBy().count()
+ 
+       testStream(df, InternalOutputModes.Complete)(
+         AddTextFileData("0", src, tmp),
+         CheckAnswer(0)
+       )
+     }
+   }
+ 
+   test("compacat metadata log") {
+     val _sources = PrivateMethod[Seq[Source]]('sources)
+     val _metadataLog = PrivateMethod[FileStreamSourceLog]('metadataLog)
+ 
+     def verify(execution: StreamExecution)
+       (batchId: Long, expectedBatches: Int): Boolean = {
+       import CompactibleFileStreamLog._
+ 
+       val fileSource = (execution invokePrivate _sources()).head.asInstanceOf[FileStreamSource]
+       val metadataLog = fileSource invokePrivate _metadataLog()
+ 
+       if (isCompactionBatch(batchId, 2)) {
+         val path = metadataLog.batchIdToPath(batchId)
+ 
+         // Assert path name should be ended with compact suffix.
+         assert(path.getName.endsWith(COMPACT_FILE_SUFFIX))
+ 
+         // Compacted batch should include all entries from start.
+         val entries = metadataLog.get(batchId)
+         assert(entries.isDefined)
+         assert(entries.get.length === metadataLog.allFiles().length)
+         assert(metadataLog.get(None, Some(batchId)).flatMap(_._2).length === entries.get.length)
+       }
+ 
+       assert(metadataLog.allFiles().sortBy(_.batchId) ===
+         metadataLog.get(None, Some(batchId)).flatMap(_._2).sortBy(_.batchId))
+ 
+       metadataLog.get(None, Some(batchId)).flatMap(_._2).length === expectedBatches
+     }
+ 
+     withTempDirs { case (src, tmp) =>
+       withSQLConf(
+         SQLConf.FILE_SOURCE_LOG_COMPACT_INTERVAL.key -> "2"
+       ) {
+         val fileStream = createFileStream("text", src.getCanonicalPath)
+         val filtered = fileStream.filter($"value" contains "keep")
+ 
+         testStream(filtered)(
+           AddTextFileData("drop1\nkeep2\nkeep3", src, tmp),
+           CheckAnswer("keep2", "keep3"),
+           AssertOnQuery(verify(_)(0L, 1)),
+           AddTextFileData("drop4\nkeep5\nkeep6", src, tmp),
+           CheckAnswer("keep2", "keep3", "keep5", "keep6"),
+           AssertOnQuery(verify(_)(1L, 2)),
+           AddTextFileData("drop7\nkeep8\nkeep9", src, tmp),
+           CheckAnswer("keep2", "keep3", "keep5", "keep6", "keep8", "keep9"),
+           AssertOnQuery(verify(_)(2L, 3)),
+           StopStream,
+           StartStream(),
+           AssertOnQuery(verify(_)(2L, 3)),
+           AddTextFileData("drop10\nkeep11", src, tmp),
+           CheckAnswer("keep2", "keep3", "keep5", "keep6", "keep8", "keep9", "keep11"),
+           AssertOnQuery(verify(_)(3L, 4)),
+           AddTextFileData("drop12\nkeep13", src, tmp),
+           CheckAnswer("keep2", "keep3", "keep5", "keep6", "keep8", "keep9", "keep11", "keep13"),
+           AssertOnQuery(verify(_)(4L, 5))
+         )
+       }
+     }
+   }
+ 
+   test("get arbitrary batch from FileStreamSource") {
+     withTempDirs { case (src, tmp) =>
+       withSQLConf(
+         SQLConf.FILE_SOURCE_LOG_COMPACT_INTERVAL.key -> "2",
+         // Force deleting the old logs
+         SQLConf.FILE_SOURCE_LOG_CLEANUP_DELAY.key -> "1"
+       ) {
+         val fileStream = createFileStream("text", src.getCanonicalPath)
+         val filtered = fileStream.filter($"value" contains "keep")
+ 
+         testStream(filtered)(
+           AddTextFileData("keep1", src, tmp),
+           CheckAnswer("keep1"),
+           AddTextFileData("keep2", src, tmp),
+           CheckAnswer("keep1", "keep2"),
+           AddTextFileData("keep3", src, tmp),
+           CheckAnswer("keep1", "keep2", "keep3"),
+           AssertOnQuery("check getBatch") { execution: StreamExecution =>
+             val _sources = PrivateMethod[Seq[Source]]('sources)
+             val fileSource =
+               (execution invokePrivate _sources()).head.asInstanceOf[FileStreamSource]
+             assert(fileSource.getBatch(None, LongOffset(2)).as[String].collect() ===
+               List("keep1", "keep2", "keep3"))
+             assert(fileSource.getBatch(Some(LongOffset(0)), LongOffset(2)).as[String].collect() ===
+               List("keep2", "keep3"))
+             assert(fileSource.getBatch(Some(LongOffset(1)), LongOffset(2)).as[String].collect() ===
+               List("keep3"))
+             true
+           }
+         )
+       }
+     }
+   }
+ 
+   test("input row metrics") {
+     withTempDirs { case (src, tmp) =>
+       val input = spark.readStream.format("text").load(src.getCanonicalPath)
+       testStream(input)(
+         AddTextFileData("100", src, tmp),
+         CheckAnswer("100"),
+         AssertOnLastQueryStatus { status =>
+           assert(status.triggerDetails.get("numRows.input.total") === "1")
+           assert(status.sourceStatuses(0).processingRate > 0.0)
+         }
+       )
+     }
+   }
++>>>>>>> v2.0.2
  }
  
  class FileStreamSourceStressTestSuite extends FileStreamSourceTest {
diff --cc sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamTest.scala
index 6c5b170,7428330..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamTest.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamTest.scala
diff --cc sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryListenerSuite.scala
index 831543a,fad24bb..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryListenerSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryListenerSuite.scala
@@@ -41,68 -44,73 +44,187 @@@ class StreamingQueryListenerSuite exten
      assert(spark.streams.active.isEmpty)
      assert(addedListeners.isEmpty)
      // Make sure we don't leak any events to the next test
-     spark.sparkContext.listenerBus.waitUntilEmpty(10000)
    }
  
++<<<<<<< HEAD
 +  test("single listener") {
 +    val listener = new QueryStatusCollector
 +    val input = MemoryStream[Int]
 +    withListenerAdded(listener) {
 +      testStream(input.toDS)(
 +        StartStream(),
 +        AssertOnQuery("Incorrect query status in onQueryStarted") { query =>
 +          val status = listener.startStatus
 +          assert(status != null)
 +          assert(status.name === query.name)
 +          assert(status.id === query.id)
 +          assert(status.sourceStatuses.size === 1)
 +          assert(status.sourceStatuses(0).description.contains("Memory"))
 +
 +          // The source and sink offsets must be None as this must be called before the
 +          // batches have started
 +          assert(status.sourceStatuses(0).offsetDesc === None)
 +          assert(status.sinkStatus.offsetDesc === CompositeOffset(None :: Nil).toString)
 +
 +          // No progress events or termination events
 +          assert(listener.progressStatuses.isEmpty)
 +          assert(listener.terminationStatus === null)
 +          true
 +        },
 +        AddDataMemory(input, Seq(1, 2, 3)),
 +        CheckAnswer(1, 2, 3),
 +        AssertOnQuery("Incorrect query status in onQueryProgress") { query =>
 +          eventually(Timeout(streamingTimeout)) {
 +
 +            // There should be only on progress event as batch has been processed
 +            assert(listener.progressStatuses.size === 1)
 +            val status = listener.progressStatuses.peek()
 +            assert(status != null)
 +            assert(status.name === query.name)
 +            assert(status.id === query.id)
 +            assert(status.sourceStatuses(0).offsetDesc === Some(LongOffset(0).toString))
 +            assert(status.sinkStatus.offsetDesc === CompositeOffset.fill(LongOffset(0)).toString)
 +
 +            // No termination events
 +            assert(listener.terminationStatus === null)
 +          }
 +          true
 +        },
 +        StopStream,
 +        AssertOnQuery("Incorrect query status in onQueryTerminated") { query =>
 +          eventually(Timeout(streamingTimeout)) {
 +            val status = listener.terminationStatus
 +            assert(status != null)
 +            assert(status.name === query.name)
 +            assert(status.id === query.id)
 +            assert(status.sourceStatuses(0).offsetDesc === Some(LongOffset(0).toString))
 +            assert(status.sinkStatus.offsetDesc === CompositeOffset.fill(LongOffset(0)).toString)
 +            assert(listener.terminationException === None)
 +          }
 +          listener.checkAsyncErrors()
 +          true
++||||||| merged common ancestors
++  test("single listener") {
++    val listener = new QueryStatusCollector
++    val input = MemoryStream[Int]
++    withListenerAdded(listener) {
++      testStream(input.toDS)(
++        StartStream(),
++        AssertOnQuery("Incorrect query status in onQueryStarted") { query =>
++          val status = listener.startStatus
++          assert(status != null)
++          assert(status.name === query.name)
++          assert(status.id === query.id)
++          assert(status.sourceStatuses.size === 1)
++          assert(status.sourceStatuses(0).description.contains("Memory"))
++
++          // The source and sink offsets must be None as this must be called before the
++          // batches have started
++          assert(status.sourceStatuses(0).offsetDesc === None)
++          assert(status.sinkStatus.offsetDesc === CompositeOffset(None :: Nil).toString)
++
++          // No progress events or termination events
++          assert(listener.progressStatuses.isEmpty)
++          assert(listener.terminationStatus === null)
++        },
++        AddDataMemory(input, Seq(1, 2, 3)),
++        CheckAnswer(1, 2, 3),
++        AssertOnQuery("Incorrect query status in onQueryProgress") { query =>
++          eventually(Timeout(streamingTimeout)) {
++
++            // There should be only on progress event as batch has been processed
++            assert(listener.progressStatuses.size === 1)
++            val status = listener.progressStatuses.peek()
++            assert(status != null)
++            assert(status.name === query.name)
++            assert(status.id === query.id)
++            assert(status.sourceStatuses(0).offsetDesc === Some(LongOffset(0).toString))
++            assert(status.sinkStatus.offsetDesc === CompositeOffset.fill(LongOffset(0)).toString)
++
++            // No termination events
++            assert(listener.terminationStatus === null)
++          }
++        },
++        StopStream,
++        AssertOnQuery("Incorrect query status in onQueryTerminated") { query =>
++          eventually(Timeout(streamingTimeout)) {
++            val status = listener.terminationStatus
++            assert(status != null)
++            assert(status.name === query.name)
++            assert(status.id === query.id)
++            assert(status.sourceStatuses(0).offsetDesc === Some(LongOffset(0).toString))
++            assert(status.sinkStatus.offsetDesc === CompositeOffset.fill(LongOffset(0)).toString)
++            assert(listener.terminationStackTrace.isEmpty)
++            assert(listener.terminationException === None)
++          }
++          listener.checkAsyncErrors()
++=======
+   test("single listener, check trigger statuses") {
+     import StreamingQueryListenerSuite._
+     clock = new StreamManualClock
+ 
+     /** Custom MemoryStream that waits for manual clock to reach a time */
+     val inputData = new MemoryStream[Int](0, sqlContext) {
+       // Wait for manual clock to be 100 first time there is data
+       override def getOffset: Option[Offset] = {
+         val offset = super.getOffset
+         if (offset.nonEmpty) {
+           clock.waitTillTime(100)
++>>>>>>> v2.0.2
          }
-       )
+         offset
+       }
+ 
+       // Wait for manual clock to be 300 first time there is data
+       override def getBatch(start: Option[Offset], end: Offset): DataFrame = {
+         clock.waitTillTime(300)
+         super.getBatch(start, end)
+       }
      }
+ 
+     // This is to make sure thatquery waits for manual clock to be 600 first time there is data
+     val mapped = inputData.toDS().agg(count("*")).as[Long].coalesce(1).map { x =>
+       clock.waitTillTime(600)
+       x
+     }
+ 
+     testStream(mapped, OutputMode.Complete)(
+       StartStream(triggerClock = clock),
+       AddData(inputData, 1, 2),
+       AdvanceManualClock(100),  // unblock getOffset, will block on getBatch
+       AdvanceManualClock(200),  // unblock getBatch, will block on computation
+       AdvanceManualClock(300),  // unblock computation
+       AssertOnQuery { _ => clock.getTimeMillis() === 600 },
+       AssertOnLastQueryStatus { status: StreamingQueryStatus =>
+         // Check the correctness of the trigger info of the last completed batch reported by
+         // onQueryProgress
+         assert(status.triggerDetails.containsKey("triggerId"))
+         assert(status.triggerDetails.get("isTriggerActive") === "false")
+         assert(status.triggerDetails.get("isDataPresentInTrigger") === "true")
+ 
+         assert(status.triggerDetails.get("timestamp.triggerStart") === "0")
+         assert(status.triggerDetails.get("timestamp.afterGetOffset") === "100")
+         assert(status.triggerDetails.get("timestamp.afterGetBatch") === "300")
+         assert(status.triggerDetails.get("timestamp.triggerFinish") === "600")
+ 
+         assert(status.triggerDetails.get("latency.getOffset.total") === "100")
+         assert(status.triggerDetails.get("latency.getBatch.total") === "200")
+         assert(status.triggerDetails.get("latency.optimizer") === "0")
+         assert(status.triggerDetails.get("latency.offsetLogWrite") === "0")
+         assert(status.triggerDetails.get("latency.fullTrigger") === "600")
+ 
+         assert(status.triggerDetails.get("numRows.input.total") === "2")
+         assert(status.triggerDetails.get("numRows.state.aggregation1.total") === "1")
+         assert(status.triggerDetails.get("numRows.state.aggregation1.updated") === "1")
+ 
+         assert(status.sourceStatuses.length === 1)
+         assert(status.sourceStatuses(0).triggerDetails.containsKey("triggerId"))
+         assert(status.sourceStatuses(0).triggerDetails.get("latency.getOffset.source") === "100")
+         assert(status.sourceStatuses(0).triggerDetails.get("latency.getBatch.source") === "200")
+         assert(status.sourceStatuses(0).triggerDetails.get("numRows.input.source") === "2")
+       },
+       CheckAnswer(2)
+     )
    }
  
    test("adding and removing listener") {
@@@ -258,49 -287,9 +401,101 @@@
      val listenerBus = spark.streams invokePrivate listenerBusMethod()
      listenerBus.listeners.toArray.map(_.asInstanceOf[StreamingQueryListener])
    }
+ }
+ 
++<<<<<<< HEAD
++  class QueryStatusCollector extends StreamingQueryListener {
++    // to catch errors in the async listener events
++    @volatile private var asyncTestWaiter = new Waiter
++
++    @volatile var startStatus: StreamingQueryInfo = null
++    @volatile var terminationStatus: StreamingQueryInfo = null
++    @volatile var terminationException: Option[String] = null
++
++    val progressStatuses = new ConcurrentLinkedQueue[StreamingQueryInfo]
++
++    def reset(): Unit = {
++      startStatus = null
++      terminationStatus = null
++      progressStatuses.clear()
++      asyncTestWaiter = new Waiter
++    }
++
++    def checkAsyncErrors(): Unit = {
++      asyncTestWaiter.await(timeout(streamingTimeout))
++    }
 +
++
++    override def onQueryStarted(queryStarted: QueryStarted): Unit = {
++      asyncTestWaiter {
++        startStatus = queryStarted.queryInfo
++      }
++    }
++
++    override def onQueryProgress(queryProgress: QueryProgress): Unit = {
++      asyncTestWaiter {
++        assert(startStatus != null, "onQueryProgress called before onQueryStarted")
++        progressStatuses.add(queryProgress.queryInfo)
++      }
++    }
++
++    override def onQueryTerminated(queryTerminated: QueryTerminated): Unit = {
++      asyncTestWaiter {
++        assert(startStatus != null, "onQueryTerminated called before onQueryStarted")
++        terminationStatus = queryTerminated.queryInfo
++        terminationException = queryTerminated.exception
++      }
++      asyncTestWaiter.dismiss()
++    }
++  }
++||||||| merged common ancestors
 +  class QueryStatusCollector extends StreamingQueryListener {
 +    // to catch errors in the async listener events
 +    @volatile private var asyncTestWaiter = new Waiter
 +
 +    @volatile var startStatus: StreamingQueryInfo = null
 +    @volatile var terminationStatus: StreamingQueryInfo = null
 +    @volatile var terminationException: Option[String] = null
++    @volatile var terminationStackTrace: Seq[StackTraceElement] = null
 +
 +    val progressStatuses = new ConcurrentLinkedQueue[StreamingQueryInfo]
 +
 +    def reset(): Unit = {
 +      startStatus = null
 +      terminationStatus = null
 +      progressStatuses.clear()
 +      asyncTestWaiter = new Waiter
 +    }
 +
 +    def checkAsyncErrors(): Unit = {
 +      asyncTestWaiter.await(timeout(streamingTimeout))
 +    }
 +
 +
 +    override def onQueryStarted(queryStarted: QueryStarted): Unit = {
 +      asyncTestWaiter {
 +        startStatus = queryStarted.queryInfo
 +      }
 +    }
 +
 +    override def onQueryProgress(queryProgress: QueryProgress): Unit = {
 +      asyncTestWaiter {
 +        assert(startStatus != null, "onQueryProgress called before onQueryStarted")
 +        progressStatuses.add(queryProgress.queryInfo)
 +      }
 +    }
 +
 +    override def onQueryTerminated(queryTerminated: QueryTerminated): Unit = {
 +      asyncTestWaiter {
 +        assert(startStatus != null, "onQueryTerminated called before onQueryStarted")
 +        terminationStatus = queryTerminated.queryInfo
 +        terminationException = queryTerminated.exception
++        terminationStackTrace = queryTerminated.stackTrace
 +      }
 +      asyncTestWaiter.dismiss()
 +    }
 +  }
+ object StreamingQueryListenerSuite {
+   // Singleton reference to clock that does not get serialized in task closures
+   @volatile var clock: ManualClock = null
  }
diff --cc sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala
index 88f1f18,31b7fe0..0000000
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala
diff --cc sql/hive-thriftserver/pom.xml
index c47d5f0,73271b6..0000000
--- a/sql/hive-thriftserver/pom.xml
+++ b/sql/hive-thriftserver/pom.xml
diff --cc sql/hive/pom.xml
index 2388d5f,bd62d26..0000000
--- a/sql/hive/pom.xml
+++ b/sql/hive/pom.xml
diff --cc sql/hive/src/test/scala/org/apache/spark/sql/catalyst/LogicalPlanToSQLSuite.scala
index ef2f756,3794f63..0000000
--- a/sql/hive/src/test/scala/org/apache/spark/sql/catalyst/LogicalPlanToSQLSuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/catalyst/LogicalPlanToSQLSuite.scala
@@@ -1113,12 -1113,28 +1113,40 @@@ class LogicalPlanToSQLSuite extends SQL
        checkSQL("select * from orc_t", "select_orc_table")
      }
    }
++<<<<<<< HEAD
 +
 +  test("inline tables") {
 +    checkSQL(
 +      """
 +        |select * from values ("one", 1), ("two", 2), ("three", null) as data(a, b) where b > 1
 +      """.stripMargin,
 +      "inline_tables")
 +  }
++||||||| merged common ancestors
++=======
+ 
+   test("inline tables") {
+     checkSQL(
+       """
+         |select * from values ("one", 1), ("two", 2), ("three", null) as data(a, b) where b > 1
+       """.stripMargin,
+       "inline_tables")
+   }
+ 
+   test("SPARK-17750 - interval arithmetic") {
+     withTable("dates") {
+       sql("create table dates (ts timestamp)")
+       checkSQL(
+         """
+           |select ts + interval 1 day, ts + interval 2 days,
+           |       ts - interval 1 day, ts - interval 2 days,
+           |       ts + interval '1' day, ts + interval '2' days,
+           |       ts - interval '1' day, ts - interval '2' days
+           |from dates
+         """.stripMargin,
+         "interval_arithmetic"
+       )
+     }
+   }
++>>>>>>> v2.0.2
  }
diff --cc sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveUtilsSuite.scala
index 667a7dd,667a7dd..0000000
deleted file mode 100644,100644
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveUtilsSuite.scala
+++ /dev/null
diff --cc sql/hive/src/test/scala/org/apache/spark/sql/hive/MetastoreRelationSuite.scala
index eec60b4,c6711c3..0000000
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/MetastoreRelationSuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/MetastoreRelationSuite.scala
@@@ -1,38 -1,56 +1,98 @@@
++<<<<<<< HEAD
 +/*
 +* Licensed to the Apache Software Foundation (ASF) under one or more
 +* contributor license agreements.  See the NOTICE file distributed with
 +* this work for additional information regarding copyright ownership.
 +* The ASF licenses this file to You under the Apache License, Version 2.0
 +* (the "License"); you may not use this file except in compliance with
 +* the License.  You may obtain a copy of the License at
 +*
 +*    http://www.apache.org/licenses/LICENSE-2.0
 +*
 +* Unless required by applicable law or agreed to in writing, software
 +* distributed under the License is distributed on an "AS IS" BASIS,
 +* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 +* See the License for the specific language governing permissions and
 +* limitations under the License.
 +*/
 +
 +package org.apache.spark.sql.hive
 +
 +import org.apache.spark.SparkFunSuite
 +import org.apache.spark.sql.catalyst.TableIdentifier
 +import org.apache.spark.sql.catalyst.catalog.{CatalogColumn, CatalogStorageFormat, CatalogTable, CatalogTableType}
 +
 +class MetastoreRelationSuite extends SparkFunSuite {
 +  test("makeCopy and toJSON should work") {
 +    val table = CatalogTable(
 +      identifier = TableIdentifier("test", Some("db")),
 +      tableType = CatalogTableType.VIEW,
 +      storage = CatalogStorageFormat.empty,
 +      schema = Seq.empty[CatalogColumn])
 +    val relation = MetastoreRelation("db", "test", None)(table, null, null)
 +
 +    // No exception should be thrown
 +    relation.makeCopy(Array("db", "test", None))
 +    // No exception should be thrown
 +    relation.toJSON
 +  }
 +}
++||||||| merged common ancestors
++=======
+ /*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+ 
+ package org.apache.spark.sql.hive
+ 
+ import org.apache.spark.sql.{QueryTest, Row}
+ import org.apache.spark.sql.catalyst.TableIdentifier
+ import org.apache.spark.sql.catalyst.catalog.{CatalogColumn, CatalogStorageFormat, CatalogTable, CatalogTableType}
+ import org.apache.spark.sql.execution.command.DDLUtils
+ import org.apache.spark.sql.hive.test.TestHiveSingleton
+ import org.apache.spark.sql.test.SQLTestUtils
+ 
+ class MetastoreRelationSuite extends QueryTest with SQLTestUtils with TestHiveSingleton {
+   test("makeCopy and toJSON should work") {
+     val table = CatalogTable(
+       identifier = TableIdentifier("test", Some("db")),
+       tableType = CatalogTableType.VIEW,
+       storage = CatalogStorageFormat.empty,
+       schema = Seq.empty[CatalogColumn])
+     val relation = MetastoreRelation("db", "test", None)(table, null, null)
+ 
+     // No exception should be thrown
+     relation.makeCopy(Array("db", "test", None))
+     // No exception should be thrown
+     relation.toJSON
+   }
+ 
+   test("SPARK-17409: Do Not Optimize Query in CTAS (Hive Serde Table) More Than Once") {
+     withTable("bar") {
+       withTempView("foo") {
+         sql("select 0 as id").createOrReplaceTempView("foo")
+         // If we optimize the query in CTAS more than once, the following saveAsTable will fail
+         // with the error: `GROUP BY position 0 is not in select list (valid range is [1, 1])`
+         sql("CREATE TABLE bar AS SELECT * FROM foo group by id")
+         checkAnswer(spark.table("bar"), Row(0) :: Nil)
+         val tableMetadata = spark.sessionState.catalog.getTableMetadata(TableIdentifier("bar"))
+         assert(!DDLUtils.isDatasourceTable(tableMetadata),
+           "the expected table is a Hive serde table")
+       }
+     }
+   }
+ }
++>>>>>>> v2.0.2
diff --cc streaming/pom.xml
index 4e0f5b1,b6a1ab7..0000000
--- a/streaming/pom.xml
+++ b/streaming/pom.xml
diff --cc tools/pom.xml
index ddc1b09,474df40..0000000
--- a/tools/pom.xml
+++ b/tools/pom.xml
diff --cc yarn/pom.xml
index b676b6d,063efee..0000000
--- a/yarn/pom.xml
+++ b/yarn/pom.xml
* Unmerged path core/src/main/scala/org/apache/spark/storage/BlockFetchException.scala
* Unmerged path external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/package-info.java
